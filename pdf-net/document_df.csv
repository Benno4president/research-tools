txt,title,temp_index
"Drone & Me:  An Exploration Into Natural HumanDrone Interaction  
Jessica R. Cauchard  Jane L. E  Kevin Y. Zhai  James A. Landay  
Stanford University, Department of Computer Science  353 Serra Mall, Stanford CA 94305-9035  {cauchard, ejane, kzhai, landay} @stanford.edu    
ABSTRACT  Personal drones are becoming popular. It is challenging to  design how to interact with these flying robots. We present  a WizardofOz (WoZ) elicitation study that informs how to  naturally interact with drones. Results show strong  agreement between participants for many interaction  techniques, as when gesturing for the drone to stop. We  discovered that people interact with drones as with a person  or a pet, using interpersonal gestures, such as beckoning the  drone closer. We detail the interaction metaphors observed  and offer design insights for humandrone interactions.  
Author Keywords  Drone; UAV; quadcopter; WizardofOz; elicitation study.  
ACM Classification Keywords  H.5.2. Information interfaces and presentation: User  Interfaces: Usercentered design.  
INTRODUCTION  Personal drones are becoming increasingly present in our  everyday environments. They are primarily being used for  outdoor activities, such as film capture, agriculture, Search  and Rescue, entertainment, and delivery. In the future, we  expect that drones will become partly, if not fully,  autonomous, and they will be able to support people in their  everyday lives. Even with full autonomy, however, people  still need to communicate with personal drones and have  the control needed to make requests and express intention.   
Imagine a personal trainer drone, that could accompany a  user on a run [12], giving realtime feedback. As a tour  guide, it could adjust language and show points of interest.  In either scenario, it is unwieldy for the user to control the  drone with a remote while paying attention to the setting. In  addition, as drones become autonomous, remotes become  redundant since the drone can compute its optimal   path  without  needing  any user input.  Also,  in  collocated   
  
Figure 1. Example of userdefined gestures (high agreement)  
scenarios it is unnatural to use a remote to interact with an  agent that people treat like an intelligent being [14].  
The need for natural interaction is supported by the HumanRobot Interaction (HRI) literature. As drones have different  characteristics than ground robots, such as not allowing  touch interaction, it is unclear whether existing techniques  can be adapted to flying robots. Our usercentric design  strategy seeks to understand how users naturally interact  with drones. We present a 19-participant WoZ elicitation  study that shows that users felt extremely comfortable  interacting with a drone. Participants used metaphors drawn  from interacting with a person or a pet, called for the drone  by name, encouraged it, and trusted it enough to bring it to  an almost unsafe distance. We see that the preferences in  the use of voice and gestures vary across tasks, leading us  to conclude that no single modality would provide suitable  natural interaction. We consider multimodal interaction a  major challenge in the future and conclude by presenting  design insights for HumanDrone Interaction (HDI).  
RELATED WORK  A variety of work investigates the nature of HRI. Projects  explored multimodal [1], gestural [4], and even prop-  based interactions techniques [8]. Guo et al. [5] show  people feel comfortable using a variety of interaction  techniques. Specifically, with a robot dog, people felt it   was more natural to use a Wii controller than a keypad.   
Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from permissions@acm.org.  UbiComp '15, September 7-11, 2015, Osaka, Japan.  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-3574-4/15/09...$15.00.   http://dx.doi.org/10.1145/2750858.2805823    
361 
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN 
HDI applications vary from running with drones [11, 12],  filming [7], creating flying displays [15, 18], and looking at  dynamically rechargeable flying objects [9]. Researchers  have also explored communicating feedback, such as intent  and directionality [19, 20]. Yet, there are major differences  between traditional robots and drones, which can fly   freely, and cannot safely be touched, requiring new  interaction techniques that are well suited for drones.  
Prior work investigated controlling a drone using face poses  and hand gestures [13] as well as a multimodal falconry  metaphor [14]. This last project leads us to believe   that drones can be “socially” adapted and accepted.   Others studied upperbody gestural interaction in a  controlled lab where users were given specific interaction  metaphors [16]. In contrast, we do not specify the type   of interaction to obtain userdefined gestures.   
There is a history of userdefined interaction techniques and  gesture elicitation studies for new technology [2, 3, 10, 22],  such as tabletops [23], mobile devices [17], and TVs [21].  
USER STUDY  To explore interactions and better understand the metaphors  and relationships that occur when users interact with  drones, we ran a userdefined interaction elicitation study.   
Methodology  We simulated the drone autonomous behavior and reactions  to user input. We chose an outdoor space to operate the  drone safely and with added flexibility compared to indoors  or to preprogramming the drone’s movements. The  experimenter using the remote stayed behind the user   but could not be fully hidden to keep direct sight of the  drone and participant for safety reasons. We found that  even with the WoZ, users felt in control of the drone. Each  task was described on a card to avoid verbally biasing   the users’ actions and modality choices (Table 1). Users   were asked to perform any action to get from start to end.  
Name  Following  
Start  The drone is flying around  
End  The drone is following you  
Table 1. Example of a task as written on a card.  
To let users interpret the task freely, we did not show the  effect of the actions (referents in [23]). For example, when  getting the drone to follow, its comfortable relative position  with respect to the participant was different for each person  and seeing the referent could have biased the interaction.  
Participants  19 volunteers (12 m), 19 to 38 y.o. (μ= 25) were recruited  from our institution and nearby companies. Their training  was in engineering (7), CS (6), other sciences (1), and nonscientific fields (5). They were rewarded $15 for their time.  
Apparatus, Setting, and Tasks  We used a DJI Phantom 2 (29x29x18cm) with prop guards  around the propellers. The study was run outdoors, partially  
protected from the wind by trees and a building. 18 tasks  (Table 3) with different levels of complexity were presented  in a random order to avoid interaction effects and   to minimize the impact of learning and fatigue.  
Procedure  Each session lasted 1 to 1.5h. One experimenter filmed and  interviewed participants while another controlled the drone.  Participants were informed of the WoZ and we emphasized  that they should ignore the experimenters’ presence when  interacting with the drone. We asked participants to   not worry about technical capabilities and to interact   in the manner that felt most natural for each task.  
The instructions and the task cards (face down) were  positioned on a stand. The participant picked up the top  card and read it outloud to confirm that they had  understood the task. They would then interact with the  drone. After each task, the participant was prompted to  recall and explain their actions using a posttask thinkaloud  technique. Participants also rated their interaction in terms  of suitability and simplicity. After completing the 18 tasks  (Part 1), the participant was given a sheet with suggestions  for interaction techniques. They were then asked to  complete a second time 4 representative tasks (Part 2) that  covered a range of category types and complexity (Table 3,  blue italic). In Part 2, we looked at whether participants  changed their interaction strategy after given suggestions.  
RESULTS  The data collected includes transcripts, videos, and posttask and postexperiment interviews during which we  collected qualitative feedback on the users’ experience.  
UserDefined Gesture Sets per Task Type  Out of the 418 interaction tasks, 4 were misunderstood by  the users, and removed from further analysis. We found 216  unique interactions: 96 gestures (body gestures not  restricted to hands and arms), 59 sounds, 53 combinations  of gesture and sound, and 8 with a prop. Given the   low usage of props, we do not count them in further  analysis. We use gesture and sound only as they encompass  the vast majority of interactions, as shown in Table 21.   
Tasks Performed  Gesture  Sound  Both  
All  86%  38%  26%  
Representative tasks (Part 1)  88%  37%  28%  
Representative tasks (Part 2)  70%  57%  33%  
Table 2. Percentage of use of interaction modalities.  
Many participants initially expressed discomfort in talking  with the drone. Gestures are quick and allowed for precise  adjustments and continuous control throughout the  interaction. Over the course of the study, participants felt  more confident in the drone’s ability. As illustrated by the  
                                                              
1 Note that rows do not sum to 100% because interactions  that used both modalities are counted in all columns.  
362 
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN 
increase from 37% to 57% in using sound for the  representative tasks in Part 2, participants started giving  voice commands. This suggests that rapport can be  established, allowing humans to accept collocated drones.   
We determine an agreement score per referent, 𝐴!,   per modality type, based on [22] where 𝑃! is the set of  proposed interactions for referent 𝑟, and 𝑃! is the subset   of identical interactions for that referent.   
(1)  𝐴! = 𝑃! 𝑃! 
! 
!!   
Table 3 summarizes the agreement scores per task   and modality. For calculating agreement, we consider  gesture and sound separately [10] to avoid overlap in  counting individual interactions and using both modalities  simultaneously. In a few cases, some participants built a  sequence of gestures/sounds into their interaction. Each  interaction is counted separately, resulting in some   overcounting, causing some agreement scores to be greater  than 1 (impacted scores highlighted in grey). Based on   prior work, scores above 0.5 show strong agreement   (44% of interactions, in bold).   
Table 3. Tasks and agreement scores per modality.  
Navigation Strategies  For navigation, most people used a repeated waving or  continuous sweep, mapping the drone’s movement directly  to their arm (as a pointer with a line extending out from the  hand). Within the body range, people were more likely to  use smaller motions with additional interactions using body  parts as reference frames for the drone’s target flying level.  For specific locations, users initially hesitated pointing at  the target or describing it verbally because of the lack of  precision. As trust increased, they were more willing to  depend on the drone’s ability to identify its spatial context.   
Pictures  Taking a photo is one of the most complex tasks. For  selfies, many realized that on top of framing and focusing, a  counter would be useful. This caused people to use sound to  avoid interfering with posing (8 participants in Part 1 and  12 in Part 2). Yet, agreement scores were high across  modalities. Almost all used the word “picture” for both  selfies and photos. People often were less confident in their  choice of gestures here, but in fact there were two that  almost everyone used: 1. using two hands as a frame,   2. holding an invisible camera and clicking the shutter  button. People also suggested having a screen on the   drone or the use of props to frame or adjust camera settings.  
  
Figure 2. Subjective ratings in percentage of participants for  interacting with the drone using a 5-point Likert scale.  
Qualitative Survey Data  Figure 2 shows subjective ratings for how natural,  physically and mentally demanding the interaction was,  how safe it felt and whether the participant felt in control.  
90% of the participants stated they felt they were in   control and 95% felt it was natural to interact with   the drone. None of the participants reported being   tired or feeling fatigue at any point in the study.  Most participants were keen on continuing to interact with  the drone. People commented on their own faults   in properly directing the drone. “It landed rather roughly  and I didn’t mean for it to, but that's also because   I didn’t really know what I was doing with my   hand gesture” [P9] or on the drone’s abilities “I don’t  know if the drone is looking at me” [P1], [I ran] “just   to see if the drone would catch up to me” [P3].   
This illustrates that the WoZ was well received and did not  affect the results of the study. Some mentioned being  careful about not losing control [P8], worried about   the drone’s height as “it could fall and break” [P16],   losing sight of the drone [P3, P14], or vice versa: “It should  know and be smart to not get lost, like a dog” [P13].  
DESIGN INSIGHTS  Throughout the study, we found trends in terms of the  interaction metaphors participants used as well as the  feedback they gave on aspects of the interaction that they  desired in the future. This section presents design insights  based on these trends.  
Category  Task name  Gesture Sound Both  
N  o i t a g i v a  n  
Within  body  frame  
Fly closer  0.80  0.61  0.28  
Fly higher (to user’s height)   0.24  0.72  0.38  
Fly lower   (from user’s height)  
0.44  0.69  0.38  
Fly sideways (small delta)   0.40  0.38  0.50  
Stop by me  0.87  0.72  0.68  
Outside  body  frame  
Fly further away (far)   0.37  0.48  0.36  
Fly sideways (large delta)  0.31  1.00  1.00  
Fly to a precise location  0.65  0.33  0.33  
Fly higher  0.30  0.69  0.33  
Fly lower  0.23  0.67  ---  
A  o i t c  n  
General  motion  
Stop motion (when flying)   1.00  0.80  0.68  
Land  0.28  1.16*  0.33  
Take off  0.32  0.28  0.28  
Relative  to user  
Follow   0.56  0.42  0.43  
Stop following   0.51  0.28  0.22  
Get attention  0.26  0.72  0.22  
Photo Take a ‘selfie’  0.37  0.63  0.19  
Take a picture of a tree  0.94*  0.85  0.59  
363 
SESSION: INTERACTING WITH ANIMALS AND FLYING ROBOTS 
Interaction Metaphors  Several interaction metaphors emerged during the study.  We observed that users treated the drone as if it were an  animate being: a person, a group of people, or even a pet.   
Interacting with a Person  The most popular metaphor was interacting with the drone  as a person. All but one mentioned this analogy. We  observed this phenomenon in the words used when talking  to the drone: “ok, we’re good”, “let’s go”, “come this way”  [P2], “please” and “thank you” [P5], along with the word  “respect” when explaining their interaction in the posttask  interviews and the fear of being impolite [P6]. One  participant asked if they “should have been more gentle  [with the drone]” [P7]. Some users cited taskspecific  metaphors, such as navigation tasks that felt like helping  someone park [P2, P3, P8]; getting the drone’s attention  was like getting attention in a classroom [P1, P11] or   a stadium [P16]; having the drone follow them would be  similar to leading a tour group [P16]; and getting the drone  to stop moving or following was like in the army [P8].  
Interacting with a Pet  Another popular metaphor was interacting with the drone as  if it were a pet, mentioned by 16 out of 19 participants.  Most participants compared it to a dog: “I’m almost  starting to command it like I would a dog. Like, ‘stay, go  over there, go fetch’.” [P18]. Some participants also  referred to the drone as mosquitos [P18] or bumblebees  [P7] due to the noise it makes, while others thought of it as  a bird [P1, P12]. We also saw this interaction strategy when  participants called the drone by whistling at it like they  would a dog [P5], talking about its “underbelly” [P10],   and saying “all right boy” [P13] and “good job”, “good  drone” [P11] when the drone did what was expected.  
Naming the Drone  The pet interaction metaphor continued when [P8] said he  would call the drone Nick, after his own dog. [P2] gave the  drone an ID number, seven other participants decided to  call it “Drone”, while [P5] felt that “Ferdy” would be more  appropriate. Similarly [P11] said she would call the drone  to gets its attention the same way she would call a friend.  
Safety and Proxemics  When designing the study, one main concern was to ensure  the participants’ safety. We thought users might be afraid of  the drone and be uncomfortable interacting with it. Instead,  their reactions could not be further from our expectations.  16 participants reported feeling safe interacting with the  drone (Figure 2). Some appeared more concerned about the  drone’s safety [P13, P15, P16]. “I’m not really worried to  get hurt, but I don’t want to also hurt the drone”[P17].  Similarly, as users became comfortable with the drone, they  got closer to it than we expected. In our preliminary look   at proxemics, 7 participants brought the drone within their  intimate space (1.5ft), 9 in their personal space (4ft),   only 3 preferred to have the drone in their social space  (10ft) at closest, and none in the public space (> 10ft) [6].  
We found several factors created discomfort, such as  propeller noise or the wind they generated, and resulted in  three users preferring the drone to be located in their social  space. When asked what aspect of the drone made users  uncomfortable, these participants again worried more about   the drone’s safety than their own. Participants also built  trust with the drone, ensuring that it would stop when they  would ask it to do so, “The more that I learn to trust it, the  more I would feel comfortable not saying as much” [P12].  
Feedback  We did not implement a feedback system for this study.   We found in a pilot study that participants wanted feedback  for the “Take a photo” task. We added a nod where the  drone tilts forward to signal that a photo was taken. Some  users commented on the drone not being as still as they  expected when stopped (hovering) and that an additional  confirmation would help their interaction. Participants were  specific about the type of feedback they wanted, using light  or sound confirmations, such as a shutter sound for pictures  [P5, P14], or responding “ok” or “I’m leaving” [P6].   [P13] suggested adding eyes to see where the drone is  looking. Others asked for a display of what the drone is  seeing (i.e., camera feed) on the drone itself [P9, P10], on   a tablet/phone [P11], or on a headmounted display [P17].  
Emergency Landing   Several participants mentioned they would like an interface  for emergency landing in case anything was going wrong or  they had to immediately stop their activity.  
FUTURE WORK   The next step is the implementation of the best technical  solution to support multimodal Input/Output for HDI,  taking into consideration the need for multiple modalities  based on the context of use. The scenarios of use we  described can then be implemented. We will also look at  proxemics in the 3D space for HDI. As several users  mentioned feeling attached to the drone, it would be  interesting to further study human emotion towards drones  and how they differ from interacting with ground robots.  
CONCLUSION  Given the current increase in popularity of personal drones,  we need to create natural interaction techniques to best  support users. We executed a WoZ elicitation study with  nineteen participants performing a range of tasks with  different levels of complexity, and found high agreement  scores between participants on how they naturally interact  with the drone. We found strong agreement on nearly half  (44%) of the gesture, voice, and multimodal interactions  that felt intuitive to participants. This was due to most  participants interacting with the drone in a similar way to  how they would with a person or a pet. We contribute a set  of design insights to develop HumanDrone Interaction. We  expect drones to become smaller and quieter so that they  will resemble humming birds, flying by the user and  coming into play when needed. Giving people natural, easy  control will enable incorporating drones into our daily lives.   
364 
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN 
REFERENCES  1. AlvarezSantos, V., Iglesias, R., Pardo, X.M., Regueiro,  C.V. and CanedoRodriguez, A. 2014. Gesturebased  interaction with voice feedback for a tourguide robot. J.  Vis. Comun. Image Represent. 25, 499-509.  
2. Connell, S., Kuo, P.-Y., Liu, L. and Piper, A.M. 2013. A  WizardofOz elicitation study examining childdefined  gestures with a wholebody interface. In Proceedings. of the  12th International Conference on Interaction Design and  Children (IDC '13), 277-280.  
3. Cooke, N.J. 1994. Varieties of knowledge elicitation  techniques. International Journal of HumanComputer  Studies. 41, 801-849.  
4. Ende, T., Haddadin, S., Parusel, S., Wusthoff, T.,  Hassenzahl, M. and AlbuSchaffer, A. 2011. A humancentered approach to robot gesture based communication  within collaborative working processes. In Proceedings of  IEEE/RSJ International Conference on Intelligent Robots  and Systems (IROS'11), 3367-3374.  
5. Guo, C. and Sharlin, E. 2008. Exploring the use of  tangible user interfaces for humanrobot interaction: a  comparative study. In Proceedings of the SIGCHI  Conference on Human Factors in Computing Systems   (CHI '08), 121-130.  
6. Hall, E.T. 1969. The hidden dimension. Anchor Books  New York.  
7. Higuchi, K., Ishiguro, Y. and Rekimoto, J. 2011. Flying  eyes: freespace content creation using autonomous aerial  vehicles. In CHI '11 Extended Abstracts on Human Factors  in Computing Systems (CHI EA '11), 561-570.  
8. Ishii, K., Zhao, S., Inami, M., Igarashi, T. and Imai, M.  2009. Designing Laser Gesture Interface for Robot Control.  In Proceedings of INTERACT '09, 479-492.  
9. Kyono, Y., Yonezawa, T., Nozaki, H., Ogawa, M., Ito,  T., Nakazawa, J., Takashio, K. and Tokuda, H. 2013.  EverCopter: continuous and adaptive overtheair sensing  with detachable wired flying objects. In Proc. of the 2013  ACM Conference on Pervasive and Ubiquitous Computing  Adjunct Publication (UbiComp '13 Adjunct), 299-302.  
10. Morris, M.R. 2012. Web on the wall: insights from a  multimodal interaction elicitation study. In Proc. of the  2012 ACM International Conference on Interactive  Tabletops and Surfaces (ITS '12), 95-104.  
11. Mueller, F., Graether, E. and Toprak, C. 2013.  Joggobot: jogging with a flying robot. In CHI '13 Extended  Abstracts on Human Factors in Computing Systems   (CHI EA '13), 2845-2846.  
12. Mueller, F.F. and Muirhead, M. 2015. Jogging with a  Quadcopter. In Proceedings of the 33rd Annual ACM  
Conference on Human Factors in Computing Systems   (CHI '15), 2023-2032.  
13. Nagi, J., Giusti, A., Caro, G.A.D. and Gambardella,  L.M. 2014. Human Control of UAVs using Face Pose  Estimates and Hand Gestures. In Proceedings of the 2014  ACM/IEEE International Conference on HumanRobot  Interaction (HRI'14), 252-253.  
14. Ng, W.S. and Sharlin, E. 2011. Collocated interaction  with flying robots. In IEEE International Symposium   on  Robot  and  Human  Interactive  Communication   (ROMAN '11), 143-149.  
15. Nozaki, H. 2014. Flying display: a movable display  pairing projector and screen in the air. In CHI '14 Extended  Abstracts on Human Factors in Computing Systems   (CHI EA '14), 909-914.  
16. Pfeil, K., Seng Lee, K. and LaViola, J. 2013. Exploring  3d gesture metaphors for interaction with unmanned aerial  vehicles. In Proceedings of the 2013 International  Conference on Intelligent User Interfaces (IUI '13),   257-266.  
17. Ruiz, J., Li, Y. and Lank, E. 2011. Userdefined motion  gestures for mobile interaction. In Proceedings of the  SIGCHI Conference on Human Factors in Computing  Systems (CHI '11), 197-206.  
18. Scheible, J., Hoth, A., Saal, J. and Su, H. 2013.  Displaydrone: a flying robot based interactive display. In  Proceedings of the 2nd ACM International Symposium on  Pervasive Displays (PerDis '13), 49-54.  
19. Szafir, D., Mutlu, B. and Fong, T. Communication of  intent in assistive free flyers. In Proceedings of the 2014  ACM/IEEE International Conference on HumanRobot  Interaction (HRI '14), 358-365.  
20.  Szafir,  D.,  Mutlu,  B.  and  Fong,  T.  2015.  Communicating Directionality in Flying Robots. In  Proceedings of ACM HumanRobot Interaction (HRI '15),  19-26.  
21. Vatavu, R.-D. and Zaiti, I.-A. Leap gestures for TV:  insights from an elicitation study. 2014. In Proceedings. of  the 2014 ACM International Conference on Interactive  Experiences for TV and Online Video (TVX '14), 131-138.  
22. Wobbrock, J.O., Aung, H.H., Rothrock, B. and Myers,  B.A. 2005. Maximizing the guessability of symbolic input.  In CHI '05 Extended Abstracts on Human Factors in  Computing Systems (CHI EA '05), 1869-1872.  
23. Wobbrock, J.O., Morris, M.R. and Wilson, A.D. 2009  Userdefined  gestures  for  surface  computing.  In  Proceedings of the SIGCHI Conference on Human Factors  in Computing Systems (CHI '09), 1083-1092.  
  
365 
SESSION: INTERACTING WITH ANIMALS AND FLYING ROBOTS 
",Drone & Me.pdf,0
"Special Issue: Computer Vision for Smart Cameras and Camera Networks 
Drone swarm patrolling with uneven coverage requirements 
ISSN 1751-9632 Received on 5th December 2019 Revised 7th April 2020 Accepted on 1st May 2020 EFirst on 14th October 2020 doi: 10.1049/ietcvi.2019.0963 www.ietdl.org 
Claudio Piciarelli1  
, Gian Luca Foresti1 
1Università degli Studi di Udine, Via delle Scienze 206, 33100 Udine, Italy 
 Email: claudio.piciarelli@uniud.it 
Abstract: Swarms of drones are being more and more used in many practical scenarios, such as surveillance, environmental monitoring, search and rescue in hardlyaccessible areas and so on. While a single drone can be guided by a human operator, the deployment of a swarm of multiple drones requires proper algorithms for automatic taskoriented control. In this study, the authors focus on visual coverage optimisation with dronemounted camera sensors. In particular, they consider the specific case in which the coverage requirements are uneven, meaning that different parts of the environment have different coverage priorities. They model these coverage requirements with relevance maps and propose a deep reinforcement learning algorithm to guide the swarm. This study first defines a proper learning model for a single drone, and then extends it to the case of multiple drones both with greedy and cooperative strategies. Experimental results show the performance of the proposed method, also compared with a standard patrolling algorithm. 
1�Introduction 
During the last years, the use of multicopter drones has gained large popularity in many practical application fields, such as agriculture [1], surveillance [2], disaster management [3], search and rescue [4], environmental data acquisition [5], medicine delivery [6] and so on. This interest in commercial, scientific and social fields led to a drastic change in the way drones are used if compared to previous applications, which were mostly confined to video acquisition. While a single drone, manually controlled by a human operator, was a typical scenario up to few years ago, current applications often rely on multiple drones (swarms) autonomously cooperating to perform a given task. This justifies the increment of scientific works on topics such as automatic drone control, path planning, smart resource management and so on. In this work, we focus on the problem of drone visual coverage: multicopter drones are equipped with cameras to observe a portion of the environment for specific tasks (e.g. surveillance) and the observed area must be optimised according to given criteria, under the assumption that the environment is too large to get a simultaneous full coverage. For example, in a surveillance application, a basic requirement could be that no area is left uncovered for more than a given amount of time, in order to avoid ‘blind spots’ in the surveillance application. A task like the one just described can be accomplished with uniform coverage, given that enough drones are available. In this paper, however, we want to investigate scenarios with uneven coverage requirements. This means that not all the areas of the environment have the same importance: some parts should be observed more frequently, or require immediate coverage, if compared to other zones. Each point of the environment thus has a given priority (or relevance, using the term that will be adopted in the rest of the paper), and the goal of the system is to find a patrolling strategy that optimally covers the environment according to these relevance requirements. For example, in a disaster management context such as a flooding or fire in extended rural areas, the zones around known buildings should have priority for visual inspection in order to quickly identify people in danger. We propose to model the problem of prioritised visual coverage as a Markov decision process, where each drone is an agent that can actuate several actions to change its state (e.g. moving forward, rotating, zooming the camera etc.) and it gets a reward (either positive or negative) in terms of coverage quality as a consequence 
of its actions. This model allows to search for a patrolling strategy using reinforcement learning, thus letting the drone learn from experience rather than explicitly giving a patrolling algorithm. The reinforcement learning algorithm will be implemented using a neural network, thanks to the Deep QNetwork architecture. The proposed model is theoretical, meaning that the set of actions and the state space do not consider all the variables of a realworld system (such as drone movement inertia, power consumption constraints etc.), however it can be used as a reference framework for possible implementations. Our basic idea of reinforcementlearningbased patrolling was already presented in [7], however in that work only the singledrone case was considered. In this paper, we extend our previous work by improving the patrolling model (see (23)) and by considering the novel scenario of multidrone swarms. In this case, we propose two different strategies, a greedy and a cooperative one, depending on the type of information shared between drones. Compared to [7], also the experimental results have been extended, even for the singledrone scenario. The paper is organised as follows. In Section 2, we review some of the most relevant works in literature regarding drone coverage tasks, while in Section 3 we recap the basic theory behind reinforcement learning and its deep learning implementations. In Section 4, the Markov decision process model for a single drone is described by defining the state space, the action space and the reward function in terms of visual coverage of relevance maps. A patrolling strategy is then implemented using the given model, which is then extended to the case of multiple drones in Section 5, where two swarm patrolling strategies are proposed. Experimental results are given in Section 6. 
2�Related works 
The problem of drone control for patrolling tasks has been studied by several authors. A survey of coverage path planning algorithms in robotics can be found in [8], while [9] is specifically focused on drones. Here, the authors propose a taxonomy of the cited methods, ranging from simple geometric flight patterns to more complex gridbased solutions considering full and partial information about the area of interest. The authors mostly focus on how to cover areas with complex geometrical shapes but, in contrast with the proposed method, none of the surveyed works explicitly considers uneven coverage requirements, neither addresses the problem using neural networks. 
IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 452 
3�Reinforcement learning essentials 
Reinforcement learning is a learning strategy in which an agent in a given state executes an action and gets an immediate reward (or penalty) as a consequence of that action. The goal is to learn from this experience, figuring out the best actions that will eventually lead to a maximisation of the total reward on the long term. This approach can be easily applied to coverageoriented drone patrolling problems, since the drone (the agent) can take several actions (e.g. moving forward, zooming in etc.) that will impact its visual coverage of the environment. It must be noted that trivial solutions, such as choosing the action that will immediately maximise the coverage, are not suited for problems with uneven coverage requirements. In this case, the best shortterm action does not necessarily lead to a good longterm solution, e.g. when the drone must necessarily cross a lowpriority area in order to reach an highpriority zone. This justifies the use of reinforcement learning as a technique to find the optimal patrolling strategy. Formally, reinforcement learning models the problem as a Markov decision process P = {S, A, τ, r, γ}, where S is a finite set of agent states and A is a finite set of actions. τ(s′ s, a) is the transition probability from state s to state s′ given that action a is executed, r(s, a) is the reward obtained by executing action a in state s and γ is a discount factor, modelling the importance of immediate, short term rewards with respect to past rewards. A fundamental property of Markov decision processes is that the transition probability is defined only in terms of s, s′ and a, thus meaning that the next state will be affected only by the current state and action, and not by the history of previous states. A policy is a probability function π(a s) denoting the probability for an agent in state s to take action a, and the goal of reinforcement learning is to find the optimal policy π ∗ that maximises the expected total discounted reward 
π ∗ = argmax π Eτ,π ∑ i = 0 
∞ γir(si, ai) (1) 
where Eτ,π means that the expected value is computed assuming that the states sequence is distributed according to τ and the actions are chosen according to π. Several methods have been proposed to solve (1), one of the most popular being Qlearning. 
3.1 Qlearning 
The Qlearning algorithm is based on the definition of a function Qπ modelling the total discounted reward that can be achieved following the policy π if action a is chosen in state s 
Qπ(s, a) = Eτ,π,s0 = s,a0 = a ∑ i = 0 
∞ γir(si, ai) . (2) 
Equation (2) can be written recursively as 
Qπ(s, a) = r(s, a) + γ∑ s′, a′ τ(s′ s, a)π(a′, s′)Qπ(s′, a′) (3) 
and it can be simplified if the optimal policy π ∗ is considered. In fact, π ∗(a, s), because of its optimality, has a binary nature: it has value 1 for the best action possible and 0 for any other action, thus the Q ∗ function for the optimal policy reduces to 
Q ∗(s, a) = r(s, a) + γ∑ s′ τ(s′ s, a) max a′ Q ∗(s′, a′) (4) 
If we restrict our analysis to deterministic problems, in which also τ(s′ s, a) can assume only binary values, (4) further simplifies to the socalled Bellman equation [20] 
Q ∗(s, a) = r(s, a) + γ max a′ Q ∗(s′, a′) (5) 
where s′ is the state reached from s by executing action a, in other words it is the only state such that τ(s′ s, a) = 1. The Qlearning algorithm computes Q ∗ using (5) and dynamic programming. It starts by filling a S × A  table with initial random values Q0 ∗(s, a) for each possible combination of states and actions, and then updates them iteratively until convergence according to the following equation: 
Qi + 1 ∗ (s, a) = r(s, a) + γ max a′ Qi ∗(s′, a′) . (6) 
In order to apply (6), Qlearning requires the knowledge of the reward r(s, a). This knowledge comes from experience: in the training phase, the agent is requested to perform actions in order to measure the corresponding reward and accumulate enough data to update Q ∗ until convergence. In theory, the actions could be always chosen randomly, but in practice it is more effective to draw some of them from the partiallylearned policy in order to evaluate the most promising parts of the stateaction space. This approach is called explorationexploitation strategy, where exploration refers to the random choice of actions to explore previously unseen stateaction pairs and exploitation refers to exploiting the current estimate Qi ∗ in order to choose the action that maximises it in the current state. A typical training starts with exploration only, and the exploitation becomes more and more frequent as the iterative process progresses. Once Q ∗ is computed, finding the optimal policy is trivial, since Qthe best action to be taken in state s is the one that maximises ∗(s, ⋅ ). 
3.2 Deep Qnetworks 
The dynamic programming approach described in Section 3.1 requires the memorisation of a S × A  table which is often 
IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 453 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
L(θ) = E Y − Q ∗(s; θ) a 
2 (7) 
representing the difference between the current estimate Q ∗(s; θ) a of the value of action a in state s and the new estimate that can be computed from the experience, defined as 
Y = r(s, a) + γ max k Q ∗(s′; θ) k . (8) 
The computation of the loss function (7) can lead to bias issues in practical implementations. Neural networks are typically trained in small batches, and if the experience data of a batch come from the same experiment (e.g. subsequent steps of the same agent) they can lead to biased computation of the expected value. In order to avoid this problem, a replay memory can be used, which consists in a large set of experience tuples. During the training phase the tuples, rather than being directly used to train the network, are stored in the replay memory. Batches are then built by sampling the replay memory with uniform distribution, thus avoiding to build batches composed only of highly correlated tuples. Another problem in the computation of the loss function comes from the network parameters θ which are used both for action selection in (8) and for action evaluation in (7). It has been proven that this could lead to biased results [23], which can be avoided by decoupling selection and evaluation using two different networks. Two of the most popular approaches are the Target Network approach [21], in which the new estimate is defined as 
Y = r(s, a) + γ max k Q ∗(s′; θ −) k (9) 
and the Double DQN approach [23]: 
Y = r(s, a) + γQ(s′; θ −) argmaxkQ(s′;θ) k (10) 
where θ − are the parameters of a second deep Qnetwork. This second network, rather than being trained independently, is generally defined in terms of the first network, either via hard update (θ − is set to θ at every fixed number of epochs) or via soft update at each epoch i, according to a temporal smoothness factor α ∈ [0, 1] 
θi − = (1 − α)θi − 1 − + αθi . (11) 
4�Single drone model 
In order to apply reinforcement learning techniques to drone patrolling tasks, we must model the drone as a Markov decision process agent. This implies defining its possible states, the actions and a reward function giving a positive or negative feedback to each action. 
4.1 State space 
The state space should consider all the relevant parameters that define the drone setup at a given time instant. We identified six parameters that directly influence the visual coverage of a drone. Formally, a drone state is a tuple s = {x, y, z, ψ, ϕ, f } defined as 
• x, y, z: spatial coordinates of the drone; 
• ψ: camera orientation angle; • ϕ: camera tilt angle; • f: camera focal length. 
The spatial coordinates x, y, z are referred to a world reference system, and are limited by the borders of the area to be monitored and by the maximum flying height the drone can reach. The camera orientation angle ψ ∈ [0, 2π] is the camera azimuth, expressed as the angle between the camera frontal axis and the Xaxis of the world reference system. The camera tilt angle ϕ ∈ [0, π/2] describes the elevation of the camera, where ϕ = 0 is the camera pointed at the horizon and ϕ = π/2 is a nadiral view. Finally, the focal length f is included in the state to model zoom cameras, and its range is hardware dependent. Observe that we did not include the drone azimuth in the state space, as this information is not relevant. The proposed work is focused on multicopter drones, which can move freely in the three spatial dimensions (as opposed to fixedwing drones), thus identifying a frontal axis is unnecessary: in the proposed framework, a drone aiming north and moving frontward is equivalent to a drone aiming east and moving leftward. 
4.2 Action space 
As done for the state space, we identified a set of drone actions that directly influence the visual coverage of the drone. The action space consists of a total of 12 actions: 
• Move {Forward | Backward | Left | Right | Up | Down }; • Rotate {Left | Right}; • Tilt {Down | Up}; • Zoom {In | Out}. 
The Move actions translate the drone in the 3D space and influence the {x, y, z} components of the drone state. Without loss of generality, the front direction is assumed to be the camera orientation angle: as described in Section 4.1, there is no need to decouple drone and camera azimuths. The camera orientation is defined by the Rotate and Tilt actions, which, respectively, influence the {ψ, ϕ} parameters. Finally, the Zoom actions change the focal length f and thus the zoom level of the camera. Note that the proposed actions do not quantify the amount of requested change, e.g. how much the drone should move when a MoveForward action is executed. Ideally, those values should be continuous, and the action space would be infinite. However, the reinforcement learning techniques described in Section 3 require a finite action space, and thus the actions must be discretised. The amount of parameter change caused by each action is thus fixed and defined a priori. For example, the experimental results discussed in Section 6 have been obtained with a Rotate step of π/16, meaning that the camera performs a full 2π rotation after 32 Rotate actions in the same direction. 
4.3 Visual coverage 
In order to evaluate the visual coverage quality of a drone, we need a way to compute the portion of the environment observed by the camera. Let us consider a reference system as the one depicted in Fig. 1, where the origin lies on the centre of the camera optics, the Yaxis points upwards and the Zaxis is initially aligned with the camera optical axis when the camera points at the horizon with no rotation. The effect of actions Rotate {Left | Right} can be modelled by a rotation matrix Rψ around the Yaxis 
Rψ = 
cos ψ 0 sin ψ 0 1 0 −sin ψ 0 cos ψ 
(12) 
while the actions Tilt { Up | Down} are modelled by a rotation around the Xaxis 
454 IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
1 0 0 0 cos ϕ sin ϕ 0 sin ϕ cos ϕ 
. (13) 
Any camera configuration can thus be modelled by a combination Rψϕ of the two matrices, observing that the tilt rotation must be applied first, in order to correctly model the motion of a gimbal camera 
Rψϕ = RψRϕ = 
cos ψ sin ψsin ϕ sin ψcos ϕ 0 cos ϕ sin ϕ −sin ψ sin ϕcos ψ cos ϕcos ψ 
. (14) 
By using the matrix (14), it is possible to project on the ground plane any point on the image plane, given the camera rotation and tilt angle. Let p = (x, y) be a pixel in the image plane. According to the camera pinhole model, the focal length f is the distance between the optics and the image plane itself, and thus the pcoordinates of the pixel in the camera reference frame are ^ = (δxx, δyy, f ), where δx, δy are the pixel sizes of the imaging sensor. If the camera is rotated and tilted, point p ^ is moved to ^p ψϕ = Rψϕp ^. By switching to a world reference system, where the camera has coordinates C = (Cx, Cy, Cz), the parametric equation of a line parallel to vector p ^ ψϕ and passing through C is 
L(t) = C + tp ^ ψϕ . (15) 
To find the intersection of L with the ground plane, it is sufficient to set the Y world coordinates to zero 
Cy + tp ^ ψϕ, y = 0 
t = − Cy/p^ ψϕ, y 
(16) 
and by substituting (16) into (15) we get the projection pg of point p on the ground plane 
pg = C − p^Cy ψϕ, y ^p ψϕ . (17) 
By using (17), it is possible to project on the ground plane any point p in the image plane given the camera angles ψ, ϕ. If the equation is applied to the four corners of the image plane, it is thus possible to compute the corners of the trapezoidal projection of the image plane on the ground plane, which is the portion of the 
environment observed by the camera (we here assume that no point lies above the horizon). We define this zone the visual coverage of the drone in state s, for now on denoted as V(s). 
4.4 Reward function 
As stated in Section 1, this works deals with the case of uneven coverage requirements. This means that not all the portions of the environment have the same priority, and some areas are more important than others and require immediate visual inspection. In [24], the authors used this approach to focus a surveillance system on the areas with highest activity, while in [25] the high priority areas were identified by audio sensors. In general, the definition of these areas is extremely contextdependent, and here we just assume that such a definition exists in the form of a relevance map. A relevance map ℳ(x, y):ℝ 2 → [0, 1] is thus a function taking as input the (x, y) coordinates of a point in the world reference system and returning a value in the range [0, 1] denoting the relevance of that point, i.e. the relative importance of getting visual coverage of that point with respect to the rest of the map. It is now possible to define the observed relevance ρ(s) of a drone in state s as the total relevance within its visual coverage 
ρ(s) =∫ ∫V(s)ℳ(x, y) dA (18) 
or, in the likely case that ℳ is discretised in a matrix 
ρ(s) = ∑ (x, y) ∈ V(s) ℳ(x, y) (19) 
where V(s) is the visual coverage of the drone, as defined in Section 4.3. The drone reward function could thus be defined in terms of its observed relevance, giving a positive reward to actions that increase ρ(s). However, additional constraints are needed in order to avoid extreme cases, such as the drone flying at the maximum possible height trying to cover the entire area. Despite in small areas this could be a viable solution, in larger environments it is most probably a useless configuration from a practical point of view because of the very low spatial resolution (pixel per meter) of the acquired images. We thus enforce a constraint on the size of the visual coverage by defining a penalty function k:ℝ → [0, 1] such as the one shown in Fig. 2 that penalises coverages which are either too small or too large for practical applications; the shape and tuning of k is of course application dependent. We thus define the constrained observed relevance (COR) as 
ρ^(s) = k V(s) ρ(s) . (20) 
The drone reward function r(s, a) can now be defined as 
r(s, a) = ρ^(s′) − ρ^(s) (21) 
that is, if the execution action a in state s leads to state s′, the corresponding reward is defined as the difference of COR between the two states s′ and s. By using such a function, the reinforcement learning system is positively rewarded each time the agent chooses actions that lead to an increase of observed relevance. Since reinforcement learning algorithms maximise the total reward on the long term, this means that the drone will try to cover highrelevance areas, even if this requires to move through lowrelevance zones in the short term. 
4.5 Patrolling 
Training a reinforcement learning network with the reward function described in Section 4.4 is not sufficient to get a patrolling algorithm. The reward function just forces the drone to move to more relevant areas in order to increase the total reward, and the process will stop once the highest possible value has been reached: the algorithm is actually a statespace explorer trying to find a path to the global maximum of the COR function. 
Fig. 1� Camera reference system   
Fig. 2� Penalty function to penalise large visual coverage   
IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 455 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
ℳs t(x, y) = ℳ(x, y)Ts t(x, y) (22) 
defined as a combination of the static map ℳ and a temporal mask Ts t:ℝ 2 → [0, 1]. The temporal mask at time instant t for a drone in state s is defined as 
Ts t(x, y) 
= 
1 for t = 0 
min (1, Ts t − 1(x, y) + δ+) for t > 0 ∧ (x, y) ∉ V(s) 
max (0, Ts t − 1(x, y) − k(V(s))δ−) for t > 0 ∧ (x, y) ∈ V(s) (23) 
where k is the penalty function defined in Section 4.4, V(s) is the visual coverage of the drone in state s defined in Section 4.3 and δ−, δ+ are constant decreasing and increasing factors. The decreasing factor is multiplied by the penalty function k(V(s)) because we do not want to model the contribution of drones that do not satisfy the area constraints, as already done in the definition of the COR (20). With this definition, the temporal relevance map ℳs t 
dynamically changes depending on the time since last observation for each point of the map. Applying the reinforcement learning algorithm on this map forces the drone to continuously move around the area searching for highrelevance areas that have not been observed since a long time, thus implementing an efficient patrolling algorithm. Finally, observe that the temporal map should now be part of the state, since it is dependent on the chosen actions rather than being static. The reward function (21) should now be rewritten as 
r({s, ℳs t}, a) = ρ^({s′, ℳs′ t + 1}) − ρ^({s, ℳs t}) (24) 
and the definitions of ρ^, ρ are consequently adapted so that the summation in (19) is performed over ℳs t rather than ℳ. 
5�Swarm model 
The model proposed in Section 4 describes how a single drone can be modelled as a Markov decision process agent, and its patrolling strategy can be defined using reinforcement learning and a proper reward function. However, the model can be extended to a swarm of drones, where multiple drones share the task of patrolling a given area trying to maximise the overall visual coverage of relevant areas. We here propose two approaches, the greedy strategy and the cooperative one. 
5.1 Greedy strategy 
The greedy strategy consists in applying the singledrone model to each drone of the swarm. This way, every agent will try to maximise its own total reward in a greedy way; this is independent from other swarm members. However, a naive application of this strategy would eventually lead all the drones to cover the same areas of the map, namely the ones with highest relevance. In order to turn the greedy strategy into a sensible patrolling algorithm, it is sufficient to request that the temporal relevance map is shared among all the swarm. With map sharing, each drone will naturally avoid the areas already observed by other members of the swarm because the temporal relevance of those areas will be decreasing 
due to coverage. The approach is still greedy since each drone does not explicitly know about the presence of other drones in the area, however this knowledge is indirectly modelled by the shared map. The shared temporal relevance map for a swarm of N drones with states S = {si}i = 1 N  is defined as 
ℳS t (x, y) = ℳ(x, y)TS t (x, y) (25) 
where the temporal mask follows the same principle of (23), but extended to all the drones in the swarm 
TS t (x, y) 
= 
1 for t = 0 
min (1, TS t − 1(x, y) + δ+) for t > 0 ∧ (x, y) ∉ ⋃ s ∈ S V(s) 
max (0, TS t − 1(x, y) − for t > 0 ∧ (x, y) ∈ ⋃ s ∈ S V(s) 
∑ s (x, y) ∈ V(s) k(V(s))δ−) 
. 
(26) 
This way, the temporal mask increases if the point (x, y) is not observed by any drone, but it is decreased multiple times if it is observed by several drones, to encourage the multiple coverage of zones where the relevance is particularly high. The reward function (24) is consequently redefined as 
r({s, ℳS t }, a) = ρ^({s′, ℳS′ t + 1}) − ρ^({s, ℳS t }) (27) 
The computation of the shared map can be done by a central processing node and sent at each time interval to all the swarm. This is particularly suitable if the entire computation is done offline: in this case the algorithm is just used to precompute a patrolling strategy which is subsequently executed. If the algorithm must be applied online, a distributed approach would be preferred; this case is covered in the next section. 
5.2 Cooperative strategy 
In the case of cooperative strategy, each drone is aware of the rest of the swarm and their states, so that collaborative strategies can be implemented directly, rather than indirectly via the shared map as in the greedy strategy. However, rather than explicitly defining the cooperative models, we rely on reinforcement learning to automatically learn them from experience. The idea is to use S = {si}i = 1 N , the set of all the states of the drones, as a new global state. The implementation is straightforward, since the reward function for agent in state s and taking action a leading to state s′ is defined as 
r({S, ℳS t }, a) = ρ^({s′, ℳS′ t + 1}) − ρ^({s, ℳS t }) (28) 
However, this is the same as (27), except for S being made explicit in the function input. The difference between the cooperative and greedy strategy in fact does not lie in the reward function, but in the way rewards are related to states, and this is what 
Fig. 3� Two competing drones for the same highrelevance area   
456 IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
5.3 Deep learning implementation 
In order to implement the greedy and cooperative techniques, we adopted the Deep QNetwork approach described in Section 3.2 with a linearly decreasing ratio between exploration and exploitation steps. In order to avoid training biases, the Double 
DQN model has been used (10) with replay memory and soft update of the second network. For the greedy strategy, the network input is {s, ℳs t}, this is the drone state and the current shared temporal relevance map, and the output is a ℝ 12 vector with the estimated Q({s, ℳS t }, a) values for each possible action a. However, rather than modelling the state s as a tuple, we propose to represent it visually, with a binary image showing the visual coverage V(s) of the drone. This way, the information about the observed area is immediately available from the input data and does not have to be estimated from the agent state tuple, thus achieving a simpler model and a faster convergence rate. In order to simplify the input, the two images are centred on the drone position, rotated by the drone orientation angle, and cropped to a fixed size. This way, the network input represents the surrounding of the drone, aligned with its orientation. The two images are finally stacked to form a 64 × 64 × 2 input tensor (Fig. 4).  For  the  cooperative  strategy,  we  adopted  a  similar representation to model the network input {S, ℳs t}. A new binary image is created, representing the visual coverage of all the drones, and it is added to the input with the same centring, rotation and cropping procedures described above, in order to create a 64 × 64 × 3 input tensor. The choice of keeping the representations of V(s) and V(S) separately (second and third elements in the input stack, respectively) rather than relying on V(S) alone (which contains V(s)) is used to feed the network with an explicit information to discriminate between the drone on which the network is being executed and the remaining ones. The final input for cooperative mode is shown in Fig. 4. The rest of the network is the same for the two approaches, and consists of a single convolutional layer, composed of 16 8 × 8 filters with stride = 2. The convolutional layer is followed by two 1024 × 1 fully connected layers and a 12 × 1 output layer representing the Q values for all the possible drone actions. All the layers use ReLU activation functions, except for the output layer, where the activation function is linear. The network topology is shown in Fig. 5.  The training is done by creating a random relevance map and a random number of drones, with random initial states, at each epoch. The system then evolves for 20 steps (i.e. 20 actions for each drone), either by choosing random actions or by using the partially trained network (exploration/exploitation). During this procedure, old and new states, actions and rewards are stored in the replay memory. After 20 steps, a random batch of samples is extracted from the replay memory and used to compute the loss function and train the network. The process is repeated for every epoch. 
6�Experimental results 
The proposed model has been implemented and tested in Python using Tensorflow and the Keras library. As described in Section 5.3, the adopted model is a Double Deep QNetwork with soft update of the second network and replay memory. Each epoch 
Fig. 4� Network input example (a) Randomlygenerated relevance map and two drones with their visual coverage, (b) Shared temporal relevance map, centred and rotated on drone 0, (c) Visual coverage of drone 0, (d) Visual coverage of all drones, as seen from drone 0 Network input for the greedy strategy is composed of (b) and (c) stacked; network input for the cooperative strategy also adds (d) to the stack   
Fig. 5� Proposed network structure   
IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 457 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
6.1 Convergence to highrelevance zones 
We started the evaluation process by checking if the proposed approach can really control a drone so that it can always converge to highrelevance zones. In order to do the test, we relied on the greedy network and used it in a singledrone configuration with the temporal map update disabled. This way, the expected behaviour is that the drones will converge towards the zone with highest relevance and, once reached, they will keep patrolling the same zone. In order to have a reference measure, as a first experiment we generated a random relevance map (Fig. 6) and searched for its optimal COR (20) using a bruteforce, exhaustive search. Since each one of the six state parameters has been discretised in 32 values, the total amount of possible states is 32 6 and its exhaustive search would require ∼280 h if performed on the adopted hardware. In order to make the problem tractable, we halved the number of discretisation steps of each state parameter, thus finding the bruteforce optimal COR = 0.079 in ∼2.8 h. We then executed the proposed method 100 times on the same map, with random initial conditions and measured the final COR to check if it was similar to the brute force one. On average, the final COR was 0.075, thus 95% of the best possible result. The proposed method can thus find drone configurations with a coverage that is close to the best possible one, although in a fraction of time: on average, each run found the optimal COR in 0.61 s. Fig. 6 shows the best result as well as some of the approximated solutions. It is also interesting to measure the efficiency of the proposed method in finding the best solution efficiently, this is in a small number of actions. We thus measured the number of actions required by each run in order to find the final solution, and divided it by the minimum number of steps possible, which is easily computed once the initial and ending states are known. The average ratio is 1.54, thus we can expect the proposed method to reach the optimal configuration in ∼50% more steps than the best possible path in the state space. The proposed experiment however used a single relevance map, and it is not suited for computing robust results, since each brute force computation is extremely time consuming. We thus developed another test that can be more easily applied on several maps. On each map, we ran 20 agents for 50 time intervals and measured the COR of each drone. The starting state of each drone is chosen randomly. If the proposed algorithm works, we expect all the COR values to converge to a similar final score, meaning that every drone has reached an approximation of the global maximum. Fig. 7 shows this behaviour, where it is clearly seen that after roughly 20 steps all the drones reached stable and similar COR values. Small fluctuations still exist, since our model do not explicitly consider the ‘do nothing’ action (to avoid getting stuck) and thus the drone keeps moving around the found optimal area. In order to measure this behaviour numerically, we analysed the final COR values by normalising them so that their mean is set to 1 and by computing their standard deviation σ. The normalisation step is required to get comparable results between different maps. We repeated the test on 50 different relevance maps, and the results are shown in Table 2. As it can be seen, the final standard deviation is relatively small, with an average value of σ = 0.13. This means that the majority of the final COR values lie within their mean ± 13%, thus proving the capability of the system to reach highrelevance areas. 
6.2 Single drone patrolling 
As mentioned in Section 4.5, patrolling behaviour is achieved by enabling the temporal update of the relevance map. With temporal update, observed areas gradually decrease their relevance, thus making more convenient to move to a different state in search for higher rewards. With patrolling enabled, a drone should thus avoid static behaviours in which it keeps monitoring always the same zone. Fig. 8a shows the path of a drone image centre, projected on the ground plane, with patrolling enabled. As it can be seen, the nonstatic behaviour is evident. 
Table 1 Deep QNetwork training hyperparameters Hyperparameter Value discount factor γ (1) 0.99 replay memory size 104 
batch size 64 optimiser Adam Adam learning rate 10−4 
epochs (greedy) 105 
epochs (coop) 3 × 105 
soft update α (11) 0.001 
  
Fig. 6� Optimal configuration found by brute force (black) and some approximations computed by the proposed method (other colours). For each configuration the visual coverage (trapezoidal area) and the line connecting the drone with the projection of the image centre are shown   
Fig. 7� COR values of 20 drones running on the same map for 50 steps (no temporal update of the map)   
458 IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Gt = 1 − ∑ (x, y) 
ℳs ℳst(x, y) 0(x, y) . (29) 
The summation part of (29) is the ratio between the current relevance of the entire map at time t and the initial total relevance (i.e. the relevance map without temporal updates). The ratio thus has a maximum value of 1, and lower values denote a good patrolling, since it means that the drone covered highrelevance areas making their temporal relevance decrease. Gt is then defined in order to have higher scores for good patrolling patterns. Fig. 8b shows the global coverage score for the experiment shown in Fig. 8a. After 200 iterations, Gt reaches a stable value around 0.15, meaning that the total temporal relevance for t > 200 is ∼85% of the original relevance. This measure alone is not meaningful, since it depends on many factors such as the size of the map, the speed of temporal updates and so on, however it is useful for comparative results. The proposed system is thus compared with a standard, naive zigzag patrolling scheme. In this case, the drone follows a predefined path, disregarding the relevance values, uniformly spanning all the environment following a path like the one shown in Fig. 9, alternating vertical and horizontal scans. Fig. 10 shows the global score over the same relevance map of both the proposed approach and the zigzag pattern. As it can be seen, the proposed method outperforms the naive strategy, with an average Gt = 0.184, more than 50% better than the average Gt = 0.119 obtained by the naive patrolling. Table 3 shows the results of 50 tests on different maps, reporting the average Gt for both methods and the consequent performance boost. The behaviour shown in Fig. 10 is confirmed, and the average achieved performance boost of the proposed method is 41.95%. 
6.3 Swarm patrolling 
In Section 5, two swarm patrolling models have been proposed, a greedy approach and a cooperative one. The greedy approach consists of each drone acting independently from the others, since the only collaboration happens indirectly by sharing the same temporal relevance map. The cooperative one instead uses an explicit representation of the states of any other drone that can be used to explain and predict temporal map changes and thus plan a better patrolling strategy. Since this predictive feature has not been explicitly coded, we rely on reinforcement learning to learn it from experience, and thus comparative results are required to show if a performance gap between the two approaches actually exists. The global coverage (29) can be used again as a quality metric of the computed patrolling paths. Fig. 11 shows the global coverages of the greedy and cooperative approaches on the same map, with four drones starting from the same initial states. The cooperative approach indeed seems to perform better than the basic greedy approach, with an average global coverage of 0.505 and 0.355, respectively. In order to confirm this result, more experiments have been conducted, as shown in Table 4. In this case, we considered a swarm of 2, 3, 4 and 5 drones, respectively. For each case, we ran 20 tests on random relevance maps. In each test, both the greedy 
Table 2 Standard deviations of 20 normalised COR values on 40 different relevance maps Test no. σ Test no. σ Test no. σ Test no. σ 
0 0.0547 10 0.1074 20 0.0749 30 0.0978 1 0.2317 11 0.2127 21 0.1111 31 0.0721 2 0.1001 12 0.0888 22 0.1852 32 0.1821 3 0.1335 13 0.1210 23 0.1509 33 0.0705 4 0.1966 14 0.1501 24 0.1745 34 0.1955 5 0.1049 15 0.0753 25 0.1471 35 0.1264 6 0.0909 16 0.1074 26 0.0933 36 0.1667 7 0.1646 17 0.0975 27 0.1346 37 0.1798 8 0.0729 18 0.1966 28 0.1887 38 0.1800 9 0.0658 19 0.1106 29 0.2126 39 0.0795 
  
Fig. 8� Example of single drone patrolling (a) Temporal evolution of the image centre of a drone, projected on the ground plane, 
while in patrolling mode; (b) Corresponding global coverage Gt   
Fig. 9� Naive patrolling pattern. After a sequence of vertical (black) and horizontal (grey) scans, the drone returns to its starting position and repeats the same pattern   
IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 459 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
6.4 Computational load 
Since the proposed theoretical framework is currently implemented as a Python simulation, we cannot measure the timings of a full senseact cycle. However, we can measure the computational time required to choose an action given a state input, in order to understand if a practical implementation is actually feasible. Choosing an action requires to run the input through the neural network. This operation in our experiments consistently requires 1.5 ms on a CPUonly implementation running on an Intel(R) Core(TM) i7-9700 K 3.60 GHz processor. We believe this time is negligible if compared to the time required to actually execute the chosen action. 
7�Conclusions 
In this work, we proposed a theoretical model to compute efficient patrolling paths for single drones and swarms of drones. The problem of uneven coverage requirements was explicitly considered: the environment zones are associated to different priority (relevance) scores, expressing the importance for an area to be visually covered by a drone. The proposed implementation uses reinforcement learning to train a deep network that selects the best action that will most likely lead to a good coverage in the long run. The proposed system was extensively tested and showed good performances, also compared to a standard patrolling scheme. 
Fig. 10� Global coverage scores Gt for the proposed and zigzag patrolling schemes (single drone)   
Table 3 Comparison of the average global coverage of the proposed method and naive zigzag patrolling schemes on 50 different maps Test no. Ours Zigzag Boost% Test no. Ours Zigzag Boost% 1 0.1836 0.1161 58.16 26 0.1546 0.1192 29.66 2 0.1671 0.1162 43.73 27 0.1581 0.1192 32.67 3 0.1421 0.1176 20.89 28 0.1649 0.1250 31.92 4 0.1800 0.1180 52.56 29 0.1648 0.1213 35.81 5 0.1788 0.1258 42.08 30 0.1557 0.1284 21.28 6 0.1551 0.1229 26.17 31 0.1701 0.1219 39.51 7 0.1650 0.1164 41.71 32 0.1481 0.1177 25.77 8 0.2005 0.1146 74.97 33 0.1870 0.1213 54.24 9 0.1555 0.1132 37.39 34 0.1679 0.1235 35.99 10 0.2203 0.1137 93.88 35 0.1473 0.1173 25.61 11 0.1580 0.1162 35.92 36 0.1675 0.1263 32.65 12 0.1832 0.1191 53.87 37 0.1790 0.1162 54.06 13 0.1697 0.1220 39.08 38 0.1571 0.1199 31.06 14 0.1315 0.1230 6.86 39 0.1693 0.1203 40.68 15 0.1759 0.1188 48.09 40 0.1563 0.1251 24.97 16 0.1715 0.1251 37.03 41 0.1492 0.1163 28.35 17 0.1545 0.1119 38.04 42 0.1685 0.1236 36.36 18 0.1817 0.1213 49.80 43 0.1594 0.1190 33.91 19 0.1566 0.1176 33.13 44 0.1861 0.1156 61.01 10 0.2098 0.1129 85.80 45 0.1749 0.1208 44.79 21 0.1671 0.1137 46.98 46 0.1592 0.1200 32.69 22 0.1705 0.1191 43.15 47 0.1659 0.1217 36.36 23 0.1565 0.1198 30.59 48 0.1973 0.1248 58.05 24 0.1877 0.1159 61.92 49 0.1868 0.1236 51.14 25 0.1853 0.1232 50.42 50 0.1735 0.1179 47.19 
Each test lasts 500 steps. The boost% column shows the improvement of the proposed versus zigzag approach.   
Fig. 11� Global coverage scores Gt of the greedy and cooperative swarm models with four drones, on the same map and same initial drone states   
460 IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
8�Acknowledgment 
This work is partially supported by the PNRM project ‘Proactive CounterUAV’ (a2018.045). 
9�References 
[1] Tripicchio, P., Satler, M., Dabisias, G., et al.: ‘Towards smart farming and sustainable agriculture with drones’. 2015 Int. Conf. on Intelligent Environments, Prague, Czech Republic, 2015, pp. 140–143 [2] Motlagh, N.H., Bagaa, M., Taleb, T.: ‘UAVbased IOT platform: a crowd surveillance use case’, IEEE Commun. Mag., 2017, 55, (2), pp. 128–134 [3] Erdelj, M., Natalizio, E., Chowdhury, K.R., et al.: ‘Help from the sky: leveraging UAVs for disaster management’, IEEE Pervasive Comput., 2017, 16, (1), pp. 24–32 [4] Silvagni, M., Tonoli, A., Zenerino, E., et al.: ‘Multipurpose UAV for search and rescue operations in mountain avalanche events’, Geomatics Natural Hazards Risk, 2017, 8, (1), pp. 18–33 [5] Cruzan, M.B., Weinstein, B.G., Grasty, M.R., et al.: ‘Small unmanned aerial vehicles (microUAVs, drones) in plant ecology’, Appl. Plant Sci., 2016, 4, (9), p. 1600041 [6] Thiels, C.A., Aho, J.M., Zietlow, S.P., et al.: ‘Use of unmanned aerial vehicles for medical product transport’, Air Med. J., 2015, 34, (2), pp. 104–108 [7] Piciarelli, C., Foresti, G.L.: ‘Drone patrolling with reinforcement learning’. Proc. of the 13th Int. Conf. on Distributed Smart Cameras, Trento, Italy, 2019 [8] Galceran, E., Carreras, M.: ‘A survey on coverage path planning for robotics’, Robot. Auton. Syst., 2013, 61, (12), pp. 1258–1276 [9] Cabreira, T.M., Brisolara, L.B., Ferreira, P.R.Jr.: ‘Survey on coverage path planning with unmanned aerial vehicles’, Drones, 2019, 3, (1), p. 4 [10] Piciarelli, C., Esterle, L., Khan, A., et al.: ‘Dynamic reconfiguration in camera networks: a short survey’, IEEE Trans. Circuits Syst. Video Technol., 2016, 26, (5), pp. 965–977 [11] Yanmaz, E., Yahyanejad, S., Rinner, B., et al.: ‘Drone networks: communications, coordination, and sensing’, Ad Hoc Netw., 2018, 68, pp. 1– 15 
[12] WischounigStrucl, D., Rinner, B.: ‘Resource aware and incremental mosaics of wide areas from smallscale UAVs’, Mach. Vis. Appl., 2015, 26, (7), pp. 885–904 [13] Bentz, W., Panagou, D.: ‘3D dynamic coverage and avoidance control in powerconstrained UAV surveillance networks’. 2017 Int. Conf. on Unmanned Aircraft Systems (ICUAS), Miami, FL, USA, 2017, pp. 1–10 [14] Di Franco, C., Buttazzo, G.: ‘Energyaware coverage path planning of UAVs’. 2015 IEEE Int. Conf. on Autonomous Robot Systems and Competitions, Vila Real, Portugal, 2015, pp. 111–117 [15] Curiac, D.I., Volosencu, C.: ‘Path planning algorithm based on Arnold Cat map for surveillance UAVs’, Def. Sci. J., 2015, 65, (6), pp. 483–488 [16] Liu, C.H., Chen, Z., Tang, J., et al.: ‘Energyefficient UAV control for effective and fair communication coverage: a deep reinforcement learning approach’, IEEE J. Sel. Areas Commun., 2018, 36, (9), pp. 2059–2070 [17] Zargar, R.R., Sohrabi, M., Afsharchi, M., et al.: ‘Decentralized area patrolling for teams of UAVs’. 2016 4th Int. Conf. on Control, Instrumentation, and Automation (ICCIA), Qazvin, Iran, 2016, pp. 475–480 [18] Luo, W., Tang, Q., Fu, C., et al.: ‘DeepSARSA based multiUAV path planning and obstacle avoidance in a dynamic environment’, in Tan, Y., Shi, Y., Tang, Q. (Eds): ‘Advances in swarm intelligence’ (Springer International Publishing, USA, 2018), pp. 102–111 [19] Koch, W., Mancuso, R., West, R., et al.: ‘Reinforcement learning for UAV attitude control’, ACM Trans. CyberPhys. Syst., 2019, 3, (2), pp. 22:1–22:21 [20] Russell, S.J., Norvig, P.: ‘Artificial intelligence: a modern approach’ (Pearson Education Limited, UK, 2016) [21] Mnih, V., Kavukcuoglu, K., Silver, D., et al.: ‘Humanlevel control through deep reinforcement learning’, Nature, 2015, 518, (7540), p. 529 [22] Mnih, V., Kavukcuoglu, K., Silver, D., et al.: ‘Playing ATARI with deep reinforcement learning’. NIPS Deep Learning Workshop, Lake Tahoe, NV, USA, 2013 [23] Van Hasselt, H., Guez, A., Silver, D.: ‘Deep reinforcement learning with double Qlearning’. Association for the Advancement of Artificial Intelligence (AAAI), Phoenix, AZ, USA, 2016, vol. 2 [24] Piciarelli, C., Micheloni, C., Foresti, G.L.: ‘Automatic reconfiguration of video sensor networks for optimal 3D coverage’. Int. Conf. on Distributed Smart Cameras, Ghent, Belgium, 2011 [25] Piciarelli, C., Canazza, S., Micheloni, C., et al.: ‘A network of audio and video sensors for monitoring large environments’, in Pal, S.K., Petrosino, A., Maddalena, L. (Eds): ‘Handbook on soft computing for video surveillance’ (CRC Press, USA, 2012), pp. 287–315 
Table 4 Comparison of the average global coverage of the greedy and cooperative swarm patrolling schemes Test no. 2 drones 3 drones 4 drones 5 drones Coop Greedy Boost% Coop Greedy Boost% Coop Greedy Boost% Coop Greedy Boost% 1 0.3097 0.2050 51.06 0.4028 0.2878 39.94 0.4459 0.3505 27.22 0.5241 0.3571 46.75 2 0.3111 0.2604 19.46 0.4877 0.2655 83.70 0.4940 0.3761 31.34 0.5767 0.3591 60.56 3 0.3079 0.1901 61.95 0.4488 0.3231 38.91 0.5569 0.3857 44.40 0.5617 0.3771 48.96 4 0.2714 0.2037 33.23 0.4524 0.2668 69.59 0.5137 0.3866 32.89 0.5363 0.4203 27.58 5 0.2854 0.2159 32.18 0.4453 0.3417 30.33 0.5236 0.3581 46.22 0.5491 0.3413 60.88 6 0.3407 0.1680 102.77 0.3911 0.2126 83.94 0.5276 0.3481 51.59 0.5294 0.3944 34.22 7 0.3389 0.2374 42.76 0.3278 0.2721 20.48 0.5553 0.3975 39.69 0.5608 0.3789 48.02 8 0.2268 0.1595 42.16 0.4549 0.3640 24.97 0.4761 0.2807 69.63 0.5823 0.4830 20.56 9 0.3039 0.2301 32.06 0.4484 0.3238 38.45 0.5392 0.3862 39.62 0.6006 0.3642 64.89 10 0.3200 0.1828 75.07 0.4072 0.2503 62.69 0.4843 0.3707 30.67 0.5946 0.3728 59.49 11 0.2859 0.1879 52.15 0.3909 0.2389 63.65 0.5410 0.3705 46.01 0.5232 0.3900 34.16 12 0.2210 0.1962 12.66 0.4443 0.3374 31.67 0.5035 0.3415 47.46 0.5566 0.3287 69.36 13 0.3431 0.2040 68.13 0.4229 0.3160 33.82 0.5300 0.3463 53.02 0.5823 0.4426 31.56 14 0.3128 0.2626 19.11 0.4398 0.3021 45.57 0.4804 0.3563 34.82 0.6036 0.4150 45.46 15 0.3099 0.2562 20.94 0.4323 0.2670 61.92 0.5219 0.3475 50.18 0.5862 0.4667 25.59 16 0.3353 0.2006 67.19 0.4381 0.2733 60.27 0.5184 0.3411 51.98 0.5707 0.3717 53.55 17 0.2623 0.1571 66.93 0.3967 0.2701 46.85 0.4918 0.3797 29.51 0.5870 0.4846 21.13 18 0.3047 0.1575 93.44 0.4562 0.3159 44.41 0.5076 0.2988 69.88 0.5806 0.4320 34.42 19 0.2933 0.2193 33.74 0.4581 0.3183 43.91 0.5227 0.3910 33.70 0.5618 0.3552 58.17 20 0.3011 0.1917 57.10 0.4704 0.2786 68.81 0.5088 0.3836 32.62 0.5473 0.3095 76.84 Avg. 0.2992 0.2043 49.20 0.4308 0.2912 49.69 0.5121 0.35982 43.12 0.5657 0.3922 46.10 
Tests have been made with a swarm of 2, 3, 4 and 5 drones. Each test lasts 500 steps. The boost% column show the improvement of the cooperative versus greedy approach.   
IET Comput. Vis., 2020, Vol. 14 Iss. 7, pp. 452-461 © The Institution of Engineering and Technology 2020 461 
 17519640, 2020, 7, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ietcvi.2019.0963 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
",Drone swarm patrolling with uneven coverage.pdf,1
"AlphaPilot: autonomous drone racing 
Philipp Foehn1 · Dario Brescianini1 · Elia Kaufmann1 · Titus Cieslewski1 · Mathias Gehrig1 · Manasi Muglikar1 · Davide Scaramuzza1 
Received: 8 February 2021 / Accepted: 19 July 2021 / Published online: 19 October 2021 © The Author(s) 2021 
Abstract This paper presents a novel system for autonomous, visionbased drone racing combining learned data abstraction, nonlinear ﬁltering, and timeoptimal trajectory planning. The system has successfully been deployed at the ﬁrst autonomous drone racing world championship: the 2019 AlphaPilot Challenge. Contrary to traditional drone racing systems, which only detect the next gate, our approach makes use of any visible gate and takes advantage of multiple, simultaneous gate detections to compensate for drift in the state estimate and build a global map of the gates. The global map and driftcompensated state estimate allow the drone to navigate through the race course even when the gates are not immediately visible and further enable to plan a near timeoptimal path through the race course in real time based on approximate drone dynamics. The proposed system has been demonstrated to successfully guide the drone through tight race courses reaching speeds up to 8 m/s and ranked second at the 2019 AlphaPilot Challenge. 
Keyword Drone racing . Agile ﬂight . Aerial vehicles 
1 Introduction 
1.1 Motivation 
Autonomous drones have seen a massive gain in robustness in recent years and perform an increasingly large set of tasks across various commercial industries; however, they are still far from fully exploiting their physical capabilities. Indeed, most autonomous drones only ﬂy at low speeds near hover conditions in order to be able to robustly sense their environment and to have sufﬁcient time to avoid obstacles. Faster and more agile ﬂight could not only increase the ﬂight range of autonomous drones, but also improve their ability to avoid 
Video of our approach:https://youtu.be/DGjwm5PZQT8 
Talk at RSS 2020:https://youtu.be/k6vGEj1ZZWc 
RSS paper:http://rpg.iﬁ.uzh.ch/docs/RSS20_Foehn.pdf 
Philipp Foehn, Dario Brescianini, and Elia Kaufmann have contributed equally. 
All authors are with the Robotics and Perception Group. 
B Philipp Foehn foehn@iﬁ.uzh.ch 
1 Dep. of Informatics, Dep. of Neuroinformatics, University of Zurich and ETH, Zurich, Switzerland 
fast dynamic obstacles and enhance their maneuverability in conﬁned spaces. Human pilots have shown that drones are capable of ﬂying through complex environments, such as race courses, at breathtaking speeds. However, autonomous drones are still far from human performance in terms of speed, versatility, and robustness, so that a lot of research and innovation is needed in order to ﬁll this gap. In order to push the capabilities and performance of autonomous drones, in 2019, Lockheed Martin and the Drone Racing League have launched the AlphaPilot Challenge1,2, anopeninnovationchallengewithagrandprizeof$1million. The goal of the challenge is to develop a fully autonomous drone that navigates through a race course using machine vision, and which could one day beat the best human pilot. While other autonomous drone races Moon et al. (2017, 2019) focus on complex navigation, the AlphaPilot Challenge pushes the limits in terms of speed and course size to advance the state of the art and enter the domain of human performance. Due to the high speeds at which drones must ﬂy in order to beat the best human pilots, the challenging visual environments (e.g., low light, motion blur), and the limited computational power of drones, autonomous drone racing 
1 https://thedroneracingleague.com/airr/ 
2 https://www.nytimes.com/2019/03/26/technology/alphapilotaidroneracing.html 
123 
Fig. 1 Our AlphaPilot drone waiting on the start podium to autonomously race through the gates ahead 
raises fundamental challenges in realtime state estimation, perception, planning, and control. 
1.2 Related work 
Autonomous navigation in indoor or GPSdenied environments typically relies on simultaneous localization and mapping (SLAM), often in the form of visualinertial odometry (VIO) Cadena et al. (2016). There exists a variety of VIO algorithms, e.g., Mourikis and Roumeliotis (2007); Bloesch et al. (2015); Qin et al. (2018); Forster et al. (2017a), that are based on feature detection and tracking that achieve very goodresultsingeneralnavigationtasksDelmericoandScaramuzza (2018). However, the performance of these algorithms signiﬁcantly degrades during agile and highspeed ﬂight as encountered in drone racing. The drone’s high translational androtationalvelocitiescauselargeopticﬂow,makingrobust feature detection and tracking over sequential images dif- ﬁcult and thus causing substantial drift in the VIO state estimate Delmerico et al. (2019). To overcome this difﬁculty, several approaches exploiting the structure of drone racing with gates as landmarks have been developed, e.g., Li et al. (2019); Jung et al. (2018); Kaufmann et al. (2018), where the drone locates itself relative to gates. In Li et al. (2019), a handcrafted process is used to extract gate information from images that is then fused with attitude estimates from an inertial measurement unit (IMU) to compute an attitude reference that guides the drone towards the visible gate. While the approach is computationally very lightweight, it struggles with scenarios where multiple gates are visible and does not allow to employ more sophisticated planning and control algorithms which, e.g., plan several gates ahead. In Jung et al. (2018), a convolutional neural network (CNN) is used to retrieve a bounding box of the gate and a lineofsightbased control law aided 
by optic ﬂow is then used to steer the drone towards the detected gate. While this approach is successfully deployed on a real robotic system, the generated control commands do not account for the underactuated system dynamics of the quadrotor, constraining this method to lowspeed ﬂight. The approach presented in Kaufmann et al. (2018) also relies on relative gate data but has the advantage that it works even when no gate is visible. In particular, it uses a CNN to directly infer relative gate poses from images and fuse the results with a VIO state estimate. However, the CNN does not perform well when multiple gates are visible as it is frequently the case for drone racing. Assuming knowledge of the platform state and the environment, there exist many approaches which can reliably generate feasible trajectories with high efﬁciency. The most prominent line of work exploits the quadrotor’s underactuated nature and the resulting differentially-ﬂat output states Mellinger et al. (2012); Mueller et al. (2015), where trajectories are described as polynomials in time. Other approaches additionally incorporate obstacle avoidance Zhou et al. (2019); Gao et al. (2019) or perception constraints Falanga et al. (2018); Spasojevic et al. (2020). However, in the context of drone racing, speciﬁcally the AlphaPilot Challenge, obstacle avoidance is often not needed, but timeoptimal planning is of interest. There exists a handful of approaches for timeoptimal planning Hehn et al. (2012); Loock et al. (2013); Ryou et al. (2020); Foehn and Scaramuzza (2020). However, while Hehn et al. (2012); Loock et al. (2013) are limited to 2D scenarios and only ﬁnd trajectories between two given states, Ryou et al. (2020) requires simulation and realworld data obtained on the track, and the method of Foehn and Scaramuzza (2020) is not applicable due to computational constraints. 
1.3 Contribution 
The approach contributed herein builds upon the work of Kaufmann et al. (2018) and fuses VIO with a robust CNNbased gate corner detection using an extended Kalman ﬁlter (EKF), achieving high accuracy at little computational cost. The gate corner detections are used as static features to compensate for the VIO drift and to align the drone’s ﬂight path precisely with the gates. Contrary to all previous works Li et al. (2019); Jung et al. (2018); Kaufmann et al. (2018), which only detect the next gate, our approach makes use of any gate detection and even proﬁts from multiple simultaneous detections to compensate for VIO drift and build a global gate map. The global map allows the drone to navigate through the race course even when the gates are not immediately visible and further enables the usage of sophisticated path planning and control algorithms. In particular, a computationally efﬁcient, samplingbased path planner (see e.g., LaValle (2006), and references therein) is employed that 
123 
plans near timeoptimal paths through multiple gates ahead and is capable of adjusting the path in real time if the global map is updated. This paper extends our previous work Foehn et al. (2020) by including a more detailed elaboration on our gate corner detection in Sect. 4 with an ablation study in Sect. 8.1, further details on the fusion of VIO and gate detection in Sect. 5, and a description of the path parameterization in Sect. 6, completed by an ablation study on the planning horizon length in Sect. 8.3. 
2 AlphaPilot race format and drone 
2.1 Race format 
From more than 400 teams that participated in a series of qualiﬁcation tests including a simulated drone race Guerra et al. (2019), the top nine teams were selected to compete in the 2019 AlphaPilot Challenge. The challenge consists of three qualiﬁcation races and a ﬁnal championship race at which the six best teams from the qualiﬁcation races compete for the grand prize of $1 million. Each race is implemented as a time trial competition in which each team is given three attempts to ﬂy through a race course as fast a possible without competing drones on the course. Taking off from a start podium, the drones have to autonomously navigate through a sequence of gates with distinct appearances in the correct order and terminate at a designated ﬁnish gate. The race course layout, gate sequence, and position are provided ahead of each race up to approximately ±3 m horizontal uncertainty, enforcing teams to come up with solutions that adapt to the real gate positions. Initially, the race courses were planned to have a lap length of approximately 300 m and required the completion up to three laps. However, due to technical difﬁculties, no race required to complete multiple laps and the track length at the ﬁnal championship race was limited to about 74 m. 
2.2 Drone specifications 
All teams were provided with an identical race drone (Fig. 1) that was approximately 0.7 m in diameter, weighed 3.4kg, and had a thrusttoweight ratio of 1.4. The drone was equipped with a NVIDIA Jetson Xavier embedded computer for interfacing all sensors and actuators and handling all computation for autonomous navigation onboard. The sensor suite included two ± 30◦ forwardfacing stereo camera pairs (Fig. 2), an IMU, and a downwardfacing laser rangeﬁnder (LRF). All sensor data were globally time stamped by software upon reception at the onboard computer. Detailed speciﬁcations of the available sensors are given in Table 1. The drone was equipped with a ﬂight controller that con- 
eI x eI y 
eI z 
eB x 
eB y 
eB z 
eC x 
eC y eC z 
Fig.2 Illustrationoftheracedronewithitsbody-ﬁxedcoordinateframe B in blue and a camera coordinate frame C in red 
trolled the total thrust f along the drone’s zaxis (see Fig. 2) and the angular velocity, ω = � ωx, ωy, ωz � , in the body-ﬁxed coordinate frame B. 
2.3 Drone model 
Bold lower case and upper case letters will be used to denote vectors and matrices, respectively. The subscripts in 
I pC B = I pB − I pC are used to express a vector from point C to point B expressed in frame I. Without loss of generality, I is used to represent the origin of frame I, and B represents the origin of coordinate frame B. For the sake of readability, the leading subscript may be omitted if the frame in which the vector is expressed is clear from context. The drone is modelled as a rigid body of mass m with rotor drag proportional to its velocity acting on it Kai et al. (2017). The translational degreesoffreedom are described by the position of its center of mass, pB = � pB,x, pB,y, pB,z � , with respect to an inertial frame I as illustrated in Fig. 2. The rotational degreesoffreedom are parametrized using a unit quaternion, qIB, where RIB = R(qIB) denotes the rotation matrix mapping a vector from the body-ﬁxed coordinate frame B to the inertial frame I Shuster (1993). A unit quaternion, q, consists of a scalar qw and a vector ˜q = � qx, qy, qz � 
and is deﬁned as q = (qw, ˜q) Shuster (1993). The drone’s equations of motion are 
m ¨pB = RIB f eB z − RIB DR⊺ IBvB − mg, (1) 
˙qIB = 1 
2 
�0 ω 
� ⊗ qIB, (2) 
where f and ω are the force and bodyrate inputs, eB z (0, 0, 1) is the drone’s zaxis expressed in its body-ﬁxed= frame B, D = diag(dx, dy, 0) is a constant diagonal matrix containing the rotor drag coefﬁcients, vB = ˙pB denotes the drone’s velocity, g is gravity and ⊗ denotes the quaternion multiplication operator Shuster (1993). The drag coefﬁcients 
123 
Table 1 Sensor speciﬁcations 
Sensor Model Rate Details 
Cam Leopard imaging IMX 264 60Hz Global shutter, color resolution: 1200 × 720 
IMU Bosch BMI088 430Hz Range: ±24g, ±34.5 rad/s resolution: 7e-4g, 1e-3 rad/s 
LRF Garmin LIDARLite v3 120Hz Range: 1–40 m resolution: 0.01 m 
Sensor Interface Perception State Estimation Planning & Control Drone Interface 
IMU 
Laser Rangeﬁnder 
Camera 4 
Camera 3 
Camera 2 
Camera 1 Gate Detection 
Visual Inertial Odometry 
EKF State Estimation 
State Prediction 
Position Control 
Attitude Control 
Angular Velocity 
Path Planning 
Total Thrust 
gate map 
trajectory 
attitude 
vehicle state 
Fig. 3 Overview of the system architecture and its main components. All components within a dotted area run in a single thread 
were identiﬁed experimentally to be dx = 0.5 kg/s and dy = 0.25 kg/s. 
3 System overview 
The system is composed of ﬁve functional groups: Sensor interface, perception, state estimation, planning and control, and drone interface (see Fig. 3). In the following, a brief introduction to the functionality of our proposed perception, state estimation, and planning and control system is given. 
3.1 Perception 
Of the two stereo camera pairs available on the drone, only the two central forwardfacing cameras are used for gate detection (see Sect. 4) and, in combination with IMU measurements, to run VIO. The advantage is that the amount of image data to be processed is reduced while maintaining a very large ﬁeld of view. Due to its robustness, multicamera capability and computational efﬁciency, ROVIO Bloesch et al. (2015) has been chosen as VIO pipeline. At low speeds, ROVIO is able to provide an accurate estimate of the quadrotor vehicle’s pose and velocity relative to its starting position, however, at larger speeds the state estimate suffers from drift. 
3.2 State estimation 
In order to compensate for a drifting VIO estimate, the output of the gate detection and VIO are fused together with the measurements from the downwardfacing laser rangeﬁnder (LRF)usinganEKF(seeSect.5).TheEKFestimatesaglobal map of the gates and, since the gates are stationary, uses the 
gate detections to align the VIO estimate with the global gate map, i.e., compensates for the VIO drift. Computing the state estimate, in particular interfacing the cameras and running VIO, introduces latency in the order of 130 ms to the system. In order to be able to achieve a high bandwidth of the control system despite large latencies, the vehicle’s state estimate is predicted forward to the vehicle’s current time using the IMU measurements. 
3.3 Planning and control 
The global gate map and the latencycompensated state estimate of the vehicle are used to plan a near timeoptimal path through the next N gates starting from the vehicle’s current state (see Sect. 6). The path is replanned every time (i) the vehicle passes through a gate, (ii) the estimate of the gate map or (iii) the VIO drift are updated signiﬁcantly, i.e., large changes in the gate positions or VIO drift. The path is tracked using a cascaded control scheme (see Sect. 7) with an outer proportionalderivative (PD) position control loop and an inner proportional (P) attitude control loop. Finally, the outputs of the control loops, i.e., a total thrust and angular velocity command, are sent to the drone. 
3.4 Software architecture 
The NVIDIA Jetson Xavier provides eight CPU cores, however, four cores are used to run the sensor and drone interface. The other four cores are used to run the gate detection, VIO, EKF state estimation, and planning and control, each in a separate thread on a separate core. All threads are implemented asynchronously to run at their own speed, i.e., whenever new data is available, in order to maximize data throughput and to 
123 
reduce processing latency. The gate detection thread is able to process all camera images in real time at 60Hz, whereas the VIO thread only achieves approximately 35Hz. In order to deal with the asynchronous nature of the gate detection and VIO thread and their output, all data is globally time stamped and integrated in the EKF accordingly. The EKF thread runs every time a new gate or LRF measurement is available. The planning and control thread runs at a ﬁxed rate of 50Hz. To achieve this, the planning and control thread includes the state prediction which compensates for latencies introduced by the VIO. 
4 Gate detection 
To correct for drift accumulated by the VIO pipeline, the gates are used as distinct landmarks for relative localization. In contrast to previous CNNbased approaches to gate detection, we do not infer the relative pose to a gate directly, but instead segment the four corners of the observed gate in the input image. These corner segmentations represent the likelihood of a speciﬁc gate corner to be present at a speciﬁc pixel coordinate. To represent a value proportional to the likelihood, the maps are trained on Gaussians of the corner projections. This allows the detection of an arbitrary amount of gates, and allows for a more principled inclusion of gate measurements in the EKF through the use of reprojection error. Speciﬁcally, it exhibits more predictable behavior for partial gate observations and overlapping gates, and allows to suppress the impact of Gaussian noise by having multiple measurements relating to the same quantity. Since the exact shape of the gates is known, detecting a set of characteristic points per gate allows to constrain the relative pose. For the quadratic gates of the AlphaPilot Challenge, these characteristic points are chosen to be the inner corner of the gate border (see Fig. 4, 4th column). However, just detecting the four corners of all gates is not enough. If just four corners of several gates are extracted, the association of corners to gates is undeﬁned (see Fig. 4, 3rd row, 2nd column). To solve this problem, we additionally train our network to extract socalled Part Afﬁnity Fields (PAFs), as proposed by Cao et al. (2017). These are vector ﬁelds, which, in our case, are deﬁned along the edges of the gates, and point from one corner to the next corner of the same gate, see column three in Fig. 4. The entire gate detection pipeline consists of two stages: (1) predicting corner maps and PAFs by the neural network, (2) extracting single edge candidates from the network prediction and assembling them to gates. In the following, both stages are explained in detail. 
4.1 Stage 1: predicting corner maps and part affinity fields 
In the ﬁrst detection stage, each input image, Iw×h×3, is mapped by a neural network into a set of NC = 4 corner maps, Cw×h×NC , and NE = 4 PAFs, Ew×h×(NE·2). Predicted corner maps as well as PAFs are illustrated in Fig. 4, 2nd and 3rd column. The network is trained in a supervised manner by minimizing the MeanSquaredError loss between the network prediction and the groundtruth maps. In the following, groundtruth maps for both map types are explained in detail. 
4.1.1 Corner maps 
For each corner class, j ∈ C j, C j := {T L,T R ,BL ,BR }, a groundtruth corner map, C∗ j(s), is represented by a singlechannel map of the same size as the input image and indicates the existence of a corner of class j at pixel location s in the image. The value at location s ∈ I in C∗ j is deﬁned by a Gaussian as 
C∗ j(s) = exp 
� 
− ∥s − s∗ j∥2 2 
σ 2 
� 
, (3) 
where s∗ j denotes the ground truth image position of the nearestcornerwithclass j.Thechoiceoftheparameterσ controls the width of the Gaussian. We use σ = 7 pixel in our implementation. Gaussians are used to account for small errors in the groundtruth corner positions that are provided by hand. Groundtruth corner maps are generated for each individual gate in the image separately and then aggregated. Aggregation is performed by taking the pixelwise maximum of the individual corner maps, as this preserves the distinction between close corners. 
4.1.2 Part afﬁnity ﬁelds 
We deﬁne a PAF for each of the four possible classes of edges, deﬁned by its two connecting corners as (k,l) ∈ EK L := {(T L,T R ), (T R,BR ), (BR,BL ), (BL,T L )}. For each edge class, (k,l), the groundtruth PAF, E∗ (k,l)(s), is represented by a twochannel map of the same size as the input image and points from corner k to corner l of the same gate, provided that the given image point s lies within distance d of such an edge. We use d = 10 pixel in our implementation. Let G∗ be the set of gates g and S(k,l),g be the set of image points that are within distance d of the line connecting the corner points s∗ k and s∗ l belonging to gate g. Furthermore, let vk,l,g be the unit vector pointing from s∗ k to s∗ l of the same 
123 
Fig. 4 The gate detection module returns sets of corner points for each gate in the input image (fourth column) using a twostage process. In the ﬁrst stage, a neural network transforms an input image, Iw×h×3 (ﬁrst column), into a set of conﬁdence maps for corners, Cw×h×4 (second column), and Part Afﬁnity Fields (PAFs) Cao et al. (2017), Ew×h×(4·2) (third column). In the second stage, the PAFs are used to associate sets of corner points that belong to the same gate. For visualization, both 
corner maps, C (second column), and PAFs, E (third column), are displayed in a single image each. While color encodes the corner class for C, it encodes the direction of the 2D vector ﬁelds for E. The yellow lines in the bottom of the second column show the six edge candidates of the edge class (T L, T R) (the T L corner of the middle gate is below the detection threshold), see Sect. 4.2. Best viewed in color (Color ﬁgure online) 
gate. Then, the part afﬁnity ﬁeld, E∗ (k,l)(s), is deﬁned as: 
E∗ (k,l)(s) = 
� vk,l,g ifs ∈ S(k,l),g, g ∈ G∗ 
0 otherwise. (4) 
As in the case of corner maps, PAFs are generated for each individual gate in the image separately and then aggregated. In case a point s lies in S(k,l),g of several gates, the vk,l,g of all corresponding gates are averaged. 
4.2 Stage 2: corner association 
At test time, discrete corner candidates, s j, for each corner class, j ∈ C j, are extracted from the predicted corner map using nonmaximum suppression and thresholding. For each corner class, there might be several corner candidates, due to multiple gates in the image or false positives. These corner candidates allow the formation of an exhaustive set of edge candidates, {(sk, sl)}, see the yellow lines in Fig. 4. Given the corresponding PAF, E(k,l)(s), each edge candidate is assigned a score which expresses the agreement of that candidate with the PAF. This score is given by the line integral 
S((sk, sl)) = � u=1 
u=0 E(k,l)(s(u)) · sl − sk 
∥sl − sk∥du, (5) 
where s(u) lineraly interpolates between the two corner candidate locations sk and sl. In practice, S is approximated by uniformly sampling the integrand. The line integral S is used as metric to associate corner candidates to gate detections. The goal is to ﬁnd the optimal assignment for the set of all possible corner candidates to gates. As described in Cao et al. (2017), ﬁnding this optimal assignment corresponds to a Kdimensional matching problem that is known to be NPHard West (2001). Following Cao et al. (2017), the problem is simpliﬁed by decomposing the matching problem into a set of bipartite matching subproblems. Matching is therefore performed independently for each edge class. Speciﬁcally, the following optimization problem represents the bipartite matching subproblem for edge class (k,l): 
max S(k,l) = � 
k∈Dk 
� 
l∈Dl S((sk, sl)) · zkl (6) 
s.t. ∀k ∈ Dk, � 
l∈Dl zkl ≤ 1 , (7) 
∀l ∈ Dl, � 
k∈Dk zkl ≤ 1 , (8) 
where S(k,l) is the cumulative matching score and Dk, Dl denote the set of corner candidates for edge class (k,l). The variable zkl ∈ {0, 1} indicates whether two corner candidates 
123 
are connected. Equations (7) and (8) enforce that no two edges share the same corner. Above optimization problem can be solved using the Hungarian method Kuhn (1955), resulting in a set of edge candidates for each edge class (k,l). With the bipartite matching problems being solved for all edge classes, the pairwise associations can be extended to sets of associated edges for each gate. 
4.3 Training data 
The neural network is trained in a supervised fashion using a dataset recorded in the real world. Training data is generated by recording video sequences of gates in 5 different environments. Each frame is annotated with the corners of all gates visible in the image using the open source image annotation software labelme3, which is extended with KLTTracking for semiautomatic labelling. The resulting dataset used for training consists of 28k images and is split into 24k samples for training and 4k samples for validation. At training time, the data is augmented using random rotations of up to 30◦ and random changes in brightness, hue, contrast and saturation. 
4.4 Network architecture and deployment 
The network architecture is designed to optimally tradeoff between computation time and accuracy. By conducting a neural network architecture search, the best performing architecture for the task is identiﬁed. The architecture search is limited to variants of UNet Ronneberger et al. (2015) due to its ability to perform segmentation tasks efﬁciently with a very limited amount of labeled training data. The best performing architecture is identiﬁed as a 5-level UNet with [12, 18, 24, 32, 32] convolutional ﬁlters of size [3, 3, 3, 5, 7] per level and a ﬁnal additional layer operating on the output of the UNet containing 12 ﬁlters. At each layer, the input feature map is zeropadded to preserve a constant height and width throughout the network. As activation function, LeakyReLU with α = 0.01 is used. For deployment on the Jetson Xavier, the network is ported to TensorRT 5.0.2.6. To optimize memory footprint and inference time, inference is performed in halfprecision mode (FP16) and batches of two images of size 592 × 352 are fed to the network. 
5 State estimation 
The nonlinear measurement models of the VIO, gate detection, and laser rangeﬁnder are fused using an EKF Kalman (1960). In order to obtain the best possible pose accuracy relative to the gates, the EKF estimates the translational and 
3 https://github.com/wkentaro/labelme 
rotational misalignment of the VIO origin frame, V, with respect to the inertial frame, I, represented by pV and qIV, jointly with the gate positions, pGi , and gate heading, ϕIGi . It can thus correct for an imprecise initial position estimate, VIO drift, and uncertainty in gate positions. The EKF’s state space at time tk is xk = x(tk) with covariance Pk described by 
xk = � pV, qIV, pG0, ϕIG0, . . . , pGN−1, ϕIGN−1 � . (9) 
The drone’s corrected pose, � pB, qIB � , can then be computed from the VIO estimate, � pVB, qVB � , by transforming it from frame V into the frame I using � pV , qIV � as 
pB = pV + RIV · pVB, qIB = qIV · qVB. (10) 
All estimated parameters are expected to be timeinvariant but subject to noise and drift. This is modelled by a Gaussian random walk, simplifying the EKF process update to: 
xk+1 = xk, Pk+1 = Pk + �tk Q, (11) 
where Q is the random walk process noise. For each measurement zk with noise R the predicted a priori estimate, x− k , is corrected with measurement function, h(x− k ), and Kalman gain, K k, resulting in the a posteriori estimate, x+ k , as 
K k = P− k H⊺ k � Hk P− k H⊺ k + R �−1 , 
x+ k = x− k + K k � zk − h(x− k ) � , 
P+ k = (I − K k Hk) P− k , (12) 
with h(x− k ), the measurement function with Jacobian Hk. However, the ﬁlter state includes a rotation quaternion 
constrained to unit norm, ∥qIV∥ != 1. This is effectively an overparameterization in the ﬁlter state space and can lead to poor linearization as well as underestimation of the covariance. To apply the EKFs linear update step on the overparameterized quaternion, it is lifted to its tangent space description, similar to Forster et al. (2017b). The quaternion qIV is composed of a reference quaternion, qIVref, which is adjusted after each update step, and an error quaternion, qVrefV, of which only its vector part, ˜qVrefV, is in the EKF’s state space. Therefore we get 
qIV = qIVref · qVrefV qVrefV = 
�� 
1 − ˜q⊺ VrefV · ˜qVrefV ˜qVrefV 
� 
(13) 
123 
from which we can derive the Jacobian of any measurement function, h(x), with respect to qIV by the chain rule as 
∂ 
∂ ˜qVrefV h(x) = ∂ 
∂qIV h(x) · ∂qIV 
∂ ˜qVrefV (14) 
= ∂ 
∂ ˜qIV h(x) · [qIVref]× 
⎡ 
⎣ 
−˜q⊺ VrefV � 
1−˜q⊺ VrefV·˜qVrefV I3×3 
⎤ 
⎦ 
(15) 
wherewearriveat (15)byusing(13) in (14) anduse[qIVref]× to represent the matrix resulting from a lefthandside multiplication with qIVref. 
5.1 Measurement modalities 
All measurements up to the camera frame time tk are passed to the EKF together with the VIO estimate, pVB,k and qVB,k, with respect to the VIO frame V. Note thate the VIO estimate is assumed to be a constant parameter, not a ﬁlter state, which vastly simpliﬁes derivations ad computation, leading to an efﬁcient yet robust ﬁlter. 
5.1.1 Gate measurements 
Gate measurements consist of the image pixel coordinates, sCoi j , of a speciﬁc gate corner. These corners are denoted with top left and right, and bottom left and right, as in j ∈ C j, C j := {T L,T R ,BL ,BR } and the gates are enumerated by i ∈ [0, N − 1]. All gates are of equal width, w, and height, h, so that the corner positions in the gate frame, Gi, can be written as pGiCoi j = 1 
2 (0, ±w, ±h). The measurement equation can be written as the pinhole camera projection Szeliski (2010) of the gate corner into the camera frame. A pinhole camera maps the gate corner point, pCoi j , expressed in the camera frame, C, into pixel coordinates as 
hGate(x) = sCoi j = 1 
[ pCoi j ]z 
� fx 0 cx 0 fy cy 
� pCoi j , (16) 
where [·]z indicates the scalar zcomponent of a vector, fx and fy are the camera’s focal lengths and � cx, cy � is the camera’s optical center. The gate corner point, pCoi j , is given by 
pCoi j =R⊺ IC � pGi + RIGi pGiCo j − pC � , (17) 
with pC and RIC being the transformation between the inertial frame I and camera frame C, 
pC = pV + RIV � pVB + RVB pBC � , (18) 
RIC =RIV RVB RBC, (19) 
where pBC and RBC describe a constant transformation between the drone’s body frame B and camera frame C (see Fig. 2). The Jacobian with respect to the EKF’s state space is derived using the chain rule, 
∂ ∂x hGate(x) = ∂hGate(x) 
∂ pCoi j (x) · ∂ pCoi j (x) 
∂x , (20) 
where the ﬁrst term representing the derivative of the projection, and the second term represents the derivative with respect to the state space including gate position and orientation, and the frame alignment, which can be further decomposed using (14). 
5.1.2 Gate correspondences 
The gate detection (see Fig. 4) provides sets of m measurements, Sˆi = {sCoˆi j0, . . . , sCoˆi jm−1}, corresponding to the 
unknown gate ˆi at known corners j ∈ C j. To identify the correspondences between a detection set Sˆi and the gate Gi in our map, we use the square sum of reprojection error. For this, we ﬁrst compute the reprojection of all gate corners, sCoi j , according to (16) and then compute the square error sum between the measurement set, Sˆi, and the candidates, sCoi j . Finally, the correspondence is established to the gate Gi which minimizes the square error sum, as in 
argmin i∈[0,N−1] 
� 
sCoˆi j ∈Sˆi 
(sCoˆi j − sCoi j )⊺(sCoˆi j − sCoi j ) (21) 
5.1.3 Laser rangeﬁnder measurement 
The drone’s laser rangeﬁnder measures the distance along the drones negative zaxis to the ground, which is assumed to be ﬂat and at a height of 0 m. The measurement equation can be described by 
hLRF(x) = [ pB]z 
[RIBeBz ]z = [ pV + RIV pV B]z 
[RIV RVBeBz ]z . (22) 
The Jacobian with respect to the state space is again derived by ∂hLRF 
∂ pV and ∂hLRF 
∂qIV and further simpliﬁed using (14). 
6 Path planning 
For the purpose of path planning, the drone is assumed to be a point mass with bounded accelerations as inputs. This simpli- ﬁcation allows for the computation of timeoptimal motion primitivesinclosedformandenablestheplanningofapproximate timeoptimal paths through the race course in real time. 
123 
Even though the dynamics of the quadrotor vehicle’s acceleration cannot be neglected in practice, it is assumed that this simpliﬁcation still captures the most relevant dynamics for path planning and that the resulting paths approximate the true timeoptimal paths well. In order to facilitate the tracking of the approximate timeoptimal path, polynomials of order four are ﬁtted to the path which yield smoother position, velocity and acceleration commands, and can therefore be better tracked by the drone. In the following, timeoptimal motion primitives based on the simpliﬁed dynamics are ﬁrst introduced and then a path planning strategy based on these motion primitives is presented.Finally,amethodtoparameterizethetimeoptimal path is introduced. 
6.1 Timeoptimal motion primitive 
The minimum times, T ∗ x , T ∗ y and T ∗ z , required for the vehicle to ﬂy from an initial state, consisting of position and velocity, to a ﬁnal state while satisfying the simpliﬁed dynamics ¨pB(t) = u(t) with the input acceleration u(t) being constrained to u ≤ u(t) ≤ u are computed for each axis individually. Without loss of generality, only the xaxis is considered in the following. Using Pontryagin’s maximum principle Bertsekas (1995), it can be shown that the optimal control input is bangbang in acceleration, i.e., has the form 
u∗ x(t) = 
� ux, 0 ≤ t ≤ t∗, 
ux, t∗ < t ≤ T ∗ x , (23) 
or vice versa with the control input ﬁrst being ux followed by ux. In order to control the maximum velocity of the vehicle, e.g., to constrain the solutions to ranges where the simpliﬁed dynamics approximate the true dynamics well or to limit the motion blur of the camera images, a velocity constraint of the form vB ≤ vB(t) ≤ vB can be added, in which case the optimal control input has bangsingularbang solution Maurer (1977) 
u∗ x(t) = 
⎧ ⎪⎨ 
⎪⎩ 
ux, 0 ≤ t ≤ t∗ 1 , 
0, t∗ 1 < t ≤ t∗ 2 , 
ux, t∗ 2 < t ≤ T ∗ x , 
(24) 
or vice versa. It is straightforward to verify that there exist closedform solutions for the minimum time, T ∗ x , as well as the switching times, t∗, in both cases (23) or (24). Once the minimum time along each axis is computed, the maximum minimum time, T ∗ = max(T ∗ x , T ∗ y , T ∗ z ), is computed and motion primitives of the same form as in (23) or (24) are computed among the two faster axes but with the ﬁnal time constrained to T ∗ such that trajectories along each axis end at the same time. In order for such a motion 
vB,x vB,y vB,z 
pB,x pB,y pB,z 
Time [s] 
Velocity [m/s] Position [m] 
0 0.5 1.0 1.5 1.96 
-10 -5 50 10 
-10 -5 50 10 
Fig. 5 Example timeoptimal motion primitive starting from rest at the origin to a random ﬁnal position with nonzero ﬁnal velocity. The velocities are constrained to ±7.5 m/s and the inputs to ±12 m/s2. The dotted lines denote the peraxis timeoptimal maneuvers 
primitive to exist, a new parameter α ∈ [0, 1] is introduced that scales the acceleration bounds, i.e., the applied control inputs are scaled to αux and αux, respectively. Fig. 5 depicts the position and velocity of an example timeoptimal motion primitive. 
6.2 Samplingbased receding horizon path planning 
The objective of the path planner is to ﬁnd the timeoptimal path from the drone’s current state to the ﬁnal gate, passing through all the gates in the correct order. Since the previously introduced motion primitive allows the generation of timeoptimal motions between any initial and any ﬁnal state, the timeoptimal path can be planned by concatenating a timeoptimal motion primitive starting from the drone’s current (simpliﬁed) state to the ﬁrst gate with timeoptimal motion primitives that connect the gates in the correct order until the ﬁnal gate. This reduces the path planning problem to ﬁnding the drone’s optimal state at each gate such that the total time is minimized. To ﬁnd the optimal path, a samplingbased strategy is employed where states at each gate are randomly sampled and the total time is evaluated subsequently. In particular, the position of each sampled state at a speciﬁc gate is ﬁxed to the center of the gate and the velocity is sampled uniformly at random such the velocity lies within the constraints of the motion primitives and the angle between the velocity and the gate normal does not exceed a maximum angle, ϕmax It is trivial to show that as the number of sampled states approaches inﬁnity, the computed path converges to the timeoptimal path. In order to solve the problem efﬁciently, the path planning problem is interpreted as a shortest path problem. At each gate, M different velocities are sampled and the arc length from each sampled state at the previous gate is set to be equal to the duration, T ∗, of the timeoptimal motion primitive that guides the drone from one state to the other. Due to the exis- 
123 
tence of a closedform expression for the minimum time, T ∗, setting up and solving the shortest path problem can be done very efﬁciently using, e.g., Dijkstra’s algorithm Bertsekas (1995) resulting in the optimal path p∗(t). In order to further reduce the computational cost, the path is planned in a receding horizon fashion, i.e., the path is only planned through the next N gates. 
6.3 Path parameterization 
Due to the simpliﬁcations of the dynamics that were made when computing the motion primitives, the resulting path is infeasible with respect to the quadrotor dynamics (1) and (2) and thus is impossible to be tracked accurately by the drone. To simplify the tracking of the timeoptimal path, the path is approximated by fourth order polynomials in time. In particular, the path is divided into multiple segments of equal arc length. Let t ∈ [tk, tk+1) be the time interval of the kth segment. In order to ﬁt the polynomials, ¯pk(t), to the kth segment of the timeoptimal path, we require that the initial and ﬁnal position and velocity are equal to those of the timeoptimal path, i.e., 
¯pk(tk) = p∗(tk), ¯pk(tk+1) = p∗(tk+1), (25) ˙¯pk(tk) = ˙p∗(tk), ˙¯pk(tk+1) = ˙p∗(tk+1), (26) 
and that the positions at t = (tk+1 − tk) /2 coincide as well: 
¯pk 
�tk+1 + tk 
2 
� = p∗ �tk+1 + tk 
2 
� . (27) 
The polynomial parameterization ¯pk(t) of the kth segment is then given by 
¯pk(t) = a4,ks4 + a3,ks3 + a2,ks2 + a1,ks + a0,k, (28) 
with s = t − tk being the relative time since the start of kth segment. The velocity and acceleration required for the drone to track this polynomial path can be computed by taking the derivatives of (28), yielding 
˙¯pk(t) = 4a4,ks3 + 3a3,ks2 + 2a2,ks + a1,k, (29) ¨¯pk(t) = 12a4,ks2 + 6a3,ks + 2a2,k. (30) 
7 Control 
This section presents a control strategy to track the near timeoptimal path from Sect. 6. The control strategy is based on a cascaded control scheme with an outer position control loop and an inner attitude control loop, where the position controlloopisdesignedundertheassumptionthattheattitude 
control loop can track setpoint changes perfectly, i.e., without any dynamics or delay. 
7.1 Position control 
The position control loop along the inertial zaxis is designed such that it responds to position errors 
pBerr,z = pBref,z − pB,z 
in the fashion of a secondorder system with time constant τpos,z and damping ratio ζpos,z, 
¨pB,z = 1 
τ 2pos,z pBerr,z + 2ζpos,z 
τpos,z ˙pBerr,z + ¨pBref,z. (31) 
Similarly, two control loops along the inertial xand yaxis are shaped to make the horizontal position errors behave like secondorder systems with time constants τpos,xy and damping ratio ζpos,xy. Inserting (31) into the translational dynamics (1), the total thrust, f , is computed to be 
f = [m � ¨pBref + g � + RIB DR⊺ IBvB]z 
[RIBeBz ]z . (32) 
7.2 Attitude control 
The required acceleration from the position controller determines the orientation of the drone’s zaxis and is used, in combination with a reference yaw angle, ϕref, to compute the drone’s reference attitude. The reference yaw angle is chosen such that the drone’s xaxis points towards the reference position 5 m ahead of the current position, i.e., that the drone looks in the direction it ﬂies. A nonlinear attitude controller similar to Brescianini and D’Andrea (2018) is applied that prioritizes the alignment of the drone’s zaxis, which is crucial for its translational dynamics, over the correction of the yaw orientation: 
ω = 2 sgn(qw) � 
q2w + q2z T−1 att 
⎡ 
⎣ qwqx − qyqz qwqy + qxqz qz 
⎤ 
⎦ , (33) 
where qw, qx, qy and qz are the components of the attitude error, q−1 IB ⊗ qIBref, and where Tatt is a diagonal matrix containing the peraxis ﬁrstorder system time constants for small attitude errors. 
8 Results 
The proposed system was used to race in the 2019 AlphaPilot championship race. The course at the championship race 
123 
pB pVB Final Gate 
Gate 1 
Start 
Final Gate Gate 4 
Gate 3 
Gate 2 
Gate 1 Start 
Final Gate Gate 4 
Gate 3 
Gate 2 
Gate 1 Start 
px[m] 
py[m] 
px[m] 
py[m] 
px[m] 
py[m] 
0 4 8 12 -10 0 10 20 30 -10 0 10 20 30 
0 
4 
8 
12 
-30 
-20 
-10 
0 
-30 
-20 
-10 
0 
8 m/s 
6 m/s 
4 m/s 
2 m/s 
0 m/s 
Fig. 6 Top view of the planned (left) and executed (center) path at the championship race, and an executed multilap path at a testing facility (right). Left: Fastest planned path in color, suboptimal sampled paths in gray. Center: VIO trajectory as pVB and corrected estimate as pB (Color ﬁgure online) 
consisted of ﬁve gates and had a total length of 74 m. A top view of the race course as well as the results of the path planning and the fastest actual ﬂight are depicted in Fig. 6 (left and center). With the motion primitive’s maximum velocity set to 8 m/s, the drone successfully completed the race course in a total time of 11.36 s, with only two other teams also completing the full race course. The drone ﬂew at an average velocity of 6.5 m/s and reached the peak velocity of 8 m/s multiple times. Note that due to missing ground truth, Fig. 6 only shows the estimated and corrected drone position. Thesystemwasfurtherevaluatedatatestingfacilitywhere there was sufﬁcient space for the drone to ﬂy multiple laps (see Fig. 6, right), albeit the course consisted of only two gates. The drone was commanded to pass 4 times through gate 1 before ﬁnishing in the ﬁnal gate. Although the gates were not visible to the drone for most of the time, the drone successfully managed to ﬂy multiple laps. Thanks to the global gate map and the VIO state estimate, the system was able to plan and execute paths to gates that are not directly visible. By repeatedly seeing either one of the two gates, the drone was able to compensate for the drift of the VIO state estimate, allowing the drone to pass the gates every time exactly through their center. Note that although seeing gate 1 in Fig. 6 (right) at least once was important in order to update the position of the gate in the global map, the VIO drift was also estimated by seeing the ﬁnal gate. The results of the system’s main components are discussed in detail in the following subsections, and a video of the results is attached to the paper. 
8.1 Gate detection 
Architecture search Due to the limited computational budget of the Jetson Xavier, the network architecture was designedtomaximizedetectionaccuracywhilekeepingalow inference time. To ﬁnd such architecture, different variants of UNet Ronneberger et al. (2015) are compared. Table 2 
Table 2 Comparison of different network architectures with respect to intersection over union (IoU), precision (Pre.) and recall (Rec.). The index in the architecture name denotes the number of levels in the UNet. All networks contain one layer per level with kernel sizes of [3, 3, 5, 7, 7] and [12, 18, 24, 32, 32] ﬁlters per level. Architectures labelled with ’L’ contain twice the amount of ﬁlters per level. Timings are measured for single input images of size 352 x 592 on a desktop computer equipped with an NVIDIA RTX 2080 Ti 
Arch. IoU Pre. Rec. #params Latency [s] 
UNet-5L 0.966 0.997 0.967 613k 0.106 
UNet-5 0.964 0.997 0.918 160k 0.006 
UNet-4L 0.948 0.997 0.920 207k 0.085 
UNet-4 0.941 0.989 0.862 58k 0.005 
UNet-3L 0.913 0.991 0.634 82k 0.072 
UNet-3 0.905 0.988 0.520 27k 0.005 
summarizes the performance of different network architectures. Performance is evaluated quantitatively on a separate test set of 4k images with respect to intersection over union (IoU) and precision/recall for corner predictions. While the IoU score only takes full gate detections into account, the precision/recall scores are computed for each corner detection. Based on these results, architecture UNet-5 is selected for deployment on the real drone due to the low inference time and high performance. On the test set, this network achieves an IoU score with the humanannotated ground truth of 96.4%. When only analyzing the predicted corners, the network obtains a precision of 0.997 and a recall of 0.918. Deployment Even in instances of strong changes in illumination, the gate detector was able to accurately identify the gates in a range of 2 − 17 m. Fig. 4 illustrates the quality of detections during the championship race (1st row) as well as for cases with multiple gates, represented in the test set (2nd/3rd row). With the network architecture explained in Sect. 4, one simultaneous inference for the leftand rightfacing camera requires computing 3.86GFLOPS (40kFLOPS per pixel). By implementingthenetworkinTensorRTandperforminginfer- 
123 
Table 3 Total ﬂight time vs. computation time averaged over 100 runs. The percentage in parenthesis is the computation time with respect to the computational time for the full track 
Ngates Flight time Computation time 
1 9.5935 s 1.66 ms 2.35% 
2 9.2913 s 18.81 ms 26.56% 
3 9.2709 s 35.74 ms 50.47% 
4 9.2667 s 53.00 ms 74.84% 
5 (full track) 9.2622 s 70.81 ms 100% 
CPC Foehn and Scaramuzza (2020) (full track) 6.520 s 4.62 · 105 ms 6524% 
ence in halfprecision mode (FP16), this computation takes 10.5 ms on the Jetson Xavier and can therefore be performed at the camera update rate. 
8.2 State estimation 
Compared to a pure VIObased solution, the EKF has proven to signiﬁcantly improve the accuracy of the state estimation relative to the gates. As opposed to the works by Li et al. (2019); Jung et al. (2018); Kaufmann et al. (2018), the proposed EKF is not constrained to only use the next gate, but can work with any gate detection and even proﬁts from multiple detections in one image. Fig. 6 (center) depicts the ﬂown trajectory estimated by the VIO system as pVB and the EKFcorrected trajectory as pB (the estimated corrections are depicted in gray). Accumulated drift clearly leads to a large discrepancy between VIO estimate pVB and the corrected estimate pB. Towards the end of the track at the two last gates this discrepancy would be large enough to cause the drone to crash into the gate. However, the ﬁlter corrects this discrepancy accurately and provides a precise pose estimate relative to the gates. Additionally, the imperfect initial pose, in particular the yaw orientation, is corrected by the EKF while ﬂying towards the ﬁrst gate as visible in the zoomed section in Fig. 6 (center). 
8.3 Planning and control 
Figure 6 (left) shows the nominally planned path for the AlphaPilot championship race, where the coloured line depicts the fastest path along all the sampled paths depicted in gray. In particular, a total of M = 150 different states are sampled at each gate, with the velocity limited to 8 m/s and the angle between the velocity and the gate normal limited to ϕmax = 30◦. During ﬂight, the path is replanned in a receding horizon fashion through the next N = 3 gates (see Fig. 6, center). It was experimentally found that choosing N ≥ 3 only has minimal impact of the ﬂight time comapred to planning over all gates, while greatly reducing the computational cost. Table 3 presents the tradeoffs between total ﬂight time and computation cost for different horizon lengths N for the track shown in Fig. 6 (left). In addition, Table 3 
shows the ﬂight and computation time of the timeoptimal trajectory generation from Foehn and Scaramuzza (2020), which signiﬁcantly outperforms our approach but is far away from realtime execution with a computation time of 462 s for a single solution. Online replanning would therefore not be possible, and any deviations from the nominal track layout could lead to a crash. Please also note that the evaluation of our method is performed in Matlab on a laptop computer, while the ﬁnal optimized implementation over N = 3 gates achieved replanning times of less than 2 ms on the Jetson Xavier and can thus be done in every control update step. Figure 6 (right) shows resulting path and velocity of the drone in a multilap scenario, where the drone’s velocity was limited to 6 m/s. It can be seen that drone’s velocity is decreased when it has to ﬂy a tight turn due to its limited thrust. 
9 Discussion and conclusion 
The proposed system managed to complete the course at a velocity of 5 m/s with a success rate of 100% and at 8 m/s with a success rate of 60%. At higher speeds, the combination of VIO tracking failures and no visible gates caused the drone to crash after passing the ﬁrst few gates. This failure could be caught by integrating the gate measurements directly in a VIO pipeline, tightly coupling all sensor data. Another solution could be a perceptionaware path planner trading off timeoptimality against motion blur and maximum gate visibility. The advantages of the proposed system are (i) a driftfree state estimate at high speeds, (ii) a global and consistent gate map, and (iii) a realtime capable near timeoptimal path planner. However, these advantages could only partially be exploited as the races neither included multiple laps, nor had complex segments where the next gates were not directly visible. Nevertheless, the system has proven that it can handle these situations and is able to navigate through complex race courses reaching speeds up to 8 m/s and completing the championship race track of 74 m in 11.36 s. While the 2019 AlphaPilot Challenge pushed the ﬁeld of autonomous drone racing, in particularly in terms of speed, 
123 
autonomous drones are still far away from beating human pilots. Moreover, the challenge also left open a number of problems, most importantly that the race environment was partially known and static without competing drones or moving gates. In order for autonomous drones to ﬂy at high speeds outside of controlled or known environments and succeed in many more realworld applications, they must be able to handle unknown environments, perceive obstacles and react accordingly. These features are areas of active research and are intended to be included in future versions of the proposed drone racing system. 
Acknowledgements This work was supported by the Intel Network on Intelligent Systems, the Swiss National Science Foundation (SNSF) through the National Center of Competence in Research (NCCR) Robotics, the SNSFERC Starting Grant, and SONY. 
Funding Open Access funding provided by Universitàt Zürich. 
Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitteduse,youwillneedtoobtainpermissiondirectlyfromthecopyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/. 
References 
Bertsekas, Dimitri P. (1995). Dynamic programming and optimal control (Vol. 1). MA: Athena scientiﬁc Belmont. Bloesch, Michael, Omari, Sammy, Hutter, Marco, & Siegwart, Roland. (2015). Robust visual inertial odometry using a direct EKFbased approach. Brescianini, D., & DâAndrea, R., (2018). Tiltprioritized quadrocopter attitude control. IEEE Transactions on Control Systems Technology, 28(2), 376–387. Cadena,C.,Carlone,L.,Carrillo,H.,Latif,Y.,Scaramuzza,D.,Neira,J., et al. (2016). Past, present, and future of simultaneous localization and mapping: Toward the robustperception age. IEEE Transactions on Robotics, 32(6), 1309–1332. Cao, Zhe., Simon, Tomas., Wei, ShihEn., and Sheikh, Yaser .(2017). Realtime multiperson 2d pose estimation using part afﬁnity ﬁelds. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7291–7299. Delmerico J, & Scaramuzza D. (2018). A benchmark comparison of monocular visualinertial odometry algorithms for ﬂying robots. Delmerico J, Cieslewski T, Rebecq H, Faessler M, & Scaramuzza D. (2019). Are we ready for autonomous drone racing? the UZHFPV drone racing dataset. Douglas Brent West. (2001). Introduction to graph theory (Vol. 2). NJ: Prentice hall Upper Saddle River. Elia K, M Gehrig, P Foehn, R Ranftl, A Dosovitskiy, V Koltun, and D Scaramuzza. (2018) Beauty and the beast: Optimal methods 
meet learning for drone racing. IEEE International Conference on Robotics and Automation (ICRA), pp 690–696. Falanga, Davide., Foehn, Philipp., Lu, Peng., and Scaramuzza, Davide .(2018). Pampc: Perceptionaware model predictive control for quadrotors. In: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1–8. Foehn, Philipp and Scaramuzza, Davide (2020) CPC: Complementary progress constraints for timeoptimal quadrotor trajectories. Preprint arXiv:2007.06255. Foehn, P., Brescianini, D., Kaufmann, E., Cieslewski, T., Gehrig, M., Muglikar, M., and Scaramuzza. D.(2020). Alphapilot: Autonomous drone racing. In Robotics: Science and Systems (RSS). 10.15607/RSS.2020.XVI.081. Forster, Christian, Zhang, Zichao, Gassner, Michael, Werlberger, Manuel, & Scaramuzza, Davide. (2017a). SVO: Semidirect visual odometry for monocular and multicamera systems. IEEE Transactions on Robotics, 33(2), 249–265. https://doi.org/10.1109/TRO. 2016.2623335 
Forster, Christian, Carlone, Luca, Dellaert, Frank, & Scaramuzza, Davide. (2017b). Onmanifold preintegration for realtime visualinertial odometry. IEEE Transactions on Robotics, 33(1), 1–21. https://doi.org/10.1109/TRO.2016.2597321 
Gao, Fei, William, Wu., Gao, Wenliang, & Shen, Shaojie. (2019). Flying on point clouds: Online trajectory generation and autonomous navigation for quadrotors in cluttered environments. Journal of Field Robotics. https://doi.org/10.1002/rob.21842 
Guerra, Winter., Tal, Ezra., Murali, Varun., Ryou, Gilhyun., and Karaman, Sertac.(2019). Flightgoggles: Photorealistic sensor simulation for perceptiondriven robotics using photogrammetry and virtual reality. arXiv:1905.11377. Hehn, M., Ritz, R., & DâAdrea, R., (2012). Performance benchmarking of quadrotor systems using timeoptimal control. Autonomous Robots,. https://doi.org/10.1007/s10514-012-9282-3. Kai, J. M., Allibert, G., Hua, M. D., & Hamel, T. (2017). Nonlinear feedback control of quadrotors exploiting ﬁrstorder drag effects. IFACPapersOnLine, 50(1), 8189–8195. Kalman, Rudolf E. (1960). A new approach to linear ﬁltering and prediction problems. Journal of Basic Engineering, 82(1), 35–45. Kuhn, H. W. (1955). The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1–2), 83–97. LaValle, S. M. (2006). Planning algorithms. Cambridge Cambridge Cambridge Cambridge: Cambridge University Press. Li, Shuo., van der Horst, Erik.,Duernay, Philipp., De Wagter, Christophe., and de Croon, Guido.(2019). Visual modelpredictive localization for computationally efﬁcient autonomous racing of a 72-gram drone. arXiv:1905.10110. Loock, W. Van., Pipeleers, G., and Swevers, J.(2013). Timeoptimal quadrotor ﬂight. In IEEE European Control of Conference (ECC). 10.23919/ECC.2013.6669253. Maurer, H. (1977). On optimal control problems with bounded state variables and control appearing linearly. SIAM Journal on Control and Optimization, 150(3), 345–362. Mellinger, D., Michael, N., & Kumar, V. (2012). Trajectory generation and control for precise aggressive maneuvers with quadrotors. The International Journal of Robotics Research. https://doi.org/ 10.1177/0278364911434236 
Moon, H., Sun, Y., Baltes, J., & Kim, S. J. (2017). The IROS 2016 competitions. IEEE Robotics Automation Magazine, 24(1), 20– 29. Moon, H., MartinezCarranza, J., Cieslewski, T., Faessler, M., Falanga, D., Simovic, A., et al. (2019). Challenges and implemented technologies used in autonomous drone racing. Intelligent Service Robotics, 12, 137–148. Mourikis, Anastasios I., and Roumeliotis, S.I.(April 2007). A multistate constraint Kalman ﬁlter for visionaided inertial navigation. 
123 
In: IEEE International Conference on Robotics and Automation (ICRA), pp. 3565–3572. Mueller MW., Hehn M., and D’Andrea R.(2015). A computationally efﬁcient motion primitive for quadrocopter trajectory generation. IEEETransactionsonRobotics.doi: https://doi.org/10.1109/TRO. 2015.2479878. 
Qin, Tong, Li, Peiliang, & Shen, Shaojie. (2018). VINSMono: A robust and versatile monocular visualinertial state estimator.IEEE Transactions on Robotics, 34(4), 1004–1020. https://doi.org/10. 1109/TRO.2018.2853729 
Ronneberger, Olaf., Fischer, Philipp., and Brox, Thomas .(2015). Unet: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer. Ryou, G., Tal, E., and Karaman, S.(2020). Multi-ﬁdelity blackbox optimization for timeoptimal quadrotor maneuvers. Robotics: Science and Systems (RSS). 10.15607/RSS.2020.XVI.032. Shuster, Malcolm D. (1993). Survey of attitude representations. Journal of the Astronautical Sciences, 410(4), 439–517. Spasojevic, Igor, Murali, Varun, & Karaman, Sertac. (2020). Joint feature selection and time optimal path parametrization for high speed visionaided navigation. 
Sunggoo, J., Sunyou, H., Heemin, S., & Shim, D. H. (2018). Perception, guidance, and navigation for indoor autonomous drone racing using deep learning. IEEE Robotics and Automation Letters, 3(3), 2539–2544. https://doi.org/10.1109/LRA.2018.2808368 
Szeliski, Richard .(2010). Computer Vision: Algorithms and Applications. Texts in Computer Science. Springer. ISBN 9781848829343. Zhou, B., Gao, F., Wang, L., Liu, C., & Shen, S. (2019). Robust and efﬁcient quadrotor trajectory generation for fast autonomous ﬂight. IEEE Robotics and Automation Letters. https://doi.org/10.1109/ LRA.2019.2927938 
Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations. 
123 
",AlphaPilot  autonomous drone racing.pdf,2
"Design and implementation of a generic multiservice framework for autonomous indoor ﬂight & An exploratory study into autonomous indoor HumanDrone Interaction 
Master Thesis hci102f19 
Aalborg University Software 
Title: SharpFlying 
Theme: Master Thesis in HumanComputer Interaction 
Project Period: Spring 2019 
Project Group: hci102f19 
Participant(s): Kasper Østergaard Helsted Steffen Darby Carlsen 
Supervisor(s): Mikael Skov 
Copies: 3 
Page Numbers: 16 
Date of Completion: 10-06-2019 
Abstract: 
In current time, research into the use of drones in terms of autonomy has become more popular. This has caused researchers to attempt and ﬁnd a singular technology and solution for solving indoor navigation with drones. Even though some of this research has proven to be successful, none of the found research has attempted to combine multiple solutions into a singular framework. Thus, we designed and implemented SharpFlying, a generic and extendable multiservice framework for autonomous indoor drones. In order to validate our framework, we implemented three proof of concept services. These services were tested, individually and together, to get an insight into whether or not a multiservice structure can perform equal to, or greater than a singular solution. Our results show that by combining multiple services, you gain vastly better results compared to a singular service. Our ﬁrst study showed how to develop autonomous drones, however, research has yet to ﬁgure out a set of design guidelines as to how we should interact with drones in an indoor environment. Based on this, we created a 3- parts exploratory study into the world of indoor autonomous HumanDrone Interaction. Our tasks focused on primary interaction during navigation, voice interaction and secondary interaction. Our results show that participants expected the drone to behave under a level of trust. Trust was a metaphor expressed by some of the participants, they related this word to the behavior of the drone and how ﬂawless they expected it to be. Our study outlines a series of design insights for future interaction development of indoor autonomous drones. 
ment with the author. 
This master thesis has been made by hci102f19, 10th semester Software students from Aalborg University. The focus of the master thesis is on indoor autonomous drone ﬂight from the perspectives of development of a multiservice framework and exploration of interaction techniques. 
This master thesis consists of two academic papers each detailing their own research in regards to autonomous drone ﬂight. 
The master thesis has been written in cooperation with the project supervisor, Mikael Skov, who we express gratitude and thanks towards. 
Aalborg University, 11th June 2019. 
Kasper Østergaard Helsted 
khelst13@student.aau.dk 
Steffen Darby Carlsen 
sdca14@student.aau.dk 
vii 
The purpose of this project is to investigate the world of autonomous drones from a development and interaction perspective. The ﬁrst article starts by consulting the related work in regards existing solutions within autonomous drone navigation. This gives an insight into how researchers has already attempted to solve the many problems of autonomous navigation, both in regards to indoor and outdoor. This showed that the related work always focused on using one technology or method to perform indoor autonomous navigation. Based on the related work, we extracted multiple approaches to indoor navigation and created the framework, SharpFlying. This framework is a servicefocused extendable and generic implementation, meaning that it is created to allow for other developers to continue the development by implementing more services using the baseclasses we have implemented. 
As a proof of concept, we created three different services: Vision-, Distanceand WiFiPositioning service. The vision service implements Canny Edge Detection, Hough Line Transformation and DBSCAN clustering in order to determine the vanishing point of the area ﬂown in. When the vanishing point has been determined, it can be used to correct the yaw of the drone, allowing it to steer down a hallway, while keeping the nose of the drone correctly aligned. The distance service uses ultra sonic sensors mounted onto a 3Dprinted hull with a Raspberry Pi ZeroW as a means of communication. This service controls the roll of the drone by using the distances measurements on either side of the drone to correct itself into the center. The WiFipositioning service was implemented as a proofofconcept of how the system could be used. In order to test the implementation of SharpFlying and the services, we designed a closeto worstcase scenario for an indoor drone. It had to navigate down a narrow hallway from different starting positions, where it had to centralize itself before landing in a designated area. The tests showed that the individual services can navigate the hallway, although with a lot of inconsistencies and mistakes. Combining the vision and distance service yielded better results and consistencies, where the drone only made one wrong landing. 
Due to the services not allowing indoor navigation and the unpredictability of actions by the implemented services, we did not deem it safe enough nor ready to be used in a real scenario. Thus, in order to get an insight into Humandrone Interaction with an indoor autonomous drone, we used Wizard of Oz (WoZ). 
ix 
With WoZ, we emulate similar behavior to that of a fully autonomous drone, however with the safety of an experimenter being able to immediately stop the drone in case of a dangerous situation. Thus, we designed an experiment with three different parts, each focusing on their own type of interaction. The ﬁrst part is about primary interaction, where the participant is following an indoor navigation drone. The second part is an indoor area investigation, where the participant, using voice commands in a remote location, controls the drone between two locations, to then investigate an area and locate two hidden objects. The third and last part is about secondary interaction. In this task, the user is sharing a narrow hallway with the drone under different conditions. 
Based on the interaction experiment, we withdraw four design insights, that future researchers within autonomous HumanDrone Interaction may consider when designing an autonomous system. This includes: trust, a metaphor explicitly stated by our participants. Trust deals with expected behavior of the drone. The participants expected the drone to be ﬂawless in its navigation and ability to know the environment it is navigating. Furthermore, the participants wanted consistency in actions and speed of the drone. Lastly, we derive how much natural language understanding in necessary to properly interact with a drone using voice commands alone. Here we draw similar results to that of related work, that the interaction level needed is similar to communicating with a pet. 
Currently, the implemented services of SharpFlying does not deal with any interaction speciﬁc measures, however, the implementation allows a service to be developed that does. Based on the results obtained through our study, the service would need to detect people and react to them by giving the person space and awaiting the person to move past the drone, before continuing. Furthermore, further research should be made into proper autostabilization in indoor environments, as the drone in the current state can be unstable, especially in intersections, however this could potentially be solved with a smaller drone. 
We conclude that a multiservice framework for controlling autonomous drones can perform better than single instances of services. Furthermore, that allowing new services to be easily added to extend the autonomous system can provide better results. Based on the results of the exploratory study, several recommendations of valid services to extend SharpFlying has been proposed as design insights. For future work, we recommend continued development of SharpFlying to behave according to our design insights, to which the results of the exploratory study should be veriﬁed. 
Kasper Østergaard Helsted Department of Computer Science Aalborg University, Denmark khelst13@student.aau.dk 
Steffen Darby Carlsen Department of Computer Science Aalborg University, Denmark sdca14@student.aau.dk 
ABSTRACT 
Drones are becoming increasing popular, however, the use of drones is limited to mostly outdoor usage. While research highlights new and smart technologies to perform indoor navigation, it is often focused on using a singular technology as a solveeverything solution. In this article, we designed and implemented SharpFlying, a generic and extendable multiservice framework for autonomous indoor drone flight. We implemented three proof of concept services based on related work to validate our framework implementation. We tested our services individually and together, which showed vast improvements when using multiple services at the same time. We contribute a framework for future researchers to use when developing autonomous drones with a multiservice structure in mind. 
KEYWORDS 
Drones, HumanComputer Interaction, HumanDrone Interaction, Indoor navigation, Autonomous flight, SharpFlying, Vision, Edge detection, Clustering 
1 INTRODUCTION 
The use of drones in indoor environment has been researched in regards to using a single technology to attempt and solve one of the many problems of indoor navigation. This includes: Recognizing tags using vision or RFID [8, 28], using deep learning for perception, guidance and navigation indoors [20] and a general use of vision to navigate the drone indoors [16, 17] or adding additional sensors to the drone [31, 33]. While some of the solutions has been developed as frameworks, none of them attempt to combine multiple approaches to indoor navigation into a framework for flying indoors with drones. In outdoor environments, navigation is often done using GPS, gyroscopes, accelerometers and some wireless communication with a base station using the camera feed. These tools are great for outdoor navigation, but is not enough when it comes to indoor navigation, which can be used for inbuilding delivery, automated inventory and indoor emergencies [29]. Additionally, A survey made in 2001 shows that humans spend upwards of 87% of their time in indoor environments, this includes office buildings, airports, venues or homes. This result displays the need of indoor navigation and that any type of framework that allows indoor navigation has a practical use. In indoor environments, a drone need to use its sensors and vision based systems to detect and avoid collisions. These are used in conjunction with obstacle avoidance strategies to find the best solution to avoid the environment [12]. Although a lot of research highlights indoor navigation strategies and how to approach this [13, 25, 28, 36], limited research could be 
found that designs and implements an indoor autonomous drone with the goal of detecting and avoiding humans [26]. However, their results does not focus on any interaction, but rather focus on the technical aspects of detecting and avoiding humans, with a singular contribution to HumanDrone Interaction (HDI), this being that the drone should move when meeting humans in a hallway. In this paper, we have developed a standalone framework, SharpFlying. SharpFlying combines multiple results from previous work by combining vision, ultra sonic sensors and WiFi positioning into one framework that helps with autonomous indoor navigation. The framework we have developed is not a fully implemented system, but rather a proof of concept. It is developed as a foundation to be build upon, where the system allows for easy expansion upon functionality by adding more services. This framework relies on the related work, and comes with a collection of developed libraries, in order to support the features which was deemed necessary for this framework. The inspiration and some of the requirements of the project is based the SciRoc Episode 12 challenge - Fast delivery of emergency pills [3]. This challenge is about an aerial robot in an emergency situation, where a firstaid kit needs to be delivered to a customer in an indoor environment as fast as possible. The robot will need to automatically detect and avoid obstacles and account for GPS being unavailable. The challenge defines that the robot must navigate in an indoor environment, detect a customer and correctly maneuver in an environment with obstacles, land and deliver the item within a 2 meter accuracy and fly back. These requirements are the point of reference of requirements for the development of SharpFlying. Initially, we will take a look at related work, to get an idea on how to approach the project and to get an insight into the current progress made by other researchers. Afterwards, we will go through multiple development cycles, each focusing on solving a problem within indoor navigation with a drone. Lastly, the different implemented services will be tested individually and in conjunction with each other, to show how each of them perform and compensate for each other. 
2 RELATED WORK 
In order to gain an understanding of current research in regards to navigation with drones, we look at the related work in regards to outdoor and indoor autonomous navigation with drones. This will give an insight into the current state of the art and novel approaches to solving outdoor and indoor navigation issues, with a focus on drones. The related work will be used to extract important considerations and guidelines for developing SharpFlying, our framework for indoor autonomous flight. 
1 
2.1 Outdoor autonomous navigation 
Within outdoor navigation, there are several approaches that attempt to either solve or optimize the field of navigation. The conventional approaches use GPS to determine the position of the drone and either waypoints or other planning algorithms to determine the most optimal route for the drone. Another approach that has evolved, especially in regards to race drones, is machine learning approaches using neural networks. 
Conventional approaches GPS is one of the most core technologies used for outdoor positioning and can be used in conjunction with a camera for fully autonomous flight without pilot input [24]. One of the most basic implementations of autonomous navigation includes way point planning, where the pilot determines a set of points the drone should navigate to, this feature is often implemented by the drone manufactures [4]. The approach of generating a flight plan based on points specified by the pilot has previously been researched in regards to precision [34, 35] and performance [19]. Carvalho et al. adapted a standard commercialized drone into an embedded system, without any reliance on a ground station for fully autonomous flight [11]. This was done using the Robot Operating System (ROS). Their work is a design and integration of the ROS framework with the robot’s hardware using an Odroid micro computer mounted on the drone. Their results show the drone being able to correctly perform a simple mission consisting of performing a takeoff, flying in a square and then landing again using GPS only. The UAV responded satisfactory during random disturbances, such as wind and GPS signal fluctuation, with a minimal number of errors. They conclude that using filtering algorithms for GPS signal fluctuations, the minor inconsistencies in flight paths might perform better. 
Machine learning & outdoor navigation Some of the related work has started to look at machine learning as an approach for outdoor navigation. Most of their motivations stem from drone racing, where each drone is controlled by a human pilot, with a firstperson view from the drone, who controls the drone at high speed using a radio transmitter. While human pilots need years of training to properly control the drone in fast paced environments, the skills are valuable to an autonomous system that must quickly and safely navigate through complex environments, which is the primary reasoning for using this as a task. Simultaneous Localization and Mapping (SLAM) can provide consistent 2D to 3D pose estimation against a known environment, but is prone to errors during any accelerated movements, due to image blurring. SLAM is great in static environments where waypoints or movement trajectories are already defined or where speed is not a concern [10]. Kaufmann et al. describes Deep Drone Racing, a 2-part system for robust and agile flight of a drone in a dynamic environment [21]. Part one is a perception system, which uses a Convolutional Neural Network (CNN) to predict a goal direction in local image coordinates, together with a navigation speed. This is generated from a single image on a forwardfacing camera. The second part is a control system. It uses the output from the perception system to generate a movement trajectory vector. Their results 
show that their system is able to navigate a complex race track, while avoiding common problems, such as drifting and a navigation in a dynamic environment. Sadeghi et al. presented CADˆ2 RL, a neural network trained purely on simulation data. Their goal is to train a neural network for obstacle detection, without having to put the drone into the real scenario [32]. Their results show that they can successfully fly and avoid obstacles. For future work they recommend including depth into the images using multiple cameras or combine their simulated training with real data, which could yield a better overall result. Giusti et al. researched a drone as a medium of visual perception of a forest trail [18]. Here, they assume that as long the drone follow the trail, that it is free of any obstacles, thus focuses on finding and following the trail. They have trained a CNN on 17,000 training frames and uses that to process an image in order to obtain a perception of the trail. They derived a twoclass problem, one has to decide whether the trail is visible or not, while the other class determines where the trail is. The biggest problems they faced were in regards to image quality, where their training data was recorded from GoPros mounted on a researchers head during a regular hike. This meant that the fastpaced motion pictures from the drone caused complications with the classifications. Furthermore, differences in light strength across the trail caused the drone to be unaware of its surroundings, which caused some crashes. In general, they were able to classify trails correctly in synthetic tests and that their system performs better than any alternatives and to a certain degree, comparable to humans. 
2.2 Indoor autonomous navigation 
There is a fairly recent body of work attempting to solve the problem of indoor navigation in GPS denied environments using one of the Parrot drone platforms [9, 16, 22]. The solutions proposed includes: the use of markers for vision to detect, vision based distance estimation and full on image processing systems. In recent years, the use of drones has expanded into reallife applications, especially in outdoor scenarios, such as monitoring and surveillance [1, 6]. Thus, research has been made in regards to HDI in outdoor environment with respect to: how users interact with drones and how they expect the drone to respond to their interaction. However, only limited research has gone into the potential interaction in indoor environments. Lioulemes et al. researched the safety challenges when humans and drones collaborate in indoor environments [26]. Their results show that humans expect the drone to behave in a similar manner to a ground vehicle, that is, that the drone should be the one maneuvering away from the human when possible. 
Followme drone Mao et al. investigated the possibilities of an indoor followme drone [27]. Their motivation is that video taping is a costly and time consuming process with possibilities for automation. They describe the challenges of being unable to use GPS or similar technologies to properly do indoor tracking. Their solution uses robust acoustic tracking to determine the distance between the user and the drone. Their results show that it is possible to find the location of a user and follow them within a specific distance, with the limitations 
2 
being unable to properly follow the user during extended periods of disconnections with the sound signals, I.E. hard concrete or a human blocking the signal [27]. 
Machine learning & indoor navigation Duggal et al. researched the use machine learning in contrast to fully autonomous indoor navigation for drones [13]. They use a single monocular camera on the front of the drone. They input an image into a Hierarchical Structured Learning (HSL) algorithm that outputs a depth map. This depth map is fed into a CNN to generate the flight planning commands for the drone. Their results show that they use ≈ 180ms to perform HSL and ≈ 200ms for the CNN to generate flight planning commands. They specify that the Bebop drone they use for their experiment takes approximately 400ms to respond, so that their computation time is justifiable. Their flying experiments show that they have an 82% successful navigation rate, however that they had problems with the drone drifting in the indoor environment and problem with depth map estimation due to changing light levels in the hallways. 
Visionbased approaches Adriano Garcia & Kanad Ghos researched autonomous indoor navigation using a stock quadcopter using offboard control [16]. Their approach to indoor navigation uses vision to determine the location of floors and walls. This is done using Canny edge detection in correlation with Hough Line Transformation. This is done to form vanishing points. The vanishing points provides a general central marker of the image and can be used for yaw correction and the position of the vanishing lines can be used as a reference for roll correction. One part of their results show that they can detect an intersection within a 50cm difference of their measured estimate. The second part of their results is in regards to navigating a narrow hallway. In total they had 20 test flights, with 15 total collisions, with only 2 of them being hard crashes. They identify the error causing the hard crashes in both cases, saying that the implementation of 180 degree turns will fix this. They manage to successfully detect, stop and turn at the intersection in 90% their tests. Because of their implementation heavily relying on being able to find vanishing points for navigation and stabilization of the drone, during turns, they had troubles correctly finding the new vanishing point, causing problems in 25% of their test cases. A similar type of research was made by Bills et al. who focused on indoor navigation in narrow staircases [8]. Their results show that their vision based solution succeeded over 80% of their tests, however that they had problems getting the drone to properly fly in the indoor environment causing air drifts and turbulence, which caused the drone to crash or touch the walls. 
Distance sensorbased approaches There already exists different types of solutions that attempt to support drone flight using distance sensors. This includes a obstacle detection and collision avoidance system using ultrasonic sensors by Gageik et al. [15]. They show that ultra sonic sensors perform well when reflecting onto straight surfaces, where it is capable of reproducing the environment with an accuracy of centimeters, however that they are unable to reliably detect objects beyond 250cm. They recommend using additional sensors, such as infrared 
sensors, to account for some of the negative aspects of using ultra sonic sensors. The use of infrared sensors as an obstacle avoidance tool has previously been researched [14, 23]. Their results show the use of the range finder in conjunction with the inertial measurement unit, that they can stabilize the drone in an indoor environment within a 2x3 meter area. They conclude that this is adequate, since the rooms they navigate in are always at least 6x6 meters. In one of the test cases in [14], the drone is flying in a narrow hallway towards a dead end. In this test, their system showed inconsistencies due to wind turbulence in the narrow indoor environment. Even though they were able to correctly navigate the area and avoid obstacles, specifically tweaking the system variables was needed in order to accommodate the narrow areas. 
3 SHARPFLYING 
Based on the related work, we have chosen to focus on three technologies. The first technology is vision, this is chosen based on the promising results of Adriano Garcia & Kanad Ghos, who managed to perform indoor navigation using deterministic wellresearched vision models. The second technology is distance using ultra sonic sensors. This is implemented in order to accommodate for the weaknesses of vision. These sensors measure the distances between the drone and environmental objects, allowing us to stop the drone from crashing into walls. Furthermore, Bills et al. described scenarios where they had problems with the air generation of the drone when indoor. We attempted to replicate their results by manually flying indoors, which showed that when the environment changes, I.E. flying past an intersection, the drone can no longer stabilize itself during hovering, causing it to increase its speed. Using ultra sonic sensors, we can detect drastic changes in the environment and accommodate for these environmental changes. Lastly, some of the distance sensorbased related work showed promising results in regards to indoor environments and that their systems are able to function in even narrow environments, as long as the system parameters are designed to accommodate for this. The outdoor approaches use GPS in order to perform selflocalization of the drone, this is not available for indoor flight. The related work simplify their experiments by ignoring the need to navigate a building between two known points, but rather follow a user or move inside a simple environment, such as a hallway with no obstacles. In order to implement indoor navigation, we will use WiFi positioning as the third technology. The goal is not to get an absolute positioning of the drone, but rather selflocalization to the point of the drone understanding where it is, such that it knows how to move between two known locations. 
4 HARDWARE 
This section describes the drone and the basestation used for the system. The drone chosen for this project is a Parrot Bebop 2 (42cm x 39cm x 9cm) modified with a custom 3D printed hull. Furthermore, the drone has 3 ultra sonic sensors (HCSR04) mounted onto the hull. The ultra sonic sensors measures in a 15°angle and has a range of 2cm to 400cm. The drone with all modifications can be seen in Figure 1. The ultra sonic sensors are connected to a Raspberry Pi Zero W, which 
3 
is also mounted on the drone. The Raspberry Pi is running Raspbian as its operating system. 
Figure 1: Image visualizing the drone setup, with ultra sonic mount. 
The Raspberry Pi offloads all of the heavy computations to a basestation. The basestation is a laptop with an i5-7300HQ @ 2.5GHz processor running Windows 10. During integration tests of the distance sensors, it was discovered that the drone propellers emit ultra sonic sound, causing some of the sensors to give invalid data. Due to this being discovered late in the project, the tests were performed by taking the hull off the drone, placing it on an office chair and imitating the movements of the drone. 
5 IMPLEMENTATION 
Figure 2: A simplified overview of the SharpFlying component architecture 
The implementation consists of 4 different modules. These are the modules outlined by the related work as necessary to implement an indoor autonomous drone. The system architecture can be seen in Figure 2. 
SharpFlying is the entry point for the system. Here the declarations of the different services are made and the results from each service is retrieved. BebopSharp is the library developed to control the drone. The Vision service contains the implementation of Canny Edge detection, Hough Lines and the DBSCAN clustering. The Distance service contains the implementation of the ultra sonic sensors. The WiFi Positioning service provides functionality to find an estimated position of the drone inside a building. All of the services return a Response together with a confidence value. A confidence value is a value that indicates the services confidence in its result. From the vision service, the confidence value is dependant on the changes between images. If we observe a large change in the results from a captured image, the confidence value will drastically go down. This will handle cases of blurred images or other cases of bad data. The entire point of SharpFlying is to be a fully asynchronous multiservice framework. This means that all services are running separately and “as fast as possible”, causing no services to bottleneck each other. This means that there is no guarantee of a looped order of actions, but that one service can publish multiple results before another service publishes theirs. 
5.1 BebopSharp 
BebopSharp is the implementation of the drone controller library. This is the library implementing all of the functionality we can perform with the drone. Initially, we implemented the bebop system in Python using PyParrot, an existing library for developing drone flight applications [7]. However, during integration of the vision service, the performance of Python failed to deliver the expected results, with this implementation only being able to process 1-10 frames per second, depending on the amount of data in each image. Thus, we decided to rewrite the entire project in a compiled language, namely C#, due the project members having previous experience in the language. However, there existed no library equivalent of PyParrot for C#. Thus we created our own library to communicate with the Bebop drone, called BebopSharp. After successfully connecting to the drone, a thread that generates the drone command packets is started. This thread ensures that if the drone retrieves a FlightVector (An object containing movement values) from SharpFlying, that the drone performs the corresponding movements. The amount of times the command generator generates and sends commands is controlled by the Constructor call to the BebopSharp class, where the user can specify the number of updates per second, per default this is 10. In order to ensure safety, a thread watcher is used. The thread watcher ensures the command generator thread is active and performs an emergency landing if we lose connection to the drone. The drone communicates using UDP between itself and the client. Based on the ARSDK Protocol documentation (Official documentation of how the drone works internally), the communication with the drone has been implemented to handle pings from the drone (the drone asking if we are still alive) and when the drone has updates to its sensors [30]. This is implemented by recursively withdrawing the first 7 bytes, containing the packet header, then based on the total packet size, we store the bytes from the data 
4 
Services 
Vision Distance Indoor positioning 
Objective Detect features in rooms Measure distance to remote objects Estimates location from WiFi strength 
Implementation Uses multiple vision based algorithms to locate room features. 
Uses an external mounted system to measure distance. Using multiple measurements, to understand distance. 
Uses external mounted system scan for WiFi and comparing to know hotspots, for positioning. 
Table 1: Simplification of implementation of different services 
array from the drone. This is performed recursively since the drone can send multiple data packets at a time. In order to parse the droneUDPpacket, we start by parsing the Datatype byte of the packet. The Datatype tells us how to respond to the packet retrieved, this is either: Ping, acknowledgement packet (The drone asking us whether we retrieved a specific packet) or sensor data. The next byte in the header is the Buffer ID. This specifies if the data is an internal drone command, a data buffer or an acknowledgement buffer. Afterwards, we have the Sequence ID. This is an identifier for the packet. Lastly, we have the Packet size. This defines is the total size of the entire data structure as a little endian integer. In order to send a FlightVector to the drone, we must first determine how the drone should move. In order to ensure the horizontal movement (Yaw & Roll) of the drone, the camera from the drone is used to extract relevant data from the images. The implementation of this is described in subsection 5.2. Vision does not provide any details about distance, thus in order to ensure the drone is in the middle of the corridor, we use ultra sonic sensors in our distance service. The implementation of this can be seen in subsection 5.3. In order to navigate indoors, we have to know where the drone is located, this is done using indoor WiFi positioning, the implementation can be seen in subsection 5.4. 
5.2 Vision service 
Vision allows us to extract information about the environment that distancebased sensors or other measurement devices can not directly provide. The vision service will be used to extract details from the live camerafeed of the drone. The vision service extends the baseservice, overriding the Input, Run and GetLatestResponse methods. These are used to give data to the service, start the service and retrieve the latest result from the service. The Vision service implements Canny Edge Detection and Hough Lines Transformation. In order to visualize the data, RenderGeometryLib is used. This library is a wrapper around the OpenCV drawing functionality and allows us to draw geometric figures on a frame and render it. DBSCANLib is the clustering library used. We are using DBSCAN clustering algorithm over the traditional Kmeans, since we can not estimate or predict the expected amount clusters in the image. GeometryLib contains base implementations of Box, Line, Point and Polygon, which is used throughout the implementation of all features. An example can be seen in EdgyLib, where the implementation of Hough Line Transformation uses Line to represent its result. 
Implementation Based on the previous research described in the related work, we decided to build upon the approach described by Adriano Garcia & Kanad Ghos in [16]. This means that we take the raw image from the drone, downscale it to 640x360px, apply Gaussian blur and run Canny Edge detection on the image. Canny Edge detection consists of 5 different steps. The first step is to apply Gaussian filtering to smooth out the image, this removes the noise. The next step is to determine the location of the horizontal, vertical and diagonal edges in the blurred image. Afterwards, a scan of the image is performed to remove unwanted pixels not apart of the edge. Furthermore, each pixel is checked if its a local maximum or if there is a local maximum closeby the gradient. The result of this step is a binary image with thin edges. The last step decides which of the edges are useful. For this, two values, maximum and minimum threshold are used. The edges that lie between the threshold values are decided to be edges based on their connectivity. After Canny Edge detection, we find the lines in the image using HLT. HLT uses the binary representation of the image from Canny Edge detection and based on: a minimum distance between points, a maximum angle between the points in a line and a minimum number of points to represent a line, the lines are returned. After finding the lines in an image, we will need to find the point with the most intersections. If there exists intersections in the image, we perform DBSCAN clustering based on the intersections. This gives us a bestcase centroid point, which is our vanishing point. If the vanishing point is outside of the centerlocation of the drone’s image, the drone will autocorrect itself to have the point in the middle. An example of the steps of the vision service can be seen in Figure 3. The topleft image is the raw input image, taken directly from the video feed of the drone. The topright image is the result of performing Canny Edge detection on the image. The bottomleft image is the result of HLT, where we find the lines in the image. The bottomright image shows the clustering, where the dots indicate points and the red dot is the centroid. 
5.3 Distance measurement service 
Using the Vision service, the drone has an understanding of the environment and can use this to autocorrect itself. However, in more advanced environments, the drone requires understanding of distances to accommodate for changes in the environment, that vision can not detect and classify. The distance service uses 3 ultra sonic sensors, one on the front and one on either side of the drone. The drone and its extra components is described in section 4. 
5 
Figure 3: The four different stages of which our Computer Vision sees each processed video frame 
The distance service extends the baseservice, overriding the Run and GetLatestResponse methods. These are used to start the service and retrieve the latest output from the service. It is dependant on UDPBase, which is a wrapper around the C# UDPClient, to which it implements a network handshake and handles disconnects. This is implemented for easier communication over UDP across multiple classes and to reduce code duplication across services. The Raspberry Pi sends the data to the basestation, which uses the data to compute an action for the drone. 
Implementation 
The implementation of the distance measurement service uses a ClientServer architecture. The server runs on the Raspberry Pi, and the client is a service for the SharpFlying system. In order for us to use the ultrasonic sensors, we have mounted a Raspberry Pi ZeroW on the drone, which uses Generalpurpose input/output (GPIO) to get the distance from each of the three ultrasonic sensors. The Raspberry Pi is also equipped with an UDP server, which is used for transmitting the data to all clients, which are listening. When a client connects to the UDP server, a handshake is made to register the client as a subscriber to the sensor data. The UDP server reads from the sensors 10 times a second. Since ultrasonic measurements can vary depending on the surroundings, we average the distance measured over a period of 10 measurements, so highly varying results does not have a large impact, but if an object suddenly comes close to the sensor, the sensor will still give enough small values for it to have an effect on the result. The server transmits the latest result four times a second to all clients as JSON. The client subscribes to the UDP server running on the Raspberry Pi and receives JSON data as fast as possible, which is then mapped to internal objects. The client calculates the correct action for the drone to perform, based on the data received from the server. The actions are separated into two different types of movements; critical and noncritical movements. The critical movements are executed first. This is done by checking if any of the sensors detect objects within a “safe distance” of the drone. The default safe distance is 30cm to either side. If an object is detected, the drone should 
move away from that object immediately. If no critical movements are needed, the noncritical movements are calculated. An example of a noncritical movement is to center the drone in a given area by using the ultrasonic sensors on each side of the drone. All of these movements are calculated in the distance service and is returned to the SharpFlying program as a Response, where the movement vector is given with a confidence value. The confidence value by the distance service is hardcoded to 75 out of 100. This means that we are 75% sure that the value retrieved from the service is correct. Since the confidence value of the vision service is fluctuating, this allows it to both be below and above the confidence of the distance service, making it possible to be at a bigger priority than the result from the distance service. The confidence value does not cause a result to be ignored. Instead, a higher confidence value causes the final movement vector to be more impacted by the service result. 
5.4 Indoor Positioning Service 
At the current stage of the implementation, we have made the drone become aware of its surroundings, however, it has no understanding of its position in the building. Thus, in order to navigate a building and move between two locations, the drone must be able to position itself. Following the specifications of the SciRoc challenge, we should assume that the area we are flying in is GPS restricted, thus another approach is used. Through previous indoor navigation research made at the House of Computer Science, Cassiopeia, we obtained an IFC (Industry Foundation Classes) File, containing router locations and their corresponding MAC addresses. 
Implementation The implementation of the indoor positioning service uses a ClientServer architecture, similar to that of the distance service. The server is the Raspberry Pi ZeroW, which was previously described in section 4. The client is a service written for the SharpFlying system. On the server, the operating system has two commands, iwlist and egrep, which allows for the scanning of nearby WiFi access points, and filtering of command output. These commands are run four times a second, to get the most accurate result. The output of the commands is parsed and mapped to an AccessPoint object. The AccessPoint object contains information about the ESSID, Signal Strength, Quality, Mac Address and Frequency of the access point. This data from the AccessPoint object is serialized into JSON and transmitted to all clients using the same UDP server as the distance measurement service. The client subscribes to the UDP server and receives a list of AccessPoint objects serialized as JSON as fast as possible. When the client parses the data from the the server, the union of the retrieved access points from the service and a list of known access points is found. The list of known access points has an associated latitude and longitude, which is used for calculating the distance from the drone to each access point. After the distances have been calculated, the access point with the smallest distance is then selected as the assumed area, where the drone is within. We have chosen to not use trilateration, due 
6 
to intermediate test results showing that due to interference from surroundings, the second nearest access points diameter was greater then the smallest diameter and offset, thus no intersections was found. In order to counteract this and due to only using the result as a general approximation of distance, the access point with the smallest distance was selected as the position of the drone. 
6 EXPERIMENT 
As presented in section 5, SharpFlying contains multiple services. Each of these services provide their own functionality to the overall system and work independently from the other services. The experiment will test the vision and distance services independently. This tests the two service’s ability to perform its intended action, for example, the vision service to modify the yaw of the drone. Furthermore, the services’ will be tested in conjunction with each other, to show how the services compensate each others weaknesses. We chose a test case that tests the system’s ability to perform fully autonomous movements in an indoor environment. Each service and both services combined will be put through the same test case. This will show the strengths and weaknesses of the services. 
Test cases In total, the system is tested in three similar test cases. The drone is placed in varying position in a hallway. The positions are: Center of the hallway and 25% of the total distance towards both right and left side of the hallway. The change in placement is made to test the properties of the system to place itself in the center of the hallway using the different services. 
6.1 Environment 
The environment is a hallway (156 cm x 800 cm) with solid walls on either side. In the hallway, there is an window at the end of the test area with direct sunlight during the tests. A figure of the test area can be seen in Figure 4. 
Figure 4: The environment used for the autonomous flight tests 
In the starting area, three positions are marked, the center position and the two locations to the right and left side of center. The left and right positions are placed 25% of the total distance from the center starting point equalling to 39 cm. The landing area is a 100 cm by 78 cm area. Since the goal of the drone is to automatically center itself, the same deviation used during takeoff is used for the landing area. The length of the landing area, 100 cm, was chosen 
due to the landing being performed manually by the experimenter from a distance, giving some room for human error. 
6.2 Data analysis 
In the experiment, there is one independent variable, the service(s). The dependant variables that will be used to determine the effect on the service(s) are: 
• Task completion time • Number of errors • Successful landing 
Task completion time is measured from when the drone takeoff command is send to when the drone is landed. The time will be measured using the Stopwatch class in C#. The number of errors is incremented every time the drone: Hits a wall or otherwise has any collisions with objects in the hallway. If the experiment ended in a full crash, it will be noted down in the table of results. The last dependant variable is whether the landing was successful or not. A successful landing implies the drone being at the end of the hallway and landing with a maximum 25% offset from the center. All of the data was collected from video recording of the experiments, output logs from SharpFlying and observers notes. The data is analysed by initially extracting the main dependant variables from the video material. Afterwards, each testrun is analysed to extract relevant flight information. The relevant flight information relates to the cause of the drone’s actions. This allows us to create a timeline of input to the services and actual output versus expected output. This will give an insight into why the drone made any given decisions. 
7 RESULTS 
In order to analyze and interpret the results from our tests, video were recorded of the different flights and the controller screen. In Table 2, a summery of the results from our tests can be seen. From this table, it can be seen that the completion time of the tests were all within a small margin of error, however, that both the vision and distance test varied the most in regards to time. 
Services 
Vision (n=9) 
Distance (n=9) 
Vision + Distance (n=9) 
Task completion time (seconds) 10.74 (2.25)* 9.7 (1.44) 10.79 (0.85) 
Number of errors 5 2 0 
Successful landings 2* 5 8 
Table 2: Combined results from test flights, * n=6 
The number of vision tests, in regards to completion times, is lower then the initial 9 tests, which were performed. This is due to the drone crashing beyond recovery, which would immediately stop the current test run. Likewise, when looking at the errors and number of successful landings in Table 2, the distance and vision service performed the worst, with substantially worse results than vision + distance. Based on the results, three different themes are 
7 
withdrawn, these are: Service validation, lack of distance awareness and correction with multiple services. 
Service validation The first theme shows that we are able to successfully validate the data input and output of the distance service. In order to validate the precision of the distance sensors, unit tests were performed. Two different sensor validation tests was made, distance reliability and movement vector test. The distance reliability test shows the precision of the sensor and the reliability of the service. The movement vector test, is a test where the distance service outputs the movement vector, and this vector is compared to an expected vector, depending on the drone’s actual distance. This movement vector is corresponding to the movement the drone should perform. These tests show whether the distance service is calculating and outputting a correct movement vector. The distance reliability test is done by placing the drone precisely 100 cm from a wall. The distance service then measures the distance 100 times and then output the minimum, maximum and average values. The results can be seen in Table 3. The results show that 
Distance unit test Distance (cm) 
Minimum distance 99 
Maximum distance 102 
Average distance 100 
Table 3: Unit tests of the distance service 
the implementation of the service works as intended and provides stable results in a calm environment. The movement vector test is done by placing the drone in a hallway. Then the distances on either side of the drone is measured with a ruler and observed using the ultra sonic sensors. The output vector from the distance service is then outputted and compared to the expected vector. This movement vector is verified to be equal to the expected output. An expected value is a movement vector with a direction towards the side with the greater distance. 
Lack of distance awareness The second theme shows the general inability to understand distance and overall interpret hallways using the individual services alone. This could be seen in the distance service, where the drone would lose track of the direction, depending on the starting position. This would in some cases make the drone rotate uncontrollably, and unable to recover to the initial direction. In total we experienced 4 successful and 5 unsuccessful flights using the distance service. During landing, in 7 of the tests, the drone was aligned with the hallway, but in 2 of the flights, the nose of the drone would not align with the hallway upon landing. These results show, that the distance service by it self was not able to detect the direction of the hallway in which it was flying. This could be observed since the drone would deviate in the yaw direction, and not recover from it. The problem most commonly occurred when the takeoff was off center, which causes the drone to try and 
recenter itself. This causes overcorrections and made the nose of the drone be unaligned with the hallway. 
Figure 5: Illustration of distance service in hallway 
In Figure 5, the hallway feature problem can be seen. This figure shows how the distance service is unable to understand if the hallway is getting wider or the drone is rotating in the yaw direction. The results form our vision service shows that the drone would correctly change its yaw to center itself, but would drift, causing major inconsistencies in its movement direction. During the tests, the drone crashed to an unrecoverable state once, and were operator aborted twice. The crash happened while the drone attempted to correct its yaw, while drifting directly towards a door frame. This caused the drone to hit the door frame and crash the drone. In the 2 aborted tests, the operator deemed the drone was moving towards an unrecoverable state. In the first aborted test, the drone was moving into an intersection, where an ongoing movement would cause a direct frontal crash with a wall. The errors observed during the tests were caused by the drone zigzagging between the walls towards the end of the hallway. These inconsistencies translated into the drone missing the landing zone. In general, the drone using only vision is unable to properly center itself in the hallway and will instead drift, which means the drone’s movements are somewhat randomly determined. 
Correction with multiple services The third theme shows that using multiple services, the overall results drastically improve. It was clear that using a single service, would not be sufficient for autonomous flight, since each service in itself have different weaknesses. For example, the vision service is unable to account for the distances in the environment, thus colliding with the walls. It does, however, properly find the center point of the hallway and can center itself. The distance service was able to traverse the hallway with no hard crashes, but with the nose of the drone misaligned from the center, causing the drone to slide sideways and missing the landing zone. In the vision + distance test, the two services run at the same time, in order to compensate for each others limitations. The result from the vision + distance test showed that in total, 8 out of 9 tests are successfully completed and 1 test where the landing zone was missed. The vision + distance tests showed the drone is able to navigate a hallway using SharpFlying and its developed services with only one unsuccessful landing and no errors during the flight. In the test, where it failed the landing, the drone compensated too much from the air generated during liftoff from the right side, causing it to steer too far left. This caused the vision service to temporary lose 
8 
the center location of the hallway, which meant that the natural drift left got the drone close to the wall, causing an emergency roll towards right. At the same time the yaw was rightheavy, this meant that the drone barely managed to avoid a wallcollision, but due to the limited length of the hallway, it did not have enough time to get to the center of the hallway before the landing zone. 
Vision (n=6) 
Distance (n=6) 
Vision + Distance (n=6) 
Task completion time (seconds) 10.81 (2.75)* 9.11 (1.34) 10.54 (0.97) 
Number of errors 5 2 0 
Successful landings 1* 1 5 
Table 4: Table showing results from tests, where the drone started offcenter, * n=4 
As can be seen in Table 2 and Table 4, when the drone started in a noncenter position, it had a higher degree of noncompleteness and a higher margin of error. The time differences are similar between all starting positions. However, when looking at the margin of error in Table 2, the error rate is similar to Table 4, however the sample size is 33% smaller, which makes the errors vastly more influential. Also the amount of successful landings suffers from the same problem as the errors. The drone is vastly better at correcting the position when using multiple services. 
Limitations During our tests some limitations became clear. The vision service is limited to travelling through a narrow hallway, where it is able to successfully identify the vanishing point and use that to autocorrect itself for successful navigation. The system has no understanding of distances. This means that it is unable to determine when it is too close to the sides and end of a hallway. The distance service is unable to understand the features of the hallway. This causes the drone to rotate uncontrollably, making it unable to recover the nose to the initial direction. We experienced turbulence when approaching the intersection in the hallway due to the air generation of the drone. Using only the vision service, the intersection is not being detected, which means we can not compensate for this. 
8 DISCUSSION 
Through the related work, we found that research has been made in regards to indoor navigation and indoor flight using a multitude of different approaches. However, none of the work used multiple different approaches in a singular framework. Thus, we designed and implemented SharpFlying, a generic and extendable framework for indoor flight. Our results show that each of our implemented services separately show strengths and weaknesses and that by combining multiple of these approaches, that the amount of weaknesses can be drastically limited. The results can be derived into two main themes: Standalone services lack awareness and accuracy increases with multiple services. Lastly, we will discuss the implementation 
in regards to optimizations related to the implementation itself and the vision service. 
Standalone Services lack awareness Our test results show, that vision service alone is able to navigate the hallway, while only modifying the yaw of the drone from all starting positions. However, it is unable to properly correct itself to be in the middle of the hallway and thus misses the landing zone 7 out of 9 times. Furthermore, if the drone experiences turbulence at the liftoff moment, the drone has a big chance of colliding with the walls or completely crashing. Similar test results was found by [16]. Their results showed that their vision system could lose the vanishing point which caused collisions. Our tests showed that having no understanding of distance or the depth of the room, caused the drone to have major inconsistencies in regards to staying centralized in the hallway. The test results show that the drone is able to find the center point of the hallway and point the nose of the drone in that direction, by modifying the yaw of the drone. However, it can not properly account for turbulence or drastic changes in terms of distance to the walls. We believe that vision based systems, which are standalone, should explore the use of machine learning approaches moreso than line detection. A standalone CNN that generates flight commands based on a depth map was created by [13], their results showed a successful navigation rate of 82%. However, they had issues with drifting, similar to how our vision service experienced. The distance service results showed that the distance service alone makes the drone able to move between two locations by only modifying the roll of the drone. The drone had three fully completed flights, where the drone was able to correctly stay center of the hallway. Coincidental, the successfully completed tests were all when the drone started at the center position. This shows that when the drone is already center, that the distance service is good at making the drone stay there, but when the drone is starting uncentered, that it fails to either successfully land, or has collisions with the environment. The test cases, where the drone did not start in the center, showed that the nose of the drone would be misaligned with the center of the hallway, causing the distance service to overcompensate the roll values, which meant the drone was unable to perform a successful landing, even though it was able to reach the landing zone. In general, these results were expected from the ultra sonic sensors, especially considering the related work of [14], where they had to finetune their distance sensor algorithm and still had problems in narrow areas due to the air generation of the drone. 
Accuracy The test results of using both vision and the distance service show that by combining multiple services and using both of their results to navigate the hallway, that the amount of errors are minimized. This causes the drone to successfully recover from almostimpact situations1. The video linked in the footnote shows the drone recovering from a noncenter starting position using the both implemented services. This test results show that the drone was able to successfully navigate the hallway and avoid collisions. 
1https://streamable.com/a0igt 
9 
When testing both services together, there were 0 errors and 1 missed landing. This missed landing was because of the drone compensating too much from the air generated during liftoff from the right side, causing it to steer too far left, making it unable to recover before landing. 
Optimizations One of the main goals of the implementation was to achieve near realtime data processing. Thus, we had to optimize the implementation into a state of “as fast as possible”. An example is with the vision service: While an image is being processed, any new image will be added as the newest available image and the old image will be discarded. This causes us to only ever use the newest available data and not have the system halt behind. Since we always assume that there is a context between the latest processed frame, whatever frames we discard should not cause the system to behave overall differently. The queue system, combined with dynamically modifying the input parameters to the primary bottleneck of the system, HLT, caused the vision service to achieve a processing frame rate equal or greater than the camera feed of the drone (30 FPS). Currently, the services, we have implemented only uses the CPU for image processing. However, it is possible to offload some of the computations to a GPU using for example CUDA with OpenCV. According to the OpenCV documentation, most primitive image processing techniques can see speed increases of upto 30x using CUDA [5]. However, using CUDA comes with a computational cost of moving the image from main memory to the GPU’s memory [2]. In this study, we designed and implemented a framework for autonomous indoor flight with a drone. The framework is designed to be generic and extendable with further services. Testing of the implemented services showed that they individually are able to perform some part of the necessary actions to perform indoor flight, but that their weaknesses far outweigh what they can do individually. In the combined service tests, where the results of both services are used, the drone shows drastic better results and is able to successfully navigate the indoor hallway and recover from almost collisions in a narrow hallway. 
9 CONCLUSION 
Considering the increase of research in regards to indoor drones and that most of the current studies focus on finding a singular, solve everything solution, for indoor navigation, we took some of the related work’s ideas and implemented them into a generic and extendable framework, SharpFlying. The framework was unit tested, to discover the strength and weaknesses of each service, which showed that each service was able to account for its own part of the control, but that the drone was unable to properly navigate a hallway consistently. We found that combining the results of our two services, vision and distance service, the framework was able to successfully navigate the indoor hallway with only a single missed landing and 0 errors during flight. We contribute a framework that future researchers can use to implement their autonomous indoor navigation systems, allowing them to easily create multiservice structures for controlling their drones. Currently, the system assumes that the developer accounts for the results from a service themselves and knows how to convert 
the results of a service into a valid movement vector. The two services we implemented each control a direction, but there are no overlap between them. Given further implementation, it would be interesting to automatically choose the correct movement based on a level of confidence from the service, in the case that multiple services would attempt to change a singular movement direction, for example roll. Furthermore, we would have liked to finish the implementation of a third service into our service structure, namely WiFi positioning. This would have allowed us to navigate indoor, rather than just fly and account for changes in the environment. We researched the use of WiFi positioning and managed to retrieve an approximate position in the building, but did not have enough time to combine the results of the positioning and navigation. 
REFERENCES 
[1] [n. d.]. Aerial Surveillance and Monitoring, observation with UAV / drone. https://www.microdrones.com/en/industryexperts/security/monitoring/ 
[2] [n. d.]. CUDA Kernel Overhead. https://www.cs.virginia.edu/~mwb7w/cuda_ support/kernel_overhead.html 
[3] [n. d.]. E12 - Fast delivery of emergency pills | SciRoc. https://sciroc.eu/ e12-fastdeliveryofemergencypills/ 
[4] [n. d.]. Flight Plan | Parrot Store Official. https://www.parrot.com/us/flightplan# customizedflightplans 
[5] [n. d.]. OpenCV CUDA documentation. https://opencv.org/cuda/ [6] [n. d.]. Surveillance Drones | Electronic Frontier Foundation. https://www.eff. org/issues/surveillancedrones 
[7] Amy McGovern. [n. d.]. PyParrot. https://github.com/amymcgovern/pyparrot [8] Cooper Bills, Joyce Chen, and Ashutosh Saxena. 2011. Autonomous MAV flight in indoor environments using single image perspective cues. In 2011 IEEE International Conference on Robotics and Automation. IEEE, 5776–5783. https://doi.org/10.1109/ICRA.2011.5980136 
[9] PierreJean Bristeau, Francois Callou, David Vissière, and Nicolas Petit. 2011. The Navigation and Control technology inside the AR.Drone micro UAV. IFAC Proceedings Volumes 44, 1 (1 2011), 1477–1484. https://doi.org/10.3182/ 20110828-6-IT-1002.02327 
[10] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jose Neira, Ian Reid, and John J. Leonard. 2016. Past, Present, and Future of Simultaneous Localization And Mapping: Towards the RobustPerception Age. (6 2016). https://doi.org/10.1109/TRO.2016.2624754 
[11] Joao Pedro Carvalho, Marco Aurelio Jucá, Alexandre Menezes, Leonardo Rocha Olivi, Andre Luis Marques Marcato, and Alexandre Bessa dos Santos. 2017. Autonomous UAV Outdoor Flight Controlled by an Embedded System Using Odroid and ROS. Springer, Cham, 423–437. https://doi.org/10.1007/ 978-3-319-43671-5{_}36 
[12] Boris Crnokic, Snjezana Rezic, and Slaven Pehar. 2017. Comparision of Edge Detection Methods for Obstacles Detection in a Mobile Robot Environment. https://doi.org/10.2507/27th.daaam.proceedings.035 
[13] Vishakh Duggal, Kumar Bipin, Utsav Shah, and K. Madhava Krishna. 2016. Hierarchical structured learning for indoor autonomous navigation of Quadcopter. In Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing - ICVGIP ’16. ACM Press, New York, New York, USA, 1–8. https://doi.org/10.1145/3009977.3009990 
[14] Nils Gageik, Paul Benz, and Sergio Montenegro. 2015. Obstacle Detection and Collision Avoidance for a UAV With Complementary LowCost Sensors. IEEE Access 3 (2015), 599–609. https://doi.org/10.1109/ACCESS.2015.2432455 
[15] Nils Gageik, Thilo Müller, and Sergio Nieto Montenegro. 2012. Obstacle Detection and Collision Avoidance Using Ultrasonic Distance Sensors for an Autonomous Quadrocopter. https://www.semanticscholar.org/paper/ ObstacleDetectionandCollisionAvoidanceUsinganGageikM%C3% BCller/65f460229474e64687ecb209f6965d45bd7d5c34 
[16] Adriano Garcia and Kanad Ghose. 2017. Autonomous indoor navigation of a stock quadcopter with offboard control. In 2017 Workshop on Research, Education and Development of Unmanned Aerial Systems (REDUAS). IEEE, 132–137. https: //doi.org/10.1109/REDUAS.2017.8101656 
[17] Adriano Garcia, Edward Mattison, and Kanad Ghose. 2015. Highspeed visionbased autonomous indoor navigation of a quadcopter. In 2015 International Conference on Unmanned Aircraft Systems (ICUAS). IEEE, 338–347. https: //doi.org/10.1109/ICUAS.2015.7152308 
[18] Alessandro Giusti, Jerome Guzzi, Dan C. Ciresan, FangLin He, Juan P. Rodriguez, Flavio Fontana, Matthias Faessler, Christian Forster, Jurgen Schmidhuber, Gianni Di Caro, Davide Scaramuzza, and Luca M. Gambardella. 2016. 
10 
A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots. IEEE Robotics and Automation Letters 1, 2 (7 2016), 661–667. https://doi.org/10.1109/LRA.2015.2509024 
[19] U. Hashmi, F. Afshan, and M. Rafiq. 2013. Performance Analysis of Different Optimal Path Planning Bug Algorithms on a Client Server Based Mobile Surveillance UGV. In 2013 4th International Conference on Intelligent Systems, Modelling and Simulation. IEEE, 30–35. https://doi.org/10.1109/ISMS.2013.37 
[20] Sunggoo Jung, Sunyou Hwang, Heemin Shin, and David Hyunchul Shim. 2018. Perception, Guidance, and Navigation for Indoor Autonomous Drone Racing Using Deep Learning. IEEE Robotics and Automation Letters 3, 3 (7 2018), 2539– 2544. https://doi.org/10.1109/LRA.2018.2808368 
[21] Elia Kaufmann, Antonio Loquercio, Rene Ranftl, Alexey Dosovitskiy, Vladlen Koltun, and Davide Scaramuzza. 2018. Deep Drone Racing: Learning Agile Flight in Dynamic Environments. (6 2018). http://arxiv.org/abs/1806.08548 
[22] Tomas Krajník, Vojtech Vonásek, Daniel Fišer, and Jan Faigl. 2011. ARDrone as a Platform for Robotic Research and Education. Springer, Berlin, Heidelberg, 172–186. https://doi.org/10.1007/978-3-642-21975-7{_}16 
[23] Kedar Kulkarni and Aditya Bharadwaj. 2010. Development of an autonomous aerial vehicle using Laser range finder. Technical Report. 
[24] Jeonghoon Kwak and Yunsick Sung. 2018. Autonomous UAV Flight Control for GPSBased Navigation. IEEE Access 6 (2018), 37947–37955. https://doi.org/10. 1109/ACCESS.2018.2854712 
[25] Michael Leichtfried, Christoph Kaltenriner, Annette Mossel, and Hannes Kaufmann. 2013. Autonomous Flight using a Smartphone as OnBoard Processing Unit in GPSDenied Environments. In Proceedings of International Conference on Advances in Mobile Computing & Multimedia - MoMM ’13. ACM Press, New York, New York, USA, 341–350. https://doi.org/10.1145/2536853.2536898 
[26] Alexandros Lioulemes, Georgios Galatas, Vangelis Metsis, Gian Luca Mariottini, and Fillia Makedon. 2014. Safety challenges in using AR.Drone to collaborate with humans in indoor environments. In Proceedings of the 7th International Conference on PErvasive Technologies Related to Assistive Environments - PETRA ’14. ACM Press, New York, New York, USA, 1–4. https://doi.org/10.1145/2674396.2674457 
[27] Wenguang Mao, Zaiwei Zhang, Lili Qiu, Jian He, Yuchen Cui, and Sangki Yun. 2017. Indoor Follow Me Drone. In Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services - MobiSys ’17. ACM Press, New York, New York, USA, 345–358. https://doi.org/10.1145/3081333.3081362 
[28] Muhammad Atif Mehmood, Lars Kulik, and Egemen Tanin. 2008. Autonomous navigation of mobile agents using RFIDenabled space partitions. In Proceedings of the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems - GIS ’08. ACM Press, New York, New York, USA, 1. https: //doi.org/10.1145/1463434.1463461 
[29] Claudio E. Palazzi and Claudio E. 2015. Drone Indoor SelfLocalization. In Proceedings of the First Workshop on Micro Aerial Vehicle Networks, Systems, and Applications for Civilian Use - DroNet ’15. ACM Press, New York, New York, USA, 53–54. https://doi.org/10.1145/2750675.2750677 
[30] Parrot. 2015. ARSDK Protocols Parrot SA. https://developer.parrot.com/docs/ bebop/ARSDK_Protocols.pdf 
[31] Roberto Sabatini, Alessandro Gardi, Subramanian Ramasamy, and Mark A. Richardson. 2014. A Laser Obstacle Warning and Avoidance system for Manned and Unmanned Aircraft. In 2014 IEEE Metrology for Aerospace (MetroAeroSpace). IEEE, 616–621. https://doi.org/10.1109/MetroAeroSpace.2014.6865998 
[32] Fereshteh Sadeghi and Sergey Levine. 2016. CAD2RL: Real SingleImage Flight without a Single Real Image. (11 2016). http://arxiv.org/abs/1611.04201 
[33] Mohammad Fattahi Sani and Ghader Karimian. 2017. Automatic navigation and landing of an indoor AR. drone quadrotor using ArUco marker and inertial sensors. In 2017 International Conference on Computer and Drone Applications (IConDA). IEEE, 102–107. https://doi.org/10.1109/ICONDA.2017.8270408 
[34] Lucas Vago Santana, Alexandre Santos Brandao, and Mario SarcinelliFilho. 2015. An automatic flight control system for the AR.Drone quadrotor in outdoor environments. In 2015 Workshop on Research, Education and Development of Unmanned Aerial Systems (REDUAS). IEEE, 401–410. https://doi.org/10.1109/REDUAS. 2015.7441033 
[35] Lucas Vago Santana, Alexandre Santos Brandao, and Mario SarcinelliFilho. 2015. Outdoor waypoint navigation with the AR.Drone quadrotor. In 2015 International Conference on Unmanned Aircraft Systems (ICUAS). IEEE, 303–311. https://doi. org/10.1109/ICUAS.2015.7152304 
[36] Sebastian Scherer, Sanjiv Singh, Lyle Chamberlain, and Mike Elgersma. 2008. Flying Fast and Low Among Obstacles: Methodology and Experiments. The International Journal of Robotics Research 27, 5 (5 2008), 549–574. https://doi.org/ 10.1177/0278364908090949 
11 
Kasper Østergaard Helsted Department of Computer Science Aalborg University, Denmark khelst13@student.aau.dk 
Steffen Darby Carlsen Department of Computer Science Aalborg University, Denmark sdca14@student.aau.dk 
ABSTRACT 
During the last 10-15 years, the world of robots has focused on the use of drones to explore areas inaccessible to humans, search/rescue and remote inspections. In this context, research has been made in regards to how autonomous drones should fly and how we interact with outdoor drones, but only limited work has focused on autonomous indoor drones and how we interact with these. In this paper, we present a 7 participant 3-parts exploratory study into autonomous indoor HumanDrone Interaction. Our tasks focused on primary interaction during navigation, voice interaction and secondary interaction. Our results show that participants expected the drone to behave under a level of trust. Trust was a metaphor expressed by some of the participants, they related this word to the behavior of the drone and how flawless they expected it to be. Our study outlines a series of design insights for future interaction development of indoor autonomous drones. 
KEYWORDS 
Drones, HumanComputer Interaction, HumanDrone Interaction, Indoor navigation, Indoor Interaction, Autonomous flight, Secondary interaction, Voice interaction, Bebop 
1 INTRODUCTION 
During the last 10-15 years, the world of robotics has largely focused on using teleoperated mobile robots equipped with cameras to get their eyes on something out of reach [25]. While in more recent times, drones has been used in regards to exploring areas completely inaccessible to humans[1], search/rescue [13] and remote inspections [22, 23]. In this context, researchers have proposed methods and approaches to autonomous outdoor drone navigation in regards to earthquakes[19], forest navigation[11] and tunnels [21, 22]. However, the area of indoor navigation and general use of autonomous indoor drones is rarely investigated. These fully autonomous drones interact differently with people and requires considerations and design decisions beyond the natural interaction techniques researched in regards to outdoor drones. The skillset required for a human teleoperator of an indoor drone includes: Being able to control the drone in an indoor environment, avoiding collisions with surrounding obstacles and to interpret the intentions of other people in the indoor environment. Given this, our work explores the use of drones in an indoor environment from a perspective of different types of interaction. Our results show that drones have a use in indoor environments, in particular in terms of indoor navigation. Furthermore, that interaction with drones requires trust, which can be achieved through the 
behaviour and flawlessness of the drone. Lastly, voice interaction with a drone performing micro movements requires the drone to understand advanced commands, while macro movements, such as moving between locations, were performed using similar commands to a GPS. 
2 RELATED WORK 
The interaction between humans and drones bring a new type interaction into the area of HumanRobot Interaction by adding an additional dimension. Being able to change the height of the robot interacted with, changes the interpretation of the interaction the user has. In the following, we will outline previous HumanDrone Interaction in regards to appeal, which Baytas et al. defined as: ""Are people willing to accept, acquire, and/or use a drone or drones, as designed, for the purpose under investigation? Do people feel psychologically comfortable and safe in interacting with the drone(s) or simply cohabiting the same environment? Do they have confidence that the drone will not inflict damage or otherwise misbehave? [5]"" Their definition of appeal and the context to drones is the primary factor investigated in this exploratory study. Thus we will, based on their definition, investigate the related work in this context. Lastly, interaction from the perspective of voice interaction is investigated in the related work. 
2.1 Appeal 
The differences between humans in regards to the level of comfort during interaction with drones are often associated with the impression of safety and stability, which are influenced by the design decisions of the drone. The understanding of what comforts people changes drastically depending on the background of the person [3, 12, 14]. Research has shown that drones meant for close range interaction should be small, as the noise and large amounts of airflow from the propellers can annoy the user causing discomfort [4, 6, 7]. The height of which a drone operates changes the comfort level of the users interacting with the drone. In a comparison between two heights, 120cm and 180cm, it was shown that participants preferred a height of 120cm [28]. Similar results was found by Han et al. who found that users allow a drone at eye to level closer than a drone above their heads [12]. Furthermore, the visual appearance of the drone is an important aspect of appeal. Different studies has investigated the impact of altering the visual appearance of the drone [16]. In contrast to the visual appearance of the drone, the movements of the drone has a big impact on the perception of comfort. Here, 
1 
studies have shown that fast mechanical and nonsmoothed movements are repellent; while steady and smooth drone movements give a positive perception of the drone. Specifically, the undesired movements often includes lack of compensation of wind changes, causing the drone to perform unwanted and often drastic changes in short periods of time, while the positive perceptions come from smooth and stable movements, such as automatic movement pans [17, 24]. The last notion of appeal is privacy and social acceptance. This issue is highlighted by the Electronic Privacy Information Center, who highlights the issue of privacy and surveillance as a concern [2]. While many studies show that users are comfortable when interacting with drones, some users showed concern in how drones will integrate into public settings. Yao et al. interviewed drone pilots in the US, to understand their privacy perceptions and practices of drones [26]. They found that drone pilots consider privacy issues very important when they fly and consider the media to overstate drones as a privacy problem. This contradicts their own previous work, where they concluded that bystanders of drones were in search of different mechanisms to further privacy [27]. The results of bystanders showing a larger concern with regards to drone usage, is common within privacy research in regards to drones [7, 10, 15]. 
2.2 Voice interaction 
Throughout the related work in regards to interaction, there is a fairly large body of work focusing on smartphone applications and joystickbased controllers. These are seen as the common interaction techniques with regard to drone piloting. In recent times, investigations and exploratory studies have focused on the use of gestures and voice control to communicate with drones [6, 8, 9, 20]. In the exploratory studies, it has been found that these two interaction techniques are the preferred techniques when interacting with social drones, with a relation to the interaction context and the personal familiarity of the participant with drones. Often, participants would display initial discomfort when interaction with drones using voice, but over time become comfortable with voice interactions and in some studies, where participants could choose between voice and gesture interactions, end up preferring to use voice, explaining that the interaction became similar to interacting with a pet [18]. 
3 EXPERIMENT 
The contribution of this paper is a exploratory study into the world of autonomous drones using Wizard of Oz (WoZ). We performed three different parts with 7 participants, each providing some insight into design decisions for the future of HDI. Our aim was to emulate autonomous drone behavior in order to gain insight into how users choose to interact with a drone in indoor scenarios. In the following, we describe the three parts and and their results. 
Indoor navigation We simulated autonomous behaviour of a drone in an indoor environment. We chose an open lobby area as the initial startoff point, to perform a safe takeoff and allow the user to better determine what they deemed a safe distance. The experimenter stayed behind the user, but could not be fully hidden, as they needed direct line 
of sight to the drone to properly control it. We found that with WoZ, users showed vastly different opinions in regards to safety and usefulness of an autonomous navigation drone. The scenario and task was explained verbally to user seconds before starting and the user was asked if they understood the task at hand. 
Indoor area investigation In order to test the interaction between a user and a voice controlled drone in an investigation scenario, two tasks were created. The user is told that they are sitting in their office, where they suddenly remember that they forgot their coffee machine and bag of coffee in another room. Instead of going to pick it up themselves, they send their drone to investigate and find them. Here they are to complete 2 tasks: Indoor navigation and Area investigation. 
Secondary Interaction In order to simulate a realworld scenario, the environment chosen mimics the worst case scenario. The environment chosen for the test, is a narrow hallway, where the user is asked to walk past the drone, while not doing anything in particular. The test was performed using WoZ. This was partly for safety reasons, since in case of a dangerous situation, the pilot would be able to abort the test. The pilot was not hidden, as they needed direct line of sight on the drone, in order to ensure safety of the participant. The scenario and task was explained verbally to the participant, to which the user was asked if they understood the task. 
Figure 1: Indoor map displaying test areas, the red path is navigation test; the blue path is secondary interaction and the purple path is indoor area investigation 
3.1 Participants 
7 volunteers (6 male), 17 to 39 years old (µ = 27) were recruited from our institution and social networks. Their training was in Computer Science, Psychology, Social Sciences, shop trained and a High School student. All of the participants knew what drones were prior to the tests, three of the participants had previously flown a drone. When testing the different parts of the experiment, we have chosen to use a limited amount of volunteers per part. For the first and third part, we are using 5 volunteers and for the second part we are using 2 volunteers. 
2 
3.2 Apparatus 
In this section we describe the drone used for our experiment. The drone used for our experiment is a Parrot Bebop 2 (42cm x 39cm x 9cm). In order to increase safety of our participants, we have modified the drone with propeller guards. The drone can be seen in Figure 2. 
Figure 2: The Parrot Bebop 2 with mounted propeller guards 
The propeller guards are mounted on the base of each propeller, and ensures that if the drone hits a wall or a person, the guard takes the blow, and not the propeller, thus minimizing damage to the drone and its surroundings. The drone has been limited in terms of speed and acceleration, these limitations are created for the drone to easier be controlled in limited space. The limitations set on the drone are the minimum in all directions, except yaw, where the recommended speed are used. 
3.3 Procedure 
The experiment lasted 30 minutes to 45 minutes. One of the experimenters performed all of the communication with the participant before the experiment and recorded during the experiment. The other experimenter controlled the drone and asked followup questions during the interview. The participant was told verbally about each of the tasks before the task started by reading the scenario aloud. They were asked if they understood the scenario and to ask the experimenters if they had any questions about the task. At this point the task would begin. After each task, the participant was interviewed about their interaction. If the participant was partaking in the navigation and secondary interaction task, the order of which task they performed first was counterbalanced according to a Latin Square design. 
3.4 Environment 
Our experiment was carried out at the Computer Science building of Aalborg University, Cassiopeia. The experiment consisted of three different parts, indoor navigation, indoor area investigation a and secondary interaction. The indoor navigation starts in the lobby of the building and moves towards room 0.1.32. The route is marked as the red path in Figure 1. This route was chosen since it starts from a location a user would be expected to start navigation from, and ends in an area where multiple turns has been taken. Lastly, the area flown in was also chosen due to being secluded from the group room clusters and having multiple doors at the ending location. The indoor area investigation starts outside of a cluster and moves towards room 2.1.03, where the participant will perform the 
room investigation. The route is marked as the purple path, which can be seen in Figure 1. The route was chosen since it requires multiple voice commands in order to navigate and it allows participants to move the drone in all directions and through both wide and narrow areas. 
Figure 3: The indoor area investigation and the object to be found 
Inside of the room, the participant had to find two objects, a coffee machine and a coffee bag, which can be seen in Figure 3. We chose to hide the objects at different heights and to make them hard to spot in general. This was done to make the investigation of the area nontrivial and force the participant to control the drone in all dimensions. The areas where the two objects were hidden could not be found without changing the default height of the drone nor without actively navigating beyond the center of the room. The last part, the secondary interaction was done in one of the hallways inside the Cassiopeia. The hallway measures 1.56m x 12m. The path is marked on Figure 1 with a blue path. The participant starts at the end of the blue line, furthest away from the circle marked with red in Figure 1 and were asked to walk around the corner, assess the situation, and act as they would normally. 
4 TASKS 
In order to test the interaction with an autonomous drone in an indoor environment, a series of different tasks was created. Each task tests its own approach to indoor drone interaction with an autonomous drone. 
Indoor navigation In the indoor navigation task, the user has to be navigated from the entrance of a building to a target room. The user takes out their smartphone and starts the navigation of the drone. After the drone takes off, the user follows the drone in their preferred way. 
Indoor area investigation In the indoor area investigation task, the user has forgotten two objects at a previously visited location and wants to find these objects again. The user has connected to their remote voice controlled drone and wants to investigate the area. 
3 
First, the user has to tell the drone to perform a takeoff, after which it will follow their commands, moving towards a group room. This part of the task requires the user to determine how they wish to control the drone when performing broad a navigational task, which can be seen as similar to a GPS giving you directions. Second part of the task is after arriving at the target location, a group room. The user has to fly into the room and find two objects, a coffee bag and a coffee machine. These objects are hidden in such a manner that they can not be found without using all types of movements the drone can perform. 
Secondary Interaction In the secondary interaction task, the user has to interact with the drone in a narrow hallway under different scenarios. The user is asked to wait around a corner of the hallway and when their hear the drone taking off, count to 10 (allowing the experimenters to properly stabilize the drone and prepare it for the next scenario) before moving around the corner towards the end of the hallway. In the center of the hallway, the user will meet the drone in different scenarios, these being: 
• On contact, land. • Continue past the participant • On contact, stop movement and await the participant to pass 
These scenarios were performed at two different heights depending on the height of the participant, at their knees and at chest height. The starting height was counterbalanced according to a Latin Square design. 
5 FINDINGS 
In this section, we will first present the overall results of the experiment, to then go into specific aspects of the overall results of the exploratory study, by pointing out tendencies in the results and explaining these. When following a drone, the participants expected the drone to know all possible scenarios and know how to properly react. This includes, always giving a 100% correct navigation, even in advanced buildings and to correctly handle larger crowds of humans in a navigational area. However, how the drone should handle larger crowds, none of the participants knew. In correlation to being able to properly navigate crowded areas, the participants expressed the importance of the safety of any interactors with the drone. One of the repeated expectations of the drone was to detect and react properly to other humans during flight. In the navigation task, two participants said that trust is an important factor when using an autonomous system, like a drone. The participants expect the system to be flawless and always show correct directions and not misinterpret the environment it is flying in. This shows that using and interacting with a fully autonomous drone requires trust. This trust is based on the drone’s ability to perform to its requirements and successfully interpret the intentions of the users. Whether this is for the drone to move faster, steer around ongoing people or land. In regards to interpreting the intentions of people, the secondary interaction task showed that the participants preferred when the drone would move to the side and hold still, sharing the space similar to that of another person. In correlation to the general behaviour of the drone, the secondary interaction 
task showed that the participants preferred when the drone was at chest height. However, two of the participants disagreed with this, saying that the lower the drone would fly, the safer they felt. These two participants were also visible scared in the recordings of the task, showing that as users become more comfortable with the drone, the more they wish to have the drone in a similar height to how they usually interact with people. When the participants where asked to interact with a drone using voice commands, two different types of commands were observed. The first type of interaction was a macro focused interaction. Here, the participants chose to use simple, general directional commands, such as; Move left, move right and move to the intersection. Considering that the participants were familiar with the environment and that there were only two intersections where they had to choose a direction, it makes sense that they choose to navigate using macro commands in this area. However, considering the wording using when explaining the tasks, explicitly telling them the room number they were flying to, it is surprising to us that none of them used the drone as a GPS, that they would tell where to move to, rather than telling it how to move. After getting to the room, the participants had to find two objects. At this point, they changed their interaction to micro adjustments, telling the drone to rotate slightly right. Another noticeable difference was that they chose to, instead of telling the drone how to move, told it what to look at. An example of a command used by a participant was: “I want to take a closer look at the windowsill over there”. This type of command show, that in order for a drone to properly cater to voice commands, it will need to have a certain level of understanding of natural language. In general, the interactions with the followme navigation drone were simple and similar between the participants. They would choose their destination on the application and slowly follow behind the drone. During the straight paths of the drone’s route, they would follow the drone relatively close, keeping up with the drone in terms of tempo. When at the intersection, they would prematurely slow down, keeping distance from the drone and waiting for the drone to manoeuvre the corner. It shows, that when the drone had to perform a different action than move straight, that users chose to keep their distance, waiting for the drone to finish its action. The users expressed that a drone, depending on the environment it is used in, is a functional navigational tool. In particular, if the indoor environment is large enough and the infrastructure of the area needing navigation allows for it. However, there are certain expectations to the abilities and behaviour of the drone. The drone should follow the pace of the follower, while not exceeding a regular walking pace. Furthermore, it should be able to handle the environment and any changes that happen to it during the navigation. This includes meeting other people and moving objects. 
6 DISCUSSION 
We made an exploratory study into the world of autonomous drone flight by deploying a series of different WoZ tasks. The tasks focus primarily on the interaction between a user and a working autonomous drone in a navigation context. Throughout this study, we found trends in terms of the interaction techniques and expectations of drone behaviour from the feedback of the participants and 
4 
the analysis of video footage. This section presents and discusses design insights based on these trends. 
6.1 Drone behaviour 
Several different expectations in terms of drone behaviour was observed through the interviews and task recordings. We observed that users expected the drone to act under a level of trust and that the height of the drone and interaction action is important to consider. 
Trust One of the repeating metaphors, said explicitly by two participants, was that the interaction with an indoor drone requires a high level of trust. We observed this through the interviews of the participants, where they expected the drone to react properly in all situations, where situations range from meeting people during indoor navigation to always correctly navigating the user to their destination. The last point of trust mentioned by the participants is in regards to the speed of the drone. They wanted consistent and predictable movement, that they could follow while walking. The system the participants searched for was a flawless, consistent and predictable system, which shows that if drones were to be used for indoor tasks, that the development of specialised drones, that only complete a singular task, although is very good at this particular task, is a potential future area of research. Similar results have been found by other researchers, who found that the noise and airflow from the propellers can cause discomfort for the users [4, 6, 7]. In regards to the consistency and predictability of drone movements, related work found that users expect smooth and stable movements by the drone [17, 24]. This shows that even in different scenarios, the fundamentals of interactions with drones remain the same and that trusting a drone is an important aspect of ensuring a user is comfortable. 
Height and safety A variable modified during the secondary interaction task was height. The participants met the drone in a narrow hallway, where it would perform one of three actions at two different heights. Here, the participants preferred the drone to be at chest height, saying that although the drone is able to do more damage when at chest height, that being able to keep direct line of sight with the drone is more important than having the drone in the proximity of the least damaging area of the body. Two of the participants fully disagreed with this testimony, saying that they preferred the drone as low it could get. This shows that although our task showed a slight favour in regards to the drone being at chest height, that future research can further task with how users prefer to interact with a fully autonomous drone in indoor environments. Related work, primarily focusing on the concept of a social drone, have found that users feel more comfortable around a drone between 1.2m and eye level of the participant [12? ]. Although our work used two different heights, it still shows some connection between height and feel of safety. 
Interaction The last behavioural expectation observed during the experiment is in regards to the action performed by the drone when approached by a person in a hallway. Here, our results show that 3 out of 5 participants preferred the drone to pull over and wait for them to pass the drone, while the last 2 wanted the drone to land. Expanding upon this task, by using areas of different sizes and a drone developed for indoor usage might yield more consistent results. This is largely because of the participants preferring the drone to hold still in the air, mentioning that the sound of the drone changed drastically when performing a landing, making it seem more scary. This could be counteracted by using a smaller form factor of a drone, perhaps designed for indoor use. The related work has shown that users prefer a drone in an indoor environment to be small, as a small drone generates less noise and air [4, 6, 7]. 
6.2 Voice interaction 
Two types of interaction methods were used by the participants during the voice interaction tasks. This was either macro or micro adjustments of the drone. The type of command changed how the user interacted with the drone, going from general and vague commands to specific, object targeted commands. 
Natural language understanding Macro commands were used by the participants when moving between locations, the commands consisted of simple directional commands, similar to how a GPS proposes directions. During this interaction, the drone was never told how to move, rather which direction it should move. This was changed after entering the room, where the commands changed into how the drone should move or commands specifying where the drone should look. This shows, that if the drone needs to perform micro adjustments, a greater level of natural language processing is required in order for a user to feel understood in their interaction with the drone. On the other hand, the simplicity of the directional commands makes it easier to have a drone perform navigational tasks using voice commands. One of the comparisons used in regards to drones, is that it is similar to that of a pet during interaction [18]. This shows that the level of understanding necessary to properly control a drone in an indoor environment, can be compared to the same communication level of interaction with a pet. This can, in future work, be used as a guideline of how much natural language understanding is necessary in the context of voice interaction with a drone. 
7 CONCLUSION 
Given the current increase in research in regards to fully autonomous indoor drones, we need to create and investigate natural interaction techniques to best support users. We executed one exploratory experiment with 7 users performing three different tasks and found design principles with high agreement among participants. We found several similar ties between participants in their expectations to how they interact with a drone, both when directly interacting and during secondary interaction. We contribute a set of design insights to develop the field of autonomous indoor HumanDrone Interaction. 
5 
In the future, there are primarily two points of interest based on our research. The first is to primarily focus on the areas where our exploratory study showed to be statistical insignificant. This is especially in regards to the preferred height of the drone, where our study showed mixed results between knee and chest height. The second point of interest is to use the proposed design insights and use those in the development of an autonomous indoor flight system for drones. The understanding of how a user expects a drone to behave and interact with an user, both in regards to direct interaction, during navigation, and secondary interaction, can be used to design a future system that is not only able to fulfil its goal, but also designed for human interaction. Due to the complications of flying in confined spaces, some of the participants noticed that an experimenter flew the drone, rather than an automated system. We believe that this might cause some of the participants to feel safer than they really were. In order to further investigate and validate the results of our study, recreating our experiment with a fully autonomous drone might yield different results compared to ours. 
REFERENCES 
[1] [n. d.]. Drones help write new history of Caribbean – ScienceDaily. https: //www.sciencedaily.com/releases/2016/10/161017123746.htm 
[2] [n. d.]. EPIC - Domestic Unmanned Aerial Vehicles (UAVs) and Drones. https: //epic.org/privacy/drones/ 
[3] Majed Al Zayer, Sam Tregillus, Jiwan Bhandari, Dave FeilSeifer, and Eelke Folmer. 2016. Exploring the Use of a Drone to Guide Blind Runners. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility - ASSETS ’16. ACM Press, New York, New York, USA, 263–264. https://doi.org/10. 1145/2982142.2982204 
[4] Mauro Avila Soto, Markus Funk, Matthias Hoppe, Robin Boldt, Katrin Wolf, and Niels Henze. 2017. DroneNavigator. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility - ASSETS ’17. ACM Press, New York, New York, USA, 300–304. https://doi.org/10.1145/3132525.3132556 
[5] Mehmet Aydin Baytas, Damla Çay, Yuchong Zhang, Mohammad Obaid, Asim Evren Yantaç, and Morten Fjeld. 2019. The Design of Social Drones. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19. ACM Press, New York, New York, USA, 1–13. https://doi.org/10.1145/ 3290605.3300480 
[6] Jessica R. Cauchard, Jane L. E, Kevin Y. Zhai, and James A. Landay. 2015. Drone and me. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing - UbiComp ’15. ACM Press, New York, New York, USA, 361–365. https://doi.org/10.1145/2750858.2805823 
[7] Victoria Chang, Pramod Chundury, and Marshini Chetty. 2017. Spiders in the Sky. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ’17. ACM Press, New York, New York, USA, 6765–6776. https: //doi.org/10.1145/3025453.3025632 
[8] ChienFang Chen, KangPing Liu, and NengHao Yu. 2015. Exploring interaction modalities for a selfie drone. In SIGGRAPH Asia 2015 Posters on - SA ’15. ACM Press, New York, New York, USA, 1–2. https://doi.org/10.1145/2820926.2820965 
[9] Jane L. E, Ilene L. E, James A. Landay, and Jessica R. Cauchard. 2017. Drone and Wo. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ’17. ACM Press, New York, New York, USA, 6794–6799. https: //doi.org/10.1145/3025453.3025755 
[10] Markus Funk and Markus. 2018. Humandrone interaction let’s get ready for flying user interfaces! Interactions 25, 3 (4 2018), 78–81. https://doi.org/10.1145/ 3194317 
[11] Alessandro Giusti, Jerome Guzzi, Dan C. Ciresan, FangLin He, Juan P. Rodriguez, Flavio Fontana, Matthias Faessler, Christian Forster, Jurgen Schmidhuber, Gianni Di Caro, Davide Scaramuzza, and Luca M. Gambardella. 2016. A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots. IEEE Robotics and Automation Letters 1, 2 (7 2016), 661–667. https://doi.org/10.1109/LRA.2015.2509024 
[12] Jeonghye Han and Ilhan Bae. 2018. Social Proxemics of HumanDrone Interaction. In Companion of the 2018 ACM/IEEE International Conference on HumanRobot Interaction - HRI ’18. ACM Press, New York, New York, USA, 376–376. https: //doi.org/10.1145/3173386.3177527 
[13] YaoHua Ho, YuRen Chen, and LingJyh Chen. 2015. Krypto. In Proceedings of the First Workshop on Micro Aerial Vehicle Networks, Systems, and Applications for Civilian Use - DroNet ’15. ACM Press, New York, New York, USA, 3–8. https: 
//doi.org/10.1145/2750675.2750684 
[14] Walther Jensen, Simon Hansen, and Hendrik Knoche. 2018. Knowing You, Seeing Me. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18. ACM Press, New York, New York, USA, 1–12. https://doi.org/ 10.1145/3173574.3173939 
[15] Brennan Jones, Kody Dillman, Richard Tang, Anthony Tang, Ehud Sharlin, Lora Oehlberg, Carman Neustaedter, and Scott Bateman. 2016. Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones. In Proceedings of the 2016 ACM Conference on Designing Interactive Systems - DIS ’16. ACM Press, New York, New York, USA, 1123–1135. https: //doi.org/10.1145/2901790.2901847 
[16] Kari Daniel Karjalainen, Anna Elisabeth Sofia Romell, Photchara Ratsamee, Asim Evren Yantac, Morten Fjeld, and Mohammad Obaid. 2017. Social Drone Companion for the Home Environment. In Proceedings of the 5th International Conference on Human Agent Interaction - HAI ’17. ACM Press, New York, New York, USA, 89–96. https://doi.org/10.1145/3125739.3125774 
[17] Bomyeong Kim, Hyun Young Kim, and Jinwoo Kim. 2016. Getting home safely with drone. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing Adjunct - UbiComp ’16. ACM Press, New York, New York, USA, 117–120. https://doi.org/10.1145/2968219.2971426 
[18] Hyun Young Kim, Bomyeong Kim, and Jinwoo Kim. 2016. The Naughty Drone. In Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication - IMCOM ’16. ACM Press, New York, New York, USA, 1–6. https://doi.org/10.1145/2857546.2857639 
[19] Tomas Krajník, Vojtech Vonásek, Daniel Fišer, and Jan Faigl. 2011. ARDrone as a Platform for Robotic Research and Education. Springer, Berlin, Heidelberg, 172–186. https://doi.org/10.1007/978-3-642-21975-7{_}16 
[20] Mohammad Obaid, Felix Kistler, Gabriele Kasparaviči¯ut˙e, Asim Evren Yantaç, and Morten Fjeld. 2016. How would you gesture navigate a drone?. In Proceedings of the 20th International Academic Mindtrek Conference on - AcademicMindtrek ’16. ACM Press, New York, New York, USA, 113–121. https://doi.org/10.1145/ 2994310.2994348 
[21] Tolga Ozaslan, Giuseppe Loianno, James Keller, Camillo J. Taylor, Vijay Kumar, Jennifer M. Wozencraft, and Thomas Hood. 2017. Autonomous Navigation and Mapping for Inspection of Penstocks and Tunnels With MAVs. IEEE Robotics and Automation Letters 2, 3 (7 2017), 1740–1747. https://doi.org/10.1109/LRA.2017. 2699790 
[22] Tolga Özaslan, Shaojie Shen, Yash Mulgaonkar, Nathan Michael, and Vijay Kumar. 2015. Inspection of Penstocks and Featureless Tunnellike Environments Using Micro UAVs. Springer, Cham, 123–136. https://doi.org/10.1007/ 978-3-319-07488-7{_}9 
[23] Chathura Suduwella, Akarshani Amarasinghe, Lasith Niroshan, Charith Elvitigala, Kasun De Zoysa, and Chamath Keppetiyagama. 2017. Identifying Mosquito Breeding Sites via Drone Images. In Proceedings of the 3rd Workshop on Micro Aerial Vehicle Networks, Systems, and Applications - DroNet ’17. ACM Press, New York, New York, USA, 27–30. https://doi.org/10.1145/3086439.3086442 
[24] Daniel Szafir, Bilge Mutlu, and Terrence Fong. 2014. Communication of intent in assistive free flyers. In Proceedings of the 2014 ACM/IEEE international conference on Humanrobot interaction - HRI ’14. ACM Press, New York, New York, USA, 358–365. https://doi.org/10.1145/2559636.2559672 
[25] Waypoint Robotics. 2019. What Are Autonomous Robots? https:// waypointrobotics.com/blog/whatautonomousrobots/ 
[26] Yaxing Yao, Huichuan Xia, Yun Huang, and Yang Wang. 2017. Free to Fly in Public Spaces. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ’17. ACM Press, New York, New York, USA, 6789–6793. https://doi.org/10.1145/3025453.3026049 
[27] Yaxing Yao, Huichuan Xia, Yun Huang, and Yang Wang. 2017. Privacy Mechanisms for Drones. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ’17. ACM Press, New York, New York, USA, 6777–6788. https://doi.org/10.1145/3025453.3025907 
[28] Alexander Yeh, Photchara Ratsamee, Kiyoshi Kiyokawa, Yuki Uranishi, Tomohiro Mashita, Haruo Takemura, Morten Fjeld, and Mohammad Obaid. 2017. Exploring Proxemics for HumanDrone Interaction. In Proceedings of the 5th International Conference on Human Agent Interaction - HAI ’17. ACM Press, New York, New York, USA, 81–88. https://doi.org/10.1145/3125739.3125773 
6 
",HCI102f9_Master_Thesis___SharpFlying.pdf,3
"https://doi.org/10.1007/s10846-022-01676-3 
REGULAR PAPER 
Trajectory Planning and Optimization for Minimizing Uncertainty  in Persistent Monitoring Applications 
Michael Ostertag1  · Nikolay Atanasov1 · Tajana Rosing2 
Received: 6 January 2022 / Accepted: 10 June 2022  © Springer Nature B.V. 2022 
Abstract This paper considers persistent monitoring of environmental phenomena using unmanned aerial vehicles (UAVs). The  objective is to generate periodic dynamically feasible UAV trajectories that minimize the estimation uncertainty at a set of  points of interest in the environment. We develop an optimization algorithm that iterates between determining the observation periods for a set of ordered points of interest and optimizing a continuous UAV trajectory to meet the required observation periods and UAV dynamics constraints. The interestpoint visitation order is determined using a Traveling Salesman  Problem (TSP), followed by a greedy optimization algorithm to determine the number of observations that minimizes the  maximum steadystate eigenvalue of a Kalman filter estimator. Given the interestpoint observation periods and visitation  order, a minimumjerk trajectory is generated from a bilevel optimization, formulated as a convex quadratically constrained  quadratic program. The resulting Bspline trajectory is guaranteed to be feasible, meeting the observation duration, maximum velocity and acceleration, region enter and exit constraints. The feasible trajectories outperform existing methods by  achieving comparable observability at up to 47% higher travel speeds, resulting in lower maximum estimation uncertainty. 
Keywords Informative path planning · Persistent monitoring · Aerial systems: perception and autonomy 
1 Introduction 
Persistent sensing and data collection is important for tracking environmental phenomena such as atmospheric pollution  [1, 2] and wildfire ignition at the wildlandurban interface  [3, 4]. Early detection of dangerous conditions is a critical factor in mitigating damages to ecological systems and  human infrastructure [5]. Persistent monitoring methods  have utilized satellite data, humanpiloted aircraft, and networks of stationary sensors, optimizing their placement for a  
variety of resource constraints [6–9]. Satellite data is often at  too low resolution (e.g. pixel widths of 1 km for MODIS and  375 m for VIIRS) or too infrequent to detect rapidly emerging trends [10–13]. Regular flights of humanpiloted vehicles  is useful for ongoing tracking efforts in localized regions but  is too expensive for largescale monitoring. Dense sensor  deployments can monitor the environment effectively but as  the size of the region grows, sensor network deployment and  maintenance can quickly become cost prohibitive. Stationary  sensing networks also suffer from information redundancy  since many environmental phenomena are highly temporally  and spatially correlated. Recent persistent monitoring research has focused on  developing motion planning and control techniques for  mobile sensing robots, such as unmanned aerial vehicles  (UAVs), that can monitor large environments and deliver  highfrequency, highresolution data more effectively  [14–22]. Technological advancements in sensing, computation, and communication enable UAVs to supply critical  realtime data for a variety of applications, depending on the  equipped visual, thermal, chemical, or depth sensors. The goal of persistent monitoring is to minimize estimation uncertainty by planning an optimal trajectory for  
  * Michael Ostertag  mosterta@ucsd.edu 
  Nikolay Atanasov  
  natanasov@eng.ucsd.edu 
  Tajana Rosing  
  tajana@ucsd.edu 
1  Department of Electrical and Computer Engineering,  University of California, San Diego, 9500 Gilman Drive,  La Jolla 92093, CA, USA 
2  Department of Computer Science and Engineering,  University of California, San Diego, 9500 Gilman Drive,  La Jolla 92093, CA, USA 
/ Published online: 16 August 2022 
Journal of Intelligent & Robotic Systems (2022) 106: 2 
the sensing platforms. Probabilistic filtering techniques  are employed to integrate the sensed information over time  into an estimate the signal of interest. Kalman filtering is  commonly used [22, 23], including generalizations such  as extended and unscented Kalman filtering [24], particle  filtering [25], and variational inference [26]. Researchers in persistent monitoring have used a variety  of objective functions for quantifying the effectiveness of  an informative trajectory. We chose to minimize the maximum eigenvalue of the steadystate covariance matrix,  which intuitively represents a bound on the uncertainty of  the model at any POI and is commonly referred to as the  spectral radius or Eoptimality [27]. Other potential objective functions include the trace of the covariance matrix,  which is the sum of the uncertainty, or the determinant of  the covariance matrix, which is the volume of uncertainty  [28]. By optimizing for the steadystate uncertainty, the  developed controller may perform suboptimally for initial  cycles of a periodic traversal of a trajectory, but it can be  shown that the uncertainty evolution converges at an exponential rate from any initial covariance to a cyclic pattern  of values (Sec IV.A in [29]). Our previous work [22] proposes an initial solution  for a UAV flying along a prescribed sensing trajectory.  The objective was to plan a velocity profile along the path  that minimizes the maximum eigenvalue of the estimation  covariance matrix of a Kalman filter. In contrast to other  works, which only optimize to minimize the revisitation  frequency [30], the method balances the number of consecutive measurements at a point with the frequency of  visitation. While the generated trajectory obeyed maximum velocity constraints of the UAV, the sensing trajectory was not continuous, and resulted in paths that were  infeasible to follow (Fig. 1). 
Contributions We present a planner for feasible trajectories  for persistent monitoring with UAVs. Modern approaches  [14, 22, 23, 31] ignore the dynamics of monitoring platforms, resulting in violations of full observability assumptions during simulations with controllers that track a trajectory. In contrast, our proposed feasible Trajectory  Optimization for Persistent monitoring (fTOP) plans a  trajectory, adhering to a UAV’s dynamic constraints while  minimizing estimation uncertainty, with the following  contributions: 
1. Bilevel optimization that plans a minimumjerk, three  times continuously differentiable trajectory that minimizes steadystate estimation uncertainty, 2. Greedy Knockdown Algorithm to determine the optimal number of observations of each POI to minimize  worstcase bound on estimation uncertainty for periodic  sampling, 
3. Secondorder cone program (SOCP) for calculating  Bspline trajectory coefficients that enforce constraints  of vehicle dynamics. 
The remaining paper is organized as follows. Section 2  outlines related work relevant to trajectory planning for persistent monitoring. Section 3 describes the problem formulation. Section 4 provides background information necessary  for the formulation of our optimization. Section 5 describes  our optimization approach for planning feasible trajectories.  Section 6 compares our proposed method against baseline  and existing works. The paper concludes with Section 7. 
2  Related Work 
Our contribution is at an intersection of several related  research areas in persistent monitoring. Inspired by the periodic sensing results from the stationary sensing literature [8,  9], we extend Kalman filter convergence results to persistent  monitoring scenarios. We focus on planning dynamically  feasible trajectories for robotic sensing platforms that better meet observability assumptions, which are commonly  assumed but often unachieveable in existing works without  significant reduction in robot performance. In this section,  we provide a brief overview of relevant literature in sensor  scheduling, trajectory planning for persistent monitoring,  and feasible trajectories. 
Fig. 1  Our proposed fTOP algorithm attempts to plan trajectories  that adhere to the dynamic constraints of the system, resulting in trajectories that can be tracked and remaining within the sensing distance of a POI. The FirstOrder and Direct Planner generate infeasible trajectories with discontinuities in the velocity profile 
2   Page 2 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
2.1  Sensor Scheduling 
In sensor scheduling, a group of stationary sensors attempts  to measure and approximate a phenomenon in a resourceefficient manner by selecting a subset of sensors to sample at  each time step. Initial work calculates bounds on the estimation uncertainty by forming sensor scheduling as a convex  problem problem and creating heuristic and openloop policies [6, 32]. Zhao et al. [29] advance the sensor scheduling  problem for infinite time horizons, proving that for linear  Gaussian processes and sensing functions the steadystate  solution is independent of initial estimation covariance and  the optimal estimation can be approximated arbitrarily close  by a finite, periodic schedule. Greedy scheduling algorithms  work well in many cases, approaching optimal results at  reduced computational complexity. Jawaid and Smith [7]  leverage submodularity of the estimation error to show that  greedy approaches can approach optimal results, and additional efforts prove greedy algorithms can be optimal for  specific cases of uncorrelated noise [8, 9]. The primary difference between persistent monitoring  with robotic agents and sensor scheduling is that there is  a nonnegligible switching cost between targets for robotic  sensing platforms, which must physically move to measure  new regions. Despite the difference, we can leverage and  extend sensor scheduling results [9, 29] by modeling our  POIs as uncorrelated and proposing a finite, periodic schedule with an additional period where no observations occur. 
2.2  Trajectory Planning for Persistent Monitoring 
Instead of relying on fixedposition sensors for estimation,  an alternate costeffective approach for sensing across a large  area is UAVmounted sensors [33]. The simplest formulation  of persistent monitoring a set of discrete POIs is the patrol  problem [34], where every POI must be visited with the  objective of minimizing time between revisits. Minimizing  time between revisits results in a nearoptimal strategy when  all POIs have equivalent uncertainty in state transitions, but  are suboptimal when the POIs have different uncertainty  characteristics. For POIs with varying rates of uncertainty, persistent  monitoring trajectories can be planned that optimize one of  several different metrics of uncertainty, such as maximum  eigenvalue, log determinant, or trace of an estimate covariance matrix [28]. Decomposing trajectory generation into  separate path planning and velocity control problems allows  for developing optimal solutions for constrained problems  with reduced complexity [35]. The first approaches for planning persistent monitoring trajectories optimize wait times,  visit orders, and velocity along a given 1-dimensional path. For a given 1-dimensional path, the persistent monitoring  problem can be simplified into calculating a required length  
of observation time at each POI prior to travelling to the  next. The length of observation time is modelled based on  different accumulation and clearing models of uncertainty.  Smith et al. [14] represent uncertainty at POIs is modelled  as a linear, continuous accumulation model with uncertainty  reduction directly proportional to the time spent measuring a  point, planning a periodic velocity as a composition of basis  functions that minimized the maximum accumulation value.  Similar works expand the concepts to different uncertainty  accumulation models that clear in linear proportion to the  distance of nearby robotic platforms [15] and for symmetric,  nonlinear accumulation models [16]. Ostertag et al. [22]  plans a velocity along a prescribed trajectory by determining the optimal dwell times using a greedy application of  an Kalman filter at an infinitetime horizon. Researchers  have planned trajectories in twodimensional space using  the concept of infinitesimal perturbation analysis, which  approximates the gradient of the cost function but with limited guarantees of optimality [18, 20]. Related to persistent monitoring is the field of informative trajectory planning where a robotic agent optimizes for  information gain while travelling through an environment  but does not necessarily measure POIs periodically or with  full observability. A common formulation is the orienteering  problem [36], whereby an agent travels along a graph and  collects rewards for visiting locations, attempting to maximize the reward with a limited resource spent on each edge  taken. For problems formatted on continuous planes, optimal  solutions are NPhard, but recent work in samplingbased  path planning [19, 37, 38] and finitehorizon, searchbased  path planning [39] show promising results for exploration  problems, which are similar but not immediately applicable  to persistent monitoring. 
2.3  Trajectory Representation for UAVs 
When dealing with real UAV systems, trajectory continuity is important to consider since trajectories with abrupt  changes in velocity or acceleration, such as are common  in simple waypoint paths, will result in high tracking error  when a UAV is operating at high velocities. Continuous trajectories are more complicated to solve, so researchers have  simplified the problem into graph searches using a discrete  number of fixed motion primitives [40, 41]. While these  approaches are useful for planning realtime actions due to  the limited number of movement patterns, they are almost  always suboptimal. To handle the complexity of developing continuous trajectories, researchers have formulated the trajectory optimization as finding coefficients for a set of basis functions. By  limiting to searching for the set of coefficients, the solution  space is sufficiently limited to formulate the problem in a  tractable manner. Objective functions commonly minimize  
Page 3 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
the snap (4th derivative) [40, 42–45] or jerk (3rd derivative)  [46, 47] of the trajectory. Quadrotors are differentially flat  [48], meaning that for a selected set of trajectory parameters,  typically position in three dimensions and yaw, the control  inputs can be precisely calculated from the first three derivatives of the trajectory. Initial work in developing trajectories for quadrotors uses  piecewise polynomial basis functions. The polynomial basis  functions have issues with numerical stability and are difficult to fit to nonlinear constraints on maximum velocity  and acceleration. Mellinger et al. [42] formulate the optimization as a quadratic program optimizing for coefficients of  piecewise twotime continuously differentiable polynomial  basis curves, solving for an arbitrary time range and scaling  to meet any required constraints. The constraints are linear, however, and checked only at discrete time instances,  which results in a large number of linear constraints and  no guarantee of meeting maximum velocity or acceleration  constraints. Mueller et al. [40] resolves the numerical stability of polynomial basis functions by for end point conditions  instead of coefficients, proving a simple mapping between  end point conditions and the coefficients. Richter et al. [44]  improves the speed to solve the optimization by solving an  unconstrained quadratic optimization followed by a check  on constraints. Polynomial bases are numerically illconditioned for complex paths, and as the order of the polynomial  increases to meet pathing demands, properties such as curvature and the extrema of derivatives must be found using  numerical rootfinding methods. To solve the shortcomings of polynomial basis functions,  researchers began exploring Bezier curves and Bsplines.  This class of curves are defined by a set of control points, are  numerically stable, and have beneficial properties of convexity and linear formulation of derivatives. The last property  enables simple calculation for a bound on the maximum  magnitude of lower order derivatives. Bsplines have a limited support for any given point in time with the support  transition in accordance to a set of coefficients called knots.  The limited support enables further numerical stability and  complex maneuvers. Ding et al. [49] leverage the properties  of Bsplines to efficiently solve for a path using a kinodynamic search and elastic optimization to fine tune control  points. Zhou et al. [50] utilize a similar method, but model  the path as a nonuniform Bspline with an iterative time  adjustment method, adjusting the knot spacing to achieve  more aggressive trajectories. Due to the convex hull property  of Bsplines, both methods place constraints on the control  points to avoid collisions. Due to the advantages of numerical stability, continuity,  and complex pathing while meeting dynamic constraints  able to meet dynamic constraints of a robotic platform,  we plan our trajectories by developing optimizations for  the control points of Bsplines. Unlike [50], our proposed  
trajectory optimization minimizes the magnitude of jerk,  which enables simple computation for a 4thorder Bspline. 
3  Problem Formulation 
Given a set of points of interest (POIs) in the environment,  we focus on planning a dynamically feasible trajectory for a  quadrotor robot to estimate a timedependent value of interest at these points with minimum uncertainty. Prior to stating the problem formally, we define the environment model,  quadrotor dynamics, and the sensing model. The paper follows the convention of denoting matrices by capitalized bold  symbols A and vectors by lowercase bold symbols a. 
3.1  Environment Model 
Consider a set of N POIs spread throughout an environment  Q = {퐪1, ..., 퐪N} where 퐪i ∈ ℝ3 . The objective is to track the  values xk = [x1(tk), ..., xN(tk)]⊺ of an environmental phenomenon at the N POIs over discrete time steps tk, tk+ 1, …. Time  is discretized with sampling frequency fs and associated  period f −1 s  such that tk+1 = tk + f −1 s ∀ k ∈ ℤ≥0 . To simplify  notation, variables that depend on time are denoted with a  subscript k where appropriate, e.g. Ak = A(tk). We assume  the values at each location are independent and time varying  according to the following discretized model: 
where mk are independent increments drawn from a Gaussian distribution with mean value change μk and a diagonal  covariance Wf −1 s . 
3.2  Quadrotor Model 
A quadrotor equipped with a sensor is attempting to estimate  the phenomenon at each POI. The state of the quadrotor  is defined by its orientation R ∈ SO(3), worldframe position 퐬 ∈ ℝ3 , velocity ̇퐬 ∈ ℝ3 , acceleration ̈퐬 ∈ ℝ3 , and jerk  ⃛퐬 ∈ ℝ3 . The control inputs of the quadrotor are the bodyframe thrust f ∈ ℝ≥0 and rotational velocity 흎 ∈ ℝ3 , which  are directly related to the rotational velocity of the four  rotors [42]. The quadrotor state evolves with the following  dynamics: 
(1) xk+1 = xk + mk, 
(2) mk ∼ N(흁k, Wf −1 s ), 
(3) ̈퐬 = Re3f∕m + ge3, 
(4) ̇R = R ̂흎, 
2   Page 4 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
where m is the mass of the quadrotor, g is the acceleration  due to gravity, e3 is the unit vector for zaxis in the world  frame, and ̂⋅ ∶ ℝ3 ↦ 픰픬(3) denotes the hat map, which maps  3-D axisangle vectors to a skewsymmetric matrix. The quadrotor system is differentially flat with respect to  its position s and yaw angle ψ states [48]. This means that,  given a desired timeparametrized flat trajectory sd, ψd, we  can recover the control inputs f and ω that will track the  desired trajectory: 
where Rd and ̇Rd are the desired rotation and rotational  velocity. The desired orientation Rd is obtained from rotation about yaw ψd and desired acceleration ad according to  [42]. The desired rotation and acceleration are provided by  the planned trajectory that is generated in Section 5, which  the UAV is attempting to track. The quadrotor has bounds on its velocity and acceleration due to physical constraints. The total thrust that can be  generated by the motors is limited, resulting in a maximum  acceleration bound. To account for the lack of air drag in the  simplified dynamics model, we also introduce a maximum  velocity constraint. Hence, all intended quadrotor trajectories should lie in the admissible set: 
where C3(ℝ, ℝ3) denotes the space of threetimes continuously differentiable functions ℝ ↦ ℝ3 , vmax is the maximum  velocity magnitude, and amax is the maximum acceleration  magnitude. 
3.3  Sensing Model 
A sensor is mounted on the quadrotor that can sense a POI  qi if it is within a given closed, convex region Bi, which represents the union of all UAV positions such that qi is in the  sensor field of view. An example trajectory and observation  region are shown in Fig. 2. The sensor captures observations  with a sampling frequency of fs and each measurement is  corrupted by additive noise: 
(5) f = m‖̈퐬d + ge3‖, 
(6) ̂흎 = R⊤ d ̇Rd, 
(7) S ∶= {퐬 ∈ C3(ℝ, ℝ3) ∣ ‖̇퐬‖ ≤ vmax, ‖̈퐬‖ ≤ amax}, 
(8) yk = 퐇kxk + 휼, 
(9) 휼 ∼ N(0, V), 
(10) 퐇i,j(tk) = 
{ 
1, i = j and 퐬(tk) ∈ Bi, 
0, otherwise, 
where Hk is a binary observation matrix and η is additive,  Gaussian observation noise with a positive semidefinite  covariance matrix V. The matrix Hk models the sensor field  of view by assigning 1 to visible POIs and 0, otherwise.  Equation 8 models a sensor that provides direct measurements of the features xk at the POIs. For example, a downwardfacing visiblelight or thermal camera used for vegetation imaging or temperature estimation can be modeled by  Eq. 8. The true value of the environmental phenomenon is continuous in time but due to the discretetime observations, xk  is estimated using a discretetime Kalman filter. The filter  maintains a mean estimate zk and a covariance matrix Σk,  whose values are predicted using the environment model in  Eq. 2 and updated using the sensor measurements and the  sensing model in Eq. 8. As shown in Fig. 3, the predict stage  incorporates the Gaussian noise of the environment process  to obtain a predicted estimate z− k and associated uncertainty  
횺− k , and the update stage applies the information gained from  an observation yk to obtain an improved estimate zk with  uncertainty Σk. The estimated value is important for data  collection and is introduced for completeness, but our trajectory optimization is only concerned with minimizing the  covariance of the estimation. Despite the discrete updates of  the Kalman filter, the precise values of the covariance matrix  can be calculated at any point t using the following: 
Fig. 2  Persistent monitoring example of 3 POIs and their associated  observation regions. (top) The minimumjerk trajectory must spent  a different amount of time within each observation region to meet  the constraints of 3 observations in B1, 2 observations in B2, and 1  observation in B3. The trajectory is constrained to remain within each  observation region. (bottom) Evolution of the covariance over time.  When not observed, the uncertainty grows linearly in proportion to W  
Page 5 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
where ck is the result of the Riccati Kalman filter update and  δ(t − tk) is the Dirac delta function. Each observation reduces the estimation uncertainty of a  POI by a monotonically decreasing amount. This property of  diminishing returns of the Kalman filter has been exploited  to optimize sensor selection and scheduling in previous  works [8, 9, 51, 52]. As we describe later, the number of  consecutive observations of a POI di will be an important  optimization target due to the inherent trade off between  reducing the uncertainty of individual POIs and reducing  total cycle time, impacting the uncertainty of all POIs as  described in Section 4.1. 
3.4  Problem Statement 
Our objective is to plan a trajectory for a sensing quadrotor  that results in a minimum estimation uncertainty across all  POIs. While researchers have used a variety of optimization  goals [28], we choose to minimize the maximum eigenvalue  of the steadystate covariance matrix, which bounds the estimation uncertainty at any POI and is commonly referred to  as Eoptimality [27]. When considering persistent sensing over an infinite time  horizon, optimal trajectories have been proven to be approximately periodic [29]. Hence, we restrict our focus to the  class of periodic trajectories, which maintain full observability of the POIs at every loop. An infinite time horizon  is selected for the optimization goal because any initial  covariance will converge to an infinitehorizon steadystate  condition exponentially fast for a periodic trajectory [29],  removing the influence of initial conditions. Additionally,  
(11) ̇횺(t) = 
{ 
W + ck𝛿(t − tk) , 퐬(tk) ∈ Bi, 
W , otherwise, 
(12) ck = −횺k퐇⊺ 
k (퐇k횺k퐇⊺ 
k + V)−1퐇k횺k, 
we obtain a solution that works for any initial position along  the trajectory, which is equivalent to starting the trajectory at  any point within a sensing period. While suboptimal in some  instances, removing the reliance on specific timing of sampling allows for a lower cycle time and guarantees a computable upper bound for the maximum estimation uncertainty. 
Problem 1 (Minimizing Steady‑State Uncertainty subject  to Trajectory Feasibility Constraints) For a quadrotor with  dynamics in Eqs. 3-6 and POIs Q , find a periodic, timeparameterized trajectory s with loop period T to minimize  the maximum eigenvalue 휆∞ max of the steadystate Kalman  filter covariance matrix Σ that evolves as defined in Eq. 11  at an infinitetime horizon for any initial sampling position: 
where S is the admissible set of trajectories defined in Eq. 7  and 휆∞ max is defined as: 
where 휆max is the maximum eigenvalue of Σ. 
Our approach to generating a feasible minimum steadystate uncertainty trajectory considers three subproblems: 
1. Determine POI visitation order, 2. Calculate optimal number of consecutive observations, 3. Generate a dynamically feasible trajectory. 
As outlined in Fig. 4, the POI visitation order is calculated  once for a given set of POIs q. Then, the Greedy Knockdown  Algorithm calculates the optimal observation periods d that  minimize the maximum estimation uncertainty. The Feasible  Trajectory Optimizer for Persistent Monitoring (fTOP) calculates minimumjerk trajectories that meet the observation  periods d and dynamic constraints of the system. If the feasible trajectory timing does not match the optimal observation  
(13) 
퐬∗ = arg min T,퐬 
(휆∞ max(T, 퐬)) 
s.t. 퐬 ∈ S 
(14) 휆∞ max(T, 퐬) = lim sup 
t→∞ 
[ 
max 
t≤휏≤t+T 휆max(휏, 퐬(휏)) 
] 
Fig. 3  The Riccati update procedure for a discrete Kalman filter consists of two stages: a Predict Stage that updates the mean and covariance of the estimate based on the known system model and an Update  Stage that refines the prediction according to the measured value and  observation noise 
Fig. 4  Algorithm flow of the Feasible Trajectory Optimizer for Persistent Monitoring (fTOP), which computes optimal observation  periods and feasible trajectories in an iterative manner 
2   Page 6 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
periods, then the Greedy Knockdown Algorithm and proposed fTOP algorithm are run iteratively until convergence. 
4  Optimal Observation Lengths 
Our goal is to minimize the estimation uncertainty of a set  of POIs by determining the optimal observation length for  each POI. This presents an interesting trade off; the more  time spent observing any single POI, the more uncertain we  become about the current value of the other POIs. In this section, we outline how to calculate the maximum  eigenvalue of the Kalman filter estimation covariance at  steady state and describe the tradeoff between minimizing loop time and decreasing the uncertainty for a single  POI. Then, we describe an approach to calculate the optimal  observation lengths for a set of POIs. First, we determine an  optimal visitation order for the POIs, which is equivalent to  finding the shortest cyclic path through the POIs. For calculating the optimal observation lengths for each POI, we  implement the Greedy Knockdown Algorithm (GKA) from  our previous work [22] with updated stopping criteria. The  output of GKA is an optimal continuous observation length  for each POI, which is used as a constraint when calculating  feasible paths in Section 5. 
4.1  Steady‑State Kalman Filter Bounds 
For a discrete Kalman filter, a periodic sequence of observations will result in the estimation covariance to converge  exponentially fast [29] to a periodic steadystate covariance  Σ(t) where for a loop with cycle time T, Σ(t) = Σ(t + T) as  t → ∞ . We define the steadystate value for the ith eigenvalue immediately prior and after observation d as 휆(d) i  and  
휆−(d) i  with the maximum value at steadystate 휆i = 휆(1) i  for  simplicity. For POIs with uncorrelated values, the eigenvalues are ordered such that λi corresponds to the maximum  uncertainty of POI qi. The maximum eigenvalue of the  steadystate covariance matrix is guaranteed to be bounded  if every POI is measured at least once per cycle. 
Lemma 1 (Bounded Uncertainty Guarantee 9) If all POIs Q  are observed at least once during a cycle of duration T, the  maximum eigenvalue of the steadystate Kalman filter covariance Σ will be bounded as lim t→∞휆max(t) ≤ b for some finite  
constant b. 
During each measurement of POI qi, di ∈ ℤ≥1 consecutive observations are made where each observation is binary  as described in Eq. 10. Additional observations result in a  reduction in uncertainty such that 𝜆(d+1) i < 𝜆(d) i  , but the  uncertainty reduction decreases for each observation added  
such that 𝜆(d) i − 𝜆(d+1) i < 𝜆(d−1) i − 𝜆(d) i  . An example of the  evolution of the eigenvalues over time at steady state can  be seen in Fig. 2. For uncorrelated POIs at steady state, the reduction in  uncertainty from the di consecutive observations is followed by a linear increase in uncertainty in proportion to  W. The lack of correlation means that each eigenvalue  evolves independently with the same cycle period T. For  the purpose of computing the maximum eigenvalue of the  steadystate covariance, the observations of each POI can  be shifted in time to align, such that the first observation  of each POI occurs at the same time instance. The rearrangement of observation times can be captured by a new  condensed observation matrix ̃퐇k where k is the number of  discrete observations. Let Ei be the elementary matrix with  1 at its ith diagonal entry and 0 everywhere else, then ̃퐇k  is defined as: 
where ̂d = max 
i di . The matrix ̃퐇k is a condensed observation  
matrix where ̃퐇1 is the identity matrix, indicating all POIs  are observed at least once, and the last nonzero observation  matrix ̃퐇̂d contains 1s for the POIs that had the highest number of observations. With all observations condensed, the  maximum uncertainty for each POI λi with the modified time  alignment will occur immediately prior to the application of  ̃퐇1 , which is described more formally in Eq. 17. For a periodic cycle where T mod f −1 s = 0 , the uncertainty can be calculated precisely. Under these conditions,  each observation per loop will be taken at the exact location  as the previous loop. Aperiodic cycles where T mod f −1 s ≠ 0  can be converted to periodic by raising T to the nearest f −1 s   multiple. Using ̃퐇k , the worstcase interobservation time is  given as: 
where ⌈⋅⌉ is the ceiling function. For high sampling frequencies, the change in cycle time will result in minimal change  in overall error, but for lower sampling frequencies, which  may occur with slowresponding air quality sensors or certain image processing applications, this rounding can impact  the overall performance of the system. The worstcase steadystate uncertainties ̃횺 resulting from  using ̃퐇k and ̂T forms the following system of equations: 
(15) ̃퐇k = 
�∑ 
{i∣di≥k}퐄i, 1 ≤ k < ̂d, 
0, ̂d ≤ k, 
(16) ̂T = ⌈Tfs − ̂d⌉f −1 
s , 
(17) 
̃횺j = ̃횺i+퐖f −1 s − ̃횺i ̃퐇i 
( ̃퐇⊺ i ̃횺i ̃퐇i+Vi 
)−1 ̃퐇⊺ i 횺i ∀ 1 ≤ i < ̃d, j = i + 1 
̃횺1 = ̃횺̂d+퐖 ̂T − ̃횺̂d ̃퐇̂d ( ̃퐇⊺ 
̂d ̃횺̂d ̃퐇̂d+V̂d )−1 ̃퐇⊺ 
̂d ̃횺̂d 
Page 7 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
where ̃횺1 contains the maximum uncertainty for all POIs  immediately prior to the first observation, such that  [ ̃횺1]i,i = 𝜆i . For smaller values of ̂d , the equations can be  quickly solved directly, but for larger values of ̂d , the problem can be solved efficiently using structurepreserving  algorithms [53]. 
Proposition 1 (Uncertainty Bound) If all POIs Q are  observed at least once during a cycle of duration T, the  eigenvalues of the steadystate Kalman filter covariance Σ  will be bounded as: 
where λi is the ith eigenvalue of ̃횺1 as calculated in Eq. 17  from a modified observation matrix ̃퐇 formed from the number of consecutive observations di of each POI in Eq. 15 and  the worstcase interobservation time ̂T. 
The uncertainty bound can be computed for a worstcase  interobservation time and number of observations for each  POI. The remainder of Section 4 explores how the cycle time  and observation periods are selected to minimize this bound. 
4.2  Visitation Order 
The cycle time T can be split into two components: Ti, the  time spent observing qi, and Ti→i+1 , the travel time between  successive observation regions Bi and Bi+ 1. In this section,  we present a solution for minimizing the interobservation  travel time by formulating the problem as a standard TSP. Modern solutions to the TSP involve minimizing the  total cost to visit all points and finding a Hamiltonian tour  that only visits each POI once. For our application, the cost  between POIs is the time required to travel between them.  A stateoftheart exact TSP solver is Concorde [54], which  leverages LinKernighan heuristics, branchandbound  methods, and other upperand lowerbound methods to  converge to solutions quickly for realworld problems but  still with exponential complexity for worstcase situations. We restrict our trajectory to the class of periodic trajectories that is periodic for position and all higher order derivatives, such that: 
Due to being periodic, the trajectory finishes a loop with  travel from the last qN to the first POI q1. For a given visitation order, the travel time between POIs  can be fixed by selecting entrance and exit points for the  observation region Bi for each POI, which separates the  problem into parallel subproblems of generating a trajectory  
(18) lim t→∞휆max(t) ≤ max i 휆i 
(19) 퐬(t + T) = 퐬(t), T = 
N ∑ 
i=1 
( 
Ti + Ti→i+1 
) . 
segment within each observation region and the trajectory  between consecutive observation regions as illustrated in  Fig. 2. Within each Bi, the trajectory segment si(t) begins  at an entrance point 퐬in i = 퐬i(0) and ends at an exit point  
퐬out i = 퐬i(Ti) , both of which are located within the closed  observation region 퐬in i , 퐬out i ∈ Bi . When the UAV is not in an  observation region, the estimation uncertainty grows linearly  with time. The interpoint travel time between observation regions is  minimized by minimizing the distance between consecutive,  convex observation regions Bi and Bj while maximizing the  trajectory velocity ‖̇퐬‖ = vmax with ‖̈퐬‖ = 0 as follows: 
where 퐬in∗ j  and 퐬out∗ i  are the optimal entrance and exit positions for observation region Bj and Bi, respectively, Ti→j is  the interpoint travel time, and j = i + 1 mod N closes the  cycle so that the last point leads into the first point in the  visitation order. The distance between regions will be  unique, but in the case where the entrance and exit positions  are not unique, the interpoint trajectory closest to the direct  path from one point to the next is used. The interpoint travel  times Ti→j are used in calculation of the total loop time and  are input into the Greedy Knockdown Algorithm in  Section 4.3. The fixed entrance and exit positions for each observation  region enable the trajectory s to be decomposed into trajectories that are within an observation region si and interpoint  trajectories that are between observation regions 퐬i→j , such  that: 
The interpoint trajectories are fully defined as the linear  interpolation between 퐬out∗ i  and 퐬in∗ j  with ‖̇퐬‖ = vmax and  
‖̈퐬‖ = 0 . Continuity is maintained between the trajectory  segments by constraining the trajectory within an observation region to begin at the entrance position and end at the  exit position with 0 acceleration and velocities aligned with  the incoming and outgoing trajectories. By constraining the  trajectory at each entrance and exit position, trajectories  within each observation region can be calculated independent of each other by the optimization outlined in  Section 5.2. Although we generate a visitation order by solving a TSP,  the interpoint travel times and entrance and exit positions  can be calculated for any arbitrary visitation order of POIs.  However, the visitation order has a direct impact on the  
(20) 퐬in∗ j , 퐬out∗ i = arg min 퐬in 
j ∈Bj,퐬out 
i ∈Bi 
‖퐬in 
j −퐬out 
i ‖ 
vmax , 
(21) Ti→j = 
‖퐬in∗ 
j −퐬out∗ 
i ‖ 
vmax , 
(22) 퐬 = 퐬1 ∪ 퐬1→2 ∪ 퐬2 ∪ … ∪ 퐬N ∪ 퐬N→1. 
2   Page 8 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
observation periods calculated in Section 4.3 and the trajectories generated in Section 5, so any change to the visitation order would require reexecuting the iterative trajectory  generation portion of fTOP. 
4.3  Greedy Knockdown Algorithm 
Prior to generating the trajectory si through each observation  region Bi, the length of time spent observing each POI must be  determined. Since we constrict the trajectory to visit each POI  once per cycle for one or more consecutive observations, the  goal is to determine the number of consecutive observations  that will minimize the worstcase steadystate uncertainty. To  accomplish this, we extend the Greedy Knockdown Algorithm from our previous work [22] by updating the method  with an improved stopping criterion and more efficient computation. We chose to implement a greedy algorithms because  they have been shown to be nearoptimal for approximate  supermodular metrics [52], such as the meansquared error  and spectral norm of the estimation covariance matrix, and  optimal when state and sensor noise are uncorrelated [9]. Our proposed solution GKA, shown in Algorithm 1, determines the optimal number of observations of each POI di from  the interpoint travel times Ti→j , the uncertainty in the environmental model W, and the covariance of the noise in the  observation model V. The initial run of GKA leverages the  assumption that each POI is observed at least once and sets di  = 1 ∀ i. Following runs of GKA calculate the number of initial  observations from the minimum traversal time of each subregion, an output of the proposed fTOP algorithm in Section 5.2. 
GKA is a greedy algorithm that proceeds in iterations  denoted by a (Algorithm 1). The algorithm may progress  for several iterations beyond the optimum, so the iteration  
that contains the optimal number of iterations and the final  iteration may be different. For each iteration, a bound on the maximum steadystate uncertainty for each point is calculated by forming  ̃퐇 (Algorithm 1 Line 4) and computing the steadystate  uncertainty (Algorithm 1 Line 10) using the techniques  outlined in Section 4.1. The bound is the result of solving  the nested Kalman filter formulation in Eq. 17 with the  special observation matrix ̃퐇 . An observation is added  to the POI with the largest estimation uncertainty as  denoted by the ith diagonal element in ̃횺 (Algorithm 1  Lines 6-8). The algorithm continues until termination conditions are met that guarantee the minimum upperbound  on the maximum eigenvalue has been found. The last step  is to check all iterations and return the optimal number of  observations of each POI di associated with the minimum  upperbound. Each iteration of GKA adds an observation to the POI  with the current maximum estimation uncertainty, and  this addition has competing effects. Firstly, the additional  sequential observation reduces the estimation uncertainty  of the POI being observed. As the number of consecutive  observations of a single POI increases, the uncertainty of  that POI reduces exponentially and asymptotically to the  value that a stationary sensor with the sample sampling  characteristics could achieve if left at the POI. Secondly,  each observation increases the overall cycle time by f −1 s  (i.e.  
Ta+1 = Ta + f −1 s  ), resulting in a linear increase in uncertainty  of all other POIs. The GKA stopping criterion in Algorithm 1 Line 9 identifies the point when additional observations of a POI can no longer improve the maximum estimation uncertainty (Fig. 5). Since the reduction in uncertainty decreases for each subsequently added observation, after some number of observations, the reduction from adding an observation will be less  than the uncertainty increase when an observation is taken  at another point. Once this critical point is reached, GKA  is stopped after the maximum uncertainty for any two following iterations is higher than the maximum uncertainty at  the latch point (Algorithm 1 Line 9). Algorithm 1 uses the  array notation ̃𝜆[a, ∶] to denote the eigenvalues associated  with iteration a of the algorithm generated with the modified observation matrix ̃퐇 . For simplicity in describing the  progression of the algorithm and stopping criterion, it is useful to express the eigenvalues for a specific iteration a and  consecutive observation di in a single term ̃𝜆(di,a) i  . Formally,  the stopping criterion is stated as follows. 
Proposition 2 (GKA Stopping Criterion) For the iterative  Greedy Knockdown Algorithm, the minimum steadystate  uncertainty is reached at an optimal number of observations  d∗ and associated total number of observations a∗. The optimal point is reached at or before any two POIs have reached  
Page 9 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
the condition where any additional observation will result in  an increase in uncertainty for either POI such that: 
where ̃𝜆(di,a) i  is the ith eigenvalue of the maximum covariance matrix ̃횺1 for di consecutive observations during round  a of GKA and wi = [W]i,i. The stopping point a∗ is reached  when di ≥ d∗ i  for two or more i. Since the maximum eigenvalue for future observations a > a∗ increases monotonically,  the minimum upperbound of the maximum eigenvalue has  been reached or passed at iteration a∗. 
Proof See Appendix A.1 
The result of GKA is the number of observations required  for each POI to minimize the upperbound on the steadystate  uncertainty. Each of the di observations requires a period  of f −1 s  , which serves as constraints for the length of the  observation period. In the next section, the trajectory optimization generates a smooth trajectory that ensures that the  required observation period while adhering to the dynamic  constraints of the UAV. 
5  Minimum Uncertainty Trajectories 
From the observation lengths calculated by GKA (i.e. dif −1 s  )  and physical constraints of the platform, we generate a feasible trajectory that meets these constraints while minimizing  the average jerk. As shown in Eq. 22, the whole trajectory  can be decomposed into interpoint trajectories and trajectories within an observation region si. The interpoint trajectories are fully defined by a linear interpolation between  consecutive observation regions with maximum velocity  and no acceleration. In this section, we present a method  for generating the remaining trajectories within the observation region using Bspline trajectories to ensure adherence  to constraints on the dynamics of the system, the required  
(23) d∗ i = arg min 
di 
[ 
ĩ𝜆(di,a) − ̃𝜆(di+1,a+1) i < wif −1 s 
] 
observation length, and trajectory continuity at each transition between trajectories within and between observation  regions. First, we present a brief background on Bspline trajectories and then we present our proposed fTOP algorithm  for generating feasible trajectories within each observation  region. 
5.1  B‑Spline Trajectory 
To generate C3 continuous trajectories that meet constraints  on velocity and acceleration, we leverage a Bspline representation. Bspline curves have an attractive property that  the curve and its derivatives are contained within the convex  hull of their control points and offer controllable levels of  continuity and numerical stability due to their limited, timevarying support. Bsplines are piecewise polynomials defined for each  subregion i by a set of control points Ci, kth order basis  functions Nk(t), and knots τj. The kth order basis functions can be calculated using the Coxde Boor algorithm  as follows: 
where  
[Nk(t)] 
j is the jth basis function of Nk(t). Each basis function provides limited support over the  interval τj ≤ t < τj+k. At any given point along the curve,  a Bspline function will have a maximum support of k + 1  different basis functions, transitioning between basis functions at each knot. We focus on developing trajectories with  uniform Bsplines, which have a uniform spacing between  all knots, such that Δτ = τj + 1 − τj. 
(24) 퐬i = 퐂iNk(t) 
(25) 
[N1(t)] 
j = 
{ 
1 , 𝜏j ≤ t < 𝜏j+1 
0 , otherwise 
(26) 
[Nk(t) ] 
j = 
t−휏j 휏j+k−휏j 
[ Nk−1(t) ] 
j + 휏j+k+1−t 
휏j+k+1−휏j+1 [ Nk−1(t) ] 
j+1, 
Fig. 5  (left) The control points  and resulting trajectory of a  BSpline of order 4 with three  different time points marked  (t = 0.9, t = 1.8, and t = 2.7).  (right) The colorcoded basis  functions Nk(t) corresponding  to each control point. Note that  at any time point, the position  is calculated from a weighted  average of up to 4 control points  and the current value of the  basis function 
2   Page 10 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
At the transition points defined by the knots τk, the curve  has Ck− 1 continuity. The derivative of the Bspline bases  results in a linear combination of lower order bases, which  can be succinctly represented as: 
where Ci is a matrix of the control points, A(a) is a Toeplitz  matrix that captures the relationship between C and the control points of the ath derivative C(a), and Nk − 1(t) is vector  of the value of each basis function at time t. For the derivation of Eq. 27 see Appendix A.2. ∑Due to the fact that the basis functions sum to unity  jNj,k(t) = 1 , ∀t, the value of a function defined by Bsplines  will remain within the convex hull created by the control  points of the support. As shown in Eq. 27, the derivative In  the case where a robotic platform has the same maximum  allowable velocity and acceleration for the entirety of its  trajectory, the constraints are guaranteed to be met if the  magnitude of all control points c are less than the limiting  value (i.e. vmax or amax as defined in S). 
5.2  Trajectory Optimization with f‑TOP 
Trajectories that are optimized to be informative must take  the dynamic constraints of the sensing platform into account.  If a trajectory cannot be followed and observation locations  are different from planned, the resulting uncertainty in an  estimate can be arbitrarily bad. Our proposed Feasible Trajectory Optimization for Persistent Monitoring (fTOP) takes  the point order and required number of observations for each  POI and generates a feasible trajectory that guarantees the  correct number of observations are taken within each sensing region while meeting dynamic constraints of the UAV. The proposed fTOP optimization has a multiobjective  formulation, primarily to minimize the maximum steadystate uncertainty as formulated in Eq. 13 and secondarily to  provide the smoothest possible path as shown in Eq. 28a. By  applying the results from Section 4.2 and the output of GKA  from Section 4.3, minimizing the maximum steadystate  uncertainty can be simplified to minimizing the total travel  time subject to constraints on entrance and exit conditions in  Eq. 28c and the minimum number of required observations  in Eq. 28e for each observation region. The secondary objective of a smooth path is formulated  as a minimization of the L2-norm of the jerk of the trajectory with a feasibility constraint that the trajectory remains  within the valid state space S as defined in Eq. 7. By restricting the trajectory s to S in Eq. 28b, the maximum velocity  and acceleration are bounded by the limits of the platform.  
(27) da𝐬(t) 
dta = 
( 
1 
Δ휏a 
) 
𝐂i𝐀(a)Nk−1(t), 
As smoothness is a secondary objective, the coefficient 𝜖 can  be set arbitrarily small so that it does not affect the primary  goal. The full multiobjective formulation is as follows: 
where the trajectory is segmented as described in Eq. 22.  Note that since GKA assumes that the UAV remains within  the observation region of a POI for a continuous period of  time, the additional constraint of si ∈ Bi is applied. The optimization and constraints from Eq. 28a can be  reformulated using Bsplines into a quadratically constrained  quadratic program (QCQP). The primary objective is simplified by removing the interpoint travel times Ti→j , which  are constant due to the entrance and exit constraints, and  the time within each observation region is updated with the  number of control points and a fixed, uniform knot spacing  such that Ti = MiΔτi. The secondary objective of minimum  jerk is converted into a linear relationship of control points  using Eq. 27, which form a convex hull of achievable values  for the trajectory derivatives. For the special case of a 4thorder Bspline with k = 4, the objective can be simplified  into a quadratic form (see Appendix A.3) as shown below: 
s.t. Eqs. (28b) and (28c) 
where 퐂i ∈ ℝMi×3 is the matrix of all control points and  ci,j is a vector of the jth control point for the ith observation region, A(3) is a thirdorder Toeplitz matrix capturing the relationship between control points of Bspline  derivatives as in Eq. 27, the first and second order control points are computed as ̇퐜ij = (퐜ij − 퐜i(j−1))∕Δ𝜏i and  ̈퐜ij = (퐜ij − 3퐜i(j−1) + 3퐜i(j−2) − 퐜i(j−3))∕Δ𝜏2 i  , i ∈{1,...,N} is the  POI index, and j ∈{1,...,Mi} is the interior control point  index. Note that Mi only refers to the control points within  the observation region. When implementing, k − 1 additional control points must be added outside of the observation region to define the entrance and exit conditions  in Eq. 28c where si is computed from control points and  Bspline basis functions as described in Eq. 24. 
(28a) min퐬 
N∑ 
i=1 
� 
Ti + Ti→i+1 
� 
+ 𝜖 ∫ ‖⃛퐬(t)‖2dt 
(28b) s.t. 퐬 ∈ S 
(28c) 퐬in i = 퐬in∗ i 퐬out i = 퐬out∗ i ∀ i 
(28d) 퐬i ∈ Bi ∀ i 
(28e) Ti =∫ {t∣퐬(t)∈Bi}1 dt ≥ difs ∀ i, 0≤t<T 
(29a) min 
퐂i,Mi,Δ휏i 
N∑ 
i=1 
� 
MiΔ휏i + 휖퐂⊺ i 퐀(3)퐀(3)⊺퐂i 
� 
(29b) cij ∈ Bi ∀ i, j 
(29c) ‖ ̇퐜ij‖2 ≤ vmax ‖̈퐜ij‖2 ≤ amax ∀ i, j 
(29d) MiΔ휏i ≥ di∕fs ∀ i 
Page 11 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
The objective of the optimization problem in Eq. 29a  has a linear and a quadratic component that defines the  meansquare of the jerk with quadratic constraints on the  magnitude of position, velocity, and acceleration. This  form of QCQP can be efficiently solved by modern convex optimization solvers with interiorpoint methods [55].  While the problem is solvable, the constraints of the problem are restrictive with many values of Δτ resulting in  infeasible conditions. The feasibility of the constraints is  closely linked to the time that the robotic platform remains  within the observation region MiΔτ, the size of the observation region Bi, and the dynamic constraints. Larger Mi  values result in more flexibility in the planner in terms of  the number of control points, and so long as the robotic  platform can remain within the sensing region of a point,  which is determined by vmax and amax , and has a sufficient  number of control points, then it can meet the enter and  exit constraints of the region of interest. To better optimize the search for a feasible domain, we  introduce a bilevel formulation of the optimization problem  with slack variables that guide the selection of appropriate  Mi and Δτ values. The outer problem controls the number  of control points and knot spacing while the inner problem  minimizes the slack of the constraints, which is added to  provide direct information for how close the solver is to a  feasible solution. The inner problem refits the constraints  from Eq. 29a with slack variables for the velocity constraint  πv, acceleration constraint πa, and overall slack π. The  outer problem can adjust Mi and Δτi with gradient descent  methods to better approximate good values for subsequent  iterations. The inner problem is defined for a given observation region Bi as a secondorder cone program (SOCP) as  follows: 
s.t. Eqs. (28b), (28c), and (29b) 
where ̇퐜(x) ij  , ̇퐜(y) ij  , and ̇퐜(z) ij  represent the first, second, and  third entry of ̇퐜ij , respectively, πi is an upperbound on the  slack, 𝜖π is an added weight to the acceleration, and the  slacked constraints are formulated as a secondorder cone.  An ndimensional quadratic cone is defined as: 
(30a) f(Mi, Δ휏i) = min 퐂i 
Πi + 휖휋휋(a) 
i + 휖퐂⊺ i 퐀 (3) 퐀 (3)⊺퐂i 
(30b) (𝜋(v) i vmax, ̇퐜(x) ij , ̇퐜(y) ij , ̇퐜(z) ij ) ∈ Q4 ∀j 
(30c) (𝜋(a) i amax, ̈퐜(x) ij , ̈퐜(y) ij , ̈퐜(z) ij ) ∈ Q4 ∀j 
(30d) Πi ≥ 휋(v) i Πi ≥ 휋(a) i 
(31) Qn = 
{ 
퐱 ∈ ℝn ∣ x1 ≥ √ 
x2 2 + x2 3 +...+ x2 n 
} 
and the normalized velocity and acceleration constraints are  transformed from Eq. 29b to the following form: 
where πv and πa are the slack for the velocity and acceleration constraints, respectively. While it may look difficult to  solve a SOCP with NMi cone constraints, each region of  interest is independent due to the entrance and exit constraints. Each independent problem results in 2Mi fourthorder cones and a positional constraint to remain within  the convex optimization region, which is easily solvable by  modern standards. The final bilevel optimization consists of a higher level  controller that sets the knot spacing and number of control  points that are guided to a feasible region and a set of independent lowerlevel problems that depend on knot spacing  and number of control points f(Mi,Δτi) that can be solved in  parallel as follows: 
where the inner problem f(Δτi,Mi) is defined in Eq. 30a. In the inner problem objective statement in Eq. 30a, the  reason for an added focus on minimizing the maximum  acceleration is twofold. First, the minimum normalized  slack on velocity (πv) is 1 since the entrance and exit conditions for each observation region have the UAV travelling at  vmax . In order to further guide good solutions after the slack  in velocity has been removed, an additional focus needs to  be added to the acceleration slack. Secondly, the maximum  acceleration along a trajectory has a large impact on the  ability of a UAV to track a trajectory since maximum acceleration is directly linked to the maximum force produced by  the motors, which is a limiting factor on performance. The  further the acceleration is from the upper limit, the better a  UAV can recover from tracking errors. It is important to note that the minimum sensing time  
dif −1 s  may differ from the amount of time it takes to traverse the region of interest ΔτiMi. When they differ (i.e.  Δτi≠ΔτiMi), the GKA from Section 4.3 is rerun with the  new minimum traversal times to determine if a more optimal  series of observations can be obtained. The iteration continues until convergence, which is achieved when the Greedy  Knockdown Algorithm maintains its number of observations  for two consecutive cycles. 
(32) ‖ ̇퐜ij‖2∕vmax ≤ 𝜋v 
(33) ‖̈퐜ij‖2∕amax ≤ 𝜋a 
(34a) min Mi,Δ휏i 
N∑ 
i=1 
f(Mi, Δ휏i) 
(34b) s.t. MiΔ휏i ≥ di∕fs ∀ i 
2   Page 12 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
6  Simulation Results 
6.1  Simulation Setup 
We evaluate the fTOP algorithm against three baseline algorithms in a simulated scenario requiring monitoring a set of  randomly distributed points over a 50 acre region (450-by- 450 m). We assume the environmental model from Eq. 2 is  zero mean and distribute the covariance growth W in accordance to a Chisquared distribution. We vary the number of  points in the region between 10 and 80, which can represent  points that an expert selects as useful for inputting into an  environmental model in the air quality example or points  that have high probability of event detection from thermal  or RGB camera sensors in forest fire detection scenarios. Each POI can be measured within a 10 m circular observation region that does not overlap with the observation  regions of other POIs. The circular region approximates the  performance of a UAV that is mounted with a camera on a  gimbal system. Despite aggressive maneuvers, the camera  can remain pointed at the POI. We use a UAV model based  on a commercially available Phantom 4 quadrotor, which has  a mass of 1.216 kg, maximum velocity of 20 m/s, maximum  acceleration of 19.2 m/s2, and battery life of 20 min. The  maximum acceleration is the maximum magnitude in the  XY plane, which was calculated using the maximum lift  capability and required lift to maintain altitude against the  acceleration due to gravity. During simulation, limits were  placed on each motor individually. Aggressive maneuvers at  the acceleration limit can result in tracking instability when  using a realistic controller. The test configurations include the maximum velocity and  acceleration and more conservative configurations in 10%  steps for maximum velocities vmax ∈ {10, 12, 14, 16, 18, 20}  m/s and two maximum acceleration configurations at 90%  and 100% with amax ∈ {17.3, 19.2} . Each test configuration was run for 10 randomly drawn configurations for the  expected battery lifetime of the platform (20 min), which  resulted in a variable number of average loops, dependent  on maximum velocity and number of POIs. 
6.2  Comparison Algorithms 
We compared fTOP to three baseline approaches: a naive  constant max velocity speed controller (Constant), a velocity controller obtained using the algorithm proposed by  [14] with uncertainty decay estimates equal to a firstorder  approximation of a Kalman filter (Linear), and a velocity  controller that leverages the GKA but without feasibility  constraints, similar to [22] (GKA 2019). The Constant method calculates the visitation order by  solving a TSP as described in Section 4.2 and then traverses  
the POIs at maximum velocity. The method assumes linear  travel between successive points and does not take observations or dynamic constraints into consideration. These  assumptions lead to trajectories with instantaneous velocity  changes at each POI, which results in tracking errors due to  the dynamic limits of the UAV, and potentially unobserved  points at higher maximum velocities or for POIs with small  observation regions. The Linear method is based on the uncertainty models  used in [14], which include a linear increase in uncertainty  when a POI is not being observed and a linear decrease when  it is. Since the steady state uncertainty of the Kalman filter  is achieved when, for each loop, the reduction in uncertainty  equals the increase in uncertainty for each cycle of the loop,  a firstorder approximation of the reduction rate of the discrete Kalman filter is the product of the loop decrease and  the sampling rate, i.e., TWfs where W is the uncertainty  growth from Eq. 2 and T is the loop time at steady state.  A UAV trajectory is obtained using the velocity controller  proposed by Smith et al. [14]. While Linear is an improvement over Constant, it still lacks a guarantee that all points  of interest will be observed. The GKA 2019 method calculates a POI visitation order  by solving a TSP as described in Section 4.2, and optimizes  the velocity profile over the resulting path using [22]. The  method attempts to meet the required observation periods  by reducing velocity within the regions of interest. The  method plans a trajectory that minimizes an upper bound  on the steadystate error, but like the Constant and Linear  methods, GKA 2019 generates trajectories with instantaneous velocity changes. 
6.3  Results 
All results for the Constant, Linear, GKA 2019, and fTOP  trajectory planners were captured for the same POI placement and uncertainty growth rates. The minimum upper  bound on the steadystate error was approximated by the  maximum eigenvalue of the covariance matrix of the estimation over the last loop of each test. All uncertainty results  were normalized to the bound on max uncertainty of the  fTOP algorithm. Linear, GKA 2019, and fTOP guarantee that each POI  will be observed at least once by remaining within the observation region for each POI longer than the sampling period.  The Constant planner lacks this guarantee of observability  because it travels at maximum velocity. Due to the cyclic  nature of the plans, not observing a POI results in an unstable condition whereby the uncertainty for the unobserved  POI will grow unboundedly. 
Page 13 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
Despite that the trajectories are optimized to visit all  POIs, when the plans are followed by a nonlinear, geometric  controller [56] that obeys the UAV dynamics, the actual coverage can differ from the planned coverage. The Constant,  Linear, and GKA 2019 plan trajectories without incorporating acceleration limits, resulting in trajectories that cannot be followed. When POIs were sufficiently far from each  other, the UAV controller converged to the planned trajectories during the straight flight segments between observation regions. As the number of POIs increased, resulting in  a dense configuration that required high agility, the Constant, Linear, and GKA 2019 trajectories regularly resulted  in missing observations of POIs. Examples of trajectories  and differences between planned and actual coverage can be  seen in Fig. 6, and a quantitative comparison between the  planned and simulated trajectories is provided as the Max  XY Error in Table 1. Paths planned by fTOP at the performance limits (i.e.  
vmax = 20 m/s and amax = 19.2 m/s2 ) also suffered from  tracking errors. This is due to a mismatch between maximum magnitude constraints on the dynamics with which  the trajectories are constructed and the actual drone model,  which includes limits on the maximum force for each motor.  Depending on the maneuver, the required correction to follow the desired trajectory can max out the force ability of an  individual motor. For paths further within the performance  limits, the proposed fTOP algorithm generates paths that  can be followed more reliably. The actualized coverage, which is the percentage of  POIs that received at least one observation per loop, can be  observed in Fig. 7 and Table 1. As the maximum allowable  velocity increases, the coverage of the POIs drops. For the  plans that do not obey maximum acceleration constraints  
(i.e. Constant, Linear, GKA 2019), the UAV can only  achieve 50% of the maximum velocity before reaching unstable conditions. The plans by fTOP incorporate maximum  acceleration constraints, resulting in plans that are feasible.  Several variants of plans were generated for maximum acceleration amax values of 90% and 100% of the maximum listed  accelerations. Despite the lower loop time of the the naive (Constant)  and baseline (Linear and GKA 2019) methods, the maximum uncertainty, Max Covar as seen in Table 1, is substantially worse than the result of fTOP due to the actualized coverage of the planned trajectories. By incorporating  maximum acceleration constraints and planning feasible  paths, the robotic platforms are able travel 47% faster while  maintaining the same coverage. The higher achievable speed  results in lower average loop times for a given coverage level  and a significant improvement in maximum uncertainty of  the measurement at each POI. Note that the Max Covar is  normalized to the minimum eigenvalue from any method for  a given trial, such that a value of 0.0 means that the method  had the minimum uncertainty for a single trial. The best performing method was the fTOP algorithm  with acceleration limited to 90% of the maximum value of  the platform. Despite the slightly improved loop times of  fTOP with higher accelerations, the trajectories planned to  the limit of the system resulted in errors due to the imprecise  tracking of the controller, which updated the trajectory plan  online at 100 Hz, causing compounding error when operating at the system limited in complex maneuvers. By limiting  the maximum acceleration and velocity to 90% of the maximum value of the platform, the controller was better able to  control the system and achieve the required observations in  a minimum time. 
Fig. 6  Simulated trajectories for a quadrotor robot obtained from  the constant velocity, GKA 2019, and proposed fTOP methods. The  POIs are shown with their associated sensing regions (light gray).  
Subplots (a) - (d) focus on regions that highlight the differences in  the methods and how the performance of the proposed fTOP method  enables more observations 
2   Page 14 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
Fig. 7  Comparison of methods showing the impact of not planning  with higherorder dynamic constraints. Increasing maximum velocity  constraint results in naive planners creating infeasible trajectories that  result in missing observations. As a region increases in numbers of  
POIs, the maneuvers to measure all points become more aggressive,  resulting in naive methods requiring significantly lower speeds to  achieve similar average observability. Results generated are average  of 10 random trials for each condition 
Table 1  Statistics for each simulated method for 10 randomized  distributions of 80 POIs with random uncertainty growth rates.  A  graphical summary of important results can be seen in Fig. 7 (right).  Max Covar is the maximum eigenvalue of estimation covariance nor- 
malized to the smallest maximum eigenvalue from any algorithm for  the same trial. Max XY Error is the maximum distance between the  planned and simulated trajectory for any time point in the given trial.  Coverage is the percentage of POIs with at least one observation 
The bolded entries highlight test configurations where the controller was unable to follow the planned trajectory 
Method Constraints Max Covar (%) Max XY Error (m) Coverage (%) Loop Time (s) 
vmax amax Avg Min Max Avg Min Max Avg Min Max Avg 
Constant 10 – 79.7 36.8 379.5 21.5 14.3 53.0 99.6 97.5 100.0 327.6 12 – 169.1 20.7 449.9 29.5 21.8 52.4 97.3 90.0 100.0 273.0 14 – 308.0 175.0 419.4 45.9 28.6 > 100 90.1 82.5 97.5 234.0 16 – 400.0 332.6 465.4 >100 50.6 > 100 68.5 0.0 82.5 204.7 Linear 10 – 79.7 36.8 379.5 21.5 14.3 53.0 99.6 97.5 100.0 327.6 12 – 164.9 20.7 449.9 29.0 21.8 52.4 97.5 90.0 100.0 273.0 14 – 295.7 91.0 419.4 44.5 28.7 82.1 89.5 81.2 96.2 234.0 16 – 378.8 292.7 465.4 >100 50.6 > 100 75.0 61.3 85.0 204.7 GKA 2019 10 – 55.7 38.7 117.8 18.8 14.4 24.5 99.8 97.5 100.0 332.6 12 – 153.7 22.3 365.8 27.9 20.4 52.7 97.9 93.8 100.0 280.8 14 – 304.6 116.6 442.8 44.7 26.3 91.2 89.4 80.0 96.2 242.8 16 – 364.5 268.6 442.8 62.9 36.3 94.1 78.9 71.2 86.2 215.0 fTOP 12 17.3 29.0 11.3 163.3 7.7 5.8 10.0 100.0 100.0 100.0 266.6 (90% accel) 14 17.3 80.7 0.0 379.5 13.0 8.8 19.9 99.3 96.2 100.0 238.2 16 17.3 59.0 0.0 312.0 13.3 8.1 21.8 99.3 97.5 100.0 222.1 18 17.3 131.4 0.0 352.9 23.6 10.3 73.6 97.6 93.8 100.0 222.0 fTOP 12 19.2 51.1 0.0 377.5 10.3 6.2 16.2 99.8 98.8 100.0 264.4 (100% accel) 14 19.2 128.1 4.4 379.5 66.0 7.1 > 100 98.2 95.0 100.0 235.0 16 19.2 154.1 3.0 328.8 19.3 12.2 25.6 96.5 92.5 100.0 216.8 18 19.2 169.7 25.6 343.6 44.3 14.4 > 100 95.4 85.0 100.0 209.3 
Page 15 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
7  Conclusion 
Persistent monitoring using mobile robotic platforms  offers substantial benefits over stationary sensors and lowresolution satellite imagery. When planning trajectories  for UAVs, it is critical to optimize for observation periods and to constrain the trajectory to meet the dynamics  constraints of the sensing platform to ensure a feasible  trajectory. The fTOP method generates optimal observation periods using the Greedy Knockdown Algorithm  and forms continuous, feasible trajectories by optimizing the control points and time knots of a Bspline in a  novel bilevel formulation. The resulting trajectories have  guarantees on their position, velocity, and acceleration,  meeting provided constraints. Simulations for randomized  POI distributions show that the trajectories planned using  the fTOP framework outperform related methods, achieving the same level of observability at 47% higher travel  speeds, resulting in significant performance gains. Future work will focus on applying the fTOP algorithm  in physical experiments. For persistent monitoring in the  real world, the generated trajectory must involve rendezvous with a recharging station at regular intervals, placing additional requirements for the planned trajectories.  Online replanning would be needed to deal with dynamic  obstacles or changing objectives. Lastly, the fTOP algorithm could be extended to include environmental models  that capture measurement correlation. 
Appendix  
A.1 GKA Stopping Criterion (Section 4.3) 
For uncorrelated POIs, each additional observation results  in a decrease in the eigenvalue of estimation uncertainty  for a POI, such that 𝜆(di−1,a) i > 𝜆(di,a) i  where 휆(di,a) i  is the  eigenvalue associated with the estimation uncertainty of  POI qi for di observations and a total observations. An  additional observation of a different point qj results in an  uncertainty increase as 𝜆(di,a+1) i > 𝜆(di,a) i + wif −1 s  for j≠i. To  see this relationship, note that 휆(di,a) i = 휌(휆(di,a) i ) + awif −1 s   where ρ(⋅) is the reduction in uncertainty due to an added  observation, a known monotonic function. When a POI meets the stopping condition  
𝜆 (d∗ 
ii ,a) − 𝜆 
(d∗ 
ii +1,a+1) < wif −1 s  , then it follows that  
𝜆 (d∗ 
ii ,a) < 𝜆 
(d∗ 
ii +1,a+1) + wif −1 
s < 𝜆(d∗+1,a+2) i  , which states that  when the stopping condition is met, the uncertainty λi can  only increase after the set of observations at qi and at any  other POI. In GKA, an observation is added to the POI  with the highest uncertainty. When any two or more POI  
meet the stopping condition, then adding an observation  to any POI that meets the stopping criterion will result in  an overall increase in uncertainty. 
A.2 B‑Spline Derivatives (Section 5.1) 
The derivative for a Bspline is defined as: 
For uniform spacing Δτ = τj + 1 − τj, the derivative simplifies as: 
where A(1) captures the interaction between basis functions. With A(a) capturing the linear interaction between  basis functions for derivative a, further derivatives take  the form: 
A.3 B‑Spline Objective Reformulation (Section 5.2) 
The primary objective from from Eq. 28a can be simplified  by noting that Ti→i+1 is constant and will not affect the minimization goal and that Ti = MiΔτi. The secondary objective,  the average jerk power, can be replaced by the linear combination of control points and bases in Eq. 27. The result is  an updated cost function as follows: 
The secondary objective can be further modified for the  specific case of minimizing the square of jerk magnitude  for k = 4. For a 4rdorder representation of position, the 3rd  derivative results in a basis function of order 1, which is  either 1 for τi ≤ t < τi + 1 and 0 otherwise as shown in Eq. 25.  Since the limited domain of the base does not overlap with  
(35) ̇s(t) = 
Mi∑ 
j=1 
퐜i,j dNk(t) 
dt 
(36) 
[ 
dNk(t) 
dt 
] 
j = k 
( 
[Nk−1(t)]j 휏j+k−휏j − [Nk−1(t)]j+1 휏j+k+1−휏j+1 
) 
(37) 
[ 
dNk(t) 
dt 
] 
j = 1 Δ휏 
([Nk−1(t)]j − [Nk−1(t)]j+1 
) 
(38) ̇s(t) = 
Mi∑ 
j=1 
퐜i,j Δ𝜏 
�[Nk−1(t)]j − [Nk−1(t)]j+1 
� 
(39) = 1 Δ휏 퐂i퐀(1)Nk−1(t) 
(40) das(t) 
dta = 
( 
1 
Δ휏a 
) 
퐂i퐀(a)Nk−1(t) 
(41) 
minc, Δ휏, Mi 
N∑ 
i=1 
� MiΔ휏i + 휖 
Δ휏6 i 퐂⊺ i 퐀(3)∫ Nk−3N⊺ k−3dt 퐀(3)⊺퐂i 
� 
2   Page 16 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
any other base, the outer product of the 0thorder basis function results in I and the integral results in ΔτI, where I is the  identity matrix as seen below: 
Since 𝜖 is an arbitrarily small scale factor, we incorporate 1/Mi and 1/Δτ5 into 𝜖, resulting in the reformulated  objective: 
Author Contributions Conceptualization: Michael H. Ostertag, Nikolay  Atanasov; Methodology: Michael H. Ostertag, Nikolay Atanasov;  Software, investigation, and analysis: Michael H. Ostertag; Writing  - original draft preparation: Michael H. Ostertag; Writing - review  and editing: Michael H. Ostertag, Nikolay Atanasov, Tajana Rosing;  Supervision: Nikolay Atanasov, Tajana Rosing 
Funding We gratefully acknowledge support from the National Science  Foundation National Robotics Initiative (CNS-1830399). 
Code Availability The custom code used for generating the results  found in this paper can be accessed at https:// github. com/ UCSD-  SEELab/ persi stentmonit oring. 
Declarations  
Conflict of Interests The authors have no relevant financial or nonfinancial interests to disclose. 
Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long  as you give appropriate credit to the original author(s) and the source,  provide a link to the Creative Commons licence, and indicate if changes  were made. The images or other third party material in this article are  included in the article’s Creative Commons licence, unless indicated  otherwise in a credit line to the material. If material is not included in  the article’s Creative Commons licence and your intended use is not  permitted by statutory regulation or exceeds the permitted use, you will  need to obtain permission directly from the copyright holder. To view a  copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. 
References 
 1. Alvear, O., Zema, N.R., Natalizio, E., Calafate, C.T.: Using uavbased systems to monitor air pollution in areas with poor accessibility. J. Adv. Transp. 2017. https:// doi. org/ 10. 1155/ 2017/ 82043  53 (2017) 
 2. Alvear, O., Calafate, C.T., Zema, N.R., Natalizio, E., HernándezOrallo, E., Cano, J. -C., Manzoni, P.: A discretized approach  
(42) ∫ Nk−3N⊺ k−3dt = ∫ N0N⊺ 0dt = Δ휏퐈 
(43) min 
퐂i,Mi,Δ휏i 
N ∑ 
i=1 
( 
MiΔ휏i + 휖퐂⊺ i 퐀(3)퐀(3)⊺퐂i 
) 
to air pollution monitoring using uavbased sensing. Mob.  Netw. Appl. 23(6), 1693–1702 (2018). https:// doi. org/ 10. 1007/  s11036- 018- 1065-4 
 3. Allison, R.S., Johnston, J.M., Craig, G., Jennings, S.: Airborne  optical and thermal remote sensing for wildfire detection and  monitoring. Sensors 16(8), 1310 (2016). https:// doi. org/ 10. 3390/  s1608 1310 
 4. Bushnaq, O.M., Chaaban, A., AlNaffouri, T.Y.: The role of uaviot networks in future wildfire detection. IEEE Internet of Things  Journal. https:// doi. org/ 10. 1109/ JIOT. 2021. 30775 93 (2021) 
 5. Aslan, Y.E., Korpeoglu, I., Ulusoy, Ö.: A framework for use of  wireless sensor networks in forest fire detection and monitoring.  Comput. Environ. Urban. Syst. 36(6), 614–625 (2012). https:// doi.  org/ 10. 1016/j. compe nvurb sys. 2012. 03. 002 
 6. Le Ny, J., Feron, E., Dahleh, M.A.: Scheduling continuoustime  kalman filters. IEEE Trans. Autom. Control 56(6), 1381–1394  (2011). https:// doi. org/ 10. 1109/ TAC. 2010. 20959 70 
 7. Jawaid, S.T., Smith, S.L.: Submodularity and greedy algorithms  in sensor scheduling for linear dynamical systems. Automatica  61, 282–288 (2015). https:// doi. org/ 10. 1016/j. autom atica. 2015.  08. 022 
 8. Asghar, A.B., Jawaid, S.T., Smith, S.L.: A complete greedy  algorithm for infinitehorizon sensor scheduling. Automatica 81,  335–341 (2017). https:// doi. org/ 10. 1016/j. autom atica. 2017. 04. 018 
 9. Zhang, H., Ayoub, R., Sundaram, S.: Sensor selection for kalman  filtering of linear dynamical systems: complexity, limitations and  greedy algorithms. Automatica 78, 202–210 (2017). https:// doi.  org/ 10. 1016/j. autom atica. 2016. 12. 025 
 10. Justice, C., Giglio, L., Korontzi, S., Owens, J., Morisette, J., Roy,  D., Descloitres, J., Alleaume, S., Petitcolin, F., Kaufman, Y.: The  modis fire products. Remote Sens. Environ. 83(1-2), 244–262  (2002). https:// doi. org/ 10. 1016/ S0034- 4257(02) 00076-7 
 11. Koltunov, A., Ustin, S.L., Prins, E.M.: On timeliness and accuracy  of wildfire detection by the goes wfabba algorithm over california  during the 2006 fire season. Remote Sens. Environ. 127, 194–209  (2012). https:// doi. org/ 10. 1016/j. rse. 2012. 09. 001 
 12. Schroeder, W., Oliva, P., Giglio, L., Csiszar, I.A.: The new viirs  375 m active fire detection data product: Algorithm description  and initial assessment. Remote Sens. Environ. 143, 85–96 (2014).  https:// doi. org/ 10. 1016/j. rse. 2013. 12. 008 
 13. Koltunov, A., Ustin, S.L., Quayle, B., Schwind, B., Ambrosia,  V.G., Li, W.: The development and first validation of the goes  early fire detection (goesefd) algorithm. Remote Sens. Environ.  184, 436–453 (2016). https:// doi. org/ 10. 1016/j. rse. 2016. 07. 021 
 14. Smith, S.L., Schwager, M., Rus, D.: Persistent robotic tasks:  Monitoring and sweeping in changing environments. IEEE Trans.  Robot. 28(2), 410–426 (2012). https:// doi. org/ 10. 1109/ TRO. 2011.  21744 93 
 15. Cassandras, C.G., Lin, X., Ding, X.: An optimal control approach  to the multiagent persistent monitoring problem. IEEE Trans.  Autom. Control 58(4), 947–961 (2013). https:// doi. org/ 10. 1109/  TAC. 2012. 22255 39 
 16. Song, C., Liu, L., Feng, G., Xu, S.: Optimal control for multiagent persistent monitoring. Automatica 50(6), 1663–1668  (2014). https:// doi. org/ 10. 1016/j. autom atica. 2014. 04. 011 
 17. Atanasov, N., Le Ny, J., Daniilidis, K., Pappas, G.J.: Information  Acquisition with Sensing Robots: Algorithms and Error Bounds.  In: Robotics and Automation (ICRA), IEEE International Conference On, Pp. 6447–6454. https:// doi. org/ 10. 1109/ ICRA. 2014.  69078 11 (2014) 
 18. Lin, X., Cassandras, C.G.: An optimal control approach to the  multiagent persistent monitoring problem in twodimensional  spaces. IEEE Trans. Autom. Control 60(6), 1659–1664 (2015).  https:// doi. org/ 10. 1109/ TAC. 2014. 23597 12 
 19. Lan, X., Schwager, M.: Rapidly exploring random cycles: Persistent estimation of spatiotemporal fields with multiple sensing  
Page 17 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
robots. IEEE Trans. Robot. 32(5), 1230–1244 (2016). https:// doi.  org/ 10. 1109/ TRO. 2016. 25967 72 
 20. Zhou, N., Cassandras, C.G., Yu, X., Andersson, S.B.: Optimal  eventdriven multiagent persistent monitoring with graphlimited  mobility. IFACPapersOnLine 50(1), 2181–2186 (2017) 
 21. Yu, X., Andersson, S.B., Zhou, N., Cassandras, C.G.: Optimal  dwell times for persistent monitoring of a finite set of targets. In:  American Control Conference (ACC). https:// doi. org/ 10. 23919/  ACC. 2017. 79638 17, vol. 2017, pp 5544–5549. IEEE (2017) 
 22. Ostertag, M., Atanasov, N., Rosing, T.: Robust velocity control  for minimum steady state uncertainty in persistent monitoring  applications. In: 2019 American Control Conference (ACC).  https:// doi. org/ 10. 23919/ ACC. 2019. 88143 76, pp 2501–2508.  IEEE (2019) 
 23. Pinto, S.C., Andersson, S.B., Hendrickx, J.M., Cassandras, C.G.:  Optimal periodic multiagent persistent monitoring of a finite set  of targets with uncertain states. In: 2020 American Control Conference (ACC). https:// doi. org/ 10. 23919/ ACC45 564. 2020. 91473  76, pp 5207–5212. IEEE (2020) 
 24. Metia, S., Oduro, S., Sinha, A.: Pollutant profile estimation using  unscented Kalman filter. In: Advances in Control, Signal Processing and Energy Systems. https:// doi. org/ 10. 1007/ 978- 981- 32- 9346- 5_2, pp 17–28. Springer (2020) 
 25. Garg, S., Ayanian, N.: Persistent monitoring of stochastic spatiotemporal phenomena with a small team of robots. In: Robotics:  Science and Systems (RSS) Conference. https:// doi. org/ 10. 15607/  RSS. 2014.X. 038, vol. 2014. Robotics: Science and Systems (2014) 
 26. Zhao, S., Ma, Y., Huang, B.: Probabilistic monitoring of sensors in  statespace with variational bayesian inference. IEEE Trans. Ind.  Electron. 66(3), 2154–2163 (2018). https:// doi. org/ 10. 1109/ TIE.  2018. 28380 88 
 27. Kiefer, J.: Optimum experimental designs. Journal of the Royal Statistical Society: Series B (Methodological) 21(2), 272–304 (1959).  https:// doi. org/ 10. 1111/j. 2517- 6161. 1959. tb003 38.x 
 28. Yang, C., Kaplan, L., Blasch, E.: Performance measures of covariance and information matrices in resource management for target  state estimation. IEEE Trans. Aerosp. Electron. Syst. 48(3), 2594– 2613 (2012). https:// doi. org/ 10. 1109/ TAES. 2012. 62376 11 
 29. Zhao, L., Zhang, W., Hu, J., Abate, A., Tomlin, C.J.: On the optimal  solutions of the infinitehorizon linear sensor scheduling problem.  IEEE Trans. Autom. Control 59(10), 2825–2830 (2014). https://  doi. org/ 10. 1109/ TAC. 2014. 23142 22 
 30. Hari, S.K.K., Rathinam, S., Darbha, S., Kalyanam, K., Manyam,  S.G., Casbeer, D.: Bounding algorithms for persistent monitoring of  targets using unmanned vehicles. In: 2019 International Conference  on Unmanned Aircraft Systems (ICUAS), pp. 615–621. https:// doi.  org/ 10. 1109/ ICUAS. 2019. 87981 34 (2019) 
 31. Hari, S.K.K., Rathinam, S., Darbha, S., Kalyanam, K., Manyam,  S.G., Casbeer, D.: Persistent monitoring of dynamically changing  environments using an unmanned vehicle (2018) 
 32. Joshi, S., Boyd, S.: Sensor selection via convex optimization. IEEE  Trans. Signal Process. 57 (2), 451–462 (2008). https:// doi. org/ 10.  1109/ TSP. 2008. 20070 95 
 33. Tang, L., Shao, G.: Drone remote sensing for forestry research and  practices. J. Forestry Res. 26(4), 791–797 (2015). https:// doi. org/  10. 1007/ s11676- 015- 0088-y 
 34. Portugal, D., Pippin, C., Rocha, R.P., Christensen, H.: Finding optimal routes for multirobot patrolling in generic graphs. In: IEEE/ RSJ International Conference on Intelligent Robots and Systems  (IROS), pp. 363–369. https:// doi. org/ 10. 1109/ IROS. 2014. 69425 85  (2014) 
 35. Kant, K., Zucker, S.W.: Toward efficient trajectory planning: The  pathvelocity decomposition. Int. J. Robot. Res. 5(3), 72–89 (1986).  https:// doi. org/ 10. 1177/ 02783 64986 00500 304 
 36. Yu, J., Schwager, M., Rus, D.: Correlated orienteering problem and  its application to persistent monitoring tasks. IEEE Trans. Robot.  32(5), 1106–1118 (2016). https:// doi. org/ 10. 1109/ TRO. 2016. 25934  50 
 37. Hollinger, G.A., Sukhatme, G.S.: Samplingbased robotic information gathering algorithms. Int. J. Robot. Res. 33(9), 1271–1287  (2014). https:// doi. org/ 10. 1177/ 02783 64914 533443 
 38. Ghaffari Jadidi, M., Valls Miro, J., Dissanayake, G.: Samplingbased incremental information gathering with applications to  robotic exploration and environmental monitoring. Int. J. Robot.  Res. 38(6), 658–685 (2019). https:// doi. org/ 10. 1177/ 02783 64919  844575 
 39. Atanasov, N., Le Ny, J., Daniilidis, K., Pappas, G.J.: Decentralized  active information acquisition: Theory and application to multirobot slam. In: 2015 IEEE International Conference on Robotics  and Automation (ICRA). https:// doi. org/ 10. 1109/ ICRA. 2015. 71398  63, pp 4775–4782. IEEE (2015) 
 40. Mueller, M.W., Hehn, M., D’Andrea, R.: A computationally efficient motion primitive for quadrocopter trajectory generation. IEEE  Trans. Robot. 31(6), 1294–1310 (2015). https:// doi. org/ 10. 1109/  TRO. 2015. 24798 78 
 41. Liu, S., Atanasov, N., Mohta, K., Kumar, V.: Searchbased motion  planning for quadrotors using linear quadratic minimum time control. In: 2017 IEEE/RSJ International Conference on Intelligent  Robots and Systems (IROS). https:// doi. org/ 10. 1109/ IROS. 2017.  82061 19, pp 2872–2879. IEEE (2017) 
 42. Mellinger, D., Kumar, V.: Minimum snap trajectory generation and  control for quadrotors. In: 2011 IEEE International Conference on  Robotics and Automation. https:// doi. org/ 10. 1109/ ICRA. 2011.  59804 09, pp 2520–2525. IEEE (2011) 
 43. Tang, M., Tong, R., Wang, Z., Manocha, D.: Fast and Exact Continuous Collision Detection with Bernstein Sign Classification. ACM  Transactions on Graphics 33(6). https:// doi. org/ 10. 1145/ 26612 29.  26612 37 (2014) 
 44. Richter, C., Bry, A., Roy, N.: Polynomial trajectory planning for  aggressive quadrotor flight in dense indoor environments. In: Inaba,  M., Corke, P. (eds.) Robotics Research: The 16th International  Symposium ISRR. Springer Tracts in Advanced Robotics. https://  doi. org/ 10. 1007/ 978-3- 319- 28872-7_ 37, pp 649–666. Springer  (2016) 
 45. Tang, S., Kumar, V.: Autonomous flight. Annual Review of Control  Robotics, and Autonomous Systems 1(1), 29–52 (2018). https:// doi.  org/ 10. 1146/ annur evcontr ol- 060117- 105149 
 46. Liu, S., Watterson, M., Mohta, K., Sun, K., Bhattacharya, S.,  Taylor, C.J., Kumar, V.: Planning dynamically feasible trajectories for quadrotors using safe flight corridors in 3-D complex environments. IEEE Robotics and Automation Letters  2(3), 1688–1695 (2017). https:// doi. org/ 10. 1109/ LRA. 2017.  26635 26 
 47. Liu, S., Mohta, K., Atanasov, N., Kumar, V.: SearchBased motion  planning for aggressive flight in SE(3). IEEE Robot. Autom. Lett.  3(3), 2439–2446 (2018). https:// doi. org/ 10. 1109/ LRA. 2018. 27956  54 
 48. Van Nieuwstadt, M.J., Murray, R.M.: Realtime trajectory generation for differentially flat systems. International Journal of Robust  and Nonlinear Control 8(11), 995–1020 (1998). https:// doi. org/ 10.  1002/ (SICI) 1099- 1239(199809) 8: 11% 3C995:: AIDRNC373%  3E3.0. CO;2-W 
 49. Ding, W., Gao, W., Wang, K., Shen, S.: An Efficient BsplineBased  Kinodynamic Replanning Framework for Quadrotors. https:// doi.  org/ 10. 1109/ TRO. 2019. 29263 90 (2019) 
 50. Zhou, B., Gao, F., Wang, L., Liu, C., Shen, S.: Robust and efficient  quadrotor trajectory generation for fast autonomous flight. IEEE  
2   Page 18 of 19 Journal of Intelligent & Robotic Systems (2022) 106: 2 
Robotics and Automation Letters 4(4), 3529–3536 (2019). https://  doi. org/ 10. 1109/ LRA. 2019. 29279 38 
 51. Tzoumas, V., Jadbabaie, A., Pappas, G.J.: Sensor placement for  optimal kalman filtering: Fundamental limits, submodularity,  and algorithms. In: 2016 American Control Conference (ACC).  https:// doi. org/ 10. 1109/ ACC. 2016. 75249 14, pp 191–196. IEEE  (2016) 
 52. Chamon, L.F., Pappas, G.J., Ribeiro, A.: Approximate supermodularity of kalman filter sensor selection. IEEE Trans.  Autom. Control 66(1), 49–63 (2020). https:// doi. org/ 10. 1109/  TAC. 2020. 29737 74 
 53. Chu, E.-W., Fan, H.-Y., Lin, W.-W.: A structurepreserving doubling  algorithm for continuoustime algebraic riccati equations. Linear  Algebra and Its Applications 396, 55–80 (2005). https:// doi. org/ 10.  1016/j. laa. 2004. 10. 010 
 54. Applegate, D., Bixby, R., Chvatal, V., Cook, W.: Concorde TSP  solver (2006) 
 55. MOSEK ApS: MOSEK Optimizer API for Python 9.3.7. https://  
docs. mosek. com/ latest/ pytho napi/ index. html(2019) 
 56. Lee, T., Leok, M., McClamroch, N.H.: Geometric tracking control  of a quadrotor uav on se (3). In: 49th IEEE Conference on Decision  and Control (CDC). https:// doi. org/ 10. 1109/ CDC. 2010. 57176 52,  pp 5420–5425. IEEE (2010) 
Publisher’s Note Springer Nature remains neutral with regard to  jurisdictional claims in published maps and institutional affiliations. 
Michael H. Ostertag is a PhD candidate at UC San Diego in the  Department of Electrical and Computer Engineering, advised by Prof.  Tajana Simunic Rosing. After receiving B.S./M.S. degrees from the  Rochester Institute of Technology, he worked at BlackBox Biometrics  developing technology to help detect mild traumatic brain injury in  soldiers and athletes. His research interests include environmental  sensing using distributed sensors and autonomous robotic platforms. 
Nikolay Atanasov is an Assistant Professor of Electrical and Computer  Engineering at the University of California San Diego. He obtained a  Ph.D. degree in Electrical and Systems Engineering from the University of Pennsylvania in 2015. His research focuses on robotics, control  theory, and machine learning. He works on probabilistic models that  unify geometry and semantics in simultaneous localization and mapping (SLAM) and optimal control and reinforcement learning techniques for active perception. 
Tajana Šimunić Rosing is a Professor, a holder of the Fratamico  Endowed Chair, IEEE & ACM Fellow, and a director of System Energy  Efficiency Lab at UCSD. Her research interests are in energy efficient  computing, computer architecture, and cyberphysical systems. Prior  to coming to UCSD, she was a research scientist at HP Labs while also  leading research efforts at Stanford University. She finished her PhD  in EE in 2001 at Stanford, concurrently with finishing her Masters in  Engineering Management.  Prior to her PhD, she worked as a senior  design engineer at Altera, now Intel. 
Page 19 of 19    2 Journal of Intelligent & Robotic Systems (2022) 106: 2 
",Trajectory Planning and Optimization for Minimizing Uncertainty.pdf,4
"DOI: 10.1002/net.21818 
S P E C I A L I S S U E 
Optimization approaches for civil applications of unmanned aerial vehicles (UAVs) or aerial drones: A survey 
Alena Otto1 Niels Agatz2 James Campbell3 Bruce Golden4 Erwin 
Pesch5 
1Department of Management Information Science, University of Siegen, Siegen, Germany 
2Rotterdam School of Management, Erasmus University, Rotterdam, Netherlands 
3College of Business Administration, University of Missouri, St. Louis, Missouri 
4Robert H. Smith School of Business, University of Maryland, College Park, Maryland 
5Department of Management Information Science, University of Siegen and Center for Advanced Studies in Management, HHL, Leipzig, Germany 
Correspondence Alena Otto, Department of Management Information Science, University of Siegen, Siegen, Germany. Email: alena.otto@unisiegen.de 
Abstract 
Unmanned aerial vehicles (UAVs), or aerial drones, are an emerging technology with signiﬁcant market potential. UAVs may lead to substantial cost savings in, for instance, monitoring of diﬃculttoaccess infrastructure, spraying ﬁelds and performing surveillance in precision agriculture, as well as in deliveries of packages. In some applications, like disaster management, transport of medical supplies, or environmental monitoring, aerial drones may even help save lives. In this article, we provide a literature survey on optimization approaches to civil applications of UAVs. Our goal is to provide a fast point of entry into the topic for interested researchers and operations planning specialists. We describe the most promising aerial drone applications and outline characteristics of aerial drones relevant to operations planning. In this review of more than 200 articles, we provide insights into widespread and emerging modeling approaches. We conclude by suggesting promising directions for future research. 
K E Y W O R D S 
drones, operations planning, optimization, survey article, UAVs, unmanned aerial vehicles 
1 INTRODUCTION 
Aerial drones or unmanned aerial vehicles (UAVs) are pilotless aircraft that are used, or can potentially be used, in a wide range of civil (nonmilitary) applications. UAV technology is rapidly evolving and may signiﬁcantly change the business landscape in the coming years. The steampowered ﬂying pigeon of Archytas the Tarantine in ancient Greece, which may have been the ﬁrst aerial drone, was allegedly able to ﬂy only about 200 m [59, 102]. However, modern parcel delivery drones can ﬂy up to 15 km with about a 3 kg (6.5 pound) payload [143, 224] and ﬁxedwing drones for medical supply delivery ﬂy up to 150 km [250]. We call aerial drones simply as drones throughout this article. Drones come in a variety of designs, such as a ﬁxedwing drone, which looks like a plane, or a rotorcraft drone, which looks similar to a helicopter, though often with multiple rotors (usually up to eight). The latter has vertical takeoﬀ and landing capabilities and it can hover, which makes this design particularly attractive in close quarters, such as crowded urban areas. Tiltwing drones combine features of ﬁxedwing and rotorcraft drones using wings that can be swiveled. Drones may use diﬀerent types of propulsive power, including internal combustion engines, electric batteries, and solar and hydrogen fuel cells [38]. Some drones must be closely remote controlled by a human pilot, while fully autonomous drones are able to decide how to accomplish complex tasks in an uncertain environment [60]. 
Networks. 2018;00:1–48. wileyonlinelibrary.com/journal/net © 2018 Wiley Periodicals, Inc. 1 
Published on: 25 March 2018 
R E V I E W A R T I C L E 
Networks. 2018;72:411–458. wileyonlinelibrary.com/journal/nav © 2018 Wiley Periodicals, Inc. 411 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
2 OTTO ET AL. 
FIGURE 1 Interest in optimization of civil operations of drones is growing 
In the US, current regulations signiﬁcantly restrict operations of drones in order to safeguard the congested airspace and people and property on the ground by, for example, requiring operations to remain within the visual line of sight of a human operator. However, other countries have very diﬀerent regulations, and, even in the US and the EU, regulations have changed substantially since 2013, when the roadmaps for airspace integration of civil drones were introduced [79, 230]. As stakeholders gain experience in drone technology and as collision avoidance systems mature, it is expected that regulations will be relaxed signiﬁcantly, thus opening a range of new, attractive applications for drones. Therefore, market forecasts predict a rapid development of the commercial drone market. For example, the Teal Group Corporation, commissioned by the Federal Aviation Administration (FAA), predicts that the operational ﬂeet of small commercial drones in the US will reach about 500,000, or $3.3 billion in purchase value, by 2020 [80]. Drones already perform regular commercial operations (eg, [224, 250]), and ideas for many new potential applications regularly appear in the media. As drones play a growing role in business operations, questions of planning and optimization increase in practical and academic importance. The aim of this survey is to review optimization problems arising in the operations planning of drones in civil applications. This survey seeks to summarize the knowledge generated in 217 articles and provides insights based on many additional articles as supporting literature. Time series statistics on the surveyed literature indicate the growing academic interest in the topic (see Figure 1). For researchers, our survey provides a fast entry point into the topic, as well as directions for future F1 research and further reading. For practitioners, we provide guidance on the existing literature and point to sources on operational enhancement, cost and proﬁt optimization, and how to estimate the business payoﬀs of emerging drone technologies. Because of its breadth, the topic of our literature survey intersects with a number of other related topics that are beyond the scope of this article: 
• We do not include articles on military and security applications of drones, including patrolling applications. However, note that there is no sharp boundary between military and civil applications, since some models and methods can be used in both contexts. 
• We include only those papers that explicitly address drones in their title, abstract, or keywords using any relevant term or description. For instance, our search included, but was not limited to, such terms as “unmanned aerial vehicle,” “UAV,” “drone,” “unmanned aircraft,” “unmanned aerial system,” “UAS,” “remotely piloted aircraft,” and “remotely piloted vehicle.” Note that operations planning of drones are closely related to more general topics, such as operations planning of mobile robots (including groundbased drones) and mobile sensors as well as vehicle routing and machine scheduling, which are not part of this survey. 
• We consider complex drone operations that usually involve several tasks. Thus, we do not include articles on obstacleavoiding path planning from a given starting point to a given end point [12, 63, 108, 153, 194, 203, 220, 227, 245, 306] or on planning trajectories of drones ﬂying in formation [220]. We refer the interested reader to the work of Goerzen et al. [92], Khaksar et al. [129] and Yang et al. [307] on path planning problems. 
• We survey articles that formulate an optimization problem, describing objective functions, decision variables, and the set of feasible solutions. Note that optimization may be part of control approaches, such as motion control or communication control. 
412 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 3 
• We survey papers written in English and published in peerreviewed journals as well as in some peerreviewed conference proceedings that are indexed in the databases of Web of Science. To ensure our survey is current, we have also included the latest manuscripts from opensource archives, which we highlight in Figure 1. Due to the large number of publications, only the most recent manuscripts from opensource archives and only select conference papers are included in this survey. While we are unable to include all the publications, we believe that the reviewed articles provide a comprehensive overview of the topic. 
The existing literature surveys on drones mostly focus on speciﬁc ﬁelds of application, such as glaciology [30], coastal surveying [279], or precision agriculture [317]. A number of literature surveys investigate topics other than planning of drone operations or optimization; these are, for example, Chen et al. [44] and Sujit et al. [261] on path following and control, Gupta et al. [105] on communication protocols, Nex and Remondino [190] on drone platforms for 3D mapping, and Jawhar et al. [116] on communication aspects of drones. Other articles elaborate on adaptations of speciﬁc optimization problems to robotic applications in general, such as Galceran and Carreras [87] on coverage path planning and Robin and Lacroix [228] on target tracking. General reference works on drones include LeMieux [145], Murphy [185], Valavanis and Vachtsevanos [282], and Siciliano and Khatib [249]. The history of drones as well as a summary of present innovations and research developments can be found in Nonami [196] and van Blyenburgh [284]. Only one survey, by Coutinho and Fliege [56], relates to optimization aspects of drone operations. However, these authors study publications related to a particular optimization problem—the vehicle routing and trajectory optimization problem. For this reason, to the best of our knowledge, this article is the ﬁrst to provide a comprehensive general overview of optimization problems arising in operations planning of civil drone applications. We proceed as follows. We provide an overview of popular civil applications of drones in Section 2. In Section 3, we summarize some important characteristics of drones for consideration in operations planning. Based on our extensive survey of the literature, we then describe planning problems for drone operations in Section 4 and planning problems for combined operations of drones and other robots or vehicles in Section 5. Section 6 is devoted to strategic, tactical, and operational issues linked to drone operations. Section 7 discusses the extent to which drone operations pose new optimization problems as opposed to new instances of wellstudied ones. Section 8 outlines directions for future research and concludes our article. 
2 CIVIL APPLICATIONS OF DRONES 
Many industries can potentially beneﬁt from pilotless technology because it can reduce labor cost. Drones can operate in dangerous environments that would be inaccessible to humans. Furthermore, pilotless technology lowers the weight of the aircraft, and thus its energy consumption, by making the cockpit and environmental systems, which provide air supply, thermal control, and cabin pressurization, unnecessary. Drones do not require roads and can, thus, access locations that are diﬃcult to reach by roads. Figures 2–5 illustrate several models of drones. F2-5 The increasing utility of drones is driving a corresponding increase in their ﬁnancial impact. PricewaterhouseCoopers [170] predicts the potential market value of business services that may beneﬁt from drone technology to reach several billion dollars in a number of industries (see Figure 6). In this section, we will discuss the most promising emerging drone applications according F6 to their potential market value. 
• Physical infrastructure includes such industries as energy, roads, railways, oil and gas, and construction. Drones have already been used to examine terrain at future construction sites, to track progress at existing construction sites, to inventory the assets, and to regularly inspect facilities as part of maintenance (see the octocopter in Figure 2). Drones not only provide high resolution 3D aerial recordings at low cost, but also oﬀer safety beneﬁts by replacing humans for risky inspections. For example, the ﬁrst pilot project of Lufthansa and its partner DJI was to use drones to inspect rotor blades of wind turbines, a costly and dangerous task traditionally performed by industrial climbers [157]. 
• Agriculture is another promising application of drone technology: Drones may map soil properties, assess crop health, spray targeted fertilizer and other treatments, and monitor livestock. Japanese farmers have been using drones since the early 1990s, and as early as the mid-2000s, drones sprayed about 10% of all sprayed paddy ﬁelds in Japan [196]. Several market analysts predict agriculture to be the largest application for drones in the US in the coming years [42]. Figure 4 illustrates a popular agricultural drone, Yamaha RMAX (1 m in height, 2.75 m in length), which can carry a payload of about 28 kg and can spend about 1.5 hours in ﬂight. Figure 5 depicts a ﬁxedwing drone owned by AeroTerrascan and used for remote sensing. 
OTTO et al. 413 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
FIGURE 2 Introspect’s drone B 3.1 is used for taking aerial images. A photograph by Molnár Zsolt distributed under a CC0 1.0 license. Source: https://commons.wikimedia.org/wiki/File:Interspect_UAV_B_3.1.png [Color ﬁgure can be viewed at wileyonlinelibrary.com] 
FIGURE 3 DHL’s drone Paketcopter used in package delivery. A photograph by Frankhöﬀner distributed under a CC BYSA 3.0 Source: https://commons.wikimedia.org/wiki/File:Package_copter_microdrones_dhl.jpg [Color ﬁgure can be viewed at wileyonlinelibrary.com] 
FIGURE 4 Yamaha’s drone RMAX, powered by gasoline, is popular in agricultural applications. A photograph by Gtuav distributed under a CC BYSA 3.0 license. Source: https://commons.wikimedia.org/wiki/File:YamahaRMax.jpg [Color ﬁgure can be viewed at wileyonlinelibrary.com] 
414 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
FIGURE 5 AeroTerrascan’s drone Ai450 mapping a ﬁeld in Indonesia. A photograph by GIII distributed under a CC BYSA 4.0 license. Source: https://kids.kiddle.co/Image:Agriculture_UAV.jpg [Color ﬁgure can be viewed at wileyonlinelibrary.com] 
• Transport, or delivery applications, have recently received considerable media attention, mainly because of the prospectofdoortodoorexpressdeliveriesatlowcost.Deliveryapplicationsincludepackagedeliveriestoremote rural regions and ﬁrstand lastmile deliveries in urban and suburban areas as well as express deliveries of, for example, deﬁbrillators to treat outofhospital cardiac arrests [51] or customized parts to assembly lines. Delivery of medical supplies, especially of vaccines and blood, may signiﬁcantly improve the quality of medical service in developing countries, where road infrastructure is poor and some parts of the route may become impassable during certain seasons. Unsurprisingly, the ﬁrst productionlevel use of a delivery drone to move beyond a pilot test or demonstration project was blood delivery in Rwanda by Zipline’s ﬁxedwing drone [250]. According to expert estimates, the cost savings of the drone vaccine transport compared to the vaccine transportation by common trucks surpass the required ﬁxed cost for establishing drone infrastructure [107]. Perhaps the most critical stage in the drone delivery process is package release, because the transported goods may be damaged. Numerous technical solutions have been developed to ﬁt various applications, including drone landing at speciﬁed locations (eg, platforms or “delivery rugs”) and the use of a parachute or tether to lower the item [236, 250]. Overall, although the total distance traveled in a droneonly delivery system will likely be longer than in a truckonly delivery system due to the drone’s limited payload, drones may be faster than trucks, have a lower cost per mile to operate, and emit less CO2. Thus, they represent a greener alternative to conventional delivery modes [95]. However, regulations will potentially restrict development of the delivery segment more than other ﬁelds of application [83]. Operations of drones in urban areas will be especially regulated due to safety and privacy issues. 
Further applications of drones include security, where drones perform observations, and entertainment and media, where drones are used for ﬁlming, taking pictures, and for special eﬀects. The category other encompasses examination of disaster and accident sites for risk assessment and risk monitoring by insurance companies, mapping and surveying of opencast and underground mining sites, as well as applications of drones in telecommunications [170]. In the latter, several technological solutions have been proposed to extend internet and mobile access. Google’s project Loon and Facebook’s drone Aquila are 
FIGURE 6 Potential market value of commercial drone applications estimated by Mazur et al. [170] 
OTTO et al. 415 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
6 OTTO ET AL. 
examples of socalled highaltitude platforms that would be deployed in the stratosphere [251, 252]. These platforms serve as a gateway and establish communication links for computing devices. By comparison, lowaltitude platforms, such as ﬁxedwing drones, rotorcrafts or kiteballoons, provide advantages of rapid deployment and low cost and are most suited for temporary events[43]. Suchtemporaryeventsmaybe,forinstance,festivals,sportingevents,anddisastermanagementoperationsinregions with damaged communication infrastructure [315]. Recently, Nokia has tested drones equipped with mobile base stations to improve coverage in a rural region in Scotland [231]. Drones may also serve as cloudlets and relieve energy consumption of mobile devices by taking over computations, which are energy consuming; they may also perform data storage and provide video on demand services at camping sites [119, 120]. In internal transport, drones like Fraunhofer Institute’s ﬂying and rolling robot Bin:Go serve warehouses by lifting payloads of up to 1 kg from shelves [64]. Drones are also actively used in disaster management and in environmental monitoring, such as monitoring of bush ﬁres and icebergs or poststorm coastal erosion assessment, as well as in collecting data from sensors [6, 148, 279]. Besides that, drones may assist the blind and escort individuals as they walk alone through dangerous areas at night [155]. In sum, drones might be advantageously used in a number of industries to perform a wide range of operations. To realize the potential market value in any of the discussed applications, ﬁrms should be able to deploy drones in civil airspace safely and at low cost. Drones may be part of an integrated airspace, which will be shared with piloted aircraft, or alternatively, certain airspace zones may be reserved exclusively for drones. For example, Amazon proposed two classes of droneonly airspace: a lowspeed zone below 200 feet (61 m) and a highspeed zone between 200 and 400 feet (61–122 m). The company also recommends a no-ﬂy zone between 400 and 500 feet (122–152 m) to serve as a buﬀer zone between drones and piloted traﬃc [124]. Other experts, including those at NASA, recommend reserving specialized drone corridors, also called air highways, for groups of drones traveling in the same direction [134]. Note that, contrary to a widespread perception that drone technology has merely entered the testing phase, drones are already in production use in a number of applications, as discussed in this section, such as blood transport in Rwanda and postal service provided by delivery company DPD in a rural region of Southern France [224, 250]. 
3 CHARACTERISTICS OF DRONES RELEVANT TO OPERATIONAL PLANNING 
Drones have peculiarities that must be considered in modeling. In this section, we brieﬂy summarize common parameters and restrictions used in existing modeling approaches for drone operations (see Figure 7) and introduce some abbreviations we will F7 use to describe the contributions of the articles in this survey. 
• Speciﬁcs of motion. Drones are able to move in 3D space. Autopilots of drones are usually able to maintain ﬂight stability, hold the required altitude, and autonomously land and take oﬀ. Nevertheless, certain aspects of drone motion may need to be taken into account in planning of drone operations. One of them is the minimum turning radius restriction while changing directions in ﬂight [17, 242, 265], which is especially important for ﬁxedwing drones. For example, this restriction should be considered while spraying farmland in order to avoid blank spots. Although rotorcraft drones, such as quadcopters, may easily reverse their ﬂight direction by making sharp turns, 
FIGURE 7 Characteristics of drones relevant to operational planning 
416 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 7 
each reversal requires additional time and energy, as the drones must come to a halt before moving in a diﬀerent direction [208, 276]. Therefore, researchers look for smooth, constantspeed trajectories respecting the minimum turning radius also for rotorcraft drones [127, 152, 208]. Small and micro drones are highly susceptible to weather conditions, such as wind, which may be modeled as uncertain travel times [202]. Requirements for minimum and maximum ﬂight angles of ﬁxedwing drones should be taken into account during landings and takeoﬀs [86, 209]. Some optimization problems introduce constant setuptime parameters for each landing and takeoﬀ [274]. In photography applications, certain terrain types, such as mountains or trees, may require a speciﬁc angle of ﬂight in order to optimally position the camera [36]. 
• Limited payload. Payloads for package delivery drones do not usually exceed 3 kg (6.5 pounds), and a drone usually carries just one package per sortie [3]. Limitations on the payload are closely related to the capacity of the drone’s energy storage unit and the size and conﬁguration (and cost) of the drone. For example, to maintain a stable ﬂight, the propeller of a rotorcraft drone should generate enough thrust to counter the force of gravity. Therefore, a heavier drone needs more energy than a lighter drone to ﬂy the same distance [68]. 
• Limited ﬂight range. Most drones, with the exception of tethered drones receiving energy via a power cord, carry an energy unit of a limited capacity. Energy consumption of a drone depends on a multitude of factors, such as type of drone (ﬁxed wing vs. rotorcraft), ﬂying altitude (eg, propellers of rotorcrafts have to rotate faster at higher altitudes because of lower air density), ﬂight conditions (such as hovering vs. forward ﬂight), climbing speed, payload, and weather conditions, such as wind. The tradeoﬀ between energy consumption and ﬂying altitude is especially important in surveillance applications: a higher ﬂying altitude enables observation of a larger area at the cost of higher energy consumption [216]. The limited capacity of the energy unit is usually modeled as maximal operation time [274, 292], maximal ﬂying distance [104, 244], or the limited number of addresses a drone can visit during one ﬂight [75, 237]. Emerging technologies, such as thin ﬁlm photovoltaic panels, enable socalled energy harvesting: Small drones may recharge their batteries during ﬂight in the sunlight [162]. Battery swaps and refueling usually require assistance of a human operator [292]; however, there exist fully automated platforms able to exchange or to recharge the battery of a drone in just a couple of minutes [53, 268, 269, 275]. 
• Speciﬁcs of information processing and connectivity. Drones have to maintain communication links with the ground control station to receive instructions and transfer the collected information. Since lineofsight communications are typically required, the signal gets weaker in the shadow of buildings in urban areas, indoors, or under the crowns of trees. Additionally, transmission lines and telecommunication towers may cause signal interference. Therefore, path planning methodologies may avoid or penalize visitation of certain regions [75]. Whenever mobile devices, for example, cell phones, cannot establish a direct connection to the macrocell base station, drones may also serve as intermediaries, acting as ﬂying base stations [5, 91, 178]. Drones may use diﬀerent wireless access methods to provide communication services, which may require assignment of particular time slots and/or frequencies to the users. Another consideration is that the power density of the signal reduces as it passes through a communication channel. Moreover, the signal can vary (fade), for example, due to shadowing from obstacles or due to multipath propagation. Fading may be modeled, for example, as Rayleigh fading (more appropriate for heavily builtup urban environments), Rician fading (if lineofsight communication dominates), or Nakagamim fading [46, 98, 123, 147]. Therefore, drone positioning as a ﬂying base station depends on signal fading along the communication path, path loss, interferences, and noise. Other connectivity challenges emerge when several drones perform tasks cooperatively, since they may need to exchange information by establishing communication links that are subject to noise and dependent on transmission distance [100, 290]. Dronetodrone communication also enables drones to attend a GPSdenied area while maintaining a communication link with another drone able to receive the GPS signal [164]. The limited memory capacity of the drone should be respected in gathering data from sensors, such as RFIDs attached to wild animals or sensors for pipeline monitoring [117, 148, 159, 260]. 
• Handling by a human operator. In a number of countries, drone regulations require human operators [185, 305]. Generally, a human operator performs a number of setup operations before the drone’s takeoﬀ and, after its landing [186], he or she may have to control the drone and to examine information collected by the drone in real time. 
In the overview tables in Sections 4–6, we provide some keywords on modeling parameters of drones for each surveyed article. In these tables, articles may appear several times, for example, when the authors have provided several models describing 
OTTO et al. 417 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
8 OTTO ET AL. 
diﬀerent types of operations. Observe that some articles simply model drones as vehicles moving at a constant speed as an approximation [222, 239]. In such cases, we left the column “Drone characteristic(s)” in Tables 2−7, 9, and 10 empty. In the following sections, we review articles on drone operations (Section 4), on combined operations of drones and other vehicles, such as trucks or ships (Section 5), and on strategic and operational aspects of preparation for drone operations (Section 6). 
4 PLANNING DRONE OPERATIONS 
Possible civil drone applications are manifold and, therefore, so are the possible related planning problems. Observe that similar optimization problems may emerge in diﬀerent applications. Consider a drone that has to visit several locations, which can be modeled as the traveling salesman problem (TSP). This problem emerges in the infrastructure industry, where a drone has to inspect points of interest of a building; in agriculture, where a drone collects information on sample points in a ﬁeld; in delivery applications, where a drone delivers blood to several hospitals; and in entertainment, where a drone takes pictures of tourist sites. Therefore, we align our classiﬁcation to the problem speciﬁcs instead of areas of application. We arrange our literature review by diﬀerent types of drone operations (see Figure 8), such as F8 
• Area coverage (Section 4.1), where drones should cover a certain area with a sensor of a limited footprint, 
• Search operations (Section 4.2), where drones have to ﬁnd a stationary or moving object, 
• Routing for a set of locations (Section 4.3), where drones have to visit a discrete set of addresses, 
• Data gathering and recharging in a wireless sensor network (WSN) (Section 4.4), where drones have to gather information from a discrete set of locations while considering communication scheduling and memory capacity constraints, 
• Allocating communication links and computing power to mobile devices (Section 4.5), where drones are positioned (or routed) to provide communication links to mobile devices of suﬃcient quality, and 
• Operational aspects of a selforganizing network of drones (Section 4.6). 
See Sections 4.1–4.6 for more detailed deﬁnitions of the operation types. Figure 9 shows that most of the surveyed articles consider routing for a set of locations. Drones in these operations perform F9 surveillance and delivery tasks mostly in the context of infrastructure, agriculture, transport, and entertainment and media applications. 
FIGURE 8 Classiﬁcation of drone operations 
418 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 9 
FIGURE 9 Distribution of the surveyed articles according to types of drone operations 
We have selected the following approach for the literature review in Sections 4.1–4.6 that reﬂects the broadness of the selected topic and heterogeneity of the articles coming from diﬀerent disciplines: operations research, engineering, and informatics. In Tables 2–7, arranged by the operation type, we summarize a number of characteristics for each publication: information on methodological contribution to planning drone operations, characteristics of the drones that have been taken into account in modeling, application that motivated the publication, and whether one or several drones have been considered. The notation used in Tables 2–7 is explained in Table 1. In the accompanying text in Sections 4.1–4.6, we provide a general overview for T1 each operation type. We illustrate typical applications, outline what is speciﬁc about drone operations, discuss some emerging optimization problems, and typical objective functions. Note that because of the broadness of the topic, we cannot provide details on decision variables, constraints, and objective functions for each surveyed publication in the limited space of a journal article. Overall, the articles surveyed in Sections 4–6 propose various solution approaches to optimize operations of drones. In addition to presenting centralized optimization methods, some articles design agentbased, or decentralized, algorithms [22, 142, 321]. Such algorithms are scalable to new drones entering the system, are robust, since drones may not be connected to the control station at all times, and naturally enable parallelized computation with each drone optimizing its own local subproblem. For complex operations with heterogeneous drones belonging to diﬀerent providers, such as disaster management, coordinated planning frameworks with a clear assignment of roles and eﬃcient communication procedures have been proposed to ensure eﬃcient coordination in achieving common goals [212, 296]. Gametheoretic approaches depict drones as strategic rational or adversarial players as in situations when drones belonging to diﬀerent organizational entities compete for clients [137] or for the most energyeﬃcient position in a ﬂying formation [225]. 
4.1 Planning of area coverage 
In coverage problems, one or more drones equipped with sensors of a limited footprint have to monitor (cover) some area P, which can take diﬀerent shapes. This problem arises in disaster management, such as postearthquake assessment, in agriculture, such as in observation of vegetation indexes, and in creating digital terrain maps. The coverage path planning problem, which is to ﬁnd paths of drones equipped with sensors of a limited footprint to cover all points of area P at the lowest possible cost, is discussed in Section 4.1.1. Section 4.1.2 examines the dual planning problem of maximizing information collected from the partially covered area, given some budget constraints. Section 4.1.3 reports on stationary positioning of drones so that their sensors cover all points of P. Although coverage problems with drones are closely related to general coverage problems, some peculiarities may arise. First of all, since drones perform observations from the air, there is a tradeoﬀ between taking pictures from a higher altitude with a larger camera footprint, but lower resolution and higher energy consumption, and taking pictures from a lower altitude with a smaller camera footprint, but higher resolution and lower energy consumption. Secondly, cameras and sensors may be attached to a drone at diﬀerent orientations (eg, sideaimed cameras vs. directlydownward facing cameras) [66], so that diﬀerent shapes and positions of the camera footprint relative to the drone are possible. Moreover, more sophisticated sensors can change their orientation during ﬂight. Lastly, because drones, especially ﬁxedwing drones, may traverse long distances quickly, coverage problems involving nonconvex or disconnected areas gain importance. Table 2 provides an overview of the surveyed articles. T2 
OTTO et al. 419 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
10 OTTO ET AL. 
TABLE 1 Glossary of abbreviations 
M Mathematical model ready for input into an oﬀ-theshelf solver (eg, Gurobi, IBM ILOG Cplex) 
P Theoretical studies on problem properties, such as nontrivial complexity analysis of the problem, worstor bestcase analysis, approximation guarantees for algorithms, polynomial exact algorithms, proofs for stability and adaptivity of online algorithms, proofs for asymptotic optimality of the proposed algorithms, studies on convergence 
E Closedform solution for a nonlinear problem, exact solution with methods of convex optimization 
Control Control algorithms, such as ﬂight altitude control or nonlinear drone trajectory control, that update “on the ﬂy” based on new information as well as frameworks for cooperative control 
Control P Theoretical studies on properties of the formulated control algorithms, such as convergence 
Game P Theoretical studies on properties of the formulated games, such as existence of Nash equilibrium, stability, optimality, convergence, priceofanarchy analysis 
Game H Adaptive algorithms for individual strategic players (rational or adversary) that maximize the player’s utility and potentially converge to a Nash equilibrium 
BnB, BnC, BnP, DP, CP, CG, LR 
Branchandbound, branchandcut, branchandprice, dynamic programming, cutting plane, column generation, or Lagrangian relaxation algorithms, respectively 
Bnd Customized bounds other than LR that are not part of BnB or BnC approaches 
(Decentral) MH One or several (decentralized) metaheuristic algorithms. Metaheuristic algorithms may describe online decision making in the receding, or rolling, horizon. Articles may provide memory and computational complexity analysis of the proposed algorithms 
(Decentral) H One or several customized heuristic algorithms, apart from metaheuristic algorithms. Heuristic algorithms may describe online decision making in the receding, or rolling, horizon. Articles may provide memory and computational complexity analysis of the proposed algorithms 
EmpCS Empirical results and case studies. Empirical results derive parameters and/or constraints or provide detailed explanations of how to derive model parameters from empirical data, they may suggest elements of control other than motion control that are relevant for realworld operations (eg, computer visionbased feedback control). Case studies test suggested operations planning concepts on real drones 
Exp Computational experiments on artiﬁcial or realworld data 
4.1.1 Coverage path planning for full coverage 
Coverage path planning with drones refers to ﬁnding paths of drones such that all points of some area P are covered at least once (see general surveys by Choset [49] and Galceran and Carreras [87] on coverage path planning for robotics). Preferred paths of drones have to avoid unproductive movements, such as repeated surveillance of the same points in P or “idle” ﬂights to the next zone of operation. Common objective functions aim to minimize some distancerelated cost function [66, 199, 276, 300], completion time (in case of several drones or setup times between drone ﬂights) [17, 22, 176, 188], or energy consumption (because the drone’s energy consumption depends on the altitude of ﬂight and the direction of acceleration) [146]. Furthermore, since each turn of the drone requires additional time and energy (see Section 3), a costminimizing path will generally contain fewer turns. Therefore, cost functions are sometimes approximated as the required number of turns [149, 283]. Especially if area P contains obstacles or is nonconvex, coverage path planning is a highly nontrivial task. Therefore, some articles assume paths of speciﬁc patterns (see Figure 10a,b). For example, a drone may move in a lawn mowing pattern along F10 parallel lines that are perpendicular to a ray denoting the sweep direction. Note that for some convex area P, the number of turns is minimized if the largest diﬀerence in ℓ-coordinates between a pair of points belonging to P, when measured along the chosen sweep direction −→ℓ , is at its minimum [115, 168]. Another common pattern involves spiral trajectories [176]. Note that if terrain elevations are present, an energyeﬃcient coverage path may diﬀer greatly from conventional path patterns, such as lawn mowing paths [146]. Overall, as Li et al. [146] discuss, neglecting terrain elevation in coverage path planning may lead to a severalfold increase in coveragepath length and energy consumption. One possible solution approach to the coverage path planning problem is to partition area P with a ﬁne raster into a collection of possibly irregular cells [188, 207, 283, 300]. Afterwards, we can capture information on the connectivity of cells and distances between them in a graph (see Figure 10c). In this way, if we code the cells as nodes of the graph [218], the coverage path planning problem can be transformed into a traveling salesman or a vehicle routing problem (VRP). If we code the cells as edges of the graph, a Chinese postman problem arises [2]. Desired path properties may be enforced with additional constraints, such as 
420 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 11 
TABLE 2 Overview of the surveyed articles: Area coverage 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
Coverage path planning for full coverage (Section 4.1.1) [17] M, EmpCS, Exp several drones limited ﬂight time, sensor with a limited footprint agriculture 
[20] H, Exp several drones sensor with a limited footprint, drones have heterogeneous area coverage (sensing) capability per unit of time 
other (environmental protection and disaster management) 
[22] Decentral H, Control, EmpCS, Exp 
several drones limited ﬂight time, equations of motion, sensor with a limited footprint 
agriculture 
[66] H, Exp drone minimum turning radius, sidefacing camera, sensor with a limited footprint 
general 
[146] MH, EmpCS, Exp 
drone energy consumption depends on the altitude and speed of ﬂight, sensor with a limited footprint 
general 
[149] H, EmpCS, Exp drone sensor with a limited footprint (depends on the drone altitude), function of energy consumption (higher consumption for the turning motion compared to ﬂat ﬂying) 
general 
[168] H, Exp several drones limited ﬂight distance, sensor with a limited footprint 
other (environmental protection and disaster management) 
[176] H, EmpCS drone equations of motion (eg, no sharp turning angles allowed), sensor with a limited footprint 
agriculture 
[188] M, Exp several drones limited ﬂight time, sensor with a limited footprint other (environmental protection and disaster management) 
[199] M, Decentral H, EmpCS, Exp 
several drones limited ﬂight time, minimum turning radius, minimum separation distance between each two ﬂying drones, sensor with a limited footprint 
general 
[200] M, Decentral H, H, Exp 
several drones minimum turning radius, limited ﬂight time, sensor with a limited footprint 
general 
[207] H, Exp several drones limited ﬂight distance general 
[217] EmpCS several drones ﬂying in formation, sensor with a limited footprint other (environmental protection and disaster management) 
[276] H, Exp drone sensor with a limited footprint general 
[283] MH, EmpCS several drones energy consumption function (depends on the amplitude of the turn), sensor with a limited footprint 
agriculture 
[300] P, H, EmpCS, Exp 
drone sensor with a limited footprint, discussed: inﬂuence of wind, restrictions on the turning radius 
other (rescue) 
Coverage path planning for partial coverage: Maximum information collection (Section 4.1.2) [36] MH, EmpCS, Exp 
several drones equations of motion, sensor with a limited footprint, information collection with the sensor declines with the distance to the target 
general 
(Continued) 
OTTO et al. 421 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
12 OTTO ET AL. 
TABLE 2 (Continued) 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[74] MH, Exp drone limited ﬂight time, information collection is lower at a larger distance from the camera to the target, equations of motion, sensor with a limited footprint 
general 
[75] MH, Exp several drones limited ﬂight distance, information collection is lower at a larger distance from the camera to the target, equations of motion, sensor with a limited footprint 
general 
[76] M, H, Exp drone limited ﬂight time, uncertain ﬂight times general 
[97] Control, Exp drone limited energy, minimum turning radius, energy consumption equations (eg, depends on the turn rate), sensor with a limited footprint 
general 
[101] P, H, Exp drone, several drones 
limited ﬂight time, limited buﬀer size, communication constraints (eg, signal fades with the distance), sensor with a limited footprint 
general 
[161] P, H, Exp drone equations of motion, sensor with a limited footprint agriculture 
[184] M, H, CG, Exp several drones limited ﬂight time, minimum payload (limited number of sensors), travel time depends on the number of sensors due to increased payload 
general 
[208] MH, EmpCS, Exp 
drone minimum turning radius, limited ﬂight distance general 
[272] M, BnB, H, Exp several drones dronetotarget assignment constraints general 
[274] P, H, EmpCS, Exp 
drone limited ﬂight time, sensor with a limited footprint agriculture 
Coverage from stationary positions: Continuous observation (Section 4.1.3) [216] M, H, Exp several drones energy consumption depends on the altitude of ﬂight, limited energy pro ﬂight, sensor with a limited footprint 
other (environmental protection and disaster management) 
[235] P, H, EmpCS, Exp 
several drones minimum and maximum camera angles, sensor with a limited footprint 
general 
[321] M, H, Decentral H, Exp 
several drones energy consumption depends on the altitude of ﬂight, limited energy pro ﬂight, sensor with a limited footprint 
general 
FIGURE 10 Selected path patterns and area representations in area coverage 
422 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 13 
requiring that the drone enters and leaves certain cells from opposing sides [188]. Notice, however, that we may miss optimal solutions by discretizing area P, because we force the drone to visit centers of the cells. Several articles consider one drone covering some possibly nonconvex area P [149, 176, 276, 300]. Other articles take the limited energy capacity of drones into account, so that area coverage may require several drones, each possibly starting from diﬀerent depots [17, 188, 217]. Balampanis et al. [20] partition a nonconvex polygonal area into coverage assignments for a heterogeneous ﬂeet of drones. In Avellar et al. [17], drones must be launched or relaunched by a human operator, so they have to wait their turn if the operator is busy, delaying task completion. Independent strategic drones may negotiate a solution for partitioning P into individual areas of responsibility according to some speciﬁed auction protocol [22, 283]. Coverage of a road network is examined in Dille and Singh [66], Oh et al. [199], and Oh et al. [200]. 
4.1.2 Coverage path planning for partial coverage: Maximum information collection 
Budget constraints, such as limited energy or time, may prohibit drones from covering the whole area P, giving rise to the problem of maximizing information collection. For example, Çakici et al. [36], Ergezer and Leblebicio˘glu [74, 75], and Gramajo and Shankar [97] maximize the covered area while possibly avoiding forbidden regions. Gu et al. [101] minimize information delay between the point of information collection (such as a taken photo) and its arrival at the processing unit. Objective functions may incorporate penalties for long traveled distances [36, 184], energy expenditures [36, 75], and entering forbidden regions [36, 75]. Overall, in some applications, such as maintenance inspection and precision agriculture, information on some regions of area P may be more valuable than on other regions. Therefore, another common objective function is to maximize the value of the collected information [76, 161, 195, 208, 272, 274]. For instance, Tokekar et al. [274] and Ma and Karaman [161] examine soil analysis, that is, classiﬁcation of soils according to the nitrogen levels, with the assistance of aerial images. In the problem setting of Ma and Karaman [161], a drone may detect in ﬂight some anomalies with associated rewards, or importance scores, and may then decide to take an aerial image of some of these anomalies. The authors investigate the maximum achievable average reward in discrete and continuous problem formulations for diﬀerent motion equations of the drone and for diﬀerent detection ranges of the drone’s sensors. As discussed in Section 4.1.1, area P can be discretized and represented as a graph with information value (or rewards) assigned to each node [274]. In this case, the problem of maximum information collection becomes equivalent to the orienteering problem. In the orienteering problem, a drone cannot visit all the points of interest during its sortie. Therefore, we have to ﬁnd a tour over some (possibly not all) nodes in a way that maximizes the total collected reward (see Vansteenwegen et al. [285] for a general survey on the orienteering problem). Pˇenicka et al. [208] formulate the Dubins orienteering problem to take into account restrictions on the minimum turning radius of the drone. In Mufalli et al. [184] and Thi et al. [272], each node of the graph may be surveyed with several drones or several sensors with diﬀerent information gathering abilities. In Mufalli et al. [184], a set of drones must be equipped with sensors, where each sensor brings additional (additive) beneﬁt in surveying a node, and tours of drones starting and ending at their depots should be selected so that the potential beneﬁt is maximized and travel distance is minimized (the objective function incorporates both factors as a weighted sum). Evers et al. [76] examine an orienteering problem relevant for police and media applications. A drone has to ﬁlm several episodes during a sporting event for a certain duration within the predeﬁned time windows. The ﬁlming locations are positioned in the nodes of a graph. However, some urgent, unforeseen incident, such as a riot, may occur with a certain probability in the neighborhood of the nodes of the graph and would require the immediate attendance of the drone. In this scenario, the rewards of the node visits depend on the number of potential incidence locations in the neighborhood and on the probabilities of events occurring. InNiuetal.[195],adronecollectsinformationontransportﬂowsinroadsegments,withsomesegmentsbeingmoreimportant (eg, with frequent traﬃc jams) than others. Due to regulations, the drone can only ﬂy over the highways in the network and cannot take any shortcuts by ﬂying over the terrain between designated roadways. The authors formulate a multiobjective problem that evaluates tradeoﬀs between the value of the collected information and the ﬂying time. 
4.1.3 Coverage from stationary positions: Continuous observation 
Environmental monitoring, monitoring of transportation networks or emergency response activities may require allocation of several heterogeneous drones to stationary observational positions, that is, monitoring positions where drones hover or loiter for a long time. Pugliese et al. [216] and Zorbas et al. [321] set up optimization models in which a given set of objects should be covered by the sensor range of at least one drone. Each object should be monitored for a certain amount of time, so that if the 
OTTO et al. 423 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
14 OTTO ET AL. 
drone’s energy is insuﬃcient to cover the required time span, a new drone jumps into the surveillance position and the drone with depleted energy returns to the depot. The higher the altitude of the drone, the larger its sensor range is; however, its energy consumption also increases. The authors consider minimizing the number of drones to cover the target and the total energy consumption of the drones. In Saeed et al. [235], drones must take frontal pictures of objects. The authors look for positions and orientation to provide the minimum number of drones that will cover all the objects, which are modeled as sets of oriented line segments. 
4.2 Planning of search operations 
In the search problem with drones, which is readily observed in wildlife monitoring and search and rescue applications, a search path for one or several drones must be determined to ﬁnd an object with an unknown location. Search problems have a long and rich history in the operations research (OR) literature, and we refer the interested reader to Koopman [135, 136], Stone [255], and Alpern and Gal [11] for essentials of search theory and to Benkoski et al. [26], Hu et al. [114], and Robin and Lacroix [228] for general surveys of the literature on the search problem. Obviously, search problems with drones closely resemble search problems with piloted aircraft. Innovations in modeling and methodology in articles on drones are mainly motivated by applications that are now proﬁtable due to the low cost of drone technology when compared to piloted aircraft. Such applications include monitoring of livestock and environmental monitoring (eg, icebergs). See Table 3 for an overview of T3 the surveyed articles. In the search problem, a map of the search area P is given and is usually partitioned into a collection of cells (eg, with a grid) each labeled with the probability of ﬁnding a targeted object in this cell. Due to possible obstacles in the line of sight, such as trees or buildings, Lin and Goodrich [151] and Yao et al. [309] recommend modeling detection probabilities speciﬁc for each cell. Most publications assume a stationary object, even if the target is a person or an animal, because the speed of the object is usually much slower than that of the drone. In the case of a moving object, the probability map evolves over time [218, 219, 291]. Due to imperfect sensors, a drone detects an object with some probability π < 1 if it visits the cell where the object is located [142], so it may make sense to visit some cells several times. The probability of detecting the object may be higher in the center of the sensor’s footprint and decline at the footprint’s edge [121, 142, 191, 273]. Given the information collected by a drone and the initial base rate, we can perform a Bayesian update of the cell label [33]. A common objective is to maximize the cumulative probability of ﬁnding the object within the given time span or, alternatively, to minimize the time to achieve the desired cumulative probability of ﬁnding the object. Note that because a drone collects information in all the cells visited along its path, an optimal path does not necessarily contain a grid cell with the highest probability of ﬁnding the object. Some articles propose alternative approaches to evaluating the search map. Thus, Sujit and Ghose [256, 257] label cells with general information uncertainty indices, which decrease by a constant factor after each drone visit. The objective function in this case aims at maximum total uncertainty reduction [308]. Due to limited ﬂight duration, drones may require refueling at the depot or, for gliding drones, using thermals (rising bubbles of hot air) [191]. As a result, cells near the depot may get searched more intensively than remote cells [256, 257]. If information exchange between the drones via a ground control station is restricted, drones may have to exchange some information between each other while taking possible limitations of the communication channels into account [121, 257, 308]. Drones that belong to diﬀerent organizational entities may also interact strategically as selﬁsh, cooperative, or adversarial players [257]. In some articles, drones not only have to locate the object, but also estimate its motion [6, 109, 110, 303]. For example, Haugen and Imsland [109, 110] and Albert et al. [6] set up a control framework to estimate parameters of motion of several objects with drones equipped with imperfect sensors. The application is motivated by detection of icebergs and assessment of their direction of movement, position, and velocity, as well as estimation of ice concentration in the sea. For instance, Albert et al. [6] use a graphtheoretical approach to assign objects (icebergs) to drones and to determine tours of the drones over these objects, so that the objects with the highest observational uncertainty are visited ﬁrst. In Darbari et al. [61], the drone must discover obstacles and predict their motion in order to cover a maximum area within the limited ﬂight time while avoiding collisions. Several articles investigate the cyclicrouting drone problem, in which drones ﬂy back and forth to update information on the objects’ positions [112, 270]. 
4.3 Routing for a set of locations 
In a number of surveillance and delivery applications, drones have to perform a tour over some set of locations which starts and ends at a depot. The resulting planning problems can be modeled as generalized versions of one of the basic routing problems, such as the TSP [214, 239], the multiple TSP [8, 19, 25, 163, 222, 266, 286, 288], or the VRP [96, 125, 133, 154, 213]. 
424 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 15 
TABLE 3 Overview of the surveyed articles: Planning of search operations 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[6] M, EmpCS, Exp several drones equations of motion (minimum turning radius), sensor with a limited footprint 
other (environmental protection and disaster management) 
[33] Control, Exp drone equations of motion, sensor with a limited footprint other (rescue) 
[61] Control, Exp drone minimum turning radius, limited ﬂight time, minimum speed, limited climbrate, sensor with a limited footprint 
general 
[109] M, EmpCS, Exp several drones equations of motion, sensor with a limited footprint other (environmental protection and disaster management) 
[110] M, EmpCS, Exp several drones equations of motion, sensor with a limited footprint other (environmental protection and disaster management) 
[112] P several drones general 
[121] Control P, Decentral H, Control, Exp 
minimum turning radius, sensor with a limited footprint, target detection probability of the sensor declines with the distance to the target, limited communication range 
general 
[142] Decentral H, Exp 
several drones sensor with a limited footprint, target detection probability of the sensor declines with the distance to the target 
general 
[151] H, Exp drone sensor with a limited footprint other (rescue) 
[191] H, EmpCS, Exp drone equations of motion (gliding), equations for energy gains by visiting thermals (eg, depends on the wind strength and the time spent in the thermal), target detection probability of the sensor declines with the distance to the target 
general 
[218] M, Exp drone equations of motion, sensor with a limited footprint, target detection probability of the sensor declines with the distance to the target 
other (rescue) 
[219] M, H, Exp drone equations of motion, sensor with a limited footprint other (rescue) 
[256] H, Decentral H, Exp 
drone, several drones 
limited ﬂight time, sensor with a limited footprint general 
[257] Decentral H, Game H, H, Exp 
several drones limited ﬂight time, sensor with a limited footprint general 
[270] H, Decentral H, Exp 
several drones minimum turning radius, sensor with a limited footprint 
general 
[273] H, EmpCS, Exp drone, several drones 
equations of motion, sensor with a limited footprint, target detection probability of the sensor declines with the distance to the target 
general 
[291] H, Exp drone equations of motion, sensor with a limited footprint other (rescue, environmental protection) 
[303] Decentral H, Exp 
several drones limited communication range, sensor with a limited footprint 
general 
[308] Decentral H, Exp 
several drones minimum turning radius, sensor with a limited footprint, statistical learning unit at each drone 
general 
[309] Decentral H, Exp 
several drones minimum turning radius, sensor with a limited footprint 
general 
OTTO et al. 425 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
16 OTTO ET AL. 
We provide an overview of the articles surveyed in this section in Table 4. Note that routing for a set of locations intersects T4 with area coverage problems examined in (Section 4.1). Indeed, as an approximation, we could discretize area P and represent it as a graph, so that the coverage path planning becomes a routing problem. Recall, however, that such discretization only represents one of the possible heuristic solution approaches to area coverage, and exact solution approaches rely, for instance, on the tools of analytic geometry. Drones face a problem that some vehicles (eg, longdistance trucks and electric vehicles) also face, as drones have to periodically refuel or recharge their batteries at depots to overcome their limited travel range [103, 131, 132, 172, 253, 254, 265, 278, 312]. In contrast to trucks, however, drones may select a battery of the most suitable size for the tour, taking into account that energy consumption depends heavily on the weight of the drone [68]. Drones also ﬂy in 3D space without a road network and may have to make a detour to avoid obstacles or dangerous zones (eg, bad weather regions) [86, 173, 175, 209]. Another inﬂuence on route planning is that drones have a minimum turning radius if they travel at a constant speed [18, 55, 103, 163, 223, 242, 299]. Drones travel a path with a curvature radius that cannot fall below some minimum level, which (if it is twice diﬀerentiable almost everywhere) is referred to as a Dubins path. Studies have proven that the shortest Dubins path between two points in space, given the starting and the ﬁnal heading of the vehicle (ie, the direction in which a drone is pointing), consists either of a straightline trajectory and/or of turning left or right at the minimum turning radius [69] (see Figure 11a). F11 Several articles consider the TSP or the VRP with a restriction on the minimum turning radius, called the Dubins TSP and the Dubins VRP, respectively [55, 197, 223, 242, 299, 304, 320]. The Dubins TSP is NPhard in the strong sense and an optimal solution of the Euclidean TSP corresponding to the Dubins TSP can be arbitrarily worse than an optimal solution to the original problem, if the problem instance is suﬃciently large [197]. Rathinam et al. [223] and Savla et al. [242] propose decomposition heuristics with performance guarantees where tour planning is performed by solving a conventional Euclidean TSP or VRP and only afterward, as a downstream planning problem, is the drone’s path adjusted to respect the minimum turning radius condition. Savla et al. [242] also provide some analysis of the expected results in stochastic and dynamic versions of the problem. Ny et al. [197] show that a performance guarantee can also be provided for a very simple heuristic: random assignment of heading angles to each node, computing shortest Dubins path for each pair of nodes, and solving the resulting problem as an asymmetric TSP. Manyam et al. [163] propose several lower bounds for the Dubins multiple TSP. Woods et al. [295] study the kneighbor TSP, which is an alternative modeling approach to ﬁnd a ﬂight tour of the minimum travel distance. Whereas in the classic TSP, the length of the tour is equal to the sum of edge lengths, in the kneighbor TSP, the length also depends on the sequence of edges in this tour (see [295] for a more precise deﬁnition). Some drone operations, especially those that involve collecting samples in maintenance inspection and precision agriculture, are modeled as routing problems with neighborhoods [21, 139, 299] (see Figure 11b). For example, since nearby points in an agricultural ﬁeld are correlated [139], collecting information about just one spot/point may be suﬃciently representative of the status of the surrounding area of interest [21]. In another setting, a drone is able to collect the required data from a range of diﬀerent positions, which would depend on the footprint of the drone’s sensor [198]. Given a collection of regions, referred to as neighborhoods, in some mdimensional space and a distance function between points in this space, the routing problem with neighborhoods requires ﬁnding a minimum cost route for one or several drones so that each neighborhood is visited at least once. For instance, Gentilini et al. [90] propose a branchandcut algorithm for a TSP with ellipsoid or polyhedral neighborhoods. If we interpret neighborhoods as a set of distinct nodes of a graph, so that exactly one node of the graph in each neighborhood must be visited and each neighborhood must be visited by exactly one drone, then a oneina-set problem arises [241]. We refer the interested reader to Arkin and Hassin [16], de Berg et al. [62], and Elbassioni et al. [71] for approximation results on some variants of the TSP with neighborhoods. To address the problem of unreliable data, Niendorf et al. [193] compute stability regions for problem parameters, that is, parameter intervals, for which the computed solution will remain optimal. Motivated by surveillance for maintenance of bridges and other infrastructure subject to wind gusts between pylons, Guerrero and Bestaoui [103] elaborate on the shortest path calculation in a windy environment based on Zermelo’s navigation equations. In Alighanbari and How [9], a set of tasks is known, but their proﬁts are uncertain and data uncertainty (ie, the standard deviation of possible proﬁts) varies between tasks. The authors propose a robust online planning algorithm that penalizes reassignment of tasks. In disaster management, such as oil spills, forest ﬁres, earthquakes, or in oncall services, such as surveillance of traﬃc incidents, surveillance requests appear dynamically and have to be assigned to available drones (see Figure 11c). Because service times of such surveillance requests are often uncertain, a variant of the mvehicle dynamic traveling repairman problem often arises [28, 29]; see Bullo et al. [35] and Ritzinger et al. [226] for surveys. Enright et al. [72, 73], Pavone [204], Pavone 
426 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 17 
TABLE 4 Overview of the surveyed articles: Routing for a set of locations 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[8] M, Exp several drones drones with heterogeneous capabilities general 
[9] M, H, Exp several drones sensor measurements are prone to errors general 
[15] P, Decentral H, Exp 
several drones a general framework: no speciﬁc drone characteristics are formulated 
general 
[18] H, Exp drone minimum turning radius general 
[19] M, P, H, Exp several drones travel cost are speciﬁc for each drone (eg, because drones have diﬀerent minimum turning radii) 
general 
[21] H, Exp drone limited ﬂight distance, sensor with a limited footprint, resolution depends on the altitude of ﬂight 
agriculture 
[25] M, H, Exp several drones limited ﬂight time, maximum number of possible targets per drone 
general 
[40] M, P, Decentral H, Exp 
several drones speed of work diﬀers among drones infrastructure & construction 
[48] M, Decentral H, Exp 
several drones minimum turning radius general 
[54] M, Exp several drones limited payload, maximum speed, limited energy capacity, energy consumption depends on speed, given speed of battery recharge 
transport 
[55] M, P, H, Exp drone minimum turning radius general 
[68] M, MH, EmpCS, Exp 
several drones limited carrying capacity, energy consumption depends on the drone’s weight 
transport 
[72] P, H, Decentral H 
drone, several drones 
minimum turning radius, sensors of limited range general 
[73] P, H, Exp several drones minimum turning radius general 
[86] M, H, BnC, CP, Exp 
drone maximum rate of climb, maximum rate of descent other (air traﬃc management) 
[90] M, H, BnC, Exp drone sensor with a limited footprint general 
[96] M, H, Exp several drones minimum turning radius general 
[103] MH, H, Control, Exp 
drone limited ﬂight time, minimum turning radius, inﬂuence of wind 
infrastructure & construction 
[104] M, Exp several drones limited ﬂight distance entertainment & media 
[125] M, MH, Exp several drones limited payload transport 
[128] M, Decentral BnP, Exp 
several drones a general framework: no speciﬁc drone characteristics are formulated 
general 
[130] M, H, MH, Exp several drones limited energy, given duration of recharging other (manufacturing) 
[131] M, BnB, H, Exp several drones limited ﬂight time general 
[132] M, MH, Exp several drones limited ﬂight time general 
[133] LR, H, Exp several drones limited ﬂight time, limited payload transport 
[144] DP, H, Exp several drones energy consumption equations, equations of motion transport 
[154] M, MH, Exp several drones limited ﬂight distance other (traﬃc surveillance on roads) 
[163] M, LR, Bnd, Exp several drones minimum turning radius general 
(Continued) 
OTTO et al. 427 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
18 OTTO ET AL. 
TABLE 4 (Continued) 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[172] MH, H, Exp several drones limited ﬂight time other (environmental protection and disaster management) 
[173] Decentral MH, Exp 
several drones limited ﬂight distance other (rescue) 
[174] Decentral H, EmpCS, Exp 
several drones a general framework: no speciﬁc drone characteristics are formulated 
general 
[175] Decentral H, Control, EmpCS, Exp 
several drones equations of motion general 
[192] Decentral MH, Exp 
several drones heterogeneous drones (eg, diﬀerent equipment) general 
[193] M, H, Exp drone general 
[195] M, Exp drone limited ﬂight time other (traﬃc surveillance on roads) 
[197] P, H, Exp drone minimum turning radius general 
[198] P, H, Exp drone minimum turning radius general 
[204] M, P, H, Decentral H, Exp 
several drones minimum turning radius general 
[205] P, H, Exp several drones general 
[206] Survey on selected results of the dynamic traveling repairman problem with several drones 
[209] MH, Exp drone equations of motion other (air traﬃc management) 
[213] MH, Exp several drones limited payload transport 
[214] M, MH, Exp drone limited camera footprint infrastructure & construction 
[222] M, P, H, Bnd several drones general 
[223] P drone, several drones 
minimum turning radius general 
[233] Game H, Exp several drones communication constraints (probability of successful transmission depends on distance and message size, limited speed of data collection) 
telecommunications 
[234] Game P, Game H, Exp 
several drones communication constraints (probability of successful transmission depends on distance and message size, limited speed of data collection) 
telecommunications 
[239] MH, Exp drone general 
[240] M, Game P, Exp drone transport 
[241] MH, Exp several drones sensor with a limited footprint general 
[242] P, H drone minimum turning radius general 
[253] M, EmpCS, Exp several drones limited ﬂight time general 
[254] M, H, Exp several drones limited fuel/energy capacity, time for refueling/recharging the battery depends on the required amount of fuel/energy 
general 
[258] H, Exp drone equations of motions (eg, minimum turning radius) other (geology) 
(Continued) 
428 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 19 
TABLE 4 (Continued) 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[264] M, LR, H, Exp several drones minimum turning radius (given heading angles at each target) 
general 
[265] M, P, H, Exp drone limited ﬂight distance general 
[266] M, BnC, Exp several drones minimum turning radius general 
[278] M, P, H, EmpCS, Exp 
drone limited energy, energy consumption function general 
[280] M, P, Decentral H, EmpCS, Exp 
several drones limited communication range, equations of motion, safety distances 
general 
[286] M, P, H, Bnd, Exp 
several drones limited energy general 
[288] H, Exp several drones communication within the visual line of sight, limited communication range 
general 
[294] M, MH, Exp several drones limited carrying capacity transport 
[295] P, E drone the authors study the kneighborTSP that may describe headingangledependent distance metrics 
general 
[299] MH, Exp drone minimum turning radius general 
[304] M, H, DP, LR, Exp 
several drones minimum turning radius general 
[312] P, EmpCS, Exp drone limited energy transport 
[320] P, H, MH, Exp drone equations of motion (minimum turning radius, motion in the wind) 
general 
[319] M, LR, H, Exp several drones limited ﬂight time other (traﬃc surveillance on roads) 
et al. [205] and Pavone et al. [206] provide several algorithms that result in a bounded expected waiting time for request completion (ie, time from the request arrival to the request completion). Moreover, some algorithms guarantee system stability and have suitable properties both under lightload conditions, that is, when only a few requests per drone of low required service time arise, and under heavyload conditions. For example, Enright et al. [73] show the following policy, known as the median circling policy, to be eﬃcient under lightload conditions: Each drone loiters around the median of its Voronoi region and serves requests arriving in its region in a ﬁrstcome-ﬁrstserved order. Overall, in a number of problem settings, policies based on the ﬁrstcome-ﬁrstserved rule are mostly suitable for lightload conditions, whereas in heavyload conditions, several requests should be accumulated or batched before planning an order of service [35]. Turpin et al. [280] investigate allocation of several accumulated requests to drones and design noncollision trajectories of drones. Agentbased and gametheoretic modeling is another methodology to deal with uncertain and dynamic environments [15, 48, 128, 174, 233, 234]. In the recedinghorizon 
FIGURE 11 Illustration of some possible drone tours. * Each path between a pair of nodes in a tour is a Dubins path. For example, path BA consists of a right turn, a straight segment and a left turn, where turns are performed at the minimum turning radius r 
OTTO et al. 429 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
20 OTTO ET AL. 
setting, drones (or agents) bid for tasks that they should perform next and each task is allocated to the lowestcost bidder. In Sanjab et al. [240], a delivery drone has to ﬁnd a path from the depot to a customer while taking into account a possibility of the malicious attack that will damage the drone with some probability of success so that a new delivery drone will have to be sent to the customer. The authors formulate a zerosum game on a graph where node labels depict success probabilities of the attack and edge labels denote travel times of the drone. In some surveillance applications, such as ﬁlming sporting events, traﬃc monitoring and control, and monitoring traﬃc incidents or disaster regions, drones have to visit locations within speciﬁed time windows [104, 130, 253, 254, 319]. Moreover, soft time windows may indicate time intervals, such as overtime in a football game, when ﬁlming is most valuable for the customers [254]. Photos may have to be taken at certain angles and at a desired height to optimize resolution [258]. Moreover, some locations may be visited only by a subset of drones equipped with a suitable set of sensors [264]. Several publications consider routing issues in the context of delivery applications, where a drone may deliver several customer orders per tour before returning to the depot [68, 294]. Wen et al. [294] consider express deliveries of blood. In order to guarantee optimal environmental conditions for the transported blood, the authors decide on temperature and volume of the additional payload—which is water or ice serving as cooling and heating agents. Suzuki [213] formulates a pickup and delivery problem motivated by drones delivering goods to human packers in a warehouse. In Lee [144], drones are assembled from the most suitable variants of given modules (rotors, carriers, and batteries) to achieve low energy utilization and fast delivery for each particular customer request. Coelho et al. [54] examine routing of drones given the possible traﬃc infrastructure of cities in the future. For instance, airspace may be separated into two layers: one for heavy and fast drones and a lower one for smaller and slower drones. The city would have a set of stationary recharging stations for drones, as well as some vertical displacement points for drones from diﬀerent airspace layers to exchange packages. A special case arises when task processing times are much longer than the travel times. In this case, routing for a set of locations can be approximated as a scheduling problem. For instance, in Caraballo et al. [40], drones equipped with robotic arms perform assembly operations, such as assembly of a bridge. The authors look for a task allocation among drones that minimizes the maximum completion time. Niccolini et al. [192] consider allocation of delivery or surveillance tasks to drones as the task assignment problem. Overall, the most common objectives in routing for a set of locations include minimizing operational cost [68, 103, 130, 173, 242, 286], minimizing ﬁxed costs (mostly the number of required drones [104, 154, 213, 294]), and minimizing makespan [40, 173, 241, 312]. Several articles consider objectives related to customer service and customer satisfaction, such as expected waiting time of customers for service [73, 204], share of successfully served customer orders [213], total customer satisfaction [104], and resolution of photographed images [21]. 
4.4 Planning of data gathering and recharging in a wireless sensor network 
A wireless sensor network (WSN) describes a collection of spatially distributed wireless sensors that gather information about the environment and transmit it to the base station. Drones serve as an additional layer between a network of stationary sensors and the base station: spatially distributed sensors gather information about the environment, and drones gather data from stationary sensors and transmit it or carry it back to the base station. In a number of relevant applications, data gathering from stationary sensors without drones would be slow, costprohibitive or even impossible. Therefore, new problems involving routing of drones arise in the modeling of WSNs. For example, underwater gas pipelines, some of which extend for more than 1000 km, are monitored by periodically placed stationary sensors that are diﬃcult to access. Another example is the monitoring of wild animal habitats [301]. Regions impacted by disaster and extreme natural phenomena, such as volcanic activities and bushﬁres, can be monitored by sensors scattered from an airplane [57]. One promising application is the location of earthquake survivors by a network of sensors able to locate weak signals of mobile phones [67]. In the latter case, data gathering and information transmission to the base station should occur with the shortest possible delay. In contrast to the routing operations of drones examined in Section 4.3, data gathering operations of drones in WSNs have to respect communication, memory, and data recency constraints. For instance, the communication range is limited and reliability of data transmission depends on the communication distance. Direct information transmission from one node (sensor or drone) of the WSN to another node or the base station is called a hop. Because of the limited communication range, nodes may have to perform a multihop transmission (see Figure 12). The limited memory capacity of drones and sensors is another constraint. F12 
430 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 21 
FIGURE 12 Illustration of wireless sensor networks. Straight arrows depict a drone tour, wavy arrows depict communication links 
Drones may perform diﬀerent roles in a network of stationary sensors. For example, they may recharge sensors inductively by generating an electromagnetic ﬁeld [23]. Existing models of drones can transfer about 5–15 Watts during ﬂight. In simulations of Basha et al. [23], a lifetime increase of 100% to 300% was achieved for mediumsized networks that were recharged in this way. The expected gain depends heavily on the applied recharging policy, and the gain declines with the size of the WSN. Xu et al. [302] derive an optimal trajectory for a drone to recharge two energy receivers. Another promising application is the use of drones as ferrying nodes, which collect information from stationary sensor nodes and carry it to the base station. This may increase the range of communication and increase the throughput rate because the drone can position itself directly over the sensor to receive data. Ferrying nodes also reduce energy consumption in the WSNs and, thus, prolong the WSN’s lifetime because they reduce the number of costly multihop transmissions. Indeed, in conventional multihop transmissions in WSNs, sink nodes that are adjacent to the base station run out of power quickly and the WSN becomes disconnected. Existing models investigate various WSN architectures. Some articles examine twohop WSNs (see Figure 12), in which stationary sensors transmit the collected information directly to drones and the drones (serving as ferrying nodes) carry it to the base station [1, 7, 57, 169, 189, 237, 267, 293]. Sahingoz [237] and Mazayev et al. [169] design tours for drones. Abdulla et al. [1] consider drones performing cyclic moves along some given trajectory; in this setting, stationary sensors have to schedule time slots for data transmission to drones. Other articles consider WSNs with hierarchical data transmission, where information may have to travel many (more than two) hops via sensors and/or drones before reaching the base station (see Figure 12) [52, 67, 111, 221, 259, 260, 301]. Here, drones collect data only from a subset of sensors (socalled sink nodes or cluster heads), which in turn aggregate data from their neighboring sensors. Most articles propose a hierarchical planning approach: Stationary sensors are clustered ﬁrst and a node in each cluster is nominated to be the sink node of this cluster, then drone routes over the sink nodes are planned [39, 259]. Liang et al. [150] propose a WSN architecture with a mobile sink node. In each time period, a drone visits some stationary sensor. This sensor assumes the role of a sink node: It aggregates information from all the other stationary sensors in the network and transmits it to the drone. In this way, the role of the sink node gets periodically reassigned. A common problem of WSNs with multihop data transmission is rapid energy depletion of sensors located close to the sink node. By selecting sensors with the highest energy level to be the sink nodes, the drone distributes the energy consumption evenly over sensors and, thus, prolongs the WSN lifetime. Jawhar et al. [117] examine linear WSNs, in which stationary sensor nodes are arranged along a straight line. Such WSNs are especially relevant for monitoring of pipeline infrastructure. In Mozaﬀari et al. [181], drones have to collect information from sensors that are active only within some (potentially unknown) time windows. OtherstudiesconsiderWSNswithincompleteinformationonsensors[67, 117].Thebasestationmaynotpossessinformation on the current energy level of stationary nodes, on the number of stationary nodes, or on the amount of the information collected by each stationary node. Stationary nodes may have only some limited information and only on their neighbor nodes, as well. In such a setting, tradeoﬀs between costs, such as additional energy expenditure to collect more information on the state of the WSN, and information delays may arise. Overall, because data collection and transmission consume the limited energy of sensors and drones, a widely used objective functionistomaximizetheWSNlifetime,thatis,thetimeintervalbeforetheﬁrstsensorfailureduetoenergyexpiration[23, 150], or maximal energy consumption among sensors to transmit the collected information [318]. Some articles minimize makespan [100], total travel distance of the drone [111] and the energy required for the drone’s operations [39, 148]. 
OTTO et al. 431 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
22 OTTO ET AL. 
We refer the interested reader to Akyildiz et al. [4], Yick et al. [310], Younis and Akkaya [311], and Wang and Liu [289] on the issues of node placement and data collection in general WSNs. Table 5 provides an overview of the surveyed articles. T5 
4.5 Planning allocation of communication links and computing power to mobile devices 
To establish connectivity with a drone, a mobile device should be located within its communication range. Therefore, the ensuing planning problems involve decisions about area coverage. However, in contrast to the planning problems examined in Section 4.1, these decisions involve probability theory and nonlinear equations describing communication constraints. For instance, drones have to allocate communication links (and communication time slots) to mobile devices and select such locations that minimize interference and ensure acceptable levels of signaltonoise ratio. We refer the interested reader to Tse and Viswanath [277] on the fundamentals of wireless communication and provide an overview of the surveyed articles in Table 6. T6 Diﬀerent ways of drone employment are possible. First of all, drones may be assigned to stationary locations and serve as intermediaries to connect mobile devices to macrocell base stations (Figure 13a). The set of potential locations may be speciﬁed F13 in advance. Secondly, instead of staying in a ﬁxed location, a drone may ﬂy, for example, along a cyclic trajectory (Figure 13b). Thirdly, dronetodrone transmissions may take place so that a mobile device connected to one drone may establish links to a mobile device connected to another drone (Figure 13c). Research has considered the placement of several drones into stationary positions over some area of interest, which is closely related to the socalled disc coverage problem, but considers communication constraints [156, 160, 177–179, 247]. Coordinates of users may be known in advance [13]. Mozaﬀari et al. [180] consider a drone providing communication to mobile devices in households. Simultaneously, these households may be making use of devicetodevice communication, which causes signal interferences. Given assumptions on the distribution of devicetodevice communication channels and of mobile devices, the authors analyze the cases of stationary deployment of a drone and of mobile deployment of a drone that pauses during ﬂight to perform transmission services. Chetlur and Dhillon [46] derive analytic formulas for the coverage probability of an arbitrarily located user (eg, of a mobile cellular phone) by a network of randomly positioned drones. Alternatively, a set of potential locations for drones may be given and drones must be assigned to a subset of these locations [290]. In Mozaﬀari et al. [183], the positions of drones are known and ﬁxed. The authors partition a given geographical area into disjoint regions associated with each drone, taking into account the distribution density of the users. AlHourani et al. [5] determine an optimal altitude of a drone, positioned at given x, ycoordinates, to maximize radio coverage. Some researchers take ﬂying trajectories of drones into account [85, 88, 119, 120, 123, 159, 313]. In a number of papers, each user receives particular time slots to exchange information with the drone (socalled timedivision multiple access (TDMA)). These papers consider scheduling a drone’s communication with ground mobile devices [159] and optimizing the trajectory of the drone [297, 298]. Jeong et al. [119] schedule data computation by a drone serving as a cloudlet as well as data transmission between the cloudlet and a mobile device to minimize the total energy expenditure of the drone given its ﬁxed trajectory. The case of moving mobile user nodes is analyzed in Fotouhi et al. [85] and Jiang and Swindlehurst [123]. Zeng and Zhang [313] point out the importance of considering the propulsion energy spent on ﬂight or hovering as it is usually several times greater than the energy spent on communication. The authors calculate optimal ﬂying trajectories for a ﬁxedwing drone that has to maintain communication with a stationary ground station. Galkin et al. [88] derive analytic formulas for an optimal altitude of a randomly moving network of drones to maximize the coverage probability for the stationary user. Serving as ﬂying base stations, several loitering drones may form a multihop communication network, in which a mobile device connected with one drone may transmit data to a mobile device connected to some other drone in the network [77, 98, 99, 248]. Some articles also optimize allocation of limited transmission power between the source node and drones (as relay nodes) to improve communication quality [147, 314]. If drones belong to diﬀerent providers, they compete with each other for users as selﬁsh strategic players and gametheoretic applications arise. In Koulali et al. [137], mobile devices perform only passive scanning: They wait for welcome messages from drones and establish a link with the sender of the ﬁrst message received. Knowing the probability distribution for locations of mobile devices, drones have to decide on the duration and starting points of their beaconing intervals: The longer the beaconing intervals are, the larger the probability of encountering users, but energy consumption increases as well. The most common objectives in this section relate to minimizing cost and maximizing the quality of customer service. The communications provider must assign locations to drones so that the desired mobile connectivity is achieved at minimum cost [290] or with the minimum number of drones [160]. Because data transmission and setup operations to establish links consume the energy of drones, a common objective function is to maximize energy eﬃciency or minimize energy to achieve the desired communication quality [47]. Alternatively, the number of established communication links [32, 99] and the quality of 
432 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 23 
TABLE 5 Overview of the surveyed articles: Planning of data gathering and recharging in a wireless sensor network 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[1] Game P, Game H, Exp 
drone communication constraints (eg, signal strength depends on distance, adaptive modulation) 
general 
[7] H, Exp drone limited communication range general 
[23] Exp drone discussed: limited recharging capability of the drone general 
[39] MH, Exp drone energy consumption function (depends on data transmission and traveled distance), limited communication range 
general 
[52] Control, EmpCS, Exp 
drone equations of motion (minimum turning radius, inﬂuence of wind, uncertainty in motion), communication constraints 
general 
[57] H, Exp several drones limited communication range other (environmental protection and disaster management) 
[67] H, Exp drone communication constraints (eg, limited communication range, data collection with help of mobile agents) 
other (rescue) 
[100] M, Exp several drones limited buﬀer size for saving data, communication constraints (eg, limited communication range, limited incoming and outcoming data rates), equations of motion 
infrastructure & construction, environmental protection and disaster management 
[111] MH, Exp drone equations of motion (minimum turning radius, inﬂuence of wind), limited communication range 
other (environmental protection and disaster management) 
[117] H, Exp drone adaptable travel speed, communication constraints (limited communication range), limited data storage capacity 
infrastructure & construction, security 
[148] M, H, Exp several drones communication constraints (eg, given set of possible modulation levels for message transmission, maximum transmit power), parameters of energy consumption 
other (environmental protection and disaster management) 
[150] H, Exp drone limited ﬂight distance, limited communication range general 
[169] M, H, Exp several drones limited buﬀer size for saving data, limited communication range 
general 
[181] M, P, LR, H, Exp 
several drones communication constraints (eg, probability of successful transmission depends on distance, limited communication range), energy consumption function (limited energy, energy consumption depends on velocity vector) 
telecommunications 
[189] Control, MH, Exp 
drone limited energy, communication constraint (eg, limited communication range, message interferences are possible, communication delays) 
other (utilities) 
[221] Exp drone limited communication range general 
[237] MH several drones drone’s trajectory is a smooth (Bezier) curve, limited communication range, upper and lower bound on the number of sensor, from which a drone can collect the data 
general 
(Continued) 
OTTO et al. 433 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
24 OTTO ET AL. 
TABLE 5 (Continued) 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[259] M, MH, Exp drone limited ﬂight time, communication constraints (limited communication range, limited communication rate) 
general 
[260] M, H, Exp drone minimum turning radius, limited communication range 
general 
[267] M, BnC, Exp several drones minimum turning radius, communication constraints (limited communication range, speed to establish a communication link and to transmit data depends on the dronetosensor assignment) 
general 
[293] P several drones communication constraints (eg, timedivision multiple access (TDMA) channel access method, exponential law for the path loss) 
general 
[301] H, Exp drone limited communication range other (wild life monitoring) 
[302] M, E, Exp drone limited ﬂight time, maximum speed, limited communication range, energy transmission constraints (channel power gain depends on the distance between the drone and the energy receiver, given transmission power) 
general 
[318] M, H, Exp drone communication parameters (distribution of the fading coeﬃcients) 
general 
communication [229, 316], such as packet transmission delay [147, 182] or the coverage probability [5, 88], should be optimized given limited resources, such as the limited energy of the drone. 
4.6 Operational aspects of a selforganizing network of drones 
In some drone operations, in which communication is an issue and the base station needs to obtain recent collected information with the shortest possible delays, we may treat a set of drones as a ﬂying wireless ad hoc network. A flying adhoc network (FANET) is a dynamically selforganizing network of drones that may utilize direct dronetodrone communication [238]. FANET may be involved in any of the drone operations described in Sections 4.1–4.5. Direct dronetodrone communication may have several advantages. First, since the speed of direct data transmission between two nodes is usually faster than the ﬂying speed of a drone, multihop transmissions to the base station may increase the recency of the received information from drones that are out of the direct communication range. It may also free up the limited buﬀer size of the drone for further data collection [100, 101]. Second, communication between drones is important for ﬂying in formation [217]. Third, users can reduce the payload of some drones and economize on cost by equipping only select drones with the hardware enabling direct longdistance communication with the base station or satellites [24]. Fourth, because of the limited communication range, several drones may be required to provide connectivity to mobile devices [77, 98, 99, 248]. Literature surveys on communication protocols in FANETs can be found in Bekmezci et al. [24] and Sahingoz [238]. We refer the interested reader to Jawhar et al. [116] for an overview of communication and networking aspects of dronebased systems. We do not include articles in this section where FANET performs operations described in Sections 4.1–4.5. For example, Gu et al. [101] describe coverage path planning operations of micro drones in which one of the objective functions is to minimize information delay. Besides deciding on coverage paths of drones, the authors schedule drone dispatch times so that drones occupy optimum locations relative to each other to enable fast message transmission to the base station. Another example is found in Vilar and Shin [288], who considers recognition and mapping of important facilities in urban environments. Communication between drones is limited by the communication range and is only possible within visual line of sight. The authors formulate the multiple TSP to assign surveillance tasks to a subset of drones and place the remaining drones as relays to establish communication links with the ground station. 
434 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 25 
TABLE 6 Overview of the surveyed articles: Allocating communication links and computing power to mobile devices 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[5] E, Exp drone communication constraints (eg, path loss depends on the distance between the drone and the receiver, probability of lineofsight communication depends on distribution of buildings’ heights, density of buildings, distance between transmitter and receiver, threshold on the path loss) 
other (disaster management) 
[13] M, E, Exp drone communication constraints (eg, path loss depends on the distance between the drone and the receiver, probability of lineofsight communication depends on drone’s altitude and the distance between transmitter and receiver, threshold on the path loss, given transmission power of the drone) 
telecommunications 
[32] M, E, Exp several drones communication constraints (eg, limited communication range, threshold for the signaltonoise ratio, given transmission power, altitudedependent probability of lineofsight communication) 
telecommunications 
[46] P, Exp several drones communication constraints (eg, Nakagamim fading for wireless links, signaltonoise ratio depends on transmission power, distance between source and sink of the transmission and interferences from other transmissions) 
general 
[47] E, H, Exp drone minimum turning radius, equations of motion, communication constraints (eg, limited communication range), energy consumption equations, signaltonoise equations 
general 
[77] Control, EmpCS, Exp 
several drones communication restrictions (eg, adaptive modulation scheme, transmitted signal depends on distance) 
other (disaster management) 
[85] Game H, H, EmpCS, Exp 
several drones communication constraints (eg, limited communication range, ﬁxed transmission power, interference is possible, probability of having a lineofsight connection depends on the elevation angle of the transmission link, scenarios of the bandwidth allocation (equal, greedy)) constant speed, minimum turning radius 
telecommunications 
[88] P, Exp several drones communication constraints (eg, Nakagamam fading, building density, drone antenna beamwidth, transmission power, interference is possible) 
telecommunications 
[98] M, Decentral H, Exp 
several drones limited communication range, communication loss depending on the transmission distance, equations of motion 
general 
[99] M, Exp several drones limited communication range, equations of motion, fuel consumption depends on velocity, acceleration, altitude 
infrastructure & construction, environmental protection and disaster management 
[119] M, LR, E, Exp drone communication constraints (limited communication range, limited bandwidth, information loss depends on distance), energy consumption equations (it depends, eg, on CPU frequency, amount of transmitted information) 
telecommunications 
(Continued) 
OTTO et al. 435 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
26 OTTO ET AL. 
TABLE 6 (Continued) 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[120] M, BnB, H, Exp drone limited energy, maximum speed, energy consumption depends on data transmission and processing, ﬂying speed and acceleration, limited communication range 
telecommunications 
[123] M, P, Bnd, H, Exp 
drone communication constraints (eg, spacedivision multiple access (SDMA) or timedivision multiple access (TDMA) channel access methods, correlated Rician fading for wireless links, mutual interference depends on the correlation between the users’ channel vectors, maximum number of active uplink users), minimum turning radius 
general 
[137] Game P, Game H, Exp 
several drones limited energy capacity, energy consumption function (eg, depends on the length of the beaconing interval), limited communication range 
telecommunications 
[147] M, P, E, Exp several drones communication constraints (Rayleigh fading model for both desired signal and interference, transmission success between mobile device and drone is a random variable) 
telecommunications 
[156] E, Exp several drones communication constraints (eg, given transmission power, path loss depends on the distance between the user and the drone, no interference) 
telecommunications 
[159] P, Exp drone limited communication range, instantaneous channel capacity from the drone to a sensor depends on the distance between them 
general 
[160] H, Exp several drones limited communication range general 
[177] P, Exp drone, several drones 
communication constraints (eg, path loss depends on the distance between the drone and the receiver, altitudedependent probability to have a lineofsight connection, cases of no interference and full interference, threshold for the signaltonoise ratio) 
general 
[178] M, P, H, Exp several drones communication constraints (eg, downlink coverage probability that depends on altitude and on the antenna gain, random shadow fading coeﬃcients for lineofsight and nonlineofsight links) 
general 
[179] P, H, Exp several drones communication constraints (eg, no interference between drones, transmission rate depends on transmission power, average path loss and transmission power) 
general 
[180] P, Bnd, H, Exp drone communication constraints (eg, Rayleigh fading channel model for devicetodevice transmissions, required signaltonoise ratio threshold, given transmission power, random additional attenuation factor in case of the nonlignofsight connection, FDMA technology for establishing links with drone in parallel) 
general 
[182] M, E, Exp several drones communication constraints (eg, frequency division multiple access, path loss depends on the distance and obstacles along the path) 
telecommunications 
[183] M, P, E, Exp limited ﬂight time, communication constraints (limited communication range, interferences, signal quality depends on distance) 
general 
(Continued) 
436 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 27 
TABLE 6 (Continued) 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[229] MH, Exp several drones communication constraints (eg, random frequency hopping, signaltonoise ratio depends on the distance between the drone and the receiver as well as the distance to the macro cell base station, free space propagation) 
telecommunications 
[247] H, Exp several drones communication constraints (eg, signaltonoise ratio depends on transmission power, distance between source and sink of the transmission and interferences from other transmissions) 
telecommunications 
[290] M, MH, Exp several drones limited communication range, limited capacity on the number of (down)links 
general 
[298] M, P, H, Exp drone communication constraints (eg, timedivision multiple access (TDMA) channel access method, freespace path loss model for channel gain, given transmission power), motion constraints (eg, given altitude, maximum speed) 
telecommunications 
[313] P, E, Exp drone communication constraints (freespace path loss model), propulsion energy consumption (eg, depends on speed and acceleration) 
general 
[297] M, P, H, Exp several drones communication constraints (eg, time division multiple access (TDMA), freespace path loss, interferences are possible), maximum speed 
general 
[314] M, P, H, E, Exp drone constant speed, communication constraints (freespace path loss model for the channel power) 
general 
[316] E, Control, EmpCS, Exp 
several drones minimum turning radius, communication constraints (eg, limited communication range, signal strength depends on distance) 
general 
Several articles examine general problems of FANETs that are not related to any speciﬁc operation described in Sections 4.1–4.5 (see an overview of these articles in Table 7). For instance, Oliveira [201] and DíazBáñez et al. [65] schedule relative T7 positioning of drones in a FANET to maximize the total connection time, whereas Navaravong et al. [187] minimize total traﬃc (traﬃc intensity multiplied by the transmission distance). Aloul and Kandasamy [10] and Koupaei and Abdechiri [138] optimize fault diagnoses of the nodes of the FANET. Indeed, to maintain a stable spatial formation, drones should regularly exchange information on their positions and velocities. In case of hardware failures, some of these messages may be false. Therefore, in Koupaei and Abdechiri [138], velocities and positions of each drone should be veriﬁed by the given number of neighbors. Since 
FIGURE 13 Illustration of drone operations to allocate communication links and computing power to mobile devices 
OTTO et al. 437 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
28 OTTO ET AL. 
TABLE 7 Overview of the surveyed articles: Operations of a selforganizing network of drones 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[10] M, Exp several drones testing range general 
[65] P, H, EmpCS, Exp 
several drones limited communication range general 
[138] MH, Exp several drones limited communication range, limited testing range general 
[187] H, BnB, Exp several drones communication constraints (limited communication distance), limited energy 
general 
[201] M, P, DP, H, Exp 
several drones limited ﬂight distance general 
communication and sensing ranges of drones diﬀer, a problem of optimal assignment of drones to the given set of potential positions arises. 
5 PLANNING COMBINED OPERATIONS OF DRONES WITH OTHER VEHICLES 
Micro, mini, and small drones, which are popular in civil applications, can spend only a very limited time airborne. Therefore, combining drones with other robots or means of transportation leads to a signiﬁcant increase of eﬃciency and eﬀectiveness in performing the desired operations. For the sake of brevity, we refer to robots and means of transportation other than drones as vehicles in this section. Most of the surveyed articles that involve combined operations examine routing for a set of locations (see Section 4.3). Routing operations may take place in uncertain, dynamic environments, such as monitoring of disaster regions or suppression of wildﬁres [212, 287, 296]. Only a few articles consider other operations, such as area coverage [262, 274] (see Section 4.1) and use of drones to expand connectivity of ground vehicles [122, 141, 200, 248] (see Section 4.5). Presumably, this situation can be explained by the prominence of delivery applications in the media, which may overshadow some other interesting applications. Indeed, in the future, drones may potentially be supported, for instance, by ships in search operations [70] (see Section 4.2) and in gathering data from sensors monitoring underwater pipes (see Section 4.4). Such operations, in which the ships will serve as carriers and moving refueling depots, will also require combined operations planning. In this section, we classify articles according to the roles assigned to drones and vehicles in combined operations. For instance, both drones and vehicles may work in parallel and their operations may have (almost) equal priority. Alternatively, either drones (or vehicles) may serve as the main performance driver in combined operations. In this case, optimization problems on combined operations will often describe only the drone’s (or vehicle’s) objectives—since they have priority—and model vehicle (or drone) operations as constraints because they play a supporting role. In hierarchical planning, operations of the main performance driver (the drone or the vehicle) will be optimized ﬁrst and taken as given in planning operations of the other working unit (the vehicle or the drone). In the combined operations of drones and vehicles, synchronization may or may not be required. In many applications, some synchronization is necessary, for example, when a drone has to wait for its lowspeed carrier. No synchronization is required, on the other hand, if drones and vehicles perform independent tasks, such as independent deliveries from the central warehouse [186] or if operations of either drones or vehicles are ﬁxed as input data in the model [165, 274]. Overall, many new optimization problems seem to arise from the need to synchronize operations of drones and other vehicles. Also in this operation type, most common objectives aim at minimizing the makespan (eg, completion time of a route) [3, 41, 186, 243, 271, 292] and the cost [37, 167, 271], such as energy consumption [287] or the number of required drones [122]. Campbell et al. [37] introduce ﬁxed costs for each truck and for each drone stop. Several articles aim at optimizing customer service, in particular via the number of delivered customer orders [244], number of performed sensor measurements [274], and, in the case of drones serving as relays, communication performance [141, 296]. Objective functions may regulate cooperation of drones and other vehicles. For example, Viguria et al. [287] penalizes situations when a single task (eg, observation) is shared among several robots. 
438 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 29 
FIGURE 14 Illustrative examples of diﬀerent types of combined operations. Straight arrows depict trajectories of the vehicle, bending arrows depict trajectories of the drones, small homes correspond to customer locations, larger facilities correspond to depots 
In Sections 5.1 and 5.2, we survey problems in which either drones or vehicles, respectively, serve as the main performance driver in combined operations. Afterward, we look at articles where drones and vehicles perform tasks in parallel, either without (Section 5.3) or with synchronization (Section 5.4) (see Figure 14). F14 Table 8 lists articles according to the roles of drones and vehicles in combined operations and Table 9 provides an overview T8 T9 of the surveyed articles. 
5.1 Vehicles supporting operations of drones 
Combined operations with vehicles as supporting units ﬁt a variety of applications, including precision agriculture, package delivery, rescue operations, oceanographic sampling, and forest ﬁre or oil spill monitoring. As a rule, the support vehicle is slower, but can travel for a longer time than the drone. In a common modeling approach, the drone may land on the support vehicle to traverse some distance without expending energy. For example, in Tokekar et al. [274], an unmanned ground vehicle (UGV) can carry a drone, which has to take aerial images of as many points of interest as possible to gather information for classiﬁcation of soils based on nitrogen level. In this scenario, the drone does not refuel or recharge on the UGV. The UGV’s speed is assumed to be enough to pick up the drone at any location and at the most suitable time, so that the drone does not have to wait for the UGV. The authors model the resulting problem as an orienteering problem (see Section 4.1.2). Other studies consider diﬀerent drone/vehicle relationships. In Luo et al. [158], Mathew et al. [167] and Garone et al. [89], a drone recharges each time it lands on the support vehicle. In a common setting in delivery operations, a drone delivers one package per sortie to customers located at the nodes of a graph [81, 167]. The support truck cannot perform deliveries; it just transports the drone to some nearby street node within the drone’s range of ﬂight. The objective of Mathew et al. [167] is to ﬁnd paths for the drone and the truck that minimize the total delivery cost; thus, possible waiting times of the drone for the truck are not relevant. The authors transform the resulting problem into the asymmetric TSP. Garone et al. [89] describe a sea rescue scenario in which a drone has to visit a given set of visit points. The authors model the resulting problem in continuous space, where the low speed of the carrier restricts its possible meeting points with the drone. They prove that if the sequence of 
TABLE 8 Surveyed articles according to the roles of drones and vehicles in combined operations 
Drones as the main performance driver 
Vehicles as the main performance driver 
Drones and vehicles as equally important working units 
(Section 5.1) (Section 5.2) (Sections 5.3–5.4) 
Synchronization [89,158,312] [262] [3,37,41,58,106,140,186,215,287,292] 
No synchronization [81,165–167,274] [78,122,141,200,243,244,248] [82,186,212,271,281,296] 
OTTO et al. 439 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
30 OTTO ET AL. 
TABLE 9 Planning combined operations of drones with other vehicles 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
Vehicles supporting operations of drones (Section 5.1) [81] MH, H, Exp several drones, truck 
delivery of 1 package per sortie transport 
[89] M, P, H, Exp drone, carrier limited ﬂight time other (rescue) 
[158] M, H, Exp drone, truck limited ﬂight time general 
[165] M, P, Exp drones, recharging robots 
limited energy general 
[166] M, P, H, Exp drones, recharging robots 
limited energy general 
[167] P, Exp drone, UGV limited ﬂight time, delivery of 1 package per sortie 
transport 
[274] P, H, EmpCS, Exp 
drone, UGV limited ﬂight time, setup times for landing and takeoﬀ 
agriculture 
[312] P, EmpCS, Exp drone, UGV limited energy transport 
Drones supporting operations of vehicles (Section 5.2) [78] P, Exp several drones, several passenger vehicles 
limited communication range telecommunications 
[122] E, Exp drone, several drones, several rescue vehicles 
communication constraints (eg, limited communication range, required connectivity threshold, given transmission power, shadow fading with normal distribution that depends on the elevation angle) 
other (environmental protection and disaster management) 
[141] Decentral MH, Exp 
several drones, ground vehicles 
equations of motion, communication constraints (probability of successful communication depends on, eg, noise and distance) 
general 
[200] M, Decentral H, H, Exp 
several drones, ground vehicles 
minimum turning radius, limited ﬂight time general 
[243] MH, H, Exp drone, truck general 
[244] MH, H, Exp drone, truck limited ﬂight distance general 
[248] MH, Exp several drones, UGVs 
communication constraints general 
[262] H, Exp drone, several autonomous underwater vehicles 
limited ﬂight time other (environmental protection and disaster management) 
Drones and vehicles performing independent tasks (Section 5.3) [82] M, H, MH, Exp several drones, several oﬀ-load trucks 
1 package per delivery other (environmental protection and disaster management) 
[186] M, H, Exp several drones, truck 
limited ﬂight time, delivery of 1 package per sortie 
transport 
[212] M, Control drones, UGVs, airship 
limited water tank other (environmental protection and disaster management) 
(Continued) 
440 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 31 
TABLE 9 (Continued) 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
[271] M, Exp drone, several trucks 
limited payload transport 
[281] DP, Exp several drones, trucks 
limited payload, limited energy transport 
[296] Decentral MH, Decentral H, Exp, Control 
several drones, airships, satellites 
limited ﬂight distance, limited energy other (environmental protection and disaster management) 
Drones and vehicles as synchronized working units (Section 5.4) [3] M, P, H, Exp drone, truck limited ﬂight time, delivery of 1 package per sortie 
transport 
[37] P, Exp drone or several drones, truck 
delivery of 1 package per sortie transport 
[41] P, Exp drone, truck delivery of 1 package per sortie transport 
[58] MH, Exp several drones, trucks 
delivery of 1 package per sortie, limited ﬂight distance 
transport 
[106] M, H, MH, Exp drone, truck delivery of 1 package per sortie transport 
[140] H, Exp drone, truck one package per sortie, function of energy expenditure (depends on the velocity vector of the wind and on the velocity vector of the drone), limited energy 
transport 
[186] M, H, Exp drone or several drones, truck 
limited ﬂight time, delivery of 1 package per sortie 
transport 
[215] P several drones, trucks 
limited ﬂight time, delivery of 1 package per sortie 
transport 
[287] Decentral H, EmpCS, Exp 
several drones, UGVs 
a general framework: no speciﬁc drone characteristics are formulated 
other (environmental protection and disaster management) 
[292] P several drones, several trucks 
limited ﬂight time, delivery of 1 package per sortie 
transport 
visits is given as well as the positions of the landings/takeoﬀs of the drone within this sequence of visits, a convex optimization problem arises. In Yu et al. [312], a battery recharge takes some time, and, while the UGV is traveling, a drone may recharge its battery after landing on the UGV. Mathew et al. [165, 166] consider drones supported by a set of mobile recharging robots. Motions of the drones are taken as given, and recharging may occur only within some time window speciﬁed for each drone. The objective is to ﬁnd paths of the recharging robots with minimum total cost so that each drone meets one of the robots exactly once within its speciﬁed time window. The authors discretize time and space to transform the resulting problem into a oneina-set VRP. 
5.2 Drones supporting operations of vehicles 
Drones may serve as support units for a mobile depot. In Savuran and Karakaya [243, 244], the trajectory and speed of the mobile carrier is taken as a given (ie, it has priority). A drone takes oﬀ from the mobile carrier, serves as many customers as possible, and has to land on the mobile carrier at the end of its sortie. If the ﬂight range of the drone is long enough, the objective could be to minimize the total cost of the drone’s tour [243]; otherwise the number of customers served must be maximized [244]. Although the formulated problem seems very restrictive, it may ﬁnd important applications in future planning of ﬂoating warehouses [171]. 
OTTO et al. 441 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
32 OTTO ET AL. 
In Sujit et al. [262], a drone supports operations of autonomous underwater vehicles (AUVs) exploring the ocean. The drone periodically performs a tour from the base to visit all the AUVs, collecting information and instructing them on their next exploration paths and the next meeting points. A tradeoﬀ arises between selecting AUV paths with the largest information collection potential and adhering to a set of feasible meeting points limited by the endurance in time of the drone. Drones may also provide communication to ground vehicles. For instance, as communication relays, they may connect vehicles, for example, rescue vehicles, to the ground control station and with each other [78, 122, 141, 200, 248]. 
5.3 Drones and vehicles performing independent tasks 
In some delivery applications, drones and vehicles perform independent tasks without the need of synchronization [82, 186, 281]. One example is a study by Murray and Chu [186], who set up a delivery problem with customers located in close proximity to a warehouse. Drones and a truck pick up packages at the warehouse to deliver them to customers. Packages may be delivered either by drone, which transports a maximum of one package per sortie, or by truck, which can transport several packages but moves at a slower speed. Drones cannot land on the truck and must return to the warehouse to pick up another package. The objective is to determine the required number of drones and ﬁnd tours for the truck and for the drones that minimize makespan. Ulmer and Thomas [281] investigate the sameday delivery setting, in which randomly arriving customer orders should be accepted or declined for sameday delivery by the logistics provider. Deliveries can be performed either by vehicles or by drones. Tavana et al. [271] consider parallel operations of trucks and drones in crossdocking with the objective of minimizing completion time. Drones may deliver orders directly from a supplier to a customer, but can carry only a very limited payload. As an alternative, trucks have a much larger carrying capacity, but they have to transport the orders indirectly via an interim warehouse, called crossdock, where goods are unloaded, rebundled into vehicle loads based on their destination, and loaded onto new vehicles (see [34] on crossdocking), which takes much time because of the reloading of the goods and the limited capacity of the crossdock. Several articles examine task assignment to heterogeneous vehicles in disaster management applications [212, 296]. For example, Wu et al. [296] propose a coordination framework to assign observational tasks to drones, airships, and satellites while taking strengths and constraints of each observational resource into account. In Phan and Liu [212], a set of drones and a set of unmanned ground vehicles controlled by an airship must cooperatively predict ﬁre spread rates and drop water to extinguish the ﬁre. 
5.4 Drones and vehicles as synchronized working units 
Synchronization is usually required whenever a drone needs to land on a truck. Because of limited speeds of the drone and the truck, situations arise in which one vehicle must wait for the other one. Murray and Chu [186], Agatz et al. [3], and Ha et al. [106] formulate the TSP with a drone, or a ﬂying sidekick TSP. A truck equipped with a drone performs a tour to deliver packages from a central warehouse to customers. From time to time, the drone takes a package, delivers it to one of the customers and returns to the truck to recharge its batteries [140]. The maximum ﬂight time of the drone’s ﬂight is limited. Because of the required setup operations performed by the driver, the drone can take oﬀ and land only while the truck visits a customer node. Murray and Chu [186] demonstrate in computational experiments that the latter requirement may result in substantial synchronization costs when the drone and the truck have to wait for each other. Wang et al. [292] and Poikonen et al. [215] extended the problem setting to the VRP with a drone. They perform worstcase analyses; for example, they estimate the best possible reduction in delivery completion time if truck operations are combined with drone operations compared to truck operations only. The authors prove that even if each truck is equipped with only one drone, if drones and trucks follow the street network and if the drone speed is the same as the speed of the truck, combined operations of delivery trucks and drones may halve the delivery time in some applications. The derived bounds are similar to Amdahl’s law on the maximum speedup potential for computer programs via parallelization [14]. Poikonen et al. [215] also examine maximum potential savings in the case of diﬀerent distance metrics for trucks and drones, in the case of a limited battery life, and for a general objective function that considers not only the makespan, but also variable costs of truck and drone deployment per unit of time. The authors also establish connections between the VRP with a drone and the wellstudied min–max vehicle routing and the min–max closeenough VRPs. Carlsson and Song [41] and Campbell et al. [37] use continuous approximation methods to derive analytical formulas for the expected delivery cost and times by assuming a continuous distribution of customer locations in 2D space. Campbell et al. [37] model truck travel with the L1 (Manhattan) metric and drone travel with the L2 (Euclidean) metric. For the case of 
442 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 33 
the random and independent distribution of customers in space (ie, the Poisson distribution), the authors have demonstrated substantial savings on delivery cost (of about 10% to 40% in a realistic setting) if deliveries are performed by a truck equipped with a drone. In Daknama and Kraus [58], drones may land on any truck, provided this truck is parking in one of the speciﬁed safe locations (nodes of the graph). The authors test several metaheuristics that schedule delivery trucks and drones. Viguria et al. [287] consider ﬁre detection and extinguishing operations, where ground robots can provide a lift for a drone. 
6 STRATEGIC, TACTICAL, AND OPERATIONAL ISSUES LINKED TO DRONE OPERATIONS 
To enable successful operations of drones, a number of strategic and operational decisions have to be made (see Figure 15). F15 Strategic decisions include decisions on the integration of drone into the civil airspace (Section 6.1) and decisions on physical infrastructure, such as on positioning of depots and recharging stations (Section 6.2). Logistic operators and disaster response teams have to decide on ﬂeet compositions, such as the number, design, and equipment of drones, as well as of other robots and vehicles. Tactical and operational decisions include decisions on the scheduling of refueling, recharging or battery swap processes at recharging stations, scheduling of drone servicing in depots and warehouses, scheduling human experts who will assess the collected critical information on the ﬂy, and scheduling (predictive) maintenance of drones. In the years to come, we expect that widespread use of drone technology in civil applications will lead to a substantial amount of research on optimization problems related to these decisions. Table 10 provides an overview of the surveyed articles. T10 
6.1 Integration of drones into the civil airspace 
Proposed concepts for integration of drone operations within civil airspace include droneonly zones or corridors and shared airspace with piloted aircraft. In any of these cases, integration of drones into the civil airspace will require development of suitable air traﬃc rules and management concepts, as well as collision and dynamic automated path replanning capabilities for individual drones. These approaches, for instance, may specify a set of conﬂictresolving rules (eg, rightofway rules), types of surveillance (eg, aircraft positions are determined by central radar or based on information broadcasted by aircraft), and types of coordination (ie, whether aircraft can communicate with each other in case of conﬂict). We refer the interested reader to the surveys of Jenie et al. [118] on conﬂict detection and resolution approaches and of Pham et al. [211] on collision avoidance systems. Chen et al. [45] investigate droneonly air highways. The authors argue that although air highways will potentially increase the travel distances for drones, they oﬀer major advantages by avoiding (reducing) multiway conﬂicts and, thus, reducing the likelihood of collisions. Chen et al. [45] oﬀer a methodology to determine location of an air highway and propose control strategies for drones when they join the highway traﬃc and travel as leaders or followers of emerging platoons. Richert and 
FIGURE 15 Strategic, tactical, and operational issues linked to drone operations: Visual guidance to Section 6 
OTTO et al. 443 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
34 OTTO ET AL. 
TABLE 10 Strategic, tactical, and operational issues linked to drone operations 
Subjects of 
Publication Contribution planning Drone characteristic(s) Application 
Integration of drones into the civil airspace (Section 6.1) [45] M, Game P, Exp several drones equations of motion other (air traﬃc management) 
[86] M, H, BnC, CP, Exp 
drone maximum rate of climb, maximum rate of descent 
other (air traﬃc management) 
[209] MH, Exp drone equations of motion other (air traﬃc management) 
[225] M, P, Exp several drones fuel consumption function (eg, less fuel consumption in wind shadow) 
general 
Location of physical infrastructure (Section 6.2) [50] M, H, Exp several drones, several trucks 
energy expenditure depends on acceleration, payload, turning, kind of movements (climbing, descending, hovering), limited ﬂying distance 
other (disaster management) 
[82] M, H, MH, Exp several drones, several oﬀ-load trucks 
1 package per delivery other (disaster management) 
[93] M, MH, Exp several drones limited ﬂight distance, limited payload other (disaster management) 
[113] M, MH, Exp several drones limited energy, energy consumption is higher if the drone is carrying a package, delivery of 1 package per sortie 
transport 
[131] M, BnB, H, Exp several drones limited ﬂight time general 
[133] LR, H, Exp several drones limited ﬂight distance transport 
[263] M, BnC, Exp drone equations of motion (angular velocities), limited sensing range, GPSdenied environment - the drone has to be able to localize itself with help of the landmarks 
general 
Human operator scheduling (Section 6.3) [202] M, H, Exp several drones, human operators 
minimum turning radius, sensors with a limited footprint 
general 
[210] M, H, Exp several drones, human operators 
general 
Scheduling setup operations (Section 6.4) [126] DP, H, Exp several drones limited energy general, military 
[232] M, H, EmpCS, Exp 
several drones general, military 
Cortes [225] observe that, similar to platooning for conventional trucks [27, 31], ﬂying in a formation enables the drones that are following the lead drone to consume less energy. Since these beneﬁts are asymmetric, Richert and Cortes [225] examine a game with two strategic independent drones and design a scheme in which one drone proposes a schedule of leader–follower switches to make breaking out from the formation unattractive for both drones at any point in time. Persiani and Bagassi [209] and Furini et al. [86] assume shared airspace. The authors consider path replanning for a drone as the lowest priority participant in air traﬃc. Given scheduled routes of piloted aircraft, the drone adjusts its route in order to avoid collisions. These two studies each formulate a timedependent TSP to sequence a drone’s visit points, and determine its trajectory in space and time, considering important properties of motion such as possible ascent and descent angles. Furini et al. [86] formulate a mixedinteger program and work out several wellperforming cuts for this problem. 
444 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 35 
6.2 Location of physical infrastructure 
The existing articles mostly examine the placement of distribution and transfer centers in disaster regions to supply emergency commodities via drones [50, 82, 93]. For example, Golabi et al. [93] study the location of emergency supply facilities given budget limitations and a set of possible disaster scenarios and their probabilities. The scenarios specify possible damage to infrastructure after some disaster, such as an earthquake. Inhabitants living along intact roads (edges of the graph) can travel by themselves to the nearest aid facility, but inhabitants on damaged edges must be supplied by drones. In this case, the limited ﬂight distance of drones poses a major restriction in the problem. Working with similar scenarios, Chowdhury et al. [50] formulate an integrated facility location and VRP. The authors assume that deliveries from emergency supply centers are generally performed by trucks, and drones are involved only if the road infrastructure is damaged. Fikar et al. [82] consider the placement of a limited number of transfer points operated by relief organizations. Such transfer points, equipped with oﬀ-road vehicles and drones, dispatch supplies over regions with damaged infrastructure, so that ﬁrstand lastmile deliveries are performed with conventional means of transportation over intact roads. Interestingly, oﬀ-road vehicles proved to be more beneﬁcial than drones in detailed simulations of two rather densely populated disaster regions in Austria because of the longer unloading and loading duration of the drones. The authors modeled the speed of oﬀ-road vehicles and drones to be 45 and 135 km/h and set the loading and unloading operations at 3 and 6 minutes, respectively. Hong et al. [113] study the placement of recharging stations for delivery drones, and Kim and Morrison [131] consider automatic service centers for drones in a general context. In the latter paper, the authors simultaneously plan tours of drones to perform a given set of tasks and decide on the placement of automatic service centers where the drones can replenish their energy. Sundar et al. [263] decide on positioning of the landmarks, that is, recognizable building structures, to enable localization of drones in a GPSdenied environment. Kim et al. [133] formulate the facility location problem to position drone depots; drones serve patients in the rural area by delivering medicines and picking up blood samples and urine. Most articles aim at minimizing total cost [50, 133], taking both the ﬁxed cost of opening an additional facility [50, 263] and inventory holding cost [50] into account. In disaster applications, urgencyrelated objectives, such as average lead time for deliveries [82] and total travel time for people and vehicles [93], gain importance. 
6.3 Human operator scheduling 
A number of important civil surveillance applications, such as search and rescue or maintenance inspection, require a human operator who examines the sensory information sent by a drone on the ﬂy. In this case, the drone’s operations should consider possible idle time of the human operator, and the operator should have enough time to examine information on each point of interest (PoI). Ortiz et al. [202] propose a heuristic algorithm for path planning for drones in this setting assuming the sequences of PoI visits for each drone are given. Further, cognitive underand overloading of human operators is to be avoided by alternating demanding and less demanding tasks appropriately as well as by providing enough rest breaks. In the given time horizon, Peters and Bertuccelli [210] schedule as many monitoring tasks to several operators as possible, given routes and speeds of drones, while penalizing situations of cognitive underand overloading. 
6.4 Scheduling setup operations 
Most publications on scheduling setup operations are motivated by military applications, but their salient points can potentially be extended to civil applications. Since we expect many articles on scheduling civil setup operations in the future, we mention a few articles on military applications in this section to suggest possible research directions to the reader. According to the existing literature, the main tradeoﬀ in scheduling setup or maintenance operations of drones, such as refueling, launching, and repair, is to consider the drone’s priority to return to its tasks while not burning up fuel by exceeding its maximum waiting time [126, 232]. Jin et al. [126] optimize the refueling sequence for drones at an automatic refueling tank. Ryan et al. [232] collect data on experiencebased planning rules applied at aircraft carrier decks and compare them to optimal solutions from integer programming. Similar planning problems may emerge at (mobile) warehouses servicing a ﬂeet of drones, such as Amazon’s ﬂoating warehouse or Ford’s autolivery [84, 171]. 
OTTO et al. 445 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
36 OTTO ET AL. 
7 DRONE OPERATIONS: A SOURCE OF NEW OPTIMIZATION PROBLEMS OR A NEW APPLICATION FOR WELLSTUDIED ONES? 
Some features of drones, such as the limited payload or the limited ﬂight time, have already been extensively studied in research on capacitated vehicle routing and vehicle routing in the presence of refueling depots [94]. However, based on our survey, we can identify a number of model extensions, as well as new problems motivated by drone research. Examples of these new and challenging problems include, but are not limited to, the TSP with drones (see, eg, Agatz et al. [3], describing combined delivery operations of a drone and a truck) and the cyclical multiple access data transmission scheme (see, eg, Lyu et al. [159], describing a drone collecting information from a set of ground sensors while cycling along a speciﬁed trajectory). Instead of listing such problems, we summarize some characteristics of drones in the following that require new operations research methods. Note that such a list cannot be complete, if only because of an ongoing high degree of innovation in drone technologies. 
• Novel modeling aspects related to the speciﬁcs of motion. A number of novel aspects are related to the speciﬁcs of ﬂight. A signiﬁcant diﬀerence between operating vehicles and drones is that drones operate in three dimensions while vehicles are restricted to movements on the ground, speciﬁcally, to the road network. Furthermore, drones are more sensitive to weather conditions than regular trucks. Thus, the weather adds dynamics and complexity to these emerging optimization problems and drones add ﬂexibility in terms of constructing vertical tours for delivery or inspection. In Section 4, we have also discussed the importance of considering equations of motion of the drone and energy consumption, such as the minimum turning radius (Section 4.3) and the number of (sharp) turns (Section 4.1). 
• Novel modeling aspects related to energy consumption and payload. Energy consumption of the drone heavily depends on the payload, and the weight of the battery (or fuel) contributes a signiﬁcant share to the total weight of the drone. Interesting novel planning problems may arise from exploiting modularity in the drone’s design, and examining tradeoﬀs in the ﬂight range and the payload of the drone, such as how to equip a team of drones with sensors (of diﬀerent weights and capabilities) and batteries (of diﬀerent weights and capacities) (see Section 4.3). 
• Novel modeling aspects related to energy consumption and the quality of sensing. Observations may be performed from a number of alternative observation points often with tradeoﬀs between the ﬂight distance, the energy consumption, the number of required drones, and the quality of sensing. Recall, for instance, the tradeoﬀ between the altitude of ﬂight and the size of the sensor footprint described in Section 4.1. 
• Novel modeling aspects related to communication. Communication is an essential component of many drone operations, such as data gathering from sensors (Section 4.4), serving as communication relays and ﬂying cloudlets (Section 4.5), especially when a team of drones acts as a FANET (Section 4.6). Incorporating communication aspects into the routing and positioning of drones is a further potential source of methodological innovations. 
• Combined operations with other vehicles. Drones have to collaborate with other robots and means of transportation in a number of operations (Section 5), which may pose challenging synchronization problems (such as coordination of meeting points). 
• Novel modeling aspects related to the drones as part of the civil airspace. A number of issues arise in adopting traﬃc rules and in planning for the civil airspace to incorporate drones, especially if drones are to share the airspace already reserved for aircraft and passenger ﬂights (see Section 6.1). 
• Novel modeling aspects related to the speciﬁcs of particular drone deployments. A number of novel optimization problems will be driven by emerging applications that would be impossible or too expensive without drone technology. These are, for example, path planning for coverage of disconnected, nonconvex areas for the purposes of mapping or surveillance (Section 4.1), energy replenishment and energyaware data gathering in sensor networks (Section 4.4), and estimating motion of icebergs and ice concentration in the sea (Section 4.2). 
446 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 37 
8 CONCLUSIONS AND FUTURE RESEARCH DIRECTIONS 
In this article, we provide a literature survey on optimization approaches to civil applications of drones. Our goal is to provide a fast point of entry into the topic for interested researchers and operations planning specialists. As we have discussed in Section 2, drones represent an emerging technology and new ideas and innovations continually appear, such as ﬂoating (ﬂying) warehouses [171], automated battery switching [269, 275], and in-ﬂight payload transfer from one drone to another [246]. Although drones are not likely to replace existing technologies, they emerge as yet another option to supplement and complement available vehicles and robots. As unmanned technology, drones open up a number of applications that are currently unattractive because of a high labor cost, such as wildlife monitoring or iceberg tracking. As aerial vehicles, drones may access remote areas quickly notwithstanding the state of the roads. And as ﬂying computers, drones may serve as communication relays or cloudlets. Overall, according to several experts, the largest market potential may be related to drone applications in monitoring of infrastructure and construction sites, agriculture, and delivery applications. In some civil applications, like disaster management, transport of medical supplies or environmental monitoring, drones may even help reduce human injuries and save lives. It is, therefore, not surprising that planning of drone operations has recently attracted considerable interest and research. Thus, more than 75% of the surveyed articles have been published in the last 5years. Articles on drone operations primarily belong to engineering-, informatics-, and operations researchoriented outlets. This underscores the high degree of interdisciplinarity of this topic. One important aim of this survey is to bring these communities together. With an eye to an interdisciplinary readership, we provide a short overview of the technical peculiarities of drones (in Section 3) as well as references to some relevant topics in optimization and information technologies (in Sections 4–6). Based on the literature surveyed, we believe that the following research directions will have the most impact in the coming years: 
• As we have discussed in Section 7, drone operations pose new optimization problems and new methodological challenges, which should be addressed in future research. By leveraging stateoftheart optimization and operations research model formulations and solution methodologies, researchers can obtain better solutions and deeper insights into the emerging problems. 
• As discussed above, among the most valuable features of drone technology, and of automated technology in general, is the ﬂexibility of deployment. Therefore, future research should work out dynamic planning schemes for a range of relevant drone operations fulﬁlling a set of desired criteria on average or with a certain guarantee. Future research should also develop approaches to deal with data uncertainty [35, 206]. One important source of data uncertainty is the speciﬁcs of the drone’s motion, susceptibility to weather conditions, and the necessity of dynamic path replanning to avoid collisions. 
• More work is needed on strategic and tactical planning issues, such as determination of ﬂeet size and ﬂeet composition. Furthermore, future studies should focus on strategic questions of drone design to optimize performance, practicality, and economics. Optimization is a valuable tool to examine the bestcase and the expected potential of emerging innovations, and it, thus, can help guide engineers and managers in investment and research decisions. First of all, future studies should evaluate design options for the drones themselves, such as an automatic battery switch. Second, they should analyze alternative designs for systems of drones, other robots and vehicles, and humans, such as an automatic landing capability on a moving truck. Finally, future research should provide guidance on infrastructure development, such as layout of depots and battery recharging stations. 
• Whereas the majority of existing studies focus on minimizing completion time [186, 241, 243] or total travel distance [90, 199, 242], operations planning may signiﬁcantly beneﬁt from incorporating demand into planning models. Future research should develop suitable revenue management approaches (such as segmentation and smart pricing) that may shift demand to the most attractive services and, thus, increase the proﬁtability of commercial drone applications. 
• It is essential to understand the economic and social value of drones in various applications and work out detailed business cases. In particular, future studies should examine how individual beliefs and experience impact purchasing decisions of drone technology and drone services, and the ways in which drones are used as well as the perceived beneﬁts. These inquiries should suggest which types of the drones (eg, ﬁxedwing drones vs. rotorcrafts, expensive large ones vs. cheap small ones) are most suitable for speciﬁc tasks. In a number of 
OTTO et al. 447 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
38 OTTO ET AL. 
applications, such as delivery of vaccines to children in developing countries, quantifying the tradeoﬀs will be particularly challenging. However, without an indepth understanding of costs and beneﬁts related to the drone technology, widespread use will be slow to happen. 
• More generally, it is important to work out technological and regulatory solutions that will address public concerns of privacy and safety without impeding valuecreating drone deployment. Overly strict and rigid regulations may drive promising startups out of the market and lead to the loss of signiﬁcant beneﬁts; however, overly lax regulations may risk major accidents when certain drones are allowed to perform operations they are not yet up to. The latter could lead to the erosion of public conﬁdence in this emerging, promising technology. Current challenges, for instance, include development of low cost senseandavoid systems that enable drones to “see” unexpected hindrances and safety mechanisms to speedbrake the falling drone in case of an engine failure. Future research should suggest and analyze alternative technological and regulatory solutions. As widespread use of drone technology is impossible without public acceptance, honest discussions of the pros and cons of drones should be initiated through the public media to foster understanding and appreciation of the emerging technology. 
ORCID 
Alena Otto http://orcid.org/0000-0002-6069-9330 
Niels Agatz http://orcid.org/0000-0003-3514-201X 
James Campbell http://orcid.org/0000-0003-2951-8703 
Bruce Golden http://orcid.org/0000-0002-5270-6094 
Erwin Pesch https://orcid.org/0000-0003-0182-870X 
REFERENCES 
[1] A.E.A.A. Abdulla et al., An optimal data collection technique for improved utility in UASaided networks, IEEE INFOCOM 2014 - IEEE Conference on Computer Communications, April 2014, pp. 736–744. 
[2] E.U. Acar and H. Choset, Sensorbased coverage of unknown environments: Incremental construction of morse decompositions, Int. J. Robotics Res. 21 (2002), 345–366. 
[3] N. Agatz, P. Bouman, and M. Schmidt, Optimization approaches for the traveling salesman problem with drone, Transp. Sci.  
[4] I. Akyildiz et al., Wireless sensor networks: A survey, Comput. Netw. 38 (2002), 393–422. 
[5] A. AlHourani, S. Kandeepan, and S. Lardner, Optimal LAP altitude for maximum coverage, IEEE Wirel. Commun. Lett. 3 (2014), 569–572. 
[6] A. Albert, F.S. Leira, and L. Imsland, UAV path planning using MILP with experiments, Model. Identif. Control 38 (2017), 21–32. 
[7] T.S. Alemayehu and J.H. Kim, Eﬃcient nearest neighbor heuristic TSP algorithms for reducing data acquisition latency of UAV relay WSN, Wirel. Pers. Commun. 95 (2017), 3271–3285. 
[8] B. Alidaee, H. Gao, and H. Wang, A note on task assignment of several problems, Comput. Indus. Eng. 59 (2010), 1015–1018. 
[9] M. Alighanbari and J.P. How, A robust approach to the UAV task assignment problem, Int. J. Robust Nonlinear Control 18 (2008), 118–134. 
[10] F. Aloul and N. Kandasamy, Sensor deployment for failure diagnosis in networked aerial robots: A satisﬁabilitybased approach, Theory and Applications of Satisﬁability Testing – SAT 2007, Springer, Berlin, 2007, pp. 369–376. 
[11] S. Alpern and S. Gal, The Theory of Search Games and Rendezvous, Kluwer Academic Publishers, Dordrecht, 2003. 
[12] A. Altmann et al., Improved 3D interpolationbased path planning for a ﬁxedwing unmanned aircraft, J. Intell. Robot. Syst. 76 (2014), 185–197. 
[13] M. Alzenad et al., 3-D placement of an unmanned aerial vehicle base station (UAVBS) for energyeﬃcient maximal coverage, IEEE Wirel. Commun. Lett. 6 (2017), 434–437. 
[14] G.M. Amdahl, Validity of the single processor approach to achieving large scale computing capabilities, Proceedings of the Spring Joint Computer Conference, April 18–20, 1967, ACM, New York, 1967, pp. 483–485. 
[15] M.E. Argyle, R.W. Beard, and D.W. Casbeer, Multiteam consensus bundle algorithm, Handbook of Unmanned Aerial Vehicles, K.P. Valavanis and G.J. Vachtsevanos (eds.), Springer: Dordrecht, 2015, pp. 1491–1507. 
[16] E. Arkin and R. Hassin, Approximation algorithms for the geometric covering salesman problem, Discrete Appl. Math. 55 (1994), 197–218. 
[17] G.S.C. Avellar et al., MultiUAV routing for area coverage and remote sensing with minimum time, Sensors 15 (2015), 27783–27803. 
965–981. 
52 (2018), 
448 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 39 
[18] L. Babel, Curvatureconstrained traveling salesman tours for aerial surveillance in scenarios with obstacles, Eur. J. Oper. Res. 262 (2017), 335–346. 
[19] J. Bae and S. Rathinam, Approximation algorithm for a heterogeneous vehicle routing problem, Int. J. Adv. Robot. Syst. 12 (2015), Article ID (doi): 10.5772/6008. 
[20] F. Balampanis, I. Maza, and A. Ollero, Area partition for coastal regions with multiple UAS, J. Intell. Robot. Syst. 88 (2017), 751–766. 
[21] T.W. Bandeira et al., Analysis of path planning algorithms based on travelling salesman problem embedded in UAVs, Brazilian Symposium on Computing Systems Engineering (SBESC), 2015, pp. 70–75. 
[22] A. Barrientos et al., Aerial remote sensing in agriculture: A practical approach to area coverage and path planning for ﬂeets of mini aerial robots, J. Field Robot. 28 (2011), 667–689. 
[23] E. Basha et al., UAV recharging opportunities and policies for sensor networks, Int. J. Distrib. Sensor Netw. 11 (2015), 824260. 
[24] I. Bekmezci, O.K. Sahingoz, and S. Temel, Flying adhoc networks (FANETs): A survey, Ad Hoc Netw. 11 (2013), 1254–1270. 
[25] J. Bellingham et al., Multitask allocation and path planning for cooperating UAVs, Cooperative Control: Models, Applications and Algorithms, S. Butenko, R. Murphey, and P.M. Pardalos (eds.), Springer: Boston, MA, 2003, pp. 23–41. 
[26] S.J. Benkoski, M.G. Monticino, and J.R. Weisinger, A survey of the search theory literature, Naval Res. Logist. 38 (1991), 469–494. 
[27] C. Bergenhem et al., Overview of platooning systems, Proceedings of the 19th ITS World Congress, Oct 22–26, Vienna, Austria, 2012. 
[28] D.J. Bertsimas and G.J. van Ryzin, A stochastic and dynamic vehicle routing problem in the Euclidean plane, Oper. Res. 39 (1991), 601–615. 
[29] D.J. Bertsimas and G.J. van Ryzin, Stochastic and dynamic vehicle routing in the Euclidean plane with multiple capacitated vehicles, Oper. Res. 41 (1993), 60–76. 
[30] A. Bhardwaj et al., UAVs as remote sensing platform in glaciology: Present applications and future prospects, Remote Sens. Environ. 175 (2016), 196–204. 
[31] A.K. Bhoopalam, N. Agatz, and R. Zuidwijk, Planning of truck platoons: A literature review and directions for future research, Transp. Res. B Methodol. 107 (2018), 212–228. 
[32] R.I. BorYaliniz, A. ElKeyi, and H. Yanikomeroglu, Eﬃcient 3-D placement of an aerial base station in next generation cellular networks, 2016 IEEE International Conference on Communications (ICC), 2016, pp. 1–5. 
[33] F. Bourgault, T. Furukawa, and H.F. DurrantWhyte, Optimal search for a lost target in a Bayesian world, Field and Service Robotics: Recent Advances in Research and Applications, S. Yuta, H. Asama, E. Prassler, T. Tsubouchi, and S. Thrun (eds.), Springer, Berlin, 2006, pp. 209–222. 
[34] N. Boysen and M. Fliedner, Cross dock scheduling: Classiﬁcation, literature review and research agenda, Omega 38 (2010), 413–422. 
[35] F. Bullo et al., Dynamic vehicle routing for robotic systems, Proc. IEEE 99 (2011), 1482–1504. 
[36] F. Çakici et al., Coordinated guidance for multiple UAVs, Trans. Inst. Meas. Control 38 (2016), 593–601. 
[37] J.F. Campbell, D.C. Sweeney II, and J. Zhang, Strategic design for delivery with trucks and drones, Supply Chain Analytics Report SCMA- 2017-02-01 (2017). 
[38] B. Canis, Unmanned aircraft systems (UAS): Commercial outlook for a new industry, CRS Report: Congressional Res. Service September 9, 2015 (2015). 
[39] H.R. Cao et al., An optimization method to improve the performance of unmanned aerial vehicle wireless sensor networks, Int. J. Distrib. Sensor Netw. 13 (2017). Article ID (doi): 10.1177/1550147717705614. 
[40] L. Caraballo et al., The blockinformationsharing strategy for task allocation: A case study for structure assembly with aerial robots, Eur. J. Oper. Res. 260 (2017), 725–738. 
[41] J.G. Carlsson and S. Song, Coordinated logistics with a truck and a drone, Manag. Sci.     (2018),  
[42] N. Chamaria, Drone usage in agriculture could be a $32 billion market, Motley Fool, 2016. 
[43] S. Chandrasekharan et al., Designing and implementing future aerial communication networks, IEEE Commun. Mag. 54 (2016), 26–34. 
[44] H. Chen, X. Wang, and Y. Li, A survey of autonomous control for UAV, International Conference on Artiﬁcial Intelligence and Computational Intelligence, 2009. 
[45] M. Chen et al., Reachabilitybased safety and goal satisfaction of unmanned aerial platoons on air highways, J. Guid. Control Dyn. 40 (2017), 1360–1373. 
[46] V.V. Chetlur and H.S. Dhillon, Downlink coverage analysis for a ﬁnite 3D wireless network of unmanned aerial vehicles, IEEE Trans. Commun., 65 (2017), 4543–4558. 
[47] D.H. Choi, S.H. Kim, and D.K. Sung, Energyeﬃcient maneuvering and communication of a single UAVbased relay, IEEE Trans. Aerosp. Electron. Syst. 50 (2014), 2320–2327. 
[48] H. Choi, Y. Kim, and H. Kim, Genetic algorithm based decentralized task assignment for multiple unmanned aerial vehicles in dynamic environments, Int. J. Aeronaut. Space Sci. 12 (2011), 163–174. 
4052–4069.. 64 
OTTO et al. 449 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
40 OTTO ET AL. 
[49] H. Choset, Coverage for robotics – A survey of recent results,Ann. Math. Artif. Intell. 31 (2001), 113–126. 
[50] S. Chowdhury et al., Drones for disaster response and relief operations: A continuous approximation model, Int. J. Prod. Econ. 188 (2017), 167–184. 
[51] A. Claesson et al., Time to delivery of an automated external deﬁbrillator using a drone for simulated outofhospital cardiac arrests vs emergency medical services, J. Am. Med. Assoc. 317 (2017), 2332–2334. 
[52] J.A. Cobano et al., Data retrieving from heterogeneous wireless sensor network nodes using UAVs, J. Intell. Robot. Syst. 60 (2010), 133–151. 
[53] F. Cocchioni, A. Mancini, and S. Longhi, Autonomous navigation, landing and recharge of a quadrotor using artiﬁcial vision, International Conference on Unmanned Aircraft Systems (ICUAS), 2014, pp. 418–429. 
[54] B.N. Coelho et al., A multiobjective green UAV routing problem, Comput. Oper. Res. 88 (2017), 306–315. 
[55] I. Cohen, C. Epstein, and T. Shima, On the discretized Dubins traveling salesman problem, IISE Trans. 49 (2017), 238–254. 
[56] W.P. Coutinho and J. Fliege, The unmanned aerial vehicle routing and trajectory optimisation problem, Working paper, University Southampton, 2017. 
[57] C.T. Cunningham and R.S. Roberts, An adaptive path planning algorithm for cooperating unmanned air vehicles, Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation, 2001, vol. 4, pp. 3981–3986. 
[58] R. Daknama and E. Kraus, Vehicle routing with drones, arXiv 1705.06431v1 (2017). 
[59] K. Dalamagkidis, Aviation history and unmanned ﬂight, Handbook of Unmanned Aerial Vehicles, K.P. Valavanis and G.J. Vachtsevanos (eds.), Springer, Dordrecht, 2015, pp. 57–81. 
[60] K. Dalamagkidis, Classiﬁcation of UAVs, Handbook of Unmanned Aerial Vehicles, K.P. Valavanis and G.J. Vachtsevanos (eds.), Springer, Dordrecht, 2015, pp. 83–91. 
[61] V. Darbari, S. Gupta, and O.P. Verma, Dynamic motion planning for aerial surveillance on a ﬁxedwing UAV, arXiv 1705.08010 (2017). 
[62] M. de Berg et al., TSP with neighborhoods of varying size, J. Algorithms 57 (2005), 22–36. 
[63] L. De Filippis, G. Guglieri, and F. Quagliotti, Path planning strategies for UAVs in 3D environments, J. Intell. Robot. Syst. 65 (2012), 247–264. 
[64] DHL, Unmanned Aerial Vehicle in Logistics: A DHL Perspective on Implications and Use Cases for the Logistics Industry, DHL Customer Solutions & Innovation, Troisdorf, 2014. 
[65] J.M. DíazBáñez et al., A general framework for synchronizing a team of robots under communication constraints, IEEE Trans. Robot. 33 (2017), 748–755. 
[66] M. Dille and S. Singh, Eﬃcient aerial coverage search in road networks, AIAA Guidance, Navigation, and Control (GNC) Conference, Boston, MA, 2013. 
[67] M. Dong et al., UAVassisted data gathering in wireless sensor networks, J. Supercomput. 70 (2014), 1142–1155. 
[68] K. Dorling et al., Vehicle routing problems for drone delivery, IEEE Trans. Syst., Man, Cybernet. Syst. 47 (2017), 70–85. 
[69] L.E. Dubins, On the curves of minimal length with a constant on average curvature and with prescribed initial and terminal positions and tangents, Am. J. Math. 79 (1957), 497–516. 
[70] E. Dyrkoren and T. Berg, Norwegian work on search and rescue in Barents Sea, 33rd International Conference on Oﬀshore Mechanics and Arctic Engineering. Volume 10: Polar and Arctic Science and Technology, 2014. 
[71] K. Elbassioni, A. Fishkin, and R. Sitters, Approximation algorithms for Euclidean traveling salesman problem with discrete and continuous neighborhoods, Int. J. Comput. Geom. Appl. 19 (2009), 173–193. 
[72] J.J. Enright et al., UAV routing and coordination in stochastic, dynamic environments, Handbook of Unmanned Aerial Vehicles, K.P. Valavanis and G.J. Vachtsevanos (eds.), Springer, Dordrecht, 2015, pp. 2079–2109. 
[73] J.J. Enright et al., Stochastic and dynamic routing problems for multiple uninhabited aerial vehicles, J. Guid. Control Dyn. 32 (2009), 1152–1166. 
[74] H. Ergezer and K. Leblebicio˘glu, Path planning for UAVs for maximum information collection, IEEE Trans. Aerosp. Electron. Syst. 49 (2013), 502–520. 
[75] H. Ergezer and L. Leblebicio˘glu, 3D path planning for multiple UAVs for maximum information collection, J. Intell. Robot. Syst. 73 (2014), 737–762. 
[76] L. Evers et al., Online stochastic UAV mission planning with time windows and timesensitive targets, Eur. J. Oper. Res. 238 (2014), 348–362. 
[77] Z.M. Fadlullah et al., A dynamic trajectory control algorithm for improving the communication throughput and delay in UAVaided networks, IEEE Netw. 30 (2016), 100–105. 
[78] W. Fawaz et al., Unmanned aerial vehicles as storecarryforward nodes for vehicular networks, IEEE Access 5 (2017), 23710–23718. 
[79] Federal Aviation Administration, Integration of Civil Unmanned Aircraft Systems (UAS) in the National Airspace System (NAS) Roadmap, Tech. report 2012-AJG-502, U.S. Department of Transportation, Federal Aviation Administration, November 7, 2013. 
450 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 41 
[80] Federal Aviation Administration, FAA aerospace forecast: Fiscal years 2016–2036, Tech. report TC16-002, Federal Aviation Administration, 2016. 
[81] S.M. Ferrandez et al., Optimization of a truckdrone in tandem delivery network using kmeans and genetic algorithm, J. Ind. Eng. Manag. 9 (2016), 374–388. 
[82] C. Fikar, M. Gronalt, and P. Hirsch, A decision support system for coordinated disaster relief distribution, Expert Syst. Appl. 57 (2016), 104–116. 
[83] P. Finnegan and D. Cornell, Teal Group predicts worldwide civil UAS production will total $65 billion in its 2016 UAS market proﬁle and forecast, Teal Group (News), 2016. 
[84] S. Fogel, Ford concept uses drones and selfdriving vans for deliveries, Endogadget, 2017. 
[85] A. Fotouhi, M. Ding, and M. Hassan, Dronecells: Improving 5g spectral eﬃciency using dronemounted ﬂying base stations, arXiv 1707.02041 (2017). 
[86] F. Furini, C.A. Persiani, and P. Toth, The time dependent traveling salesman planning problem in controlled airspace, Transp. Res. B Methodol. 90 (2016), 38–55. 
[87] E. Galceran and M. Carreras, A survey on coverage path planning for robotics, Robot. Auton. Syst. 61 (2013), 1258–1276. 
[88] B. Galkin, J. Kibiłda, and L.A. DaSilva, Coverage analysis for lowaltitude UAV networks in urban environments, arXiv 1704.06214 (2017). 
[89] E. Garone et al., Cooperative mission planning for a class of carriervehicle systems, 49th IEEE Conference on Decision and Control (CDC), 2010, pp. 1354–1359. 
[90] I. Gentilini, F. Margot, and K. Shimada, The travelling salesman problem with neighbourhoods: MINLP solution, Optim. Methods Softw. 28 (2013), 364–378. 
[91] A. Giorgetti et al., Throughput per pass for data aggregation from a wireless sensor network via a UAV, IEEE Trans. Aerosp. Electron. Syst. 47 (2011), 2610–2626. 
[92] C. Goerzen, Z. Kong, and B. Mettler, A survey of motion planning algorithms from the perspective of autonomous UAV guidance, J. Intell. Robot. Syst. 57 (2010), 65–100. 
[93] M. Golabi, S.M. Shavarani, and G. Izbirak, An edgebased stochastic facility location problem in UAVsupported humanitarian relief logistics: A case study of Tehran earthquake, Nat. Hazards 87 (2017), 1545–1565. 
[94] B. Golden, S. Raghavan, and E. Wasil, The Vehicle Routing Problem: Latest Advances and New Challenges, Springer, New York, 2008. 
[95] A. Goodchild and J. Toy, Delivery by drone: An evaluation of unmanned aerial vehicle technology in reducing CO2 emissions in the delivery service industry, Transp. Res. D Transp. Environ. 
[96] Y. Gottlieb and T. Shima, UAVs task and motion planning in the presence of obstacles and prioritized targets, Sensors 15 (2015), 29734–29764. 
[97] G. Gramajo and P. Shankar, An eﬃcient energy constraint based UAV path planning for search and coverage, Int. J. Aerosp. Eng. 2017 (2017), 1–13. 
[98] A. Grancharova, E.I. Grøtli, D.T. Ho, and T.A. Johansen, UAVs trajectory planning by distributed MPC under radio communication path loss constraints, J. Intell. Robot. Syst. 79 (2015), 115–134. 
[99] E.I. Grøtli and T.A. Johansen, Path planning for UAVs under communication constraints using SPLAT! and MILP, J. Intell. Robot. Syst. 65 (2012), 265–282. 
[100] E.I. Grøtli and T.A. Johansen, Motionand communicationplanning of unmanned aerial vehicles in delay tolerant network using mixedinteger linear programming, Model. Identif. Control 37 (2016), 77–97. 
[101] Z. Gu et al., Reducing information gathering latency through mobile aerial sensor network, 2013 Proceedings IEEE INFOCOM, 2013, pp. 656–664. 
[102] D. Guedj, Le Theoreme du Perroquet (The Parrot’s Theorem), Editions du Seuil, Paris, 1998. 
[103] J.A. Guerrero and Y. Bestaoui, UAV path planning for structure inspection in windy environments, J. Intell. Robot. Syst. 69 (2013), 297–311. 
[104] F. Guerriero et al., A multiobjective approach for unmanned aerial vehicle routing problem with soft time windows constraints,Appl. Math. Model. 38 (2014), 839–852. 
[105] L. Gupta, R. Jain, and G. Vaszkun, Survey of important issues in UAV communication networks, IEEE Commun. Surveys Tutor. 18 (2016), 1123–1152. 
[106] Q.M. Ha et al., On the mincost traveling salesman problem with drone, Transportation Research Part C: Emerging Technologies, 86 2018, 597–621. 
[107] L.A. Haidari et al., The economic and operational value of using drones to transport vaccines, Vaccine 34 (2016), 4062–4067. 
[108] J. Hall and D. Anderson, Reactive route selection from precalculated trajectories - application to microUAV path planning, Aeronaut. J. 115 (2011), 635–640. 
[109] J. Haugen and L. Imsland, Monitoring an advectiondiﬀusion process using aerial mobile sensors, Unmanned Syst. 3 (2015), 221–238. 
61(2018), 58–67. 
OTTO et al. 451 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
42 OTTO ET AL. 
[110] J. Haugen and L. Imsland, Monitoring moving objects using aerial mobile sensors, IEEE Trans. Control Syst. Technol. 24 (2016), 475–486. 
[111] D. Ho et al., Optimization of wireless sensor network and UAV data acquisition, J. Intell. Robot. Syst. 78 (2015), 159–179. 
[112] H.M. Ho and J. Ouaknine, The cyclicrouting UAV problem is PSPACEcomplete, Foundations of Software Science and Computation Structures: 18th International Conference, FOSSACS 2015, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2015, London, UK, April 11–18, 2015, Proceedings, A. Pitts (ed.), Springer, Berlin, 2015, pp. 328–342. 
[113] I. Hong, M. Kuby, and A. Murray, A deviation ﬂow refueling location model for continuous space: A commercial drone delivery system for urban areas, Advances in Geocomputation: Geocomputation 2015–The 13th International Conference, D.A. Griﬃth, Y. Chun, and D.J. Dean (eds.), Springer, Cham, 2017, pp. 125–132. 
[114] J. Hu, J. Xu, and L. Xie, Cooperative search and exploration in robotic networks, Unmanned Syst. 1 (2013), 121–142. 
[115] W.H. Huang, Optimal linesweepbased decompositions for coverage algorithms, IEEE International Conference on Robotics and Automation, 2001, Vol. 1, pp. 27–32. 
[116] I. Jawhar et al., Communication and networking of UAVbased systems: Classiﬁcation and associated architectures,J. Netw. Comput. Appl. 84 (2017), 93–108. 
[117] I. Jawhar, N. Mohamed, J. AlJaroodi, and S. Zhang, A framework for using unmanned aerial vehicles for data collection in linear wireless sensor networks,J. Intelligent Robotic Syst 74 (2014), 437–453. 
[118] Y.I. Jenie et al., Taxonomy of conﬂict detection and resolution approaches for unmanned aerial vehicle in an integrated airspace, IEEE Trans. Intell. Transp. Syst 18 (2017), 558–567. 
[119] S. Jeong, O. Simeone, and J. Kang, Mobile cloud computing with a UAVmounted cloudlet: Optimal bit allocation for communication and computation, IET Commun. 11 (2017), 969–974. 
[120] S. Jeong, O. Simeone, and J. Kang, Mobile edge computing via a UAVmounted cloudlet: Optimization of bit allocation and path planning, IEEE Transactions on Vehicular Technology, 2017, pp. 1–1. 
[121] X. Ji et al., Cooperative search by multiple unmanned aerial vehicles in a nonconvex environment, Math. Prob. Eng. 2015 (2015), 1–19. 
[122] S. Jia and L. Zhang, Modelling unmanned aerial vehicles base station in groundtoair cooperative networks, IET Commun. 11 (2017), 1187–1194. 
[123] F. Jiang and A.L. Swindlehurst, Optimization of UAV heading for the groundtoair uplink, IEEE J. Sel. Areas Commun. 30 (2012), 993–1005. 
[124] T. Jiang et al., Unmanned aircraft system traﬃc management: Concept of operation and system architecture, Int. J. Transp. Sci. Technol. 5 (2016), 123–135. 
[125] X. Jiang, Q. Zhou, and Y. Ye, Method of task assignment for UAV based on particle swarm optimization in logistics, Proceedings of the 2017 International Conference on Intelligent Systems, Metaheuristics & Swarm Intelligence, 2017, pp. 113–117. 
[126] Z. Jin, T. Shima, and C.J. Schumacher, Optimal scheduling for refueling multiple autonomous aerial vehicles, IEEE Trans. Robot. 22 (2006), 682–693. 
[127] E. Kahale, P. Castillo, and Y. Bestaoui, Minimum time reference trajectory generation for an autonomous quadrotor, 2014 International Conference on Unmanned Aircraft Systems (ICUAS), 2014, pp. 126–133. 
[128] S. Karaman and G. Inalhan, Largescale task/target assignment for UAV ﬂeets using a distributed branch and price optimization scheme, IFAC Proceedings, 2008, vol. 41, pp. 13310–13317. 
[129] W. Khaksar et al., A review on mobile robots motion path planning in unknown environments, IEEE International Symposium on Robotics and Intelligent Sensors (IRIS), 2015. 
[130] Y. Khosiawan and I. Nielsen, Indoor UAV scheduling with restful task assignment algorithm, arXiv 1706.09737 (2017). 
[131] J. Kim and J.R. Morrison, On the concerted design and scheduling of multiple resources for persistent UAV operations, J. Intell. Robot. Syst. 74 (2014), 479–498. 
[132] J. Kim, B.D. Song, and J.R. Morrison, On the scheduling of systems of UAVs and fuel service stations for longterm mission fulﬁllment, J. Intell. Robot. Syst. 70 (2013), 347–359. 
[133] S.J. Kim et al., Droneaided healthcare services for patients with chronic diseases in rural areas, J. Intell. Robot. Syst. 88 (2017), 163–180. 
[134] J. Koebler, How NASA plans to open “Air Highways” for drones, Motherboard, 2014. 
[135] B.O. Koopman, The theory of search II: Target detection, Oper. Res. 4 (1956), 503–531. 
[136] B.O. Koopman, The theory of search. III: The optimum distribution of searching eﬀort, Oper. Res. 5 (1957), 613–626. 
[137] S. Koulali et al., A green strategic activity scheduling for UAV networks: A submodular game perspective, IEEE Commun. Mag. 54 (2016), 58–64. 
[138] J.A. Koupaei and M. Abdechiri, Sensor deployment for fault diagnosis using a new discrete optimization algorithm, Appl. Soft Comput. 13 (2013), 2896–2905. 
452 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 43 
[139] A. Krause, A. Singh, and C. Guestrin, Nearoptimal sensor placements in Gaussian processes: Theory, eﬃcient algorithms and empirical studies, J. Mach. Learn. Res. 9 (2008), 235–284. 
[140] A. Kundu and T. Matis, A delivery time reduction heuristic using drones under windy conditions, Proceedings of the 2017 Industrial and Systems Engineering Conference, K. Coperich, E. Cudney, and H. Nembhard (eds.), Curran Associates, Inc., Red Hook, 2017, pp. 1894–1899. 
[141] P. Ladosz, H. Oh, and W.H. Chen, Trajectory planning for communication relay unmanned aerial vehicles in urban dynamic environments, J. Intell. Robot. Syst. 89 (2018), 7–25. 
[142] P. Lanillos et al., MultiUAV target search using decentralized gradientbased negotiation with expected observation, Inf. Sci. 282 (2014), 92–110. 
[143] N. Lavars, Amazon to begin testing new delivery drones in the US, New Atlas (2015). 
[144] J. Lee, Optimization of a modular drone delivery system, 2017 Annual IEEE International Systems Conference (SysCon), 2017, pp. 1–8. 
[145] J. LeMieux (ed.), Introduction to Unmanned Systems: Air, Ground, Sea & Space, Unmanned Vehicle University Press, Phoenix, 2013. 
[146] D. Li, X. Wang, and T. Sun, Energyoptimal coverage path planning on topographic map for environment survey with unmanned aerial vehicles, Electron. Lett. 52 (2016), 699–701. 
[147] J. Li, and Y. Han, Optimal resource allocation for packet delay minimization in multilayer UAV networks, IEEE Commun. Lett. 21 (2017), 580–583. 
[148] K. Li et al., Energyeﬃcient cooperative relaying for unmanned aerial vehicles,IEEE Trans. Mobile Comput. 15 (2016), 1377–1386. 
[149] Y. Li et al., Coverage path planning for UAVs based on enhanced exact cellular decomposition method, Mechatronics 21 (2011), 876–885. 
[150] W. Liang, J. Luo, and X. Xu, Network lifetime maximization for timesensitive data gathering in wireless sensor networks with a mobile sink, Wirel. Commun. Mobile Comput. 13 (2013), 1263–1280. 
[151] L. Lin and M.A. Goodrich, Hierarchical heuristic search using a Gaussian mixture model for UAV coverage planning, IEEE Trans. Cybernet. 44 (2014), 2532–2544. 
[152] Y. Lin and S. Saripalli, Path planning using 3D Dubins curve for unmanned aerial vehicles, 2014 International Conference on Unmanned Aircraft Systems (ICUAS), 2014, pp. 296–304. 
[153] W. Liu, Z. Zheng, and K.Y. Cai, Bilevel programming based realtime path planning for unmanned aerial vehicles, Knowl. Based Syst. 44 (2013), 34–47. 
[154] X.F. Liu et al., An optimization model of UAV route planning for road segment surveillance, J. Central South Univ. 21 (2014), 2501–2510. 
[155] S.W. Loke, The internet of ﬂyingthings: Opportunities and challenges with airborne fog computing and mobile cloud in the clouds, arXiv 1507.04492 (2015). 
[156] J. Lu et al., Energyeﬃcient 3D UAVBS placement versus mobile users’ density and circuit power, arXiv 1705.06500 (2017). 
[157] Lufthansa, Der Bereich Lufthansa Aerial Services schließt Partnerschaft mit führendem Drohnenhersteller DJI (The Aerial Services Department of Lufthansa starts partnership with a leading drone producer DJI), Tech. report 26th of January 2016, Press Release, 2016. 
[158] Z. Luo, Z. Liu, and J. Shi, A twoechelon cooperated routing problem for a ground vehicle and its carried unmanned aerial vehicle, Sensors 17 (2017), 1–17. 
[159] J. Lyu, Y. Zeng, and R. Zhang, Cyclical multiple access in UAVaided communications: A throughputdelay tradeoﬀ ,IEEE Wirel. Commun. Lett. 5 (2016), 600–603. 
[160] J. Lyu et al., Placement optimization of UAVmounted mobile base stations, IEEE Commun. Lett. 21 (2017), 604–607. 
[161] F. Ma and S. Karaman, On sensing, agility and computation requirements for a datagathering agile robotic vehicle, arXiv 1704.02075 (2017). 
[162] K.C. Magoteaux, B. Sanders, and H.A. Sodano, Investigation of an energy harvesting small unmanned air vehicle, Master’s thesis, University of Dayton, 2008. 
[163] S.G. Manyam, S. Rathinam, and S. Darbha, Computation of lower bounds for a multiple depot, multiple vehicle routing problem with motion constraints, J. Dyn. Syst. Meas. Control 137 (2015), 1–5. 
[164] S.G. Manyam et al., GPS denied UAV routing with communication constraints, J. Intell. Robot. Syst. 84 (2016), 691–703. 
[165] N. Mathew, S.L. Smith, and S.L. Waslander, A graphbased approach to multirobot rendezvous for recharging in persistent tasks, 2013 IEEE International Conference on Robotics and Automation, May 2013, pp. 3497–3502. 
[166] N. Mathew, S.L. Smith, and S.L. Waslander, Multirobot rendezvous planning for recharging in persistent tasks, IEEE Trans. Robot. 31 (2015), 128–142. 
[167] N. Mathew, S.L. Smith, and S.L. Waslander, Planning paths for package delivery in heterogeneous multirobot teams, IEEE Trans. Autom. Sci. Eng. 12 (2015), 1298–1308. 
[168] I. Maza and A. Ollero, Multiple UAV cooperative searching operation using polygon area decomposition and eﬃcient coverage algorithms, Distributed Autonomous Robotic Systems, R. Alami, R. Chatila, and H. Asama (eds.), Springer, Tokyo, 2007, pp. 221–230. 
OTTO et al. 453 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
44 OTTO ET AL. 
[169] A. Mazayev, N. Correia, and S. Schütz, Data gathering in wireless sensor networks using unmanned aerial vehicles, Int. J. Wirel. Inf. Netw. 23 (2016), 297–309. 
[170] M. Mazur, J. McMillan, and A. Wi´sniewski, Clarity from above. PwC global report on the commercial applications of drone technology, Tech. report, PricewaterhouseCoopers, 2016. 
[171] J. McGauley, Amazon’s next grand plan: Blimpsized, ﬂoating warehouses, Thrillist (2017). 
[172] V. Mersheeva and G. Friedrich, Routing for continuous monitoring by multiple micro UAVs in disaster scenarios, Proceedings of the 20th European Conference on Artiﬁcial Intelligence, 2012, pp. 588–593. 
[173] Y. Miao et al., Research on dynamic task allocation for multiple unmanned aerial vehicles, Trans. Inst. Meas. Control 39 (2017), 466–474. 
[174] S. Moon, D. Hyunchul, and E. Oh, Cooperative task assignment and path planning for multiple UAVs, Handbook of Unmanned Aerial Vehicles, K.P. Valavanis and G.J. Vachtsevanos (eds.), Springer, Dordrecht, 2015, pp. 1547–1576. 
[175] S. Moon, E. Oh, and D.H. Shim, An integral framework of task assignment and path planning for multiple unmanned aerial vehicles in dynamic environments, J. Intell. Robot. Syst. 70 (2013), 303–313. 
[176] S. Moon and D.H. Shim, Study on path planning algorithms for unmanned agricultural helicopters in complex environment, Int. J. Aeronaut. Space Sci. 10 (2009), 1–11. 
[177] M. Mozaﬀari et al., Drone small cells in the clouds: Design, deployment and performance analysis, 2015 IEEE Global Communications Conference (GLOBECOM), 2015, pp. 1–6. 
[178] M. Mozaﬀari et al., Eﬃcient deployment of multiple unmanned aerial vehicles for optimal wireless coverage, IEEE Commun. Lett. 20 (2016), 1647–1650. 
[179] M. Mozaﬀari et al., Optimal transport theory for powereﬃcient deployment of unmanned aerial vehicles, 2016 IEEE International Conference on Communications (ICC), 2016, pp. 1–6. 
[180] M. Mozaﬀari et al., Unmanned aerial vehicle with underlaid devicetodevice communications: Performance and tradeoﬀs, IEEE Trans. Wirel. Commun. 15 (2016), 3949–3963. 
[181] M. Mozaﬀari et al., Mobile unmanned aerial vehicles (UAVs) for energyeﬃcient internet of things communications, IEEE Transactions on Wireless Communications, 16 (2017), 7574–7589. 
[182] M. Mozaﬀari et al., Optimal transport theory for cell association in UAVenabled cellular networks, IEEE Commun. Lett. 21 (2017), 2053–2056. 
[183] M. Mozaﬀari et al., Wireless communication using unmanned aerial vehicles (UAVs): Optimal transport theory for hover time optimization, IEEE Transactions on Wireless Communications, 16 (2017), 8052–8066. 
[184] F. Mufalli, R. Batta, and R. Nagi, Simultaneous sensor selection and routing of unmanned aerial vehicles for complex mission plans, Comput. Oper. Res. 39 (2012), 2787–2799. 
[185] R.R. Murphy, Disaster Robotics, The MIT Press, Cambridge, 2014. 
[186] C.C. Murray and A.G. Chu, The ﬂying sidekick traveling salesman problem: Optimization of droneassisted parcel delivery, Transp. Res. C Emerg. Technol. 54 (2015), 86–109. 
[187] L. Navaravong et al., Optimizing network topology to reduce aggregate traﬃc in systems of mobile robots, arXiv 1108.6087 (2011). 
[188] A. Nedjati et al., Complete coverage path planning for a multiUAV response system in postearthquake assessment, Robotics 5 (2016), 26. 
[189] J.R. Neto et al., Performance evaluation of unmanned aerial vehicles in automatic power meter readings, Ad Hoc Netw. 60 (2017), 11–25. 
[190] F. Nex and F. Remondino, UAV for 3D mapping applications: A review, Appl. Geomat. 6 (2014), 1–15. 
[191] J.L. Nguyen et al., Realtime path planning for longterm information gathering with an aerial glider, Auton. Robots 40 (2016), 1017–1039. 
[192] M. Niccolini, M. Innocenti, and L. Pollini, Multiple UAV task assignment using descriptor functions, IFAC Proceedings, 2010, vol. 43, pp. 93–98. 
[193] M. Niendorf, P.T. Kabamba, and A.R. Girard, Stability of solutions to classes of traveling salesman problems, IEEE Trans. Cybernet. 46 (2016), 973–985. 
[194] I.K. Nikolos et al., Evolutionary algorithm based oﬄine/online path planner for UAV navigation, IEEE Trans. Syst. Man, Cybernet. B Cybernet. 33 (2003), 898–912. 
[195] S. Niu et al., A method of UAVs route optimization based on the structure of the highway network, Int. J. Distrib. Sens. Netw. 11 (2015), 1–7. 
[196] K. Nonami, Prospect and recent research & development for civil use autonomous unmanned aircraft as UAV and MAV, J. Syst. Des. Dyn. 1 (2007), 120–128. 
[197] J.L. Ny, E. Feron, and E. Frazzoli, On the Dubins traveling salesman problem, IEEE Trans. Autom. Control 57 (2012), 265–270. 
[198] K.J. Obermeyer, P. Oberlin, and S. Darbha, Samplingbased path planning for a visual reconnaissance unmanned air vehicle, J. Guid. Control Dyn. 35 (2012), 619–631. 
[199] H. Oh et al., Coordinated roadnetwork search route planning by a team of UAVs, Int. J. Syst. Sci. 45 (2014), 825–840. 
454 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 45 
[200] H. Oh et al., Cooperative mission and path planning for a team of UAVs, Handbook of Unmanned Aerial Vehicles, K.P. Valavanis and G.J. Vachtsevanos (eds.), Springer, Dordrecht, 2015, pp. 1509–1545. 
[201] C.A.S. Oliveira, Discrete optimization models for cooperative communication in ad hoc networks, Proceedings on Adhoc, Mobile, and Wireless Networks, Vol. 4104 of Lecture Notes in Computer Science, 2006, pp. 73–86. 
[202] A. Ortiz, D. Kingston, and C. Langbort, MultiUAV velocity and trajectory scheduling strategies for target classiﬁcation by a single human operator, J. Intell. Robot. Syst. 70 (2013), 255–274. 
[203] I. Oz, H.R. Topcuoglu, and M. Ermis, A metaheuristic based threedimensional path planning environment for unmanned aerial vehicles, Simul. Trans. Soc. Model. Simul. Int. 89 (2013), 903–920. 
[204] M. Pavone, Dynamic vehicle routing for robotic networks, Ph.D. thesis, Massachusetts Institute of Technology, 2010. 
[205] M. Pavone, E. Frazzoli, and F. Bullo, Adaptive and distributed algorithms for vehicle routing in a stochastic and dynamic environment, IEEE Trans. Autom. Control 56 (2011), 1259–1274. 
[206] M. Pavone, K. Savla, and E. Frazzoli, Sharing the load, IEEE Robot. Autom. Mag. 16 (2009), 52–61. 
[207] K. Peng, F. Lin, and B.M. Chen, Online schedule for autonomy of multiple unmanned aerial vehicles, Sci. China Inf. Sci. 60 (2017), 072203. 
[208] R. Pˇenicka et al., Dubins orienteering problem, IEEE Robot. Autom. Lett. 2 (2017), 1210–1217. 
[209] C.A. Persiani and S. Bagassi, Route planner for unmanned aerial system insertion in civil nonsegregated airspace, Proc. Inst. Mech. Eng. G J. Aerosp. Eng. 227 (2013), 687–702. 
[210] J.R. Peters and L.F. Bertuccelli, Robust task scheduling for multioperator supervisory control missions, J. Aerosp. Inf. Syst. 13 (2016), 393–406. 
[211] H. Pham et al., A survey on unmanned aerial vehicle collision avoidance systems, arXiv 1508.07723 (2017). 
[212] C. Phan and H.H.T. Liu, A cooperative UAV/UGV platform for wildﬁre detection and ﬁghting, 2008 Asia Simulation Conference - 7th International Conference on System Simulation and Scientiﬁc Computing, 2008, pp. 494–498. 
[213] D.H. Phan and J. Suzuki, Evolutionary multiobjective optimization for the pickup and delivery problem with time windows and demands, Mobile Netw. Appl. 21 (2016), 175–190. 
[214] M.D. Phung et al., Enhanced discrete particle swarm optimization path planning for UAV visionbased surface inspection, Autom. Constr. 81 (2017), 25–33. 
[215] S. Poikonen, X. Wang, and B. Golden, The vehicle routing problem with drones: Extended models and connections, Networks 70 (2017), 34–43. 
[216] L.D.P. Pugliese et al., Modelling the mobile target covering problem using ﬂying drones, Optim. Lett. 10 (2016), 1021–1052. 
[217] M. Quaritsch et al., Networked UAVs as aerial sensor network for disaster management applications, e & i Elektrotech. Inftech. 127 (2010), 56–63. 
[218] M. Raap et al., , J. Optim. Theory Appl. 172 (2017), 965–983. 
[219] M. Raap, M. Zsifkovits, and S. Pickl, Trajectory optimization under kinematical constraints for moving target search, Comput. Oper. Res. 88 (2017), 324–331. 
[220] M. Radmanesh and M. Kumar, Flight formation of UAVs in presence of moving obstacles using fastdynamic mixed integer linear programming, Aerosp. Sci. Technol. 50 (2016), 149–160. 
[221] S. Rashed and M. Soyturk, Analyzing the eﬀects of UAV mobility patterns on data collection in wireless sensor networks, Sensors 17 (2017), 413. 
[222] S. Rathinam and R. Sengupta, Lower and upper bounds for a multiple depot UAV routing problem, Proceedings of the 45th IEEE Conference on Decision and Control, 2006, pp. 5287–5292. 
[223] S. Rathinam, R. Sengupta, and S. Darbha, A resource allocation algorithm for multivehicle systems with nonholonomic constraints, IEEE Trans. Autom. Sci. Eng. 4 (2007), 98–104. 
[224] P. Rey, Paketzustellung per Drohne: DPDgroup startet den weltweit ersten Drohnenverkehr im Linienbetrieb (Package delivery with a drone: DPDgroup starts the ﬁrst regular drone service worldwide), Press Release DPDgroup (2016). 
[225] D. Richert and J. Cortes, Optimal leader allocation in UAV formation pairs ensuring cooperation, Automatica 49 (2013), 3189–3198. 
[226] U. Ritzinger, J. Puchinger, and R.F. Hartl, A survey on dynamic and stochastic vehicle routing problems, Int. J. Prod. Res. 54 (2016), 215–231. 
[227] V. Roberge, M. Tarbouchi, and G. Labonte, Comparison of parallel genetic algorithm and particle swarm optimization for realtime UAV path planning, IEEE Trans. Ind. Inf. 9 (2013), 132–141. 
[228] C. Robin and S. Lacroix, Multirobot target detection and tracking: Taxonomy and survey, Auton. Robots 40 (2016), 729–760. 
[229] S. Rohde, M. Putzke, and C. Wietfeld, Ad hoc selfhealing of OFDMA networks using UAVbased relays, Ad Hoc Netw. 11 (2013), 1893–1906. 
[230] RPAS Steering Group, Roadmap for the integration of civil remotelypiloted aircraft systems into the European aviation system, Final report from the European RPAS Steering Group, 2013. 
Aerial vehicle searchpath optimization:A novel method for emergency operations 
OTTO et al. 455 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
46 OTTO ET AL. 
[231] M. Russon, Nokia and EE trial mobile base stations ﬂoating on drones to revolutionise rural 4G coverage, International Business Times (2016). 
[232] J.C. Ryan et al., Comparing the performance of expert user heuristics and an integer linear program in aircraft carrier deck operations, IEEE Trans. Cybernet. 44 (2014), 761–773. 
[233] W. Saad et al., A selﬁsh approach to coalition formation among unmanned air vehicles in wireless networks, 2009 International Conference on Game Theory for Networks, 2009, pp. 259–267. 
[234] W. Saad et al., Hedonic coalition formation for distributed task allocation among wireless agents, IEEE Trans. Mobile Comput. 10 (2011), 1327–1344. 
[235] A. Saeed et al., On realistic target coverage by autonomous drones, arXiv 1702.03456 (2017). 
[236] M. Sahi, Drone delivery pilot projects show strong potential, Tractica (2016). 
[237] O.K. Sahingoz, Generation of Bezier curvebased ﬂyable trajectories for multiUAV systems with parallel genetic algorithm, J. Intell. Robot. Syst. 74 (2014), 499–511. 
[238] O.K. Sahingoz, Networking models in ﬂying adhoc networks (FANETs): Concepts and challenges, J. Intell. Robot. Syst. 74 (2014), 513–527. 
[239] S. Sanci and V. Isler, A parallel algorithm for UAV ﬂight route planning on GPU, Int. J. Parallel Program 39 (2011), 809–837. 
[240] A. Sanjab, W. Saad, and T. Bas¸ar, Prospect theory for enhanced cyberphysical security of drone delivery systems: A network interdiction game, arXiv 1702.04240 (2017). 
[241] A. Sathyan, N.D. Ernest, and K. Cohen, An eﬃcient genetic fuzzy approach to UAV swarm routing, Unmanned Syst. 4 (2016), 117–127. 
[242] K. Savla, E. Frazzoli, and F. Bullo, Traveling salesperson problems for the Dubins vehicle, IEEE Trans. Autom. Control 53 (2008), 1378–1391. 
[243] H. Savuran and M. Karakaya, Route optimization method for unmanned air vehicle launched from a carrier, Lecture Notes Softw. Eng. 3 (2015), 279–284. 
[244] H. Savuran and M. Karakaya, Eﬃcient route planning for an unmanned air vehicle deployed on a moving carrier, Soft Comput. 20 (2016), 2905–2920. 
[245] S. Schopferer and F.M. Adolf, Online replanning of timeeﬃcient ﬂight paths for unmanned rotorcraft, J. Intell. Robot. Syst. 84 (2016), 277–296. 
[246] F. Schroth, IBM granted patent for package transfer between drones, DroneLife (2017). 
[247] V. Sharma, M. Bennis, and R. Kumar, UAVassisted heterogeneous networks for capacity enhancement, IEEE Commun. Lett. 20 (2016), 1207–1210. 
[248] V. Sharma et al., Eﬃcient cooperative relaying in ﬂying ad hoc networks using fuzzybee colony optimization, J. Supercomput. 73 (2017), 3229–3259. 
[249] B. Siciliano and O. Khatib (eds.), Handbook of Robotics, Springer, Berlin, 2016. 
[250] D. Simmons, Rwanda begins Zipline commercial drone deliveries, BBC World News (2016). 
[251] T. Simonite, Meet Facebook’s stratospheric Internet drone, MIT Technology Review (2015). 
[252] T. Simonite, Project loon, MIT Technology Review (2015). 
[253] B.D. Song et al., Persistent UAV service: An improved scheduling formulation and prototypes of system components, J. Intell. Robot. Syst. 74 (2014), 221–232. 
[254] B.D. Song, J. Kim, and J.R. Morrison, Rolling horizon path planning of an autonomous system of UAVs for persistent cooperative service: MILP formulation and eﬃcient heuristics, J. Intell. Robot. Syst. 84 (2016), 241–258. 
[255] L.D. Stone, Theory of Optimal Search, Academic, New York, 1975. 
[256] P.B. Sujit and D. Ghose, Search using multiple UAVs with ﬂight time constraints, IEEE Trans. Aerosp. Electron. Syst. 40 (2004), 491–509. 
[257] P.B. Sujit and D. Ghose, Twoagent cooperative search using game models with endurancetime constraints, Eng. Optim. 42 (2010), 617–639. 
[258] P.B. Sujit, B.P. Hudzietz, and S. Saripalli, Route planning for angle constrained terrain mapping using an unmanned aerial vehicle, J. Intell. Robot. Syst. 69 (2013), 273–283. 
[259] P.B. Sujit, D.E. Lucani, and J.B. Sousa, Bridging cooperative sensing and route planning of autonomous vehicles, IEEE J. Sel. Areas Commun. 30 (2012), 912–922. 
[260] P.B. Sujit, D.E. Lucani, and J.B. Sousa, Joint route planning for UAV and sensor network for data retrieval, 2013 IEEE International Systems Conference (SysCon), 2013, pp. 688–692. 
[261] P.B. Sujit, S. Saripalli, and J.B. Sousa, Unmanned aerial vehicle path following: A survey and analysis of algorithms for ﬁxedwing unmanned aerial vehicless, IEEE Control Syst. 34 (2014), 42–59. 
[262] P.B. Sujit, J. Sousa, and F.L. Pereira, UAV and AUVs coordination for ocean exploration, OCEANS 2009-EUROPE, 2009, pp. 1–7. 
[263] K. Sundar et al., Routing unmanned vehicles in GPSdenied environments, 2017 International Conference on Unmanned Aircraft Systems (ICUAS), June 2017, pp. 62–71. 
456 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
OTTO ET AL. 47 
[264] K. Sundar and S. Rathinam, A primaldual heuristic for a heterogeneous unmanned vehicle path planning problem, Int. J. Adv. Robot. Syst. 10 (2013), 1–9. 
[265] K. Sundar and S. Rathinam, Algorithms for routing an unmanned aerial vehicle in the presence of refueling depots, IEEE Trans. Autom. Sci. Eng. 11 (2014), 287–294. 
[266] K. Sundar and S. Rathinam, Algorithms for heterogeneous, multiple depot, multiple unmanned vehicle path planning problems, J. Intell. Robot. Syst. 88 (2017), 513–526. 
[267] K. Sundar, S. Venkatachalam, and S.G. Manyam, Path planning for multiple heterogeneous unmanned vehicles with uncertain service times, arXiv 1702.07647 (2017). 
[268] K.A.O. Suzuki, P. Kemper Filho, and J.R. Morrison, Automatic battery replacement system for UAVs: Analysis and design, J. Intell. Robot. Syst. 65 (2012), 563–586. 
[269] K.A. Swieringa et al., Autonomous battery swapping system for smallscale helicopters, 2010 IEEE International Conference on Robotics and Automation, 2010, pp. 3335–3340. 
[270] Z. Tang and U. Özgüner, Motion planning for multitarget surveillance with mobile sensor agents, IEEE Trans. Robot. 21 (2005), 898–908. 
[271] M. Tavana et al., Drone shipping versus truck delivery in a crossdocking system with multiple ﬂeets and products, Expert Syst. Appl. 72 (2017), 93–107. 
[272] H.A.L. Thi, D.M. Nguyen, and T.P. Dinh, Globally solving a nonlinear UAV task assignment problem by stochastic and deterministic optimization approaches, Optim. Lett. 6 (2012), 315–329. 
[273] J. Tisdale, Z. Kim, and K. Hedrick, Autonomous UAV path planning and estimation, IEEE Robot. Autom. Mag. 16 (2009), 35–42. 
[274] P. Tokekar et al., Sensor planning for a symbiotic UAV and UGV system for precision agriculture, IEEE Trans. Robot. 32 (2016), 1498–1511. 
[275] T. Toksoz et al., Automated battery swap and recharge to enable persisten UAV missions, AIAA Infotech@ Aerospace Conference, 2011. 
[276] M. Torres et al., Coverage path planning with unmanned aerial vehicles for 3D terrain reconstruction,Expert Syst. Appl. 55 (2016), 441–451. 
[277] D. Tse and P. Viswanath, Fundamentals of Wireless Communication, Cambridge University Press, Cambridge, 2005. 
[278] C.M. Tseng et al., Flight tour planning with recharging optimization for batteryoperated autonomous drones, arXiv 1703.10049 (2017). 
[279] I.L. Turner, M.D. Harley, and C.D. Drummond, UAVs for coastal surveying, Coast. Eng. 114 (2016), 19–24. 
[280] M. Turpin et al., CAPT: Concurrent assignment and planning of trajectories for multiple robots, Int. J. Robot. Res. 33 (2014), 98–112. 
[281] M.W. Ulmer and B.W. Thomas, Sameday delivery with a heterogeneous ﬂeet of drones and vehicles, Tech. report, Technical University of Braunschweig, 2017. 
[282] K.P. Valavanis and G.J. Vachtsevanos (eds.), Handbook of Unmanned Aerial Vehicles, Springer, Dordrecht, 2015. 
[283] J. Valente et al., Aerial coverage optimization in precision agriculture management: usical harmony inspired approach, Comput. Electron. Agric. 99 (2013), 153–159. 
[284] P. van Blyenburgh, UAVs: An overview, Air Space Eur. 1 (1999), 43–47. 
[285] P. Vansteenwegen, W. Souﬀriau, and D.V. Oudheusden, The orienteering problem: A survey, Eur. J. Oper. Res. 209 (2011), 1–10. 
[286] S. Venkatachalam, K. Sundar, and S. Rathinam, Twostage stochastic programming model for routing multiple drones with fuel constraints, arXiv 1711.04936 (2017). 
[287] A. Viguria, I. Maza, and A. Ollero, Distributed servicebased cooperation in aerial/ground robot teams applied to ﬁre detection and extinguishing missions, Adv. Robot. 24 (2010), 1–23. 
[288] R.G. Vilar and H.S. Shin, Communicationaware task assignment for UAV cooperation in urban environments, IFAC Proceedings, 2013, vol. 46, pp. 352–359. 
[289] F. Wang and J. Liu, Networked wireless sensor data collection: Issues, challenges, and approaches, IEEE Commun. Surveys Tutor. 13 (2011), 673–687. 
[290] H. Wang, D. Huo, and B. Alidaee, Position unmanned aerial vehicles in the mobile ad hoc network, J. Intell. Robot. Syst. 74 (2014), 455–464. 
[291] X. Wang and D. Li, Coverage path planning for UAVs in unknown directional regions, Int. J. Wirel. Mobile Comput. 8 (2015), 285–293. 
[292] X. Wang, S. Poikonen, and B. Golden, The vehicle routing problem with drones: Several worstcase results, Optim. Lett. 11 (2017), 679–697. 
[293] Z. Wei et al., Scaling laws of unmanned aerial vehicle network with mobility pattern information, IEEE Commun. Lett. 21 (2017), 1389–1392. 
[294] T. Wen, Z. Zhang, and K.K.L. Wong, Multiobjective algorithm for blood supply via unmanned aerial vehicles to the wounded in an emergency situation, PloS One 11 (2016), e0155176. 
[295] B. Woods, A. Punnen, and T. Stephen, A linear time algorithm for the 3-neighbour travelling salesman problem on a Halin graph and extensions, Discrete Optim. 26 (2017), 163–182. 
[296] G. Wu et al., Coordinated planning of heterogeneous Earth observation resources, IEEE Trans. Syst. Man Cybernet. Syst. 46 (2016), 109–125. 
A m 
OTTO et al. 457 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
48 OTTO ET AL. 
[297] Q. Wu, Y. Zeng, and R. Zhang, Joint trajectory and communication design for multiUAV enabled wireless networks, IEEE Transactions on Wireless Communication, 17 (2018), 2109–2121. 
[298] Q. Wu, Y. Zeng, and R. Zhang, Joint trajectory and communication design for UAVenabled multiple access, arXiv 1704.01765 (2017). 
[299] X. Zhang et al., A memetic algorithm for path planning of curvatureconstrained UAVs performing surveillance of multiple ground targets, Chin. J. Aeronaut. 27 (2014), 622–633. 
[300] A. Xu, C. Viriyasuthee, and I. Rekleitis, Eﬃcient complete coverage of a known arbitrary environment with applications to aerial operations, Auton. Robots 36 (2014), 365–381. 
[301] J. Xu, G. Solmaz, R. Rahmatizadeh, D. Turgut, and L. Boloni, Internet of things applications: Animal monitoring with unmanned aerial vehicle, arXiv 1610.05287 (2017). 
[302] J. Xu, Y. Zeng, and R. Zhang, UAVenabled wireless power transfer: Trajectory design and energy region characterization, arXiv 1706.07010 (2017). 
[303] S. Xu, K. , and H. Hmam, Distributed pseudolinear estimation and UAV path optimization for 3D AOA target tracking, Signal Process. 133 (2017), 64–78. 
[304] S. Yadlapalli et al., A Lagrangianbased algorithm for a multiple depot, multiple traveling salesmen problem, Nonlinear Anal. Real World Appl. 10 (2009), 1990–1999. 
[305] Yamaha Motor Corporation, Frequently asked questions. Yamaha RMAX remotely piloted helicopter, Company Website, 2017. 
[306] K. Yang, S.K. Gan, and S. Sukkarieh, An eﬃcient path planning and control algorithm for RUAV’s in unknown and cluttered environments, J. Intell. Robot. Syst. 57 (2010), 101–122. 
[307] L. Yang et al., A literature review of UAV 3D path planning, 11th World Congress on Intelligent Control and Automation (WCICA), 2014, pp. 2376–2381. 
[308] Y. Yang, A. Minai, and M. Polycarpou, Decentralized cooperative search in UAV’s using opportunistic learning, AIAA Guidance, Navigation, Control Conference Exhibit (2002). 
[309] P. Yao, H. Wang, and H. Ji, Gaussian mixture model and receding horizon control for multiple UAV search in complex environment, Nonlinear Dyn. 88 (2017), 903–919. 
[310] J. Yick, B. Mukherjee, and D. Ghosal, Wireless sensor network survey, Comput. Netw. 52 (2008), 2292–2330. 
[311] M. Younis, and K. Akkaya, Strategies and techniques for node placement in wireless sensor networks: A survey, Ad Hoc Netw. 6 (2008), 621–655. 
[312] K. Yu, A.K. Budhiraja, and P. Tokekar, Algorithms for routing of unmanned aerial vehicles with mobile recharging stations, arXiv 1704.00079 (2017). 
[313] Y. Zeng and R. Zhang, Energyeﬃcient UAV communication with trajectory optimization, IEEE Trans. Wirel. Commun. 16 (2017), 3747–3760. 
[314] Y. Zeng, R. Zhang, and T.J. Lim, Throughput maximization for UAVenabled mobile relaying systems, IEEE Trans. Commun. 64 (2016), 4983–4996. 
[315] Y. Zeng, R. Zhang, and T.J. Lim, Wireless communications with unmanned aerial vehicles: Opportunities and challenges, IEEE Commun. Mag. 54 (2016), 36–42. 
[316] P. Zhan, K. Yu, and A.L. Swindlehurst, Wireless relay communications with unmanned aerial vehicles: Performance and optimization, IEEE Trans. Aerosp. Electron. Syst. 47 (2011), 2068–2085. 
[317] C. Zhang and J.M. Kovacs, The application of small unmanned aerial systems for precision agriculture: A review, Precis. Agric. 13 (2012), 693–712. 
[318] C. Zhang, Y. Zeng, and R. Zhang, Energyeﬃcient data collection in UAV enabled wireless sensor network, arXiv 1708.00221 (2017). 
[319] J. Zhang et al., A spacetime networkbased modeling framework for dynamic unmanned aerial vehicle routing in traﬃc incident monitoring applications, Sensors 15 (2015), 13874–13898. 
[320] X. Zhang, J. Chen, and B. Xin, Path planning for unmanned aerial vehicles in surveillance tasks under wind ﬁelds, J. Central South Univ. 21 (2014), 3079–3091. 
[321] D. Zorbas et al., Optimal drone placement and costeﬃcient target coverage, J. Netw. Comput. Appl. 75 (2016), 16–31. 
How to cite this article: Otto A, Agatz N, Campbell J, Golden B, Pesch E. Optimization approaches for civil applications of unmanned aerial vehicles (UAVs) or aerial drones: A survey. Networks. 2018;00:1–48. https://doi.org/10.1002/net.21818 How to cite this article: Otto A, Agatz N, Campbell J, Golden B, Pesch E. Optimization approaches for civil applications of  unmanned aerial vehicles (UAVs) or aerial drones: A survey. Networks. 2018;72:411–458. https://doi.org/10.1002/net.21818 
Dogˇançay 
458 OTTO et al. 
 10970037, 2018, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.21818 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
",Optimizationapproachesforcivilapplicationsofunmannedaerialvehicles(UAVs)oraerialdrones Asurvey.pdf,5
"This paper describes the design of interfaces for presenting video summaries from drone swarms in the context of Search and Rescue (SAR). The use of drone swarms for SAR missions is a current topic of research, that is motivated by enabling faster search of people in distress in larger areas. Enabling this requires utilising the video feeds that each drone in the swarm can capture. This topic has been a focus of the HERD project, which the research presented in this paper is also a part of. However, previous studies conducted with the Danish Emergency Agency Management (DEMA) has revealed that viewing multiple live video feeds simultaneously for prolonged periods of time with the attentiveness necessary is impossible. Therefore, this paper looks at the concept of utilising Computer Vision to detect objects of interest from the video feeds. These detections can be presented in a summary of detections, allowing for the SAR personnel to examine the potentially important parts of the recorded footage at their own pace, thus reducing the risk of missing any essential details. Therefore, the purpose of this paper is to examine ways of designing such a summary of drone swarm detections while ensuring that the users avoid experiencing the situation awareness demons, information overload and attentional tunneling. This was done by looking into related work which describes ways of presenting video summaries, the way humans interpret images, and ways of combatting situational awareness demons. Based on this related work we constructed a set of design principles that guided the design of a number of mockups. The mockups were shown to DEMA’s head of drone operations in Jutland, Denmark and a professional drone system developer from Robotto. They provided valid feedback that alongside the aforementioned design principles guided the design and development of two functional prototypes. The prototypes both presented a summary of AI detections from a drone swarm as an interactive storyboard, and allowed for usercontrolled filtering of the summary with the purpose of minimizing information overload for the users. The prototypes presented the filtering functionality in two different ways, and this constituted the independent variable in a subsequent online user study. This user study had 8 participants, consisting of drone operators, drone system developers and students who had worked with interfaces for drone swarm systems. During the study the participants performed a task using each prototype, where the given scenario was that a SAR mission where a person had gone missing was ongoing. The goal of the task was then to identify a number of detections that showed the missing person and their personal belongings. The results of the study showed that presenting a summary in the form a story board was effective as when using one of the prototypes the participants managed to correctly identify 4.75 detections out of a possible 6 within just 5 minutes, while only marking 1 detection incorrectly. Furthermore, the participants commented that the filtering functionality aided them in avoiding information overload by limiting the detections presented in the summary to a manageable subset. However, attentional tunneling remained a challenge for participants which calls for further research into this topic. 
ANDREAS DAUGBJERG CHRISTENSEN, Department of Computer Science, Aalborg University, Denmark SHPEND GJELA, Department of Computer Science, Aalborg University, Denmark 
Drones are currently being used in Search and Rescue (SAR) missions enabling ground operators to scan a large area for people in 
distress by utilising the drone’s video feed. Research has begun looking towards drone swarms which would enable the search of 
an area faster compared to using a single drone. Many aspects of the SAR mission will be automated including path planning, and 
detecting objects of interest such as people in distress. Using AI to identify possible targets will be essential as it is not possible to have 
an operator observe live video feeds from multiple drones over a prolonged period of time with the necessary attentiveness. Therefore, 
the video feeds must be summarized with the possible objects of interest being highlighted through augmentation or annotations to 
help the operator quickly understand the video feeds recorded by the drone swarm. In this paper we examine how summarization of 
video feeds from a drone swarm can be used to aid SAR operators during missions. Furthermore, the focus is on designing a user 
interface that among other features includes filtering functionality that intends to combat the situational awareness (SA) demons, 
information overload and attentional tunnelling. To do this, we look into theories on how humans interpret and understand images, 
and how to apply these theories in the context of drone swarms used for SAR missions. These theories revolve around making it easy 
to identify the key elements in an image and do so quickly. In the project several user interface designs for highlighting the findings 
of each individual drone and summarizing the findings of the drone swarm as a whole are explored. Some of these designs were 
presented to the Danish Emergency Management Agency’s (DEMA) Head of Drone operations in the Danish region of Jutland, and a 
developer from Robotto, who work with integrating drones and AI in the context of SAR. Based on their feedback, two prototypes 
were developed that present a summary of a drone swarm’s detections as keyframes in a story board while enabling filtering of the 
summary based on time and the category of the detected objects. The prototypes were used to conduct an online study with 8 drone 
operators, drone system developers, and university students as participants. During the study participants were instructed to complete 
one task using each prototype with an ongoing SAR mission given as the imagined scenario. The results showed that presenting a 
summary as keyframes in a storyboard allowed for participants to correctly identify detections of interest with a significant degree 
of success. On average the participants correctly marked 4.75 detections out of a possible 6 when using one of the prototypes. The 
participants also stated that filtering was useful for avoiding information overload. 
1 INTRODUCTION 
Using drone swarms for SAR is becoming increasingly popular for both researchers and real life emergency agencies. 
Having multiple drones overfly an area of interest means that an area can be searched quicker than what would be 
possible with traditional methods, such as searching the area on foot or using a single drone. Though, having multiple 
drones that are all providing live video feeds to be observed by the SAR personnel leads to another challenge, that is 
both maintaining an overview of all the incoming stimulus, and noticing any details of importance. The extent of the 
challenge increases, as the size of the drone swarm increases. Utilising Computer Vision is therefore a potential way 
of complementing the rescuers’ ability to discover objects of interest and take full advantage of a large drone swarm. 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 
© 2023 Association for Computing Machinery. Manuscript submitted to ACM 
1 
Having artificial intelligence (AI) detect elements of interest in the video feeds from the drones can help the human 
operators identify and become aware of important details, which might otherwise be missed. However, the operator 
might still experience information overload if the system in a short amount of time presents multiple notifications 
alerting to detections made, which again becomes increasingly probable as the number of drones increases. This 
motivates the option of storing the detections and providing a summary of the recorded video feeds, such that the 
operator can go through these at their own pace without becoming overwhelmed. 
In this paper we develop two working prototypes that summarize the video feeds of a drone swarm with the purpose of 
providing the SAR drone operator with an overview of the recorded feeds, and alerting them to elements of potential 
interest. Among other features we focus on researching how filtering functionality can possibly combat the challenge 
of information overload while also avoiding attentional tunneling. This research is done as part of the HERD project [7] 
in collaboration with the Danish Emergency Management Agency who provide relevant domain knowledge about 
the use of drones for SAR missions, and Robotto [20] which is a company developing solutions that use drones and 
Computer Vision to aid in SAR missions. 
Our contributions are: 
• Design principles relevant for visually presenting an interactive video summary from a drone swarm in the 
context of SAR missions. 
• Two working prototypes that present an interactive video summary from a drone swarm, with implemented 
functionality that allows for filtering the summary on time and the category of the detected objects. 
• Exploration of ways for enhancing visual elements to make it easier for an operator to understand the message 
being conveyed by artificial intelligence that is detecting entities of interest such as people, clothing, vehicles etc. 
2 RELATED WORK 
This section will highlight relevant research that can help stimulate and drive development for summarizing multiple 
video feeds in a suitable manner for SAR missions. We focus on describing the parts of the research that is concerned 
with presenting video summaries, and not on algorithms for extracting key parts that tell the story. Furthermore, focus 
will be put on theories that attempt to explain how humans process information and how they make sense of what they 
are seeing. These theories can be transferred to the ways humans interpret visual stimuli, and give a set of tools that 
can improve the method of conveying messages in visual communication. 
2.1 Presenting video summaries 
Research has examined ways of summarizing video feeds to provide a quick overview of the video content. Different 
approaches have been investigated. 
In [14] they examine the methodology of video synopsis, to efficiently compress and store video footage from drones 
for later analysis. They develop a system that can detect any abnormal objects in a video, such as a person holding a 
gun. These objects are then extracted, brought to the foreground, and stitched together in a very condensed video. For 
instance, they condense videos with a length of 1-9 minutes down to 0.34-2.2 seconds. However, they do not perform 
any user study to see if it supports an observer in gaining a quick overview and understanding of the video. 
In [16] Mei et al. they develop multiple algorithms that summarize a video by extracting keyframes that accurately 
represent the content of the video. The performance of the algorithms is measured and compared to similar algorithms. 
2 
The algorithms’ keyframe summaries are compared to humanselected keyframe summaries which act as the ground- 
truth in their test. With Fscores ranging from 48.2% to 58.5% the algorithms presented in the paper all perform better 
than the other stateoftheart algorithms. However, the paper does not describe any specific method for displaying the 
keyframes in a comprehensive manner, nor do they perform any user study. 
In [17] keyframes are also extracted, however, they further identify regions of interest (ROIs) within each keyframe 
which are used to construct a collage summarizing the content of a video. They conduct a user study where they 
compare their system, Video Collage, to other video representation tools. The study showed that their way of presenting 
collages consisting of arbitrarily shaped ROIs was the most visually pleasing representation. 
Girgensohn et al. present two approaches to presenting video feed summaries from a set of stationary surveillance 
cameras [10]. They implement a timeline user interface that shows the chronological order of events detected by 
the system, with keyframes from the video feeds being attached to the timeline to represent the detected events. 
Furthermore, they implement a storyboard user interface which is presented as a collection of keyframes that show 
detected events over time. Both interfaces allow for further investigation of an event by enabling video playback. They 
do not perform any user study to evaluate the user interfaces in the paper. 
In [9] they also propose event boards which is a collection of detected events in temporal order similar to timelines 
and storyboards, as ways of presenting video summaries from multiple video feeds. They perform a user study, however, 
it is mostly focused on evaluating the quality of their video summarization technique and not the presentation of it. 
Therefore, they do not disclose how the participants were presented to the video summaries, and what their opinion on 
the presentation itself was. Though, their results were promising as they showed that the participants on average found 
the video summaries up to 89% as informative as the original video. 
A system, named CatchLive, which in realtime summarizes live streams by dividing it into sections, and allows for 
exploring the highlights from those sections in various levels of detail is presented in [26]. The highlights consist of 
a snapshot, transcription from the video clip being presented as well as highlights from the chat during that part of 
the stream. Through interviews with frequent viewers of nonsummarized live streams they initially gather feedback 
regarding the challenges that they are faced with when joining a livestream after it has begun. These include trying 
to gain an understanding of the previous parts of the stream, while not missing out on information in the current 
live stream. They perform a user study with three groups of 16-18 participants each having to test the system by 
watching a live stream about either stocks, cooking or gaming and answering interview questions afterward. Through 
the interviews they found that the participants experienced that, a timeline helped them get an overview of the live 
stream, highlights helped them identify important moments in the stream, and the timeline and highlights combined 
helped them catch up to the current stream with less interruption compared to rewinding. Though they did find that 
more information is needed to fully understand the previous parts of the live stream, and another user study showed no 
significant difference in the understanding of the stream for viewers having the summaries provided by CatchLive, 
compared to another group of viewers that did not. 
The generation of textual descriptions that summarize the content of a video is presented in [27, 28]. This allows 
for a textual description to be associated with a keyframe or short clip in a longer video, which can aid the user in 
understanding that part in particular as well as the video in its entirety. 
In [23] they implement the interactive user interface, VideoForest, that presents a video summary using a treelike 
structure with each scene in the video being presented as a branch in the tree. Their interface includes a timeline, and 
the integration of comments made by previous viewers of the video. The sentiment of these comments is also shown in 
the interface, to guide users in deciding if they should explore a specific scene further. They conduct an interview with 
3 
13 expert participants to evaluate their interface. They find that the features of the interface are helpful for highlighting 
the eyecatching and evocative parts of the video, and understand the opinions of prior audiences regarding specific 
scenes. 
2.2 Improving visual communication by enhancement 
As described above, multiple videos can be summarized through different visual representations. Furthermore, research 
has also been done in relation to how visual representations can be further enhanced to help convey a message such as 
ensuring the user is made aware of the important key elements. This can also be seen as improving the summarization 
of the visual content, which quickly allows the viewer to identify what is of note. To understand what theories and 
tools could be useful for highlighting elements, it is necessary to investigate how humans interpret images. Research 
that examines this issue is presented in the following. 
2.2.1 Human interpretation of images. 
There are different theories relating to humans interpretation of images. These include Gestalt Laws of Perception, 
Semiotics Theory, and Cognitive Load Theory. Several papers have explored their use and how effective they are. This 
will be further examined in the remainder of the section. 
Gestalt Laws of Perception. The Gestalt Laws of Perception were developed from the theory of Gestalt Psychology. 
There are several laws, also sometimes referred to as principles, that can be applied to the design of graphical interfaces 
to make them easily understood and pleasing to look at. In [24] they create a study to compare two of the Gestalt Laws, 
namely proximity and similarity to see which approach is strongest when it comes to how people consider objects to be 
part of the same group. In their paper they define similarity as having either the same color or shape. The result of their 
study reveals that there is a slight favor for a combination of both color and shape against just applying proximity. 
In [19] they talk about how the effectiveness of visual communication design can be further improved to ensure 
the message of a design is properly conveyed to the observer. One of the primary challenges they present are keeping 
designs aesthetically pleasing but also functionallylegible to ensure the intended message is understood. Color and 
contrast are two important tools at the disposal of designers for such a task, and they explore how color and contrast in 
conjunction with the Gestalt Laws can be used to improve the effectiveness of visual communication design. Through 
the paper, examples are given for several of the Gestalt Laws that emphasizes the usefulness of utilising color and 
contrast when working with visual communication design. 
Visual Semiotics. The theory of semiotics is about how meaning is created and gets communicated and finally 
interpreted by humans, based on a set of codes that they each individually hold. The codes that each individual applies 
is based on previous experiences and the context which the message that they are trying to decipher appears in. One of 
the core fundamentals is the sign, that is anything used to convey or communicate a meaning. The sign is thus said to 
be comprised of the signifier, that is the media or material used to signify a concept referred to as the signified. Finally 
denotation is the literal meaning of a sign with connotation being the implicit meaning [4, 5, 22]. Visual Semiotics is a 
subset that focuses more specifically on how images can be used to communicate a message [13]. 
In [25] they conducted a study in order to investigate how the theory of semiotics can affect poster designs for better 
or worse. This was achieved by forming two groups, an experimental group and a control group. The experimental 
group was taught the principles of semiotics and would apply these to their designs. In the end the result exhibited a 
slight favor towards the experimental group on topics like creativity, aesthetics, typography and overall score. This 
concludes that the semiotics theory helps strengthen the overall visual tension in images, in this case posters. The 
4 
paper also touches on how a designer ensures the message comes across clearly to the observer by playing around with 
icons and symbolism. Thus messages can be embedded by combining icons and symbolism, like elements being given 
different textual patterns than their natural occurring pattern. Furthermore, visual marks when designing graphics are 
important for communicating the intended message - including texts, pictures, images, colors and textures [25, p. 9]. 
Cognitive Load Theory. This theory relates to the way which humans process information, and the limitations 
that humans experience during this process. When processing information or performing some task that increases 
the cognitive load, the working memory is used to maintain an understanding of all the information being processed. 
However, the working memory’s capacity is limited and if this limitation is exceeded it results in a deterioration 
of learning and limited understanding of the information [12]. Therefore, this theory is also relevant in the field of 
humancomputer interaction and when presenting information to a user [12]. In [15] they propose multiple ways of 
reducing the user’s cognitive load in multimedia learning. For instance, they suggest taking advantage of both the 
visual and verbal channels as humans can more easily process information simultaneously if it is received through both 
visual elements and audio. Other ways, of reducing cognitive load when it is at capacity is segmenting and pretraining. 
Segmenting covers the concept of splitting the presentation of information into multiple segments and allowing for 
time in between each segment, which enables the user to focus on one task at a time. Pretraining is the idea of providing 
instructions and training the user prior to them using a system, as this may reduce cognitive load once they use the 
system. To limit the risk of exceeding the user’s cognitive capacity the authors also propose weeding and signaling. 
Weeding means that there is an emphasis on only displaying essential information, while signalling is a tool that can 
be used to highlight what is important while still having less essential information presented, should that become 
necessary. 
2.3 Situational awareness in multidrone systems 
Maintaining Situational awareness (SA) is essential for the operator of a multidrone system in the context of SAR [2]. 
Having a high level of SA makes it possible for the operator to make better decisions in an everchanging SAR mission. 
Situational awareness is a term that describes one’s awareness of the information that is necessary for completing the 
task at hand. The definition of SA is split into three levels [8], with level 1 SA being the perception of each relevant 
element in the environment. This can be limited if the system does not present all the necessary information or if 
the information is presented in such a manner that makes it difficult for the user to understand it. Level 2 SA is the 
comprehension of how the level 1 elements relate to the task goal in combination with each other, while level 3 SA is the 
projection of the elements’ future status. 
Designing a system that increases SA can be difficult and certain challenges, known as SA demons, often arise in the 
development of systems where maintaining the user’s SA is critical. Some of these demons include, attentional tunneling, 
requisite memory trap, information overload, and outoftheloop syndrome [8]. In [8] they propose a set of design 
principles that can be used to address the SA demons. For instance, information overload can be avoided by allowing 
the user to perform information filtering thus deciding on the content that they want visible. This is reiterated in [21], 
where Overview first, zoom and filter, then detailsondemand is described as a golden rule for designing comprehensible, 
predictable, and controllable interfaces. 
In [2] they collaborate with emergency responders to codesign a multidrone system for SAR that aims to address 
the SA demons described in [8], including three additional demons that they have identified through their previous 
work with Unmanned Aerial Vehicles (UAVs). By getting feedback from the emergency responders they make key 
observations regarding the SA demons and how to combat them. In regards to attentional tunneling for instance, they 
5 
learn that the emergency responders want the ability to focus on a particular detection when the system deems it 
necessary. In that case they do not see attentional tunneling as a demon but rather a necessity. However, maintaining 
an overview of the other drones is still important. Another SA demon that they seek to address is information overload. 
In regards to that, the emergency responders did not experience information overload when using the system designed 
in the study, however, some elements were distracting which made it clear that only the relevant information in the 
specific context should always be visible. Additional information should be available on user request. In their work, 
they conduct 6 codesign sessions to elicit feedback, but they do not test their system in a reallife environment which 
could have elicited different responses regarding information overload for instance, as that could more easily occur in a 
stressful situation. 
3 PREREQUISITE KNOWLEDGE REGARDING DEMA 
As our research is focused on drone swarm systems within the context of SAR, and is done in collaboration with DEMA 
we will describe their current approach to the use of drones during SAR missions. This is to get a better understanding 
of how each element of a drone swarm system, such as a video feed summary, could be integrated into their workflow. 
The current workflow at DEMA when conducting SAR missions involves a drone operator, an observer and a single 
DJI drone. An operator will manually take control and fly the drone or alternatively use the builtin functionality of the 
DJI application to program a flight path for the drone. Meanwhile the observer is monitoring a delayed video feed from 
the drone on a large monitor which is mounted to the back of their operational van. 
Multiple studies in the HERD project have already explored the use of drone swarms for SAR operations [3, 11], 
such as how to incorporate live video feeds from numerous drones at once and how to display this to the observer [6]. 
This research has resulted in the HERD system which is currently still in development. This HERD system relies on 
using a tablet for controlling the drone swarm, an application running on each of the drones, and finally a server for 
transmitting data between the tablet and drones. An illustration of the HERD system setup can be seen in Figure 1. The 
research done in this paper will be done with the purpose of highlighting features needed when integrating a video 
feed summary system into the HERD system. 
Fig. 1. The current setup of the HERD system with a single operator using the tablet to give instructions and monitor multiple drones in the air at once. The observer is positioned behind the van monitoring the video feeds of the drone swarm on a large screen. 
6 
During these studies certain issues and concerns were revealed. For instance, they would often operate in cold 
weather which would affect their ability to use their fingers to touch a screen for prolonged periods of time. Furthermore, 
user interfaces with relatively small elements were difficult to interact with due to having large hands, meaning they 
were concerned about cluttering the screen, thus resulting in little screen space for each element. It also became apparent 
that the operators have varying levels of expertise when it comes to computers, therefore, systems must be simple, 
easy to understand and straight forward to use. Finally interviews with DEMA revealed that they were anxious about 
giving up control and relying entirely on an artificial intelligent driven drone swarm system. They would rather have a 
system that supports them during their missions, by using Computer Vision for detecting entities of interest while 
always keeping the human operators in the loop. The research presented in this paper will take into account what has 
previously been highlighted during interactions with DEMA when developing appropriate mockups and prototypes. 
4 RESEARCH PROBLEM 
The prerequisite knowledge obtained from previous studies with DEMA regarding the use of drone swarms for SAR 
missions has highlighted a number of challenges and opportunities within this context. 
Interviews conducted with DEMA previously have concluded that observing more than four video feeds simultaneously 
with the care required for SAR missions would not be possible [6]. Therefore, the system must provide the observer 
with assistance that aids the observer with noticing the elements of interest. Such assistance could be provided by AI 
that utilises Computer Vision to detect and notify the user of noteworthy detections made in the video feeds. However, 
having a large drone swarm could then potentially result in many simultaneous notifications which may overwhelm 
the observer. Therefore, it could be useful to summarize all the noteworthy detections such that the observer can get a 
high level overview of the detections and examine each detection in detail when need be. 
Furthermore, operators from DEMA have expressed that observing something in the video feeds that may be of interest 
requires them to alter the flight of the drone to take a second look. This could be avoided if the system automatically 
stored that potentially important part of the video feed, such that the observer could take a second look while allowing 
the drone swarm to continue on its designated path. 
By looking into the related work it was apparent that summarization of video feeds within different contexts is a 
topic of interest to researchers. It was evident that many different techniques for presenting summaries have been 
proposed in the literature. Some of these include storyboards, timelines [9, 10], textual descriptions [28], transcriptions 
[26], condensed videos [14], collages [17], and keyframes [16]. 
Additionally, it became apparent that effectively communicating what is noteworthy through visual content is an 
important aspect of summarizing video feeds. Looking into related work on this topic revealed that different theories 
regarding human interpretation of images can be applied to communicate messages in visual content effectively. The 
theories include the Gestalt Laws of perception, Visual Semiotics theory, and Cognitive Load theory. 
However, there is a lack of work where the two topics, video summarization and communicating through visual content, 
are combined to create and evaluate different ways of presenting video summaries through user studies. Especially, in 
relation to presenting video summaries from drone swarms in the context of SAR missions. 
Finally, the work presented in [2] showed that increasing the user’s situational awareness is essential when designing 
a multidrone system for SAR. They present and address a wide variety of SA demons, providing an overview of the 
challenges that may arise in the development of such a system. Due to their broad approach however, they leave room 
for researching how to address specific SA demons such as attentional tunnelling and information overload, more 
thoroughly. Those SA demons are of particular interest as a summarization system could potentially combat them by 
7 
allowing for both maintaining an overview of the drone swarms’ findings and each individual drone’s findings in more 
details if necessary. 
Therefore, we ask the question: How can we design and develop a system for summarizing video feeds from 
drone swarms while addressing the SA demons, attentional tunnelling and information overload, in order 
to support personnel that are conducting SAR missions? 
5 DESIGN PRINCIPLES 
The following section proposes a set of design principles that will help guide the design choices made for the proceeding 
mockups and subsequent working prototypes. A subset of the principles will originate from previous studies involving 
DEMA as domain experts. The remaining principles are either inspired by or directly derived from the related work on 
how summaries can be appropriately presented, how messages can be efficiently conveyed visually, and how the SA 
demons, information overload and attentional tunneling can be avoided. 
Previous work along DEMA highlighted several interesting points worth taking into consideration. These include: 
(1) Minimal interaction with the handheld tablet due to weather conditions affecting the operators ability to 
use the tablet for an extended period of time. The path to obtain information should be kept at a minimal, and 
information should be clear to avoid excessive browsing. 
(2) Large components as the screen real estate on the tablet is small so enlarging elements to increase readability 
and to also allow for easier interaction is crucial. 
To expand the list of principles further, the related work section highlights numerous valid points that are converted 
into design principles and listed below. 
(3) Segmenting is useful to reduce the cognitive load by splitting the information into separate phases that are 
presented at different times as mentioned in [15]. 
(4) Weeding & Signaling are principles about removing irrelevant information, or highlighting certain elements 
which require additional attention [15]. Their purpose is to make the content less overwhelming to the user by 
reducing their cognitive load. 
(5) Grouping items can as seen in [24] be further strengthened by applying the Gestalt Law of Similarity using a 
combination of both colors and shapes. This will be particularly useful for constructing visual elements that 
must be understood as belonging to the same group, or at least be related to each other in one way or another. 
(6) Use of contrast to alter hue and saturation can attract attention to designated regions of an image as explained 
in [19]. Attracting the observers attention to the findings on images will decrease the time spent decoding and 
allow for faster browsing through the summaries but also retain the context of the environment surrounding the 
findings. 
(7) Utilise colors to help separate elements from the background and stand out more clearly as described in 
the Gestalt Law of Figure/Ground. Furthermore, proper use of colors can enhance the Gestalt Law of Good 
Continuation as explained in [19]. Good continuation will help emphasize and highlight relationships between 
elements such as a reading direction e.g. 
(8) Global overview should be provided to ensure that the operator’s attention is not tunneled towards a subset 
of information, while other information that may be of higher importance is not attended to. This allows the 
operator to have a complete highlevel understanding of the situation and make the optimal decision [8]. 
8 
(9) Drill down in the summary for finer level of granularity as seen in [23] with a primary window supplying an 
overview and a side panel that allows for exploring a specific section in greater detail. The primary window can 
consist of a timeline with highlights that helps the user understand what has occurred previously [26]. 
(10) Reduce display density, but don’t sacrifice coherence is a principle that aims to reduce information overload 
by spreading out the information on the screen, such that it is easier to process for the user. However, you should 
be careful with spreading the information so much that it is presented on different pages, as this can reduce 
coherence in the system [8]. 
(11) Represent information timelines is a useful technique for increasing the user’s SA, as it provides data 
regarding the recording time of the presented information which in turn may influence the user’s decision 
making [8]. 
6 INITIAL INTERVIEWS 
Two initial interviews were conducted with domain experts to give valuable insight and feedback. Prior to these 
meetings, a set of mockups were designed based on the design principles described in Section 4. These mockups were 
designed to be an integrated part of the aforementioned HERD system. The first meeting was conducted with Robotto 
on the 9th of March, 2023. The second meeting was held on the 14th of March, 2023, and representatives from both 
Robotto, University of Southern Denmark (SDU), Aalborg University (AAU) were present as well as DEMA’s Head of 
Drone Operations in Jutland. 
Fig. 2. Topleft: The summary presented as a timeline that is overlayed on top of the interactive map. Topright: A preview of the detection is visible after selecting it on the timeline. Bottomleft: The preview is enlarged and the corresponding video recording can be played from the tablet or sent straight to the large monitor. Bottomright: An enhanced keyframe with a detection of a person. The saturation of the surrounding environment has been altered and a bounding box encapsulates the detection. 
9 
At both meetings the mockups were presented and used as a basis for obtaining knowledge about what is required 
of a system that summarizes the findings of a drone swarm during a SAR mission. Some of the mockups which 
were presented can be seen in Figure 2. During the first meeting the set of mockups were printed and handed to a 
developer from Robotto, while at the second meeting the mockups were displayed with the use of a projector to allow 
all participants to follow along. 
Robotto expressed concerns about showing images in their entirety, as the operators might be using small display 
screens like handheld controllers or tablets. It is therefore beneficial to crop the images and leave the detection in focus. 
Robotto were also fond of the general concept of summarizing and presenting detections to the user and thought it was 
worth exploring as a potential feature. The representative from Robotto also held a demonstration of their own system 
that lacks any builtin summarization, but instead saved the detections to a folder for post analysis. 
In the second meeting it was primarily the representative from DEMA who provided feedback. He was quick to 
dismiss the need for information about which drone had made a specific detection. He was much more interested in 
what was detected, together with when and where a detection was made. Furthermore, timestamps showing either 
the time of detection or the time since the detection is preferred, and a timeline would be suitable for presenting this. 
Once again, DEMA also emphasized the importance of keeping the interface simple so that anyone can operate it with 
minimal technical expertise. A total of 12 different ways of enhancing the keyframes by changing HUE, saturation, 
blurring and colored borders was presented. DEMA was unable to say which of the 12 is the most suitable approach but 
suggested keeping multiple options as each individual will interpret them differently and for that reason it is best to 
provide them with numerous options. 
7 PROTOTYPE: INTERACTIVE STORYBOARD FOR SUMMARIZATION 
The feedback received from presenting the mockups in the initial interviews was processed and used to proceed with 
designing and implementing two prototypes. Additionally, the prerequisite knowledge previously obtained from DEMA 
as part of the HERD project, and the design principles which were derived from the related work were used to shape 
the design of the prototypes. This section will describe the design and implementation of the prototypes, and the design 
principles applied to the prototypes. Furthermore, it will describe how each of the prototypes are meant to facilitate a 
study that examines the user’s experience of information overload or attentional tunneling when using a system that 
summarizes detections from a drone swarm. 
7.1 Prototype design 
As mentioned two prototypes were developed and both prototypes have large similarities in their designs, with the 
major differences being the way that functionality for filtering the summary is presented. Therefore, the design choices 
behind the elements that they have in common will be described first, and the differences between them second. The 
two prototypes can be seen in Figure 3. 
A major design change that was decided upon for both prototypes which differed from the mockups presented during 
the initial interviews, was to present the summary as a storyboard containing a collection of keyframes rather than a 
timeline representing each detection as an icon. The reasons for this include DEMA expressing that showing information 
about which drone in the drone swarm had made a detection, as was the case in the mockups, was unnecessary. This 
provided the opportunity to adhere to design principle 4, weeding & signalling, as the nonessential information was 
removed. Furthermore, the prototype design presents more information by showing up to six keyframes from separate 
detections without any user interaction, which follows design principle 1, minimal interaction. Comparatively, the 
10 
mockup design required the user to click each detection on the timeline for a keyframe from that detection to be 
presented. 
(a) Prototype A: Anchored Filtering. 
(b) Prototype B: Hidden Filtering. 
Fig. 3. (a) shows the summary in Prototype A with the filtering functionality being available through the interactive icons and timeline in the bottom. (b) shows the filtering functionality page opened on Prototype B, with the summary visible behind it. 
7.1.1 Prototype A: Anchored Filtering. 
Prototype A can be seen in Figure 3a. The detections are shown as layered keyframes referred to as stacks, with each 
keyframe having a colored border representing the category of object that has been detected on that keyframe. The size 
of the keyframes is chosen with the purpose of them being large enough to view without interacting with the system, 
while also presenting as many keyframes as possible. This decision is motivated by design principle 2, large components 
as it eases the comprehension of images. Furthermore, this design is meant to provide a global overview and ensure 
that the user can have a complete understanding of the drone swarm findings during the SAR mission, thus avoiding 
attentional tunneling. 
The colored borders are linked to buttons above the timeline which also represent the category of objects that has 
been detected in the two stacks placed directly above, and this makes use of design principle 5, grouping items. 
When there are multiple detections within the same time frame they are stacked on top of each other, with a number 
in the topleft corner that represents the number of detections in the stack. Each stack can be expanded such that 
the keyframes in that stack are presented separately. Presenting the summary in this manner is inspired by [10], and 
follows design principle 3, segmenting as it compartmentalizes the detections which should reduce cognitive load. 
The timeline shows the time interval in which the detections have been made. It is split into three parts with the 
detections being placed in a stack that corresponds to the time that they were made. For instance, a detection that was 
made in the second third of the time interval shown on the timeline is placed in one of the stacks in the middle column. 
On the timeline there is a range tool, that allows the user to select a time interval. This functionality makes it possible to 
filter the summary, such that it only shows a subset of detections made in a chosen time interval. The use of a timeline 
follows design principle 11, represent information timelines as it provides the user with an understanding of when the 
detections were made, which could aid the decisionmaking process in terms of deciding which keyframes to examine. 
As mentioned, there are buttons above the timeline with icons representing the categories of objects that are in 
the detections in the two stacks above. These can be selected and deselected such that only detections from those 
userselected categories are shown. Together, with the option to filter the summary on time this functionality is 
motivated by [8] which describes how usercontrolled filtering can help reduce information overload. It also follows 
11 
design principle 9, drilldown as it allows the user to focus on specific parts of the summary while being able to easily 
zoom out and be presented with a greater overview. 
Each keyframe can be enlarged in the overlay page which opens when clicking the left side of they keyframe as is 
indicated by the magnifying glass icon. The overlay page can be seen in Figure 4. The keyframe itself is cropped such 
that it contains an enlarged view of the detected object while still showing some of the surrounding area for context 
purposes. Furthermore, the keyframe has been manipulated by changing the exposure of the surrounding area, which 
in turn highlights the detected object. This manipulation is motivated by the related work described in Section 2.2, and 
follows design principles weeding & signalling, use contrast, and utilise colors which all aim to steer the user’s attention 
towards the detected object such that the time spent decoding the image is decreased and the user’s cognitive load is 
not increased excessively. 
Fig. 4. The overlay page that is shown when clicking the magnifying glass icon. The enhanced keyframe is shown with the timestamp and category for the detection presented above. Additionally, there is a button to mark the detection as interesting. On the right, photos of the detected object from alternative angles captured by the drone swarm are shown. The overlay also serves as a carousel that enables the user to browse through the remaining keyframes in the stack onebyone by using the arrows on the left and right. 
The overlay page includes alternative angles of the chosen detection, a textual description of the category of object 
that has been detected, and a timestamp. The use of images from alternative angles was inspired by DEMA’s current 
use of Skråfoto which is a service that is provided by the Agency for Data Supply and Infrastructure where aerial 
images captured from different angles are made available. These latest iteration of Skråfoto images are from 2021 [1], 
making them somewhat outdated. Therefore, we wanted to investigate the use of images from alternative angles taken 
immediately after the first image of a detected object is taken. These images were captured by flying around the object 
and taking pictures from different angles. The user can also add the detection to a list of detections that require further 
inquiry in the SAR mission by clicking the ’Marker som interssant’ (confirm image) button. Finally, the user can click 
or swipe through the rest of the keyframes in that stack within the overlay page. 
12 
7.1.2 Prototype B: Hidden Filtering. 
The design of Prototype B is very similar to Prototype A, and therefore, the design principles that motivate many of 
the design choices are the same. The main difference is that in Prototype B the filtering options are removed from 
the summary page where the stacks of keyframes are presented. Instead they are presented on a separate page that 
can be opened by clicking the filtering button, located in the top right corner, whose functionality is represented by a 
funnel icon. Here the user can choose to filter the summary based on time and object category, which is then applied to 
the presented summary. The object category buttons have the same color as the frames’ of the detections from that 
category, which follows design principle 5, grouping items. This can be seen in Figure 3b. 
The prototype tests the hypothesis given in design principle 10, reduce display density, but don’t sacrifice coherence, 
as the summary page will be less dense because of the filtering functionality no longer being presented there. However, 
the coherence between the filtering options and the detections that are presented in the summary could become less 
clear as they are shown on two different pages. The purpose of reducing density is to also reduce information overload, 
and by presenting the summary and the filtering on two different pages the design principle, segmenting is also followed 
as applying the filtering to the summary and examining the summary is split into two distinct phases. As a consequence 
of moving the filtering from the summary, more screen space is created for the keyframes themselves, allowing them to 
become slightly bigger thus presumably making it easier to decipher compared to Prototype A. This attempts to follow 
the design principle 2, large components, though the difference in sizes between the two prototypes is subtle. 
The two prototypes are meant to facilitate a study which examines how the user interface for a summary of de- 
tections from a drone swarm should be presented, such that a high level of situational awareness is achieved by the 
user. Furthermore, there is a focus on how the filtering functionality can combat the user’s experience of information 
overload by allowing them to limit the number of detections presented based on relevant filters, such as time and 
object category. However, the filtering should not result in the user losing awareness of the overview provided by the 
summary, thus experiencing attentional tunneling. 
8 USER STUDY 
In order to answer our research question an online study was conducted with drone operators from DEMA, drone 
system developers from Robotto, and students who work with interfaces for drone swarm systems. The aim of the 
study was to investigate the overall usefulness of a summary of drone swarm detections, and whether specific features 
such as enhanced keyframes and filtering functionality are helpful for countering the SA demons, information overload 
and attentional tunneling. To answer these questions the participants were instructed to perform the task of identifying 
specific detections using each of the prototypes, and evaluating their experience immediately after each task by 
responding to a NASA TLX questionnaire [18]. Besides the questionnaire we tracked their performance during the 
task, by tracking the completion time, correctly and incorrectly identified detections, and interactions with the filtering 
functionality. Finally, the participants had to answer a set of followup questions which again aimed to elicit answers to 
the research question. 
The purpose of the study is to evaluate whether the presented prototypes help the participant gain a high level 
of situational awareness, such that they can make decisions about how to proceed with the SAR mission. The focus 
is on the manner in which usercontrolled information filtering is made available in each prototype, and how that 
relates to the user experiencing information overload or attentional tunneling. The independent variable in this study 
is the prototype that is different for each of the two tasks. As described in Section 7 the main difference between the 
13 
prototypes is how the filtering functionality is presented. We will analyze how the change in prototype affects the 
performance and overall experience of the participants when completing the tasks. 
8.1 Participants 
A remote asynchronous online study was performed with 8 participants. All participants are licensed UAV pilots. The 
participants had different occupations as 2 participants were drone operators from DEMA, 1 was a professional drone 
system developer, and 5 were university students who have all worked with user interfaces for drone swarm systems. A 
detailed summary of the participants’ details can be observed in Table 3. The standard deviation reveals a wide spectrum 
of experience in the set of participants, and that the answers came from primarily males. The difference in experience 
can be explained by the various roles of the participating people, as not all of them handle drones on a near daily basis. 
Detail Average SD 
Age: 30.13 6.51 
Years of drone experience: 3.25 3.93 
Years at DEMA: 4.13 8.07 
Table 1. Statistics of the particpants. 
Gender Amount 
Male: 6 
Female: 2 
Other: 0 
Table 2. Distribution of gender. 
Table 3. Details on the particpants’ age, experience and gender. 
The online study was conducted through a website that we developed, which guided the participant through a 
number of steps such that they had sufficient information to perform two tasks using the two prototypes, and finally 
consider some followup statements. The website was in Danish as the participants were mostly native Danish speakers. 
8.2 Study introduction 
The participants were initially given an introduction to the study, highlighting that the overall purpose of the study was 
to examine how a system summarizing detections from drone swarms used in the context of SAR missions should look 
like. They were also asked to accept a consent form which allowed us to collect data during the study for postanalysis. 
To increase the chances of receiving valid feedback from the study, we wanted to make the environment realistic by 
making the participants understand that this system would be used during a SAR mission. This was achieved by creating 
a context video showing a drone swarm taking off, a search pattern being defined, and finally the drones searching the 
area while recording and detecting objects of interest. The participants were then informed that the drone swarms 
detections were sent to a summary which they would have to interact with to identify detections which required further 
investigation. 
8.3 Task 
The participants were shown a video and a series of screenshots explaining the implemented functionality and instructing 
them how to use each of the prototypes. To ensure, that the participants becoming familiar with such a system by using 
one prototype first would not positively bias the results when using the second prototype, half of the participants used 
Prototype A first and Prototype B afterwards, and vice versa for the second half of the participants. 
14 
After the instructional video, they were given the opportunity to try the prototype for 1 minute to become familiar 
with the functionality. During this practice session the summary was made up of detections that were different from 
the ones used in the actual task. The scenario and tasks varied for each prototype, and the description for Prototype A 
was as follows: 
""A person is missing, and a SAR mission has been launched with the purpose of finding the missing person or any personal 
belongings that may lead to the person."" 
The following detailed information was provided about the missing person: 
• 23 year old male. 
• 192 cm tall. 
• Brown hair. 
• Clothing: 
– Black pants. 
– Blue and redstriped tshirt. 
– Blue and whitestriped denim jacket. 
– Black shoes. 
• Carrying a black backpack. 
• Riding a black bicycle. 
• The person was last seen at 12:40 leaving their home in a depressed state of mind. 
• A drone operator who was watching live video feeds from the drones, noticed something at 13:10 which may be 
of interest. 
The participants were instructed to identify the detections that matched for any of the items in the summary which 
showed the person himself, each of his clothing items, or his bicycle. The participant should find these detections during 
the upcoming task and confirm them as being of interest. The task lasted 5 minutes and automatically ended once 
the time limit was reached. A message would appear letting the participant know when there was 1 minute left. The 
summary contained 62 separate detections, containing either people, clothing items, vehicles, or bags. These detections 
were manually labeled to simulate detections made by an AI model. Furthermore, there were a few detections that were 
false positives, meaning they were labeled with a category, however, they did not contain any object of interest. These 
false positives were included to make the set of detections more realistic as AI models are not 100% accurate. 
This meant that there was a set of 6 detections that were defined as being of interest by the task description, and the 
participant had to identify each of these for the task to be deemed complete. 
Once the task using one prototype was completed, the participant was introduced to the other prototype in exactly 
the same way as the first. This means a slideshow of screenshots and a video was shown explaining the functionality and 
how to use it. They also got to practice using the prototype to further understand the functionality and get accustomed 
to using it. 
In the same fashion as before a task was to be completed using the prototype by the participant. The tasks were 
structurally identical for both prototypes. However, for each task the participants were instructed to identify a different 
missing person with completely different items of clothing and vehicle associated. 
15 
Immediately after trying one of the prototypes the participants were presented with a NASA TLX questionnaire that 
they had to provide answers to before continuing. The purpose of this was to record their experience after using each 
prototype and getting data regarding their immediate impression. The participants were asked the following set of 
questions from the NASA TLX questionnaire: 
• How much mental and perceptual activity was required? 
• How much time pressure did you feel due to the rate or pace at which the tasks or task elements occurred? 
• How successful do you think you were in accomplishing the goals of the task set by the experimenter? 
• How hard did you have to work (mentally) to accomplish your level of performance? 
• How insecure, discouraged, irritated, and stressed did you feel during the task? 
The participants were asked to answer each question on a scale ranging from 1 (low) to 10 (high). 
8.4 Followup statements 
To further gather feedback from the domain experts they were asked to consider a set of statements after having tried 
both prototypes. The participants could give their opinion using a fivepoint Likert scale ranging from strongly disagree 
to strongly agree. The set of statements can be seen in Table 4. Furthermore, a text box was included so the participant 
could provide their own comments to each statement regarding their experience and opinions of the prototypes. 
Followup statements 
1. 
The system attempts to sum up the drones’ detections, by presenting a keyframe from each detection in a stack across a timeline. The number of detections that are presented at once is however overwhelming. 
7. Filtering on Prototype A was intuitive. 
2. 
The option to filter on time and category helped with showing the detections that you wanted. 
8. Filtering on Prototype B was intuitive. 
3. 
Filtering was useful for avoiding too much information being shown at once. 
9. Filtering on Prototype A was easy to access. 
4. 
You experienced becoming so focused on a specific element that you lost awareness of the remaining elements. 
10. Filtering on Prototype B was easy to access. 
5. The system helped you decide which detections to examine before others. 11. 
Prototype A had the filtering options presented on the same page as the detections. This made it easier to maintain an overview when you could filter and see the result of filtering on the same page. 
6. 
The information that was presented with each detection, was useful in terms of understanding what had been detected, and deciding if it should be investigated further. 
12. 
Prototype B had a separate page for filtering the detections, and thus had fewer elements on the page showing the detections. This made the page showing the detections easier to maintain an overview of. 
Table 4. The set of followup statements that the participants provided their opinion on through a fivepoint Likert scale. For each statement they had the option to elaborate if they desired. 
16 
Questions were asked to get in depth feedback from the domain experts and to evaluate their experience of the SA 
demons, information overload and attentional tunnelling. Furthermore, questions were asked regarding the features 
implemented in the prototypes, and their preferences between the two presented prototypes. 
9 RESULTS 
With 8 participants doing the online study, a data set was collected for further analysis. We analyzed the performance 
related data recorded during the tasks, their answers to the NASA TLX questionnaire, and their subjective answers 
to a set of followup statements. These results will be described in this section, with the purpose of answering our 
research question and investigating the hypothesis that a system that presents a summary of drone swarm detections 
while allowing for filtering that summary, combats the SA demons information overload and attentional tunneling. The 
questions presented in the study and the answers provided by the participants were in Danish, but are translated to 
English when described in this section. 
9.1 Task performance 
During each task the system recorded a number of different metrics relating to their performance and their interaction 
with the summary functionality. A detailed summary of the participants’ performance and behaviour when using each 
prototype can be seen in Table 5. The table shows the mean values, standard deviation, difference between mean values, 
as well as the tand pvalues which were obtained by running paired samples Ttest to compare the data logged during 
the use of both prototypes. The pvalues are compared to a significance level of 0.05, when determining if there is a 
significant statistical difference between their performance across both prototypes. 
Performance metric Mean SD Difference t p 
Correct markings using Prototype A: 3.38 1.22 -1.38 -2.022 0.082 
Correct markings using Prototype B: 4.75 1.20 
Wrong markings using Prototype A: 5 2.74 4 3.433 0.010 
Wrong markings using Prototype B: 1 1.00 
Right to wrong ratio using Prototype A: 46.49% 21.62 -36.19 -3.183 0.015 
Right to wrong ratio using Prototype B: 82.68% 17.71 
Time to mark a detection using Prototype A: 48.79 19.32 -3.39 -0.292 0.778 
Time to mark a detection using Prototype B: 52.17 20.66 
Filter interactions using Prototype A: 17.88 12.16 3.88 1.330 0.220 
Filter interactions using Prototype B: 14.00 11.30 
Table 5. A detailed summary of the participants’ performance showing the mean values, standard deviation, difference between mean values, as well as the tand pvalues which were obtained by running paired samples Ttest to compare the data logged during the use of both prototypes. 
For each task the number of detections correctly marked as interesting by each participant was measured, to see 
if presenting the detections in a summary as is done in the two prototypes actually allows for identifying detections 
of importance. For each task 6 different detections had to be marked as interesting. When using Prototype A the 
17 
participants correctly marked 3.38 (SD=1.22) detections on average, while the participants correctly marked 4.75 
(SD=1.20) detections on average when completing the task using Prototype B. When using Prototype A the participants 
marked 5 (SD=2.74) detections wrongly on average, and when using Prototype B the participants on average only 
marked 1 (SD=1) detection wrongly. The paired sampled Ttest shows a significant difference between the number of 
wrongly marked detections. 0 participants completed the task using Prototype A, meaning no one correctly marked all 
6 detections of importance as interesting. While 3 participants (37.5%) completed the task when using Prototype B. 
To see if the summary in either of the prototypes aided the participants with identifying the correct detections 
while minimizing the number of detections incorrectly marked as interesting, we calculated the percentage of correctly 
identified detections out of all marked detections. Out of all the detections marked as interesting, 46.49% of them were 
marked correctly when using Prototype A, and 82.68% of detections marked were correct when using Prototype B. This 
is a significant difference showing that participants made less errors while simultaneously marking more detections 
correctly when using Prototype B. 
To evaluate whether the filtering is useful for identifying important detections, the number of interactions that 
each participant made with the filtering functionality was recorded. In Figure 5, a scatter plot is seen which visualizes 
the relationship between the number of filtering interactions and correctly marked detections in order to reveal any 
correlation between the two variables. The scatter plot does not seem to show any correlation between the number of 
filter interactions and correctly marked detections. Additionally, Table 5 shows that the number of filter interactions 
were quite similar between both prototypes with there being no significant statistical difference. 
Fig. 5. Scatter plot showing the relationship between the number of filter interactions and correctly marked detections for both prototypes. 
18 
When completing the tasks the participants were instructed to identify the correct keyframes that were described 
in the task description as quickly as possible. Furthermore, Figure 6 shows the mean time taken by participants to 
mark each correct detection using both prototypes. The bar chart shows that the participants on average spent slightly 
more time to identify the correct detections when using Prototype B compared to Prototype A. This is also seen in 
Table 5 as participants on average spent 48.79 seconds to correctly mark a detection using Prototype A, and 52.17 
seconds to correctly mark a detection using Prototype B. Furthermore, the bar chart shows that when using Prototype 
B, participants spent less time to correctly identify detections 4 to 6 compared to detections 1 through 3. Similarly, the 
time spent identifying detection 3 to 5 using Prototype A was slightly less than the time spent identifying the first 
couple of detections for Prototype A. 
Fig. 6. Bar chart showing the mean time taken by participants to identify a correct detection during the tasks when using each prototype. The error bars are used to represent the standard deviation for each bar. 
9.2 NASA TLX Questionnaire 
After each task the participants filled out a NASA TLX questionnaire with values ranging from 1 (low) to 10 (high). The 
average score of the responses for each prototype can be seen in Figure 7. The bar chart shows that using Prototype B 
was slightly more mentally and temporally demanding while also requiring more effort and causing more frustration. 
However, the average scores are relatively close across both prototypes. 
The participants’ average scores for these questions are quite high. For instance, the average score for mental demand 
when using Prototype A being 6.45 and 7.33 for Prototype B, and the average rating for the level of effort is 6.22 for 
Prototype A, and 6.89 for prototype B. 
19 
Fig. 7. Bar chart showing the average score from the responses to the NASA TLX questionnaire. The error bars are used to represent the standard deviation for each bar. 
9.3 Posttask evaluation 
After the participants had finished using both prototypes they were presented with 12 followup statements which 
were to be answered through a 5-point Likert scale ranging from strongly disagree to strongly agree. The statements 
can be seen in Table 4. Additionally, the participant could elaborate their answer to each statement by writing in the 
accompanying text box. Some of the questions were related to both prototypes while others were about one prototype 
specifically. A selected number of the answers provided by the participants are described in this section. 
9.3.1 Filtering. 
Statements 2 and 3 were concerned with the usefulness of filtering for limiting the amount of information shown, and 
avoiding information overload. For statement 2, 75% of the participants agreed that filtering helped with showing the 
desired detections. One participant elaborated and wrote, 
""[filtering on] category helped, but time did not seem that useful given the short timeframe."" 
and another participant stated, 
""I did not use the filtering during the first task, but did so during the second task which helped a lot with structuring my 
search, and therefore I experienced that I was faster."". 
In response to statement 3, 62.5% of the participants either agreed or strongly agreed, while 37.5% disagreed. For 
instance, one participant wrote, 
""Filtering on time was great for starting the search and focusing on the time span that was given in the task description. But 
hereafter it was difficult to keep track of how the stacks were changing when I moved the timeline"". 
20 
Similarly, statements 7 and 8 were about the intuitiveness of using filtering on Prototype A, and Prototype B, 
respectively. Here, 50% agreed, 37.5% strongly agreed and no one disagreed that filtering on Prototype A was intuitive. 
Though, the elaborated answers did reveal some confusion among the participants with filtering on Prototype A in 
particular. One participant wrote, 
""Filtering in Prototype A was much better ... However, it was very confusing that if I clicked on a bag icon in one [column], 
then the others were affected as well."". 
In regards to the filtering on Prototype B being intuitive, 62.5% agreed while 25% either disagreed or strongly 
disagreed. The responses revealed that the filtering itself was intuitive but the manner in which it was to be accessed 
was not. For instance, a participant stated that, 
""Having to navigate back and forth to filter was not very intuitive and created more frustration than benefit. However, it 
was great that there was only one icon per category, contrary to Prototype A."", 
which also related to statement 9 and 10, which were about how easy it was to access the filtering in each prototype. 
Here, 87.5% of participants either agreed or strongly agreed that filtering was easy to access in Prototype A. On the other 
hand, 62.5% either disagreed or strongly disagreed that accessing filtering in Prototype B was easy. One participant 
wrote, 
""... it was pretty cumbersome to access filtering through a button."". 
In relation to the intuitiveness and accessibility of the filtering, participants gave their opinion on statement 11 which 
stated that, having the filtering on the same page as the detections as was the case with Prototype A made it easier to 
maintain an overview. Over half of the participants either agreed or strongly agreed with this, while the remaining 
three were either neutral or disagreed. In response to the statement one participant said, 
""It was easier to get an overview, and you could easily change what you were looking for."". 
On the other hand, statement 12 postulated that, having the filtering available on a separate page as in Prototype 
B made it easier to maintain an overview of the detections. On this matter, the opinions were split with 62.5% either 
disagreeing or strongly disagreeing and 37.5% agreeing. For instance, one participant wrote, 
""It made it more confusing, as I had a better understanding of what I was filtering and the result of it when using Prototype 
A."". 
Though another participant agreed with the statement, and wrote, 
""I agree. Even though it takes more clicks it seemed more simple."". 
9.3.2 Maintaining an overview. 
In regards to statement 4, which concerned whether participants became so focused on one element that they lost 
awareness of the remaining elements, 75% either agreed or strongly agreed that they experienced this. One participant 
felt that diving into a stack resulted in losing awareness about the other stacks of detections. They wrote, 
""When you are within a stack you can easily get tunnel vision and forget that there are other stacks as well."" 
Additionally, several participants commented that it was difficult to remember what they were supposed to identify 
during the task. The reason being that the task description was not visible for the participant while they were looking 
21 
through the detections. Instead they had to open the task description which would appear on a separate overlay page, 
and then switch back to the summary to then find the items given in the task description. One participant wrote, 
""If you could have the task description open while you’re looking at the detections then you would not have to remember as 
much."". 
9.3.3 Detection information. 
Finally, statement 6 was about the information that was presented alongside each detection on the overlay page, 
and whether it was useful for understanding what had been detected and whether the detection should should be 
investigated further. Here, 62.5% of the participants agreed, with several of them mentioning the photos from alternative 
angles as being useful in particular. One commented, 
""I like the photos from alternative angles a lot, they provide a better understanding of what has been spotted."", 
and another wrote, 
""Photos from alternative angles were the most useful."". 
10 DISCUSSION 
The online study that was conducted revealed several interesting findings with some being expected and others being 
more surprising. These findings and how they relate to the research question we are attempting to answer will be 
discussed in this section. 
10.1 Designing a system for summarizing video feeds from a drone swarm 
The overall purpose of the research in this paper was to elicit valid knowledge about the manner in which a system for 
summarizing video feeds from a drone swarm should be designed and developed. This part of the research question was 
partially answered through the development of the two prototypes which were used in the subsequent study. It was 
apparent that presenting detections as a set of enhanced keyframes in an interactive storyboard, allowed for users to 
identify specific detections containing people and items which were described in a given task description. On average 
the participants correctly identified 4.75 detections out of a possible 6 in only 5 minutes, when using Prototype B. This 
confirms that the techniques such as storyboards, timelines and keyframes that are described in [9, 10, 17] as ways 
of presenting video summaries, are indeed useful and effective for that purpose. When looking at the data gathered 
from the use of each prototype, it was seen that participants were able to correctly identify more detections with less 
errors using Prototype B, compared to Prototype A. One potential reason for this could be that Prototype B had the 
filtering functionality on a separate page, meaning fewer elements on the summary page which in turn could mean the 
user being less overwhelmed. However, somewhat surprisingly several participants commented that they preferred the 
design of Prototype A, as it was frustrating to switch between a filtering page and a summary page with the detections. 
These comments seem to reinforce the two design principles 1 and 10, which are minimal interaction, and reduce display 
density, but don’t sacrifice coherence (described in [8]) respectively, as the participants want to minimize the amount 
of clicking required, as well as maintaining the coherence that is achieved when the summary of detections and the 
filtering are on the same page. However, with the participants preferring the design of Prototype A while performing 
better when using Prototype B, it could be interesting to investigate the design of filtering in such a system further in a 
future study. 
22 
10.2 Designing to avoid information overload and attentional tunneling 
The participants spent around the same time identifying the correct detections on both prototypes. Though overall, 
they became faster at identifying the correct detections throughout the task. This could be because of participants 
getting more familiar with the system during the task. However, an explanation could also be that the participants 
would look through the detections and determine if they were of interest or not. So towards the end of the task the 
participant would know which detections were definitely not of interest, leaving a smaller subset of detections that 
could potentially contain an item described in the task description. This relates to a comment provided by a participant 
who suggested that the system should allow the user to mark a detection as not of interest and subsequently remove 
that detection from the user’s view completely. This could also make the amount of information less overwhelming as 
the user proceed with the task. 
This also ties into the second part of the research question which was concerned with designing the system in a 
manner which would avoid the user experiencing the SA demons information overload and attentional tunneling. 
To avoid information overload, the design principle drilldown was followed by implementing filtering which would 
allow the user to control the information being presented. The study revealed that this was achieved to some extent, as 
participants found filtering useful as it helped them avoid having too much information shown at once. This confirms 
the guidelines for design presented in both [8] and [21]. Furthermore, we implemented drilldown functionality in 
similar ways to the techniques described in [23, 26], with positive results which further validates these techniques as 
effective. Though, we expected to see some correlation between the number of filter interactions and the number of 
correctly identified detections, however, this was not the case. A reason for this could be that the detections spanned 
across a relatively short time span, and the total number of detections in the summary was 62. Given this, filtering 
might have been less useful compared to a summary which presented a lot more detections over a larger time span. This 
could also be interesting to examine further in a future study. Finally, the aim was also to find ways of designing the 
system such that the participant would avoid experiencing attentional tunneling, thus losing awareness of the bigger 
picture. This was not quite achieved in the design of the prototypes as participants did experience becoming so focused 
on one stack of keyframes that they forgot about the rest of the summary. This confirms the challenge described in [2], 
because the ability to focus on one detection completely is essential in the context of SAR, however ensuring that focus 
is not directed to one detection in an excessive manner is difficult. Therefore, it could be interesting to investigate this 
challenge further, by experimenting with other designs such as a splitscreen view showing both a detection in detail 
and an overview of the summary or the use of notifications to ensure that the user does not forget to consider some 
parts of the available information. 
10.3 Limitations 
The research presented has a number of limitations. For instance, the detections that were used in the prototypes 
were manually captured and labelled to simulate an AI recognizing objects in a video feed. This makes the detections 
presented in our study less realistic, as a reallife system would use an autonomous drone with AI capabilities to detect 
objects of interest which could be less accurate and capture the detections differently. 
As for the study itself we only had 8 participants and only 2 of them were drone operators which is who such a 
system in envisioned for. This might limit the validity of the results gathered, even though the remaining participants 
did have experience with developing drone swarm systems. Finally, the system was envisioned to be used outside 
during an actual SAR mission. However, the setup in the study was quite different from this with the online study being 
23 
conducted inside using a computer. Having a more realistic setup, by doing a field study for instance could perhaps 
have elicited different and more realistic feedback. 
11 CONCLUSION 
In this paper we investigated how a system that presents a summary of detections made by a drone swarm in the context 
of Search and Sescue should be designed. To examine this question we looked into related work, and prior knowledge 
obtained through the HERD project to create a set of design principles that guided the design of subsequent mockups 
and eventually two prototypes. The mockups were presented to domain experts in the form of DEMA’s head of drone 
operations in Jutland and a professional drone system developer. This resulted in valid feedback which was used along 
the design principles in the development of two functional prototypes, which both presented a summary of detections 
as keyframes in a storyboard across a timeline, while allowing for usercontrolled filtering of the summary. These 
prototypes facilitated an online study that was conducted with 8 participants consisting of drone operators from DEMA, 
professional drone system developers, and students that work with interfaces for drone swarm systems. Participants 
had to complete a task of identifying detections which showed a missing person and their personal belongings by 
using each prototype. The results showed that presenting a summary of detections as done in the prototypes allowed 
for participants to identify specific detections given to them in a task description. On average participants correctly 
identified 4.75 detections out of a possible 6, in just 5 minutes when using Prototype B. 
Additionally, the purpose of the research done was to discover ways of designing such a system while avoiding 
the situational awareness demons, information overload and attentional tunneling. To avoid information overload, 
usercontrolled filtering functionality was implemented. Participants commented that they found it useful, and it 
allowed them to limit the amount of information to a nonoverwhelming amount at a time. However, the logged data 
did not show any correlation between the number of filter interactions and correctly identified detections. Furthermore, 
the study showed that attentional tunneling was experienced by the participants when using the prototypes. Finally, 
this research could be expanded upon by conducting a field study, to gain feedback from using such a system in a more 
realistic scenario. 
ACKNOWLEDGMENTS 
A special thanks to the employees at the Danish Emergency Management Agency and Robotto for taking the time to 
provide valuable feedback that helped shape the design of both prototypes. We would also like to thank everyone who 
actively participated in the study which yielded data for further analysis in order to evaluate the prototypes. 
REFERENCES 
[1] Agency for Data Supply and Infrastructure. 2021. Information om Skråfoto. https://skraafoto.dataforsyningen.dk/info.html [2] Ankit Agrawal, Sophia J. Abraham, Benjamin Burger, Chichi Christine, Luke Fraser, John M. Hoeksema, Sarah Hwang, Elizabeth Travnik, Shreya Kumar, Walter Scheirer, Jane ClelandHuang, Michael Vierhauser, Ryan Bauer, and Steve Cox. 2020. The Next Generation of HumanDrone Partnerships: CoDesigning an Emergency Response System. Conference on Human Factors in Computing Systems - Proceedings (4 2020). https: //doi.org/10.1145/3313831.3376825 
[3] Andreh Bassam Bahodi, Denmark MariaTheresa Oanh Hoang, and Denmark Rasmus Skov Buchholdt. 2022. User Interface Design for UAV Swarms in Search and Rescue. (2022). https://www.brs.dk/da/ 
[4] Steven Bradley. 2016. An Introduction To Semiotics — Signifier And Signified. https://vanseodesign.com/webdesign/semioticssignifiersignified/ [5] Steven Bradley. 2016. Denotation And Connotation - Literal And Implied Meaning. https://vanseodesign.com/webdesign/denotationconnotation/ [6] Andreas Daugbjerg Christensen, Andreas Skjoldgaard Andersen, Philip Michaelsen, and Shpend Gjela. 2022. Interfaces for Live Videostreams from Search and Rescue Drone Swarms. (12 2022), 23 pages. 
24 
[7] DIREC. 2023. HERD: HumanAI collaboration: Engaging and controlling swarms of Robots and Drones - DIREC. https://direc.dk/da/herdhumanaicollaborationengagingandcontrollingswarmsofrobotsanddrones/ 
[8] Mica R. Endsley and Debra G. Jones. 2016. Designing for Situation Awareness: An Approach to UserCentered Design, Second Edition. Designing for Situation Awareness: An Approach to UserCentered Design, Second Edition (1 2016), 1–373. https://doi.org/10.1201/b11371 
[9] Yanwei Fu, Yanwen Guo, Yanshu Zhu, Feng Liu, Chuanming Song, and Zhi Hua Zhou. 2010. Multiview video summarization. IEEE Transactions on Multimedia 12, 7 (11 2010), 717–729. https://doi.org/10.1109/TMM.2010.2052025 
[10] Andreas Girgensohn, Frank Shipman, Anthony Dunnigan, Thea Turner, and Lynn Wilcox. 2006. Support for effective use of multiple video streams in security. Proceedings of the ACM International Multimedia Conference and Exhibition (2006), 19–26. https://doi.org/10.1145/1178782.1178787 
[11] MariaTheresa Oanh Hoang, Niels Van Berkel, Mikael B Skov, Timothy Merritt, and Timothy 2022 Merritt. 2022. Challenges Arising in a MultiDrone System for Search and Rescue Challenges Arising in a MultiDrone System. (2022). https://doi.org/10.1145/3546155.3546653 
[12] Nina Hollender, Cristian Hofmann, Michael Deneke, and Bernhard Schmitz. 2010. Integrating cognitive load theory and concepts of human–computer interaction. Computers in Human Behavior 26, 6 (11 2010), 1278–1288. https://doi.org/10.1016/J.CHB.2010.05.031 
[13] IGI Global. 2023. What is Visual Semiotics | IGI Global. https://www.igiglobal.com/dictionary/visualsemiotics/48984 [14] Palash Yuvraj Ingle, Yujun Kim, and Young Gab Kim. 2022. DVS: A Drone Video Synopsis towards Storing and Analyzing Drone Surveillance Data in Smart Cities. Systems 2022, Vol. 10, Page 170 10, 5 (9 2022), 170. https://doi.org/10.3390/SYSTEMS10050170 
[15] Richard E. Mayer and Roxana Moreno. 2010. Nine Ways to Reduce Cognitive Load in Multimedia Learning. https://doi.org/10.1207/S15326985EP3801_6 38, 1 (2010), 43–52. https://doi.org/10.1207/S15326985EP3801{_}6 
[16] Shaohui Mei, Genliang Guan, Zhiyong Wang, Shuai Wan, Mingyi He, and David Dagan Feng. 2015. Video summarization via minimum sparse reconstruction. Pattern Recognition 48, 2 (2 2015), 522–533. https://doi.org/10.1016/J.PATCOG.2014.08.002 
[17] Tao Mei, Bo Yang, Shi Qiang Yang, and Xian Sheng Hua. 2009. Video collage: Presenting a video sequence using a single image. Visual Computer 25, 1 (1 2009), 39–51. https://doi.org/10.1007/S00371-008-0282-4/METRICS 
[18] NASA. 2020. TLX @ NASA Ames - Home. https://humansystems.arc.nasa.gov/groups/TLX/ [19] Zena O’connor. 2013. Colour, Contrast and Gestalt Theories of Perception: The Impact in Contemporary Visual Communications Design in Wiley Online Library. Wiley Periodicals, Inc. Col Res Appl 40 (2013), 85–92. https://doi.org/10.1002/col.21858 
[20] Robotto. 2023. About | Robotto. https://www.robotto.ai/about [21] Ben Shneiderman. 2022. HumanCentered AI. Oxford University Press. 0–377 pages. [22] Tom Streeter. 2012. Definitions of Semiotic Terms. https://www.uvm.edu/~tstreete/semiotics_and_ads/terminology.html [23] Zhida Sun, Mingfei Sun, Nan Cao, and Xiaojuan Ma. 2016. VideoForest: Interactive visual summarization of video streams based on danmu data. SA 2016 - SIGGRAPH ASIA 2016 Symposium on Visualization (11 2016). https://doi.org/10.1145/3002151.3002159 
[24] Philip Tqulnlanu and Richard N Wilton. 1998. Grouping by proximity or similarity? Competition between the Gestalt principles in vision. Perception 27 (1998), 417–430. 
[25] ChaoMing Yang and TzuFan Hsu. 2015. Applying Semiotic Theories to Graphic Design Education: An Empirical Study on Poster Design Teaching. International Education Studies 8, 12 (2015). https://doi.org/10.5539/ies.v8n12p117 
[26] Saelyne Yang, Jisu Yim, Juho Kim, and Hijung Valentina Shin. 2022. CatchLive: Realtime Summarization of Live Streams with Stream Content and Interaction Data. Conference on Human Factors in Computing Systems - Proceedings (4 2022). https://doi.org/10.1145/3491102.3517461 
[27] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli Deepmind, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. 2019. CLEVRER: CoLlision Events for Video REpresentation and Reasoning. (10 2019). https://doi.org/10.48550/arxiv.1910.01442 
[28] Zhiwang Zhang, Dong Xu, Wanli Ouyang, and Chuanqi Tan. 2020. Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization. IEEE Transactions on Circuits and Systems for Video Technology 30, 9 (9 2020), 3130–3139. https://doi.org/10.1109/TCSVT. 2019.2936526 
25 
",Interfaces for Presenting Summaries of Detections from Search and Rescue.pdf,6
"Received February 22, 2019, accepted March 13, 2019, date of publication April 9, 2019, date of current version April 22, 2019. 
Digital Object Identifier 10.1109/ACCESS.2019.2909530 
Unmanned Aerial Vehicles (UAVs): A Survey on Civil Applications and Key Research Challenges 
HAZIM SHAKHATREH 1, AHMAD H. SAWALMEH 2,3, ALA ALFUQAHA 4,5, (Senior Member, IEEE), ZUOCHAO DOU 6, EYAD ALMAITA7, ISSA KHALIL8, NOOR SHAMSIAH OTHMAN 2, ABDALLAH KHREISHAH 9, AND MOHSEN GUIZANI10, (Fellow, IEEE) 1Department of Telecommunications Engineering, Yarmouk University, Irbid 21163, Jordan 2Department of Electronics and Communications Engineering, Universiti Tenaga Nasional, Kajang 43000, Malaysia 3Electrical Engineering Department, College of Engineering, Northern Border University, Arar 73222, Saudi Arabia 4Information and Computing Technology (ICT) Division, College of Science and Engineering (CSE), Hamad Bin Khalifa University, Doha, Qatar 5Department of Computer Science, Western Michigan University, Kalamazoo, MI 49008, USA 6Novo Vivo, Inc., Palo Alto, CA 94301, USA 7Department of Power and Mechatronics Engineering, Taﬁla Technical University, Taﬁlah 66110, Jordan 8Qatar Computing Research Institute (QCRI), HBKU, Doha, Qatar 9Department of Electrical and Computer Engineering, Newark College of Engineering, New Jersey Institute of Technology, University Heights, Newark, NJ 07102, USA 10Department of Electrical and Computer Engineering, University of Idaho, Moscow, ID 83844, USA 
Corresponding author: Ala AlFuqaha (aalfuqaha@hbku.edu.qa) 
This work was supported by the Qatar National Library. The statements made herein are solely the responsibility of the authors. 
ABSTRACT The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including realtime monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than $45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and securityrelated challenges. Based on our review of the recent literature, we discuss open research challenges and draw highlevel insights on how these challenges might be approached. 
INDEX TERMS Civil infrastructure inspection, delivery of goods, precision agriculture, realtime monitoring, remote sensing, search and rescue, security and surveillance, UAVs, wireless coverage. 
I. INTRODUCTION UAVs can be used in many civil applications due to their ease of deployment, low maintenance cost, highmobility and ability to hover [1]. Such vehicles are being utilized for realtime monitoring of road trafﬁc, providing wireless coverage, remote sensing, search and rescue operations, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. The recent research literature on UAVs focuses on vertical applications without considering the challenges facing UAVs within speciﬁc vertical domains and across application domains. Also, these studies do not discuss practical ways to overcome challenges 
The associate editor coordinating the review of this manuscript and approving it for publication was Jiankang Zhang. 
that have the potential to contribute to multiple application domains. The authors in [1] present the characteristics and requirements of UAV networks for envisioned civil applications over the period 2000–2015 from a communications and networking viewpoint. They survey the quality of service requirements, networkrelevant mission parameters, data requirements, and the minimum data to be transmitted over the network for civil applications. They also discuss general networking related requirements, such as connectivity, adaptability, safety, privacy, security, and scalability. Finally, they present experimental results from many projects and investigate the suitability of existing communications technologies to support reliable aerial networks. In [2], 
48572 This work is licensed under a Creative Commons Attribution 3.0 License. For more information, see http://creativecommons.org/licenses/by/3.0/ VOLUME 7, 2019 
TABLE 1. Comparison of related work on UAV surveys in terms of applications. 
TABLE 2. Comparison of related work on UAV surveys in terms of new technology trends. 
the authors attempt to focus on research in the areas of routing, seamless handover and energy efﬁciency. First, they distinguish between infrastructure and adhoc UAV networks, application areas in which UAVs act as servers or as clients, star or mesh UAV networks and whether the deployment is hardened against delays and disruptions. Then, they focus on the main issues of routing, seamless handover and energy efﬁciency in UAV networks. The authors in [7] survey Flying AdHoc Networks (FANETs) which are adhoc networks connecting the UAVs. They ﬁrst clarify the differences between FANETs, Mobile Adhoc Networks (MANETs) and Vehicle AdHoc Networks (VANETs). Then, they introduce the main FANET design challenges and discuss open research issues. In [8], the authors provide an overview of UAVaided wireless communications by introducing the basic networking architecture and main channel characteristics. They also highlight the key design considerations as well as the new opportunities to be explored. The authors of [9] present an overview of legacy and emerging public safety communications technologies along with the spectrum allocation for public safety usage across all the frequency bands in the United States. They conclude that the application of UAVs in support of public safety communications is shrouded by privacy concerns and lack of comprehensive policies, regulations, and governance for UAVs. In [10], the authors survey the applications implemented using cooperative swarms of UAVs that operate as distributed processing system. They classify the distributed processing applications into the following categories: 1) general purpose distributed processing applications, 2) object detection, 3) tracking, 4) surveillance, 5) data collection, 6) path planning, 7) navigation, 8) collision avoidance, 9) coordination, 10) environmental monitoring. However, this survey does not consider the challenges facing UAVs in these applications and the potential role of new technologies in UAV uses. The authors of [3] provide a comprehensive survey on UAVs, 
highlighting their potential use in the delivery of Internet of Things (IoT) services from the sky. They describe their envisioned UAVbased architecture and present the relevant key challenges and requirements. In [4], the authors provide a comprehensive study on the use of UAVs in wireless networks. They investigate two main use cases of UAVs; namely, aerial base stations and cellularconnected users. For each use case of UAVs, they present key challenges, applications, and fundamental open problems. Moreover, they describe mathematical tools and techniques needed for meeting UAV challenges as well as analyzing UAVenabled wireless networks. The authors of [5] provide a comprehensive survey on available AirtoGround channel measurement campaigns, large and small scale fading channel models, their limitations, and future research directions for UAV communications scenarios. In [6], the authors provide a survey on the measurement campaigns launched for UAV channel modeling using low altitude platforms and discuss various channel characterization efforts. They also review the contemporary perspective of UAV channel modeling approaches and outline some future research challenges in this domain. UAVs are projected to be a prominent deliverer of civil services in many areas including farming, transportation, surveillance, and disaster management. In this paper, we review several UAV civil applications and identify their challenges. We also discuss the research trends for UAV uses and future insights. The reason to undertake this survey is the lack of a survey focusing on these issues. Tables 1 and 2 delineate the closely related surveys on UAV civil applications and demonstrate the novelty of our survey relative to existing surveys. Speciﬁcally, the contributions of this survey can be delineated as: 
• Present the global UAV payload market value. The payload covers all equipment which are carried by UAVs such as cameras, sensors, radars, LIDARs, 
VOLUME 7, 2019 48573 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 1. Overall structure of the survey. 
communications equipment, weaponry, and others. We also present the market value of UAV uses. 
• Provide a classiﬁcation of UAVs based on UAV endurance, maximum altitude, weight, payload, range, fuel type, operational complexity, coverage range and applications. 
• Present UAV civil applications and challenges facing UAVs in each application domain. We also discuss the research trends for UAV uses and future insights. The UAV civil applications covered in this survey include: realtime monitoring of road trafﬁc, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. 
• Discuss the key challenges of UAVs across different application domains, such as charging challenges, collision avoidance and swarming challenges, and networking and security challenges. Our survey is beneﬁcial for future research on UAV uses, as it comprehensively serves as a resource for UAV applications, challenges, research trends and future insights, as shown in Figure 1. For instance, UAVs can be utilized for providing wireless coverage to remote areas such as Facebook’s Aquila UAV [11]. In this aplication, UAVs need to return periodically to a charging station for recharging, due to their limited battery capacity. To overcome this challenge, solar panels installed on UAVs harvest the received solar energy and convert it to electrical energy for long endurance ﬂights [12]. We can envisage Laser powerbeaming as a future technology to provide supplemental energy at night when solar energy is not available or is minimal at high latitudes during winter. This would enable such UAVs to ﬂy day and night for weeks or possibly months without landing [13]. The rest of the survey is organized as follows. We start with an overview of the global UAV market value and UAV classiﬁcation in Part I of this survey (Sections II and III). In Part II (Sections IVXI), we present UAV civil applications 
and the challenges facing UAVs in each application domain, and we also discuss the research trends and future insights for UAV uses. In Part III (Sections XII and XIII), we discuss the key challenges of UAV civil applications and conclude this study. Finally, a list of the acronyms used in the survey is presented in Section XIV. To facilitate reading, Figure 2 provides a detailed structure of the survey. Readers interested in a certain UAV application are encouraged to read Part I, their application of interest from Part II, and then Part III (cf. Figure 1). All application speciﬁc discussions are isolated in Part II while Parts I and III provide discussions that are common to all applications. 
PART I: MARKET OPPORTUNITY AND UAV CLASSIFICATION II. MARKET OPPORTUNITY UAVs offer a great market opportunity for equipment manufacturers, investors and business service providers. According to the PwC report [14], the addressable market value of UAV uses is over $127 billion as shown in Figure 3. Civil infrastructure is expected to dominate the addressable market value of UAV uses, with market value of $45 billion. A report released by the Association for Unmanned Vehicle Systems International, expects more than 100, 000 new jobs in unmanned aircrafts by 2025 [15]. To know how to operate a UAV and meet job requirements, a person should attend training programs at universities or specialized institutes. Global UAV payload market value is expected to reach $3 billion by 2027 dominated by North America, followed by AsiaPaciﬁc and Europe [16]. The payload covers all equipment which are carried by UAVs such as cameras, sensors, radars, LIDARs, communications equipment, weaponry, and others [17]. Radars and communications equipment segment is expected to dominate the global UAV payload market with a market share of close to 80%, followed by cameras and sensors segment with around over 11% share and weaponry segment with almost 9% share [16] as shown in Figure 4. 
48574 VOLUME 7, 2019 
FIGURE 2. Detailed structure of the survey. 
Business Intelligence expects sales of UAVs to reach $12 billion in 2021, which is up by a compound annual growth rate of 7.6% from $8.5 billion in 2016 [18]. This future growth is expected to occur across three main sectors: 1) Consumer UAV shipments which are projected to reach 29 million in 2021; 2) Enterprise UAV shipments which are projected to reach 805, 000 in 2021; 3) Government UAVs for combat and surveillance. According to Bard Center for the Study of UAVs, U.S. Department of Defense allocated a budget of $4.457 billion for UAVs in 2017 [19]. All these statistics show the economic importance of UAVs and their applications in the near future for equipment 
manufacturers, investors and business service providers. Smart UAVs will provide a unique opportunity for UAV manufacturers to utilize new technological trends to overcome current challenges of UAV applications. To spread UAV services globally, a complete legal framework and institutions regulating the commercial use of UAVs are needed [20]. 
III. UAV CLASSIFICATION Unmanned vehicles can be classiﬁed into ﬁve different types according to their operation. These ﬁve types are unmanned ground vehicles, unmanned aerial vehicles, unmanned surface vehicles (operating on the surface of water), unmanned 
VOLUME 7, 2019 48575 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 3. Predicted value of UAV solutions in key industries (billion). 
FIGURE 4. Global UAV payload market predictions 2027. 
underwater vehicles, and unmanned spacecrafts. Unmanned vehicles can be either remote guided or autonomous vehicles [21]. There has been many research studies on unmanned vehicles that reported progress towards autonomic systems that do not require human interactions. In line with the concept of autonomy with respect to humans and societies, technical systems that claim to be autonomous must be able to make decisions and react to events without direct interventions by humans [22]. Therefore, some fundamental elements are common to all autonomous vehicles. These elements include: ability of sensing and perceiving the environment, ability of analyzing, communicating, planning and decision making using onboard computers, as well as acting which requires vehicle control algorithms. UAV features may vary depending on the application in order for them to ﬁt their speciﬁc task. Therefore, any 
classiﬁcation of UAVs needs to take into consideration their various features as they are widely used for a variety of civilian operations [23]. The use of UAVs as an aerial base station in communications networks, can be categorized based on their operating platform, which can be a Low Altitude Platform (LAP) [24]–[26] or High Altitude Platform (HAP) [27]–[29]. LAP is a quasistationary aerial communications platform that operates at an altitude of less than 10 km. Three main types of UAVs that fall under this category are vertical takeoff and landing (VTOL) vehicles, aircrafts, and balloons. On the other hand, HAP operates at very high altitude above 10 km, and vehicles utilizing this platform are able to stay for a long time in the upper layers of the stratosphere. Airships, aircrafts, and balloons are the main types of UAVs that fall under this category. These two categories of aerial communications platforms are shown in Figure 5. More speciﬁcally, in each platform the main UAV types and examples of each type are illustrated in this ﬁgure. A comparison between HAP and LAP is presented in Table 3. This table also summarizes the main UAV types for each platform, and its performance parameters. Speciﬁcally, these parameters are UAV endurance, maximum altitude, weight, payload, range, deployment time, fuel type, operational complexity, coverage range, applications and examples for each UAV type is also shown in this table. 
PART II: UAV APPLICATIONS IV. SEARCH AND RESCUE (SAR) In the wake of new scientiﬁc developments, speculations shot up with regard to the future potential of UAVs in the context of public and civil domains. UAVs are believed to be of immense advantage in these domains, especially in support of public safety, search and rescue operations and disaster management. In case of natural or manmade disasters like ﬂoods, Tsunamis, or terrorist attacks, critical infrastructure including water and power utilities, transportation, and telecommunications systems can be partially or fully affected by the disaster. This necessitates rapid solutions to provide communications coverage in support of rescue operations [1]. When the public communications networks are disrupted, UAVs can provide timely disaster warnings and assist in speeding up rescue and recovery operations. UAVs can also carry medical supplies to areas that are classiﬁed as inaccessible. In certain disastrous situations like poisonous gas inﬁltration, wildﬁres, avalanches, and search for missing persons, UAVs can be used to play a support role and speed up SAR operations [41]. Moreover, UAVs can quickly provide coverage of a large area without ever risking the security or safety of the personnel involved. 
A. UAVBASED SAR SYSTEM SAR operations using traditional aerial systems (e.g., aircrafts and helicopters) are typically very costly. Moreover, aircrafts require special training, and special permits for taking off and landing areas. However, using UAVs in SAR 
48576 VOLUME 7, 2019 
FIGURE 5. UAV classification. 
TABLE 3. Platform classification of UAV types, and performance parameters. 
operations reduces the costs, resources and human risks. Unfortunately, every year large amounts of money and time are wasted on SAR operations using traditional aerial systems [41]. UAVs can contribute to reduce the resources needed in support of more efﬁcient SAR operations. There are two types of SAR systems, single UAV systems, and MultiUAV systems. A single UAV system is illustrated in Figure 6. In the ﬁrst step, the rescue team deﬁnes the search region, then the search operation is started by scanning the target area using a single UAV equipped with vision or thermal 
cameras. After that, realtime aerial videos/images from the targeted area are sent to the Ground Control System (GCS). These videos and images are analyzed by the rescue team to direct the SAR operations optimally [42]. In MultiUAV systems, UAVs with onboard imaging sensors are used to locate the position of missing persons. The following processes summarize the SAR operations that use these systems. Firstly, the rescue team conduct path planning to compute the optimal trajectory of the SAR mission. Then, each UAV receives its assigned path from the GCS. Secondly, 
VOLUME 7, 2019 48577 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 6. Use of single UAV systems in SAR operations, mountain avalanche events. 
the search process is started. During this process, all UAVs follow their assigned trajectories to scan the targeted region. This process utilizes object detection, video/image transmission and collision avoidance methods. Thirdly, the detection process is started. During this process, a UAV that detects an object hovers over it, while the other UAVs act des to facilitate coordination between all the UAVs and communications with the GCS. Afterword, UAVs switch to data dissemination mode and setup a multihop communications link with the GCS. Finally, the location of the targeted object, and related videos and images are transmitted to the GCS. Figure 7, illustrates the use of multiUAV systems in support of SAR operations [43]. The Quadrotor drones are most commonly used in SAR missions, e.g. the FALCON [32]. 
B. HOW SAR OPERATIONS UTILIZE UAVs SAR operations are one of the primary usecases of UAVs. Their use in SAR operations attracted considerable attention and became a topic of interest in the recent past. SAR missions can utilize UAVs as follows: 
1) Taking high resolution images and videos using onboard cameras to survey a given target area (stricken region). Here, UAVs are used for post disaster aerial assessment or damage evaluation. This helps to evaluate the magnitude of the damage in the infrastructure caused by the disaster. After assessment, rescue teams can identify the targeted search area and commence SAR operations accordingly [44]. 2) SAR operations using UAVs can be performed autonomously, accurately and without introducing additional risks [41]. In the Alcedo project [45], a prototype was developed using a lightweight quadrotor UAV equipped with GPS to help in ﬁnding lost persons. In a Capstone project [46], using UAVs in support of SAR operations in snow avalanche scenarios is explored. The used UAV utilizes thermal infrared imaging and Geographic Information System (GIS) data. In such scenarios, UAVs can be utilized to ﬁnd avalanche victims or lost persons. 
FIGURE 7. Use of multiUAV systems in SAR operations, locate GPS coordinates for the missing persons. 
3) UAV can be also used to deliver food, water and medicines to the injured. Although the use of UAVs in SAR operations can help to present potential dangers to crews of the ﬂight, UAVs still suffer from capacity scale problems and limitations to their payloads. In [47], a UAV with vertical takeoff and landing capabilities was designed. Even, a high power propellant system has been added to allow the UAV to lift heavy cargo between 10-15 Kg, which could include medicine, food, and water. 4) UAVs can act as aerial base stations for rapid service recovery after complete communications infrastructure damage in disaster stricken areas. This helps in SAR operations as illustrated in Figures 8 and 9 for outdoor and indoor environments, respectively. 
C. CHALLENGES 1) LEGISLATION In the United States, the FAA does not currently permit the use of swarms of autonomous UAVs for commercial applications. But it is possible to adjust the regulations to allow this type of use. Swarms of UAVs can be used to coordinate the operations of SAR teams [48]. 
48578 VOLUME 7, 2019 
FIGURE 8. Use UAV to provide wireless coverage for outdoor users. 
FIGURE 9. Use UAV to provide wireless coverage for indoor users. 
2) WEATHER Weather conditions pose a challenge to UAVs as they might result in deviations in their predetermined paths. In cases of natural or manmade disasters, such as Tsunamis, Hurricanes, or terrorist attacks, weather becomes a tough and cardinal challenge. In such scenarios, UAVs may fail in their missions as a result of the detrimental weather conditions [49]. 
3) ENERGY LIMITATIONS Energy consumption is one of the most important challenges facing UAVs. Usually, UAVs are battery powered. UAV batteries are used for UAV hovering, wireless communications, data processing and image analysis. In some SAR operations, UAVs need to be operated for extended periods of time over disaster stricken regions. Due to the power limitations of UAVs, a decision must be taken on whether UAVs should perform data and image analysis onboard in realtime, or data should be stored for later analysis to reduce the consumed power [2], [50]. 
D. RESEARCH TRENDS AND FUTURE INSIGHTS 1) IMAGE PROCESSING SAR operations using UAVs can employ image processing techniques to quickly and accurately ﬁnd targeted objects. Image processing methods can be used in autonomous single and multiUAV systems to locate potential targets in support 
of SAR operations. Moreover, location information can be augmented to aerial images of target objects [51]. UAVs can be integrated with target detection technologies including thermal and vision cameras. Thermal cameras (e.g., IR cameras) can detect the heat proﬁle to locate missing persons. Two stage template based methods can be used with such cameras [52]. Vision cameras can also help in the detection process of objects and persons [53], [54]. Methods that utilize a combination of thermal and vision cameras have been reported in the literature as in [52]. In SAR operations, image processing can be done either at the GCS, post target identiﬁcation, or at the UAV itself, using onboard processors with realtime image processing capabilities. In [55], the authors implemented a target identiﬁ- cation method using onboard processors, and the GCS. This method utilizes image processing techniques to identify the targeted objects and their coarse location coordinates. Using terrestrial networks, the UAV sends the images and their GPS locations to the GCS. Another possible approach requires the UAV to capture and save high resolution videos for later analysis at the GCS. 
2) MACHINE LEARNING Machine learning techniques can be applied on images captured by UAVs to help in SAR operations [56], [57]. In [57], the authors propose machine learning techniques applied to images captured by UAVs equipped with vision cameras. In their study, pretrained Convolutional Neural Network (CNN) with trained linear Support Vector Machine (SVM) is used to determine the exact image/video frame at which a lost person is potentially detected. Figure 10. illustrates a block diagram of a SAR system utilizing UAVs in conjunction with machine learning technology [57]. SAR operations that employs UAVs with machine learning technologies face many challenges. UAV is battery powered, so there is signiﬁcant limitation on its onboard processing capability. Moreover, protection against adversarial attacks on the employed machine learning techniques pose another important challenge. Furthermore, reliable and realtime communications with the GCS given QoS and energy constraints is another challenge [57]. 
3) FUTURE INSIGHTS Based on the reviewed current literature focusing on SAR scenarios using UAVs, we believe there is a need for more research on the following: • Data fusion and decision fusion algorithms that integrate the output of multiple sensors. For example, GPS can be integrated with Forward looking infrared (FLIR) sensors and thermal sensors to realize more accurate detection solution [52]. 
• While traditional machine learning techniques have demonstrated their success on UAVs, deep learning techniques are currently off limits because of the limitations on the onboard processing capabilities and power resources on UAVs. Therefore, there is a need to design 
VOLUME 7, 2019 48579 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 10. Block diagram of SAR system using UAVs with machine learning technology. 
and implement onboard, low power, and efﬁcient deep learning solutions in support of SAR operations using UAVs [58]. 
• Design and implementation of powerefﬁcient distributed algorithms for the realtime processing of UAV swarm captured videos, images, and sensing data [57]. 
• New lighter materials, efﬁcient batteries and energy harvesting solutions can contribute to the potential use of UAVs in long duration missions [50]. 
• Algorithms that support UAV autonomy and swarm coordination are needed. These algorithms include: ﬂight route determination, path planning, collision avoidance and swarm coordination [59], [60]. 
• In MultiUAV systems, there are many coordination and communications challenges that need to be overcome. These challenges include QoS communications between the swarm of UAVs over multihop communications links and with the GCS [43]. 
• More accurate localization and mapping systems and algorithms are required in support of SAR operations. Nowadays, GPS is used in UAVs to locate the coordinates UAVs and target objects but GPS is known to have coverage and accuracy issues. Therefore, new algorithms are needed for data fusion of the data received from multiple sensors to achieve more precise localization and mapping without coverage disruptions. 
• The use of UAVs as aerial base stations is still nascent. During disasters, UAVs can act as aerial base stations to help trapped people and rescue teams during SAR missions. Therefore, more research is needed to study the use of such systems in providing communications coverage when the public communications network is disrupted or operates at its maximum capacity [24]. 
V. REMOTE SENSING UAVs can be used to collect data from ground sensors and deliver the collected data to ground base stations [61]. UAVs equipped with sensors can also be used as aerial sensor network for environmental monitoring and disaster management [62]. Numerous datasets originating from UAVs remote sensing have been acquired to support the research teams, 
serving a broad range of applications: crop monitoring, yield estimates, drought monitoring, water quality monitoring, tree species, disease detection, etc [63]. In this section, we present UAV remote sensing systems, challenges during aerial sensing using UAVs, research trends and future insights. 
A. REMOTE SENSING SYSTEMS There are two primary types of remote sensing systems: active and passive remote sensing systems [64]. In active remote sensing system, the sensors are responsible for providing the source of energy required to detect the objects. The sensor transmits radiation toward the object to be investigated, then the sensor detects and measures the radiation that is reﬂected from the object. Most active remote systems used in remote sensing applications operate in the microwave portion of the electromagnetic spectrum and hence it makes them able to propagate through the atmosphere under most conditions [64]. The active remote sensing systems include laser altimeter, LiDAR, radar, ranging instrument, scatterometer and sounder. In passive remote sensing system, the sensor detects natural radiation that is emitted or reﬂected by the object as shown in Figure 11. The majority of passive sensors operate in the visible, infrared, thermal infrared, and microwave portions of the electromagnetic spectrum [64]. The passive remote sensing systems include accelerometer, hyperspectral radiometer, imaging radiometer, radiometer, sounder, spectrometer and spectroradiometer. In Figure 12, we show the classiﬁcations of UAV aerial sensing systems. In Table 4, we make a comparison among the UAV remote sensing systems based on the operating frequency and applications. The common active sensors in remote sensing are LiDAR and radar. A LiDAR sensor directs a laser beam onto the surface of the earth and determines the distance to the object by recording the time between transmitted and backscattered light pulses. A radar sensor produces a twodimensional image of the surface by recording the range and magnitude of the energy reﬂected from all objects. The common passive sensor in remote sensing is spectrometer. A spectrometer sensor is designed to detect, measure, and analyze the spectral content of incident electromagnetic radiation. 
48580 VOLUME 7, 2019 
FIGURE 11. Active Vs. passive remote sensing. 
FIGURE 12. Classification of UAVs aerial sensing systems. 
Fixed wing UAVs such as Viking aircraft [33]–[35], and VTOL Quadrotor such as LIDAR [32] can be used in remote sensing applications for real time assessment, large scale mapping, and monitoring activities for different applications. 
B. IMAGE PROCESSING AND ANALYSIS The image processing steps for a typical UAV mission are described in details by the authors in [65] and [66]. The process ﬂow is the same for most remotely sensed imagery processing algorithms. First, the algorithm utilizes the log ﬁle from the UAV autopilot to provide initial estimates for the position and orientation of each image. The algorithm then applies aerial triangulation process in which the algorithm reestablishes the true positions and orientations of the images from an aerial mission. During this process, the algorithm generates a large number of automated tie points for 
conjugate points identiﬁed across multiple images. A bundleblock adjustment then uses these automated tie points to optimize the photo positions and orientations by generating a high number of redundant observations, which are used to derive an efﬁcient solution through a rigorous leastsquares adjustment. To provide an independent check on the accuracy of the adjustment, the algorithm includes a number of check points. Then, the oriented images are used to create a digital surface model, which provides a detailed representation of the terrain surface, including the elevations of raised objects, such as trees and buildings. The digital surface model generates a dense point cloud by matching features across multiple image pairs [66]. At this stage, a digital terrain model can be generated, which is referred to as a bareearth model. A digital terrain model is a more useful product than a surface model, because the high frequency noise associated with vegetation cover is removed. After the algorithm generates a digital terrain model, orthorectiﬁcation process can then be performed to remove the distortion in the original images. After orthorectiﬁcation process, the algorithm combines the individual images into a mosaic, to provide a seamless image of the mission area at the desired resolution [67]. Figure 13 summarizes the image processing steps for remotely sensed imagery. 
C. FLIGHT PLANNING Although each UAV mission is unique in nature, the same steps and processes are normally followed. Typically, a UAV mission starts with ﬂight planning [65]. This step depends on speciﬁc ﬂightplanning algorithm and uses a background map or satellite image as a reference to deﬁne the ﬂight area. Extra data is then included, for example, the desired 
VOLUME 7, 2019 48581 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
TABLE 4. UAV aerial sensing systems. 
ﬂying altitude, the focal length and orientation of the camera, and the desired ﬂight path. The ﬂightplanning algorithm will then ﬁnd an efﬁcient way to obtain overlapping stereo imagery covering the area of interest. During the ﬂightplanning process, the algorithm can adjust various parameters until the operator is satisﬁed with the ﬂight plan. As part of the mission planning process, the camera shutter speed settings must satisfy the different lighting conditions. If exposure time is too short, the imagery might be too dark to discriminate among all key features of interest, but if it is too long, the imagery will be blurred or will be bright. Next, the generated ﬂight plan is uploaded to the UAV autopilot. The autopilot uses the instructions contained in the ﬂight plan to ﬁnd climb rates and positional adjustments that enable the UAV to follow the planned path as closely as possible. The autopilot reads the adjustments from the global navigation and satellite system and the initial measurement unit several times per second throughout the ﬂight. After the ﬂight completion, the autopilot download a log ﬁle, this ﬁle contains information about the recorded UAV 3D placements throughout the ﬂight, as well as information about when the camera was triggered. The information in log ﬁle is used to provide initial estimates for image center positions and camera orientations, which are then used as inputs to recover the exact positions of surface points [67]. 
D. CHALLENGES 1) HOSTILE NATURAL ENVIRONMENT UAVs can be utilized to study the atmospheric composition, air quality and climate parameters, because of their ability to access hazardous environments, such as thunderstorms, hurricanes and volcanic plumes [68]. The researchers used UAVs 
for conducting environmental sampling and ocean surface temperature studies in the Arctic [69]. The authors in [70] modify and test the Aerosonde UAV in extreme weather conditions, at very low temperatures (less than −20 ◦C) to ensure a safe ﬂight in the Arctic. The aim of the work was to modify and integrate sensors onboard an Aerosonde UAV to improve the UAV’s capability for its mission under extreme weather conditions such as in the Arctic. The steps to customize the UAV for the extreme weather conditions were: 1) The avionics were isolated; 2) A fuelinjection engine was used to avoid carburetor icing; 3) A servosystem was adopted to force ice breaking over the leading edge of the airfoil. In Barrow, Alaska, the modiﬁed UAVs successfully demonstrated their capabilities to collect data for 48 hours along a 30 km2 rectangular geographical area. When a UAV collects data from a volcano plume, a rotary wing UAV was particularly beneﬁcial to hover inside the plume [71]. On the other hand, a ﬁxed wing UAV was suitable to cover longer distances and higher altitudes to sense different atmospheric layers [72]. In [73], the authors presented a successful eyepenetration reconnaissance ﬂight by Aerosonde UAV into Typhoon Longwang (2005). The 10 hours ﬂight was split into four ﬂight legs. In these ﬂight legs, the UAV measured the wind ﬁeld and provided the tangential and radial wind proﬁles from the outer perimeter into the eye of the typhoon at the 700 hPa layer. The UAV also took a vertical sounding in the eye of the typhoon and measured the strongest winds during the whole ﬂight mission [69]. 
2) CAMERA ISSUES The radiometric and geometric limitations imposed by the current generation of lightweight digital cameras are 
48582 VOLUME 7, 2019 
FIGURE 13. Image processing for a UAV remote sensing image. 
outstanding issues that need to be addressed. The current UAV digital cameras are designed for the general market and are not optimized for remote sensing applications. The current commercial instruments tend to be too bulky to be used with current lightweight UAVs, and for those that do exist, there is still a question of calibration process with conventional sensors. Spectral drawbacks include the fact that spectral response curves from cameras are usually poorly calibrated, which makes it difﬁcult to convert brightness values to radiance. However, even cameras designed speciﬁcally for UAVs may not meet the required scientiﬁc benchmarks [67]. Another drawback is that the detectors of camera may also become saturated when there are high contrasts, for example when an image covers both a dark forest and a snow covered ﬁeld. Another drawback is that many cameras are prone to vignette, where the centers of images appear brighter than the edges. This is because rays of light in the centers of the image have to pass through a less optical thickness of the camera lens, and are thus low attenuated than rays at the edges of the image. There are a number of techniques that can be 
taken into account to improve the quality of image: 1) microfourthirds cameras with ﬁxed interchangeable lenses can be used instead of having a retractable lens, which allows for much improved calibrations and image quality; 2) a simple step that can make a big difference in the processing stage is to remove images that are blurred, under or overexposed, or saturated [67]. 
3) ILLUMINATION ISSUES The shadows on a sunny day are clear and well deﬁned. These weather conditions can cause critical problems for the automated image matching algorithms used in both triangulation process and digital elevation model generation [67]. When clouds move rapidly, shaded areas can vary between images obtained during the same mission, therefore the aerial triangulation process will fail for some images, and also will result in errors for automatically generated digital elevation models. Furthermore, the automated color balancing algorithms used in the creation of image mosaics may be affected by the patterns of light and shade across images. This can cause mosaics with poor visual quality. Another generally observed illumination effect is the presence of image hotspots, where a bright points appear in the image. Hotspots occur at the antisolar point due to the effects of bidirectional reﬂectance, which is dependent on the relative placement of the image sensor and the sun [67]. 
E. RESEARCH TRENDS AND FUTURE INSIGHTS 1) MACHINE LEARNING In remote sensing, the machine learning process begins with data collection using UAVs. The next step of machine learning is data cleansing, which includes cleansing up image and/or textualbased data and making the data manageable. This step sometimes might include reducing the number of variables associated with a record. The third step is selecting the right algorithm, which includes getting acquainted with the problem we are trying to solve. There are three famous algorithms being used in remote sensing:1) Random forest; 2) Support vector machines; 3) Artiﬁcial neural networks. An algorithm is selected depending on the type of problem being solved. In some scenarios, where there are multiple features but limited records, support vector machines might work better. If there are a lot of records but less features, neural networks might yield a better prediction/classiﬁcation accuracy. Normally, several algorithms will be applied on a dataset and the one that works best is selected. In order to achieve a higher accuracy of the machine learning results, a combination of multiple algorithms can also be employed, which is referred to as ensemble. Similarly, multiple ensembles will need to be applied on a dataset, in order to select the ensemble that works the best. It is practical to choose a subset of candidate algorithms based on the type of problem and then use the narrowed down algorithms on a part of the dataset and see which one performs best. The ﬁrst challenge in machine learning is that the training segment of the dataset should have 
VOLUME 7, 2019 48583 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 14. Machine learning in UAV remote sensing. 
an unbiased representation of the whole dataset and should not be too small as compared to the testing segment of the dataset. The second challenge is overﬁtting which can happen when the dataset that has been used for algorithm training is used for evaluating the model. This will result in a very high prediction/classiﬁcation accuracy. However, if a simple modiﬁcation is performed, then the prediction/classiﬁcation accuracy takes a dip [74]. The machine learning steps utilized by UAV remote sensing are shown in Figure 14. 
2) COMBINING REMOTE SENSING AND CLOUD TECHNOLOGY Use of digital maps in risk management as well as improving data visualization and decision making process has become a standard for businesses and insurance companies. For instance, the insurance company can utilize UAV to generate a normalized difference vegetation index (NDVI) map in order to have an overview of the hail damage in corn. The geographic information system (GIS) technology in the cloud utilizes the NDVI map generated from UAV images to provide an accurate and advanced tool for assisting with crop hail damage insurance settlements in minimal time and without conﬂict, while keeping expenses low [75]. 
3) FREE SPACE OPTICAL FSO technology over an UAV can be utilized in armed forces, where military wireless communications demand for secure transmission of information on the battleﬁeld. Remote sensing UAVs can utilize this technology to disseminate large amount of images and videos to the ﬁghting forces, mostly in a real time. Near Earth observing UAVs can be utilized 
to provide high resolution images of surface contours using synthetic aperture radar and light detection and ranging. Using FSO technology, aerial sensors can also transmit the collected data to the command center via onboard satellite communication subsystem [76]. 
4) FUTURE INSIGHTS Some of the future possible directions for this application are: 
• Camera stabilization during ﬂight [67] is one of the issues that needs to be addressed, in the employment of UAV for remote sensing. 
• Battery weight and charging time are critical issues that affect the duration of UAV missions [77]. The development and incorporation of lightweight solar powered battery components of UAV can improve the duration of UAV missions and hence it reduces the complexity of ﬂight planning. 
• The temporal digital surface models produced from aerial imagery using UAV as the platform, can become practical solution in mass balance studies for example mass balance of any particular metal in sanitary landﬁlls, chloride in groundwater and sediment in a river. More speciﬁcally, the UAVbased mass balance in a debriscovered glacier was estimated at high accuracy, when using highresolution digital surface models differencing. Thus, the employment of UAV save time and money when compared with the traditional method of stake drilling into the glaciers which was laborintensive and time consuming [78], [79]. 
• The tracking methods utilized on highresolution images can be practical to ﬁnd an accurate surface velocity and glacier dynamics estimates. In [80], the authors suggested a differential band method for estimating velocities of debris and nondebris parts of the glaciers. The ondemand deployment of UAV to obtain highresolution images of a glacier has resulted in an efﬁcient tracking methods when compared with satellite imagery which depends on the satellite overpass [79]. 
• UAV remote sensing can be as a powerful technique for ﬁeldbased phenotyping with the advantages of high efﬁciency, low cost and suitability for complex environments. The adoption of multisensors coupled with advanced data analysis techniques for retrieving crop phenotypic traits have attracted great attention in recent years [81]. 
• It is expected that with the advancement of UAVs with larger payload, longer ﬂight time, lowcost sensors, improved image processing algorithms for Big data, and effective UAV regulations, there is potential for wider applications of the UAVbased ﬁeld crop phenotyping [81]. 
• UAV remote sensing for ﬁeldbased crop phenotyping provides data at high resolutions, which is needed for accurate crop parameter estimations. The derivation of crop phenotypic traits based on the spectral reﬂection information using UAV as the platform has shown good 
48584 VOLUME 7, 2019 
TABLE 5. Summary of UAV specifications, applications and technology used in construction and infrastructure inspection. 
accuracy under certain conditions. However, it showed a low accuracy in the research on the nondestructive acquisition of complex traits that were indirectly related to the spectral information [81]; 
• Image processing of UAV imagery faces a number of challenges, such as variable scales, high amounts of overlap, variable image orientations, and high amounts of relief displacement arising from the low ﬂying altitudes relative to the variation in topographic relief [67]. Researchers need to ﬁnd efﬁcient ways to overcome these challenges in future studies. 
VI. CONSTRUCTION & INFRASTRUCTURE INSPECTION As already mentioned in the market opportunity section II, the net market value of the deployment of UAV in support of construction and infrastructure inspection applications is about 45% of the total UAV market. So there is a growing interest in UAV uses in large construction projects monitoring [82] and power lines, gas pipelines and GSM towers infrastructure inspection [83], [84]. In this section, we ﬁrst present a literature review. Then, we show the uses of UAVs in support of infrastructure inspection. Finally, we present the challenges, research trends and future insights. 
A. LITERATURE REVIEW In construction and infrastructure inspection applications, UAVs can be used for realtime monitoring construction project sites [85]. So, the project managers can monitor the 
construction site using UAVs with better visibility about the project progress without any need to access the site [82]. Moreover, UAVs can also be utilized for high voltage inspection of the power transmission lines. In [86]–[89], the authors used the UAVs to perform an autonomous navigation for the power lines inspection. The UAVs was deployed to detect, inspect and diagnose the defects of the power line infrastructure. In [90], the authors designed and implemented a fully automated UAVbased system for the realtime power line inspection. More speciﬁcally, multiple images and data from UAVs were processed to identify the locations of trees and buildings near to the power lines, as well as to calculate the distance between trees, buildings and power lines. Furthermore, TIR camera was employed for bad conductivity detection in the power lines. UAVs can also be used to monitor the facilities and infrastructure, including gas, oil and water pipelines. In [84], the authors proposed the deployment of smallUAV (sUAV) equipped with a gas controller unit to detect air and gas content. The system provided a remote sensing to detect gas leaks in oil and gas pipelines. Table 5 summarizes some of the construction and infrastructure inspection applications using UAVs. More speciﬁcally, this table presents several types of UAV used in construction and infrastructure inspection applications, as well as the type of sensors deployed for each application and the corresponding UAV speciﬁcations in terms of payload, altitude and endurance. 
VOLUME 7, 2019 48585 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 15. The deployment of UAVs for construction and infrastructure inspection. 
B. THE DEPLOYMENT OF UAVs FOR CONSTRUCTION & INFRASTRUCTURE INSPECTION APPLICATIONS In this section, we present several speciﬁc example of the deployment of UAV for construction and infrastructure inspection. Figure 15 illustrates the classiﬁcation of these deployments. 
• Oil/gas and wind turbine inspection: In 2016, it was reported that Paciﬁc Gas and Electric Company (PG&E) performed drone tests to inspect its electric and gas services for better safety and reliability with the authorization from Federal Aviation Administration (FAA) [95]. The inspections focused on hardtoreach areas to detect methane leaks across its 70,000-squaremile service area. In the future, PG&E plans to extend the drone tests for storm and disaster response. Cyberhawk is one of world’s premier oil and gas company that uses UAV for inspection [96]. Furthermore, it has completed more than 5,000 structural inspections, including: 1) oil and gas; 2) wind turbine; and 3) live ﬂare. UAV inspection conducted by Cyberhawk provides a bunch of photos, as well as conducts close visual and thermal inspections of the inspected assets. Industrial SkyWorks [97] uses drones for building inspections and oil/gas inspections in North America. Furthermore, a powerful machine learning algorithm, BlueVu, has been developed to efﬁciently handle the captured data. To sum up, it provides the following solutions: – Asset inspections and data acquisition; – Advanced data processing with 2D and 3D images; – Detailed reports of the inspected asset (i.e., annotations, inspector comments, and recommendations). 
• Critical land building inspection (e.g., cell tower): AT&T owns about 65,000 cell towers that need to be inspected, repaired or installed. The video analytic team at AT&T Labs has collaborated with other forces (e.g., Intel, Qualcomm, etc.) to develop faster, better, more efﬁcient, and fully automated cell tower inspection using UAVs [98]. One of the approaches is to employ deep learning algorithm on high deﬁnition (HD) videos to detect defects and anomalies in real time. Honeywell InView inspection service has been launched to provide industrial critical structure inspections [99]. It combines the Intel Falcon 8+ UAV system with 
Honeywell aerospace and industrial technology solutions. Speciﬁcally, the Honeywell InView inspection service can achieve: 1) safety of the workers; 2) improved efﬁciency; and 3) advanced data processing. 
• Infrastructure internal inspection: Maverick has provided industrial UAV inspection services for equipment, piping, tanks, and stack internals in western Canada since 1994 [100]. It provides a dedicated services for assets internal inspection using Flyability ELIOS. Maverick also provides post data processing that analyzes data using measurement software and CAD modeling. 
• Extreme condition inspection: Bluestream offers UAV inspection services for onshore and offshore assets [101]. Its services are particularly suitable for: 1) onshore and offshore live ﬂare inspections; 2) topside, splash zone and under deck inspections; and 3) hard to access infrastructure inspection. 
C. CHALLENGES There are several challenges in utilizing UAVs for construction and infrastructure inspection: 
• Some of the challenges in using UAVs for infrastructure inspection are the limited energy, short ﬂight time and limited processing capabilities [85]. 
• Limited payload capacities for sUAVs is a big challenge. The onboard loads could include optical wavelength range camera, TIR camera, color and stereo vision cameras, different types of sensors such as gas detection, GPS, etc., [94]. 
• There is a lack of research attention to multiUAV cooperation for construction and infrastructure inspection applications. MultiUAV cooperation could provide wider inspection scope, higher error tolerance, and faster task completion time. 
• Another challenge is to allow autonomous UAVs that can maneuver an indoor environment with no access to GPS signals [102]. 
D. RESEARCH TRENDS AND FUTURE INSIGHTS 1) MACHINE LEARNING Machine learning has become an increasingly important artiﬁcial intelligence approach for UAVs to operate autonomously. Applying advanced machine learning 
48586 VOLUME 7, 2019 
FIGURE 16. Illustration of How CNNs work [58]. 
algorithms (e.g., deep learning algorithm) could help the UAV system to draw better conclusions. For example, due to its improved data processing models, deep learning algorithms could help to obtain new ﬁndings from existing data and to get more concise and reliable analysis results. UAV inspection program at AT&T uses deep learning algorithms on HD videos to detect defects and anomalies in real time [98]. Industrial SkyWorks introduces advanced machine learning algorithms to process 2D and 3D images [97]. More speciﬁcally, deep learning is useful for feature extraction from the raw measurements provided by sensors onboard a UAV (details about UAV sensor technology is presented in the Precision Agriculture section). Convolutional Neural Networks (CNNs) is one of the main deep learning feature extractors used in the area of image recognition and classiﬁcation which has been proven very effective [58]. Figure 16 illustrates one example of how CNNs works. 
2) IMAGE PROCESSING Construction and infrastructure inspection using UAVs equipped with an onboard cameras and sensors, can be efﬁciently operated when employing image processing techniques. The employment of image processing techniques allows for monitoring and assessing the construction projects, as well as performing inspection of the infrastructure such as surveying construction sites, work progress monitoring, inspection of bridges, irrigation structures monitoring, detection of construction damage and surfaces degradation [91], [103]. In [91], presented an integrated data acquisition and image processing platform mounted on UAV. It was proposed to be used for the inspection of infrastructures and real time structural health monitoring (SHM). In the proposed framework, a real time image and data will be sent to the GCS. Then the images and data will be processed using image processing unit in the GCS, to facilitate the diagnosis and inspection process. The authors proposed to combine HSV thresholding and hat transform for cracks detection on the concrete surfaces. Figure 17 presents the crack detection algorithm block diagram proposed in [91]. In [87], the authors proposed for the deployment of UAV with a visionbased system that consists of a color camera, TIR camera and a transmitter to send the captured images to the GCS. Then these images will be processed in the 
FIGURE 17. Proposed approach for crack detection algorithm [91]. 
GCS, to be used in the inspection and estimation of the real temperature of the power lines joints. In [90], a fully automatic system to determine the distance between power lines and the trees, buildings and any others obstacles was proposed. The authors designed and developed visionbased algorithms for processing the HD images that were obtained using HD camera equipped on a UAV. The captured video was sent to the computer in the GCS. At the GCS, the video was converted into the consecutive images and were further processed to calculate the distance between the power line and the obstacles. 
3) FUTURE INSIGHTS Based on the reviewed articles on construction and infrastructure inspection applications using UAVs, we suggest these future possible directions: 
• More research is required to improve UAVs battery life time to allow longer distance and to increase the UAV ﬂight time [85]. 
• For future research, it is important to propose and develop an accurate, autonomous and realtime power lines inspection approaches using UAVs, including ultrasonic sensors, TIR or color cameras, image processing and data analysis tools. More speciﬁcally, to propose and develop techniques to monitor, detect and diagnose any power lines defects automatically [104]. 
• More advanced data collection, sharing and processing algorithms for multiUAV cooperation are required, in order to achieve faster and more efﬁcient inspections. 
• The researchers should also focus on the improvement of the autonomy and safety for UAVs to maneuver in the congested and indoor environment with no or weak GPS signals [102]. 
VII. PRECISION AGRICULTURE UAVs can be utilized in precision agriculture (PA) for crop management and monitoring [105], [106], weed detection [107], irrigation scheduling [108], disease detection [109], pesticide spraying [105] and gathering data from ground sensors (moisture, soil properties, etc.,) [110]. The deployment of UAVs in PA is a costeffective and time saving technology which can help for improving crop yields, farms productivity and proﬁtability in farming systems. Moreover, UAVs facilitate agricultural management, weed monitoring, and pest damage, thereby they help to meet these challenges quickly [111]. 
VOLUME 7, 2019 48587 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
TABLE 6. A comparison between UAVs, traditional manned aircraft and satellite based system for PA. 
In this section, we ﬁrst present a literature review of UAVs in PA. Then, we show the deployment of UAV in PA. Moreover, we present the challenges, as well as research trends future insights. 
A. LITERATURE REVIEW UAVs can be efﬁciently used for small crop ﬁelds at low altitudes with higher precision and lowcost compared with traditional manned aircraft. Using UAVs for crop management can provide precise and real time data about speciﬁc location. Moreover, UAVs can offer a high resolution images for crop to help in crop management such as disease detection, monitoring agriculture, detecting the variability in crop response to irrigation, weed management and reduce the amount of herbicides [105], [106], [112]–[114]. In Table 6, a comparison between UAVs, traditional manned aircraft and satellite based system is presented in terms of system cost, endurance, availability, deployment time, coverage area, weather and working conditions, operational complexity, applications usage and ﬁnally we present some examples from the literature. Table 7 summarizes some of the precision agriculture applications using UAVs. More speciﬁcally, this table presents several types of UAV used in precision agriculture applications, as well as the type of sensors deployed for each application and the corresponding UAV speciﬁcations in terms of payload, altitude and endurance. 
B. THE DEPLOYMENT OF UAV IN PRECISION AGRICULTURE In [119], the authors presented the deployment of UAVs in precision agriculture applications as summarized in Figure 18. The deployment of UAV in precision agriculture are discussed in the following: 
• Irrigation scheduling: There are four factors that needs to be monitored, in order to determine a need for 
irrigation: 1) Availability of soil water; 2) Crop water need, which represents the amount of water needed by the various crops to grow optimally; 3) Rainfall amount; 4) Efﬁciency of the irrigation system [120]. These factors can be quantiﬁed by utilizing UAVs to measure soil moisture, plantbased temperature, and evapotranspiration. For instance, the spatial distribution of surface soil moisture can be estimated using highresolution multispectral imagery captured by a UAV, in combination with ground sampling [121]. The crop water stress index can also be estimated, in order to determine water stressed areas by utilizing thermal UAV images [108]. 
• Plant disease detection: In the U.S., it is estimated that crop losses caused by plant diseases result in about $33 billion in lost revenue every year [122]. UAVs can be used for thermal remote sensing to monitor the spatial and temporal patterns of crop diseases presymptomatically during various disease development phases and hence farmers may reduce the crop losses. For instance, aerial thermal images can be used to detect early stage development of soilborne fungus [123]. 
• Soil texture mapping: Some soil properties, such as soil texture, can be an indicative of soil quality which in turn inﬂuences crop productivity. Thus, UAV thermal images can be utilized to quantify soil texture at a regional scale by measuring the differences in land surface temperature under a relatively homogeneous climatic condition [124], [125]. 
• Residue cover and tillage mapping: Crop residues is essential in soil conservation by providing a protective layer on agricultural ﬁelds that shields soil from wind and water. Accurate assessment of crop residue is necessary for proper implementation of conservation tillage practices [126]. In [127], the authors demonstrated that aerial thermal images can explain more than 95% of the variability in crop residue cover amount compared to 77% using visible and near IR images. 
• Field tile mapping: Tile drainage systems remove excess water from the ﬁelds and hence it provides ecological and economic beneﬁts [128]. An efﬁcient monitoring of tile drains can help farmers and natural resource managers to better mitigate any adverse environmental and economic impacts. By measuring temperature differences within a ﬁeld, thermal UAV images can provide additional opportunities in ﬁeld tile mapping [129]. 
• Crop maturity mapping: UAVs can be a practical technology to monitor crop maturity for determining the harvesting time, particularly when the entire area cannot be harvested in the time available. For instance, UAV visual and infrared images from barley trial areas at Lundavra, Australia were used to map two primary growth stages of barley and demonstrated classiﬁcation accuracy of 83.5% [130]. 
• Crop yield mapping: Farmers require accurate, early estimation of crop yield for a number of reasons, including crop insurance, planning of harvest and storage 
48588 VOLUME 7, 2019 
TABLE 7. Summary of UAV specifications, applications and technology used in precision agriculture. 
requirements, and cash ﬂow budgeting. In [131], UAV images were utilized to estimate yield and total biomass of rice crop in Thailand. In [132], UAV images were also utilized to predict corn grain yields in the early to midseason crop growth stages in Germany. The authors in [133] presented several types of sensor that were used in UAVbased precision agriculture, as summarized in Table 8. 
C. CHALLENGES There are several challenges in the deployment of UAVs in PA: 
• Thermal cameras have poor resolution and they are expensive. The price ranges from $2000-$50,000 
depending on the quality and functionality, and the majority of thermal cameras have resolution of 640 pixels by 480 pixels [119]. 
• Thermal aerial images can be affected by many factors, such as the moisture in the atmosphere, shooting distance, and other sources of emitted and reﬂected thermal radiation. Therefore, calibration of aerial sensors is critical to extract scientiﬁcally reliable surface temperatures of objects [119]. 
• Temperature readings through aerial sensors can be affected by crop growth stages. At the beginning of the growing season, when plants are small and sparse, temperature measurements can be inﬂuenced by reﬂectance from the soil surface [119]. 
VOLUME 7, 2019 48589 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 18. The deployment of UAVs in precision agriculture applications. 
• In the event of adverse weather, such as extreme wind, rain and storms, there is a big challenge of UAVs deployment in PA applications. In these conditions, UAVs may fail in their missions. Therefore, small UAVs cannot operate in extreme weather conditions and even cannot take readings during these conditions. 
• One of the key challenges is the ability of lightweight UAVs to carry a highweight payload, which will limit the ability of UAVs to carry an integrated system that includes multiple sensors, highresolution and thermal cameras [134]. 
• UAVs have short battery life time, usually less than 1 hour. Therefore, the power limitations of UAVs is one of the challenges of using UAVs in PA. Another challenge, when UAVs are used to cover large areas, is that it needs to return many times to the charging station for recharging [105], [109], [112]. 
D. RESEARCH TRENDS AND FUTURE INSIGHTS 1) MACHINE LEARNING The next generation of UAVs will utilize the new technologies in precision agriculture, such as machine learning. Hummingbird is a UAVenabled data and imagery analytics business for precision agriculture [135]. It utilizes machine learning to deliver actionable insights on crop health directly to the ﬁeld. The process ﬂow begins by performing UAV surveys on the agricultural land at critical decisionmaking points in the growing season. Then, UAV images is uploaded to the cloud, before being processed with machine learning techniques. Finally, the mobile app and web based platform provides farmers with actionable insights on crop health. The advantages of utilizing UAVs with machine learning technology in precision agriculture are: 1) Early detection of crop diseases; 2) Precision weed mapping; 3) Accurate yield forecasting; 4) Nutrient optimization and planting; 5) Plant growth monitoring [135]. 
2) IMAGE PROCESSING UAVbased systems can be used in PA to acquire highresolution images for farms, crops and rangeland. It can also be utilized as an alternative to satellite and manned aircraft imaging system. Processing of these images is one of the most rapidly developing ﬁelds in PA applications. 
The Vegetation Indices (VI) can be produced using image processing techniques for the prediction of the agricultural crop yield, agricultural analysis, crop and weed management and in diseases detection. Moreover, the VIs can be used to create vigor maps of the speciﬁcsite and for vegetative covers evaluation using spectral measurements [118], [136]. In [113], [118], [136], [137], most of the VIs found in the literature have been summarized and discussed. Some of these VIs are: 
• Green Vegetation Index (GVI). 
• Normalized Difference Vegetation Index (NDVI). 
• Green Normalized Difference Vegetation index (GNDVI). 
• Soil Adjusted Vegetation Index (SAVI). 
• Perpendicular Vegetation Index (PVI). 
• Enhanced Vegetation Index (EVI). Many researchers utilized VIs that are obtained using image processing techniques in PA. The authors in [118] presented agricultural analysis for vineyards and tomatoes crops. A UAV with Tetracam multispectral camera was deployed to take aerial image for crop. These images were processed using PixelWrench2 (PW2) software which came with the camera and it will be exported in a triband TIFF image. Then from the contents of this images VIs such as NDVI [138], GNDVI [139], SAVI [140] can be extracted. In [141], the authors used UAVs to take aerial images for rangeland to identify rangeland VI for different types of plant in Southwestern Idaho. In the study, image processing and analysis was performed in three steps as shown in Figure 19. More speciﬁcally, the three steps were: 
• OrthoRectiﬁcation and mosaicing of UAV imagery [141]. A semiautomated orthorectiﬁcation approach were developed using PreSync procedure [142]. 
• Clipping of the mosaic to the 50m × 50m plot areas measured on the ground. In this step, image classiﬁcation and segmentation was performed using an objectbased image analysis (OBIA) program with Deﬁniens Developer 7.0 [143], where the acquisition image was segmented into homogeneous areas [141]. 
• Image classiﬁcation: In this step, hierarchical classiﬁcation scheme along with a rule based masking approach were used [141]. 
48590 VOLUME 7, 2019 
TABLE 8. UAV sensors in precision agriculture applications. 
FIGURE 19. Steps of image processing and analysis for identifying rangeland VI [141]. 
3) FUTURE INSIGHTS Based on the reviewed articles focusing on PA using UAVs, we suggest these future possible directions: 
• With relaxed ﬂight regulations and improvement in image processing, georeferencing, mosaicing, and classiﬁcation algorithms, UAV can provide a great potential for soil and crop monitoring [119], [144]. 
• The next generation of UAV sensors, such as 3p sensor [145], can provide onboard image processing and in-ﬁeld analytic capabilities, which can give farmers instant insights in the ﬁeld, without the need for cellular connectivity and cloud connection [146]. 
• More precision agricultural researches are required towards designing and implementing special types of cameras and sensors onboard UAVs, which have the ability of remote crop monitoring and detection of soil and other agricultural characteristics in real time scenarios [111]. 
• UAVs can be used for obtaining highresolution images for plants to study plant diseases and traits using image processing techniques [147]. 
VIII. DELIVERY OF GOODS UAVs can be used to transport food, packages and other goods [148]–[151] as shown in Figure 20. In healthcare 
ﬁeld, ambulance drones can deliver medicines, immunizations, and blood samples, into and out of unreachable places. They can rapidly transport medical instruments in the crucial few minutes after cardiac arrests. They can also include live video streaming services allowing paramedics to remotely observe and instruct onscene individuals on how to use the medical instruments [152]. In July 2015, the Federal Aviation Administration (FAA) approved the ﬁrst delivery of medical supplies using UAVs at Wise, Virginia [153]. With the rapid demise of snail mail and the massive growth of eCommerce, postal companies have been forced to ﬁnd new methods to expand beyond their traditional mail delivery business models. Different postal companies have undertaken various UAV trials to test the feasibility and proﬁtability of UAV delivery services [154]. Quadrotor drones can be used for goods delivery. Delivery drones examples, commercial delivery drone services, and the future technology of delivery drones can be found in [154]. In this section, we present the UAVbased goods delivery system and its challenges as shown in Figure 21. 
A. UAVBASED GOODS DELIVERY SYSTEM In UAVbased goods delivery system, a UAV is capable of traveling between a pick up location and a delivery location. The UAV is equipped with control processor and GPS module. It receives a transaction packet for the delivery operation that contains the GPS coordinates and the identiﬁer of a package docking device associated with the order. Upon arrival of a UAV at the delivery location, the control processor checks if the identiﬁer of a package docking device matches the device identiﬁer in the transaction packet, performs the package transfer operation, and sends conﬁrmation of completion of the operation to an originator of the order [155]. If the identiﬁer of a package docking device at the delivery point does not match the device identiﬁer in the transaction packet, the UAV communication components transmit a request over a shortrange network such as bluetooth or WiFi. The request may contain the device identiﬁer, or network address of the package docking device. Under the assumption that the package docking device has not moved outside of the 
VOLUME 7, 2019 48591 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 20. Autonomous ECommerce. 
FIGURE 21. UAV delivery applications and challenges. 
range of UAV communication, the package docking device having the network address transmits a signal containing the address of a new location. The package docking device may then transmit updated GPS coordinates to the UAV. The UAV is rerouted to the new address based on the updated GPS location [155]. In Figure 22, we present the ﬂowchart of UAV delivery system. 
B. CHALLENGES 1) LEGISLATION In the United States, the FAA regulation blocked all attempts at commercial use of UAVs, such as the Tacocopter company for food delivery [156]. As of 2015, delivering of packages with UAVs in the United States is not allowed [157]. Under current rules, companies are permitted to operate commercial UAVs in the United States, but only under certain conditions. Their UAVs must be ﬂown within a pilot’s line of sight and those pilots must get licenses. Commercial operators also are restricted to ﬂy their UAVs during daylight hours. Their UAVs are limited in size, altitude and speed, and UAVs are generally not allowed to ﬂy over people or to operate beyond visual line of sight [158]. 
FIGURE 22. UAV delivery of goods system. 
2) LIABILITY INSURANCE Some UAVs can weigh up to 25 kg and travel at speeds approaching 45 m/s. A number of media reports describes severe lacerations, eye loss, and soft tissue injuries caused by UAV accidents. In addition to the risk of injuries or property damage from a UAV crash, ubiquitous UAV uses also create other types of accidents, such as automobile accidents due to distraction from low-ﬂying UAVs, injuries caused by dropped cargo, liability for damaged goods, or accidents resulting from a UAV’s interference with aircraft. Liability for UAV use, however, is not limited to personal injury or property 
48592 VOLUME 7, 2019 
FIGURE 23. Amazon airspace model for the safe integration of UAV systems. 
damage claims, UAVs present an enormous threat to individual privacy [159]. 
3) THEFT The main concerns of utilizing UAVs for data gathering and wireless delivery, are cyber liability and hacking. UAVs that are used to gather sensitive information might become targets for malicious software seeking to steal data. A hacker might even usurp control of the UAV itself for the purpose of illegal activities, such as theft of its cargo or stored data, invasion of privacy, or smuggling. Liability for utilizing a UAV does not ﬁt neatly into the coverage offered by the types of liability insurance policies that most individuals and businesses currently possess [159]. 
4) WEATHER Similar to light aircrafts, UAVs cannot hover in all weather conditions. The capability to resist certain weather conditions is determined by the speciﬁcations of the UAV [160]. In pre- ﬂight planning, it is clear that advanced weather data will play an essential role in ensuring that UAVs can ﬂy their weathersensitive missions safely and efﬁciently to deliver commercial goods. During ﬂight operations, weather data affects ﬂight direction, path elevation, operation duration and other in-ﬂight variables. Wind speeds in particular are an essential component for a smooth UAVbased operations and thus should be factored in the operation planning and deployment phases. In post-ﬂight analysis, by analyzing data through advanced weather visualization dashboards, we can improve the UAV ﬂight operations to ensure future mission success [161]. 
5) AIR TRAFFIC CONTROL Air trafﬁc control is an essential condition for coordinating large ﬂeets of UAVs, where regulators will not permit 
largescale UAV delivery missions without such systems in place [162]. Amazon designs an airspace model for the safe integration of UAV systems as shown in Figure 23. In this proposed model, the ‘‘lowspeed localized trafﬁc’’ will be reserved for: 1) Terminal nontransit operations such as surveying, videography and inspection; 2) Operations for lesserequipped UAVs, e.g. ones without sophisticated senseandavoid technology. The ‘‘highspeed transit’’ will be reserved for well equipped UAVs as determined by the relevant performance standards and rules. The ‘‘no ﬂy zone’’ will serve as a restricted area in which UAV operators will not be allowed to ﬂy, except in emergencies. Finally, the ‘‘prede- ﬁned low risk locations’’ will include areas like designated academy of model aeronautics airﬁelds, altitude and equipage restrictions in these locations will be established in advance by aviation authorities [163]. 
C. RESEARCH TRENDS AND FUTURE INSIGHTS 1) MACHINE LEARNING With machine learning, UAVs can ﬂy autonomously without knowing the objects they may encounter which is important for largescale UAV delivery missions. Qualcomm’s tech shows more advanced computing that can actually understand what the UAV encountered in midair and creates a ﬂight route. They show how the UAV processing and decisionmaking technology is nimble enough to allow UAVs to operate in unpredictable settings without using any GPS. All of the UAV computational tasks, like the machine learning and ﬂight control, happens on 12 grams processor without any offboard computing [164]. Some of challenges are: 1) We need to put a lot of effort to ﬁnd efﬁcient methods to do unsupervised learning, where collecting large amounts of unlabeled data is nowadays becoming economically less expensive [58]; 2) Realworld problems with high number of states can turn the problem intractable with current 
VOLUME 7, 2019 48593 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
techniques, severely limiting the development of real applications. An efﬁcient method for coping with these types of problems remains as an unsolved challenge [58]. 
2) NAVIGATION SYSTEM Researchers are developing navigation systems that do not utilize GPS signals. This could enable UAVs to ﬂy autonomously over places where GPS signals are unavailable or unreliable. Whether delivering goods to remote places or handling emergency tasks in hazardous conditions, this type of capability could signiﬁcantly expand UAVs’ usefulness [165]. Researchers from GPU maker Nvidia are currently working on a navigation system that utilizes visual recognition and computer learning to make sure UAVs don’t get lost. The team believes that the system has already managed the most stable GPSfree ﬂight to date [166]. Some of challenges are: 1) The design of computing devices with lowpower consumption, particularly GPUs, is a challenge and active working ﬁeld for embedded hardware developers [58]; 2) While a few UAVs can already travel without a UAV operators directing their routes, this technology is still emerging. Over the next few years, systemfailure responses, adaptive routing, and handoffs between user and UAV controllers should be improved [167]. 
3) FUTURE INSIGHTS Some of the future possible directions for this application are: 
• The energy density of lithiumion batteries is improving by 5%-8% per year, and their lifetime is expected to double by 2025. These improvements will make commercial UAVs able to hover for more than an hour without recharging, enabling UAVs to deliver more goods [167]. 
• Detectandavoid systems which help UAVs to avoid collisions and obstacles, are still in development, with strong solutions expected to emerge by 2025 [167]. 
• UAVs currently travel below the height of commercial aircraft due to the collision potential. The methods that can track UAVs and communicate with airtrafﬁccontrol systems for typical aircraft are not expected to be available before 2027, making highaltitude missions impossible until that time [167]. 
• To make UAV delivery practical, automation research is required to address UAVs design. UAVs design covers creating aerial vehicles that are practical, can be used in a wide range of conditions, and whose capability rivals that of commercial airliners; this is a signiﬁcant undertaking that will need many experiments, ingenuity and contributions from experts in diverse areas [168]. 
• More research is needed to address localization and navigation. The localization and navigation problems may seem like simple problems due to the many GPS systems that already exist, but to make drone delivery practical in different operating conditions, the integration of low cost sensors and localization systems is required [168]. 
• More research is needed to address UAVs coordination. 
Thousands of UAV operators in the air, utilizing the same resources such as charging stations and operating frequency, will need robust coordination which can be studied by simulation [168]. 
IX. REALTIME MONITORING OF ROAD TRAFFIC Automation of the overall transportation system cannot be automated through vehicles only [169]. In fact, other components of the endtoend transportation system, such as tasks of ﬁeld support teams, trafﬁc police, road surveyors, and rescue teams, also need to be automated. Smart and reliable UAVs can help in the automation of these components. UAVs have been considered as a novel trafﬁc monitoring technology to collect information about trafﬁc conditions on roads. Compared to the traditional monitoring devices such as loop detectors, surveillance video cameras and microwave sensors, UAVs are costeffective, and can monitor large continuous road segments or focus on a speciﬁc road segment [170]. Data generated by sensor technologies are somewhat aggregated in nature and hence do not support an effective record of individual vehicle tracks in the trafﬁc stream. This restricts the application of these data in individual driving behavior analysis as well as calibrating and validating simulation models [171]. Moreover, disasters may damage computing, communications infrastructure or power systems. Such failures can result in a complete lack of the ability to control and collect data about the transportation network [172]. 
A. LITERATURE REVIEW UAVs are getting accepted as a method to hasten the gathering of geographic surveillance data [173]. As autonomous and connected vehicles become popular, many new services and applications of UAVs will be enabled [169]. Recognition of moving vehicles using UAVs is still a challenging problem. Moving vehicle detection methods depend on the accuracy of image registration methods, since the background in the UAV surveillance platform changes frequently. Accurate image registration methods require extensive computing power, which affects the realtime capability of these methods [174]. In [174], Qu et al. studied the problem of moving vehicle detection using UAV cameras. In their proposed approach, they used convolutional neural networks to identify vehicles more accurately and in realtime. The proposed approach consists of three steps to detect moving vehicles: First, adjacent frames are matched. Then, frame pixels are classiﬁed as background or candidate targets. Finally, a deep convolutional neural network is trained over candidate targets to classify them into vehicles or background. They achieved detection accuracy of around 90% when evaluating their method using the CATEC UAV dataset. In [175], the authors introduce a vehicle detection and tracking system based on imagery data collected by a UAV. This system uses consecutive frames to generate the vehicle’s dynamic information, such as positions and velocities over time. Four major modules have been developed in this study: 
48594 VOLUME 7, 2019 
image registration, image feature extraction, vehicle shape detecting, and vehicle tracking. Some unique features have been introduced into this system to customize the vehicle and trafﬁc ﬂow and use them together in multiple contiguous images to increase the system’s accuracy of detecting and tracking vehicles. A framework is presented in [170] to support realtime and accurate collection of trafﬁc ﬂow parameters, including speed, density, and volume, in two travel directions simultaneously. The proposed framework consists of the following four features: (1) A framework for estimating multidirectional trafﬁc ﬂow parameters from aerial videos (2) A method combining the KanadeLucasTomasi (KLT) tracker, kmeans clustering, and connected graphs for vehicle detection and counting (3) Identifying trafﬁc streams and extracting trafﬁc information in a realtime manner (4) The system works in daytime and nighttime settings, and is not sensitive to UAV movements (i.e., regular movement, vibration, drifting, changes in speed, and hovering). A challenge that this framework faces is that their algorithm sometimes recognizes trucks, buses, and other large/heavy vehicles as multiple passenger cars. A realtime framework for the detection and tracking of a speciﬁc road segment using low and midaltitude UAV video feeds was presented in [176]. This framework can be used for autonomous navigation, inspection, trafﬁc surveillance and monitoring. For road detection, they utilize the GraphCut algorithm because of its efﬁcient and powerful segmentation performance in 2-D color images. For road tracking, they develop a tracking technique based on homography alignment to adjust one image plane to another when the moving camera takes images of a planar scene. In [172], the authors develop a processing procedure for fast vehicle detection, which consists of three stages; preclassiﬁcation with a boosted classiﬁer, blob detection, and ﬁnal classiﬁcation using SVM. In [177], the authors propose to integrate collected video data from UAVs with trafﬁc simulation models to enhance realtime trafﬁc monitoring and control. This can be performed by transforming collected video data into ‘useful trafﬁc measures’ to generate essential statistical proﬁles of trafﬁc patterns, including trafﬁc parameters such as meanspeed, density, volume, turning ratio, etc. However, a main issue with this approach is the limitation of ﬂying time for UAVs which could hover to obtain data for a few hours a day. The work in [178] addresses the security issues of road trafﬁc monitoring systems using UAVs. In this work, the role of a UAV in a road trafﬁc management system is analyzed and various situational security issues that occur in trafﬁc management are mitigated. In their proposed approach, the UAV’s intelligent systems analyze realtime trafﬁc as well as security issues and provide the appropriate mitigation commands to the trafﬁc management control center for rerouting. Instead of image processing, the authors used sensor networks and graph theory for representing the road network. They also devised different situational security scenarios to assess the 
road trafﬁc management. For example, a car without an RFID tag that enters a defense area or government building area is considered a potential security risk. Based on a research study by Kansas Department of Transportation (KDOT) [179], the use of UAVs for KDOT’s operations could lead to improved safety, efﬁciency, as well as reduced costs. The study also recommends the use of UAVs in a range of applications including bridge inspection, radio tower inspection, surveying, road mapping, highmast light tower inspection, stockpile measurement, and aerial photography. However, the study indicates that UAVs are not recommended to replace the current methods of trafﬁc data collection in KDOT operations which rely on different kinds of sensors (e.g., weight, loop, piezo). However, UAVs can complement existing data collection projects to gather data in small increments of time in certain trafﬁc areas. Based on their survey, battery life and ﬂight time of UAVs may limit the samples of trafﬁc data. For example, current UAV technology cannot collect 24-hour continuous data. Chen et al. [180] proposed a video relay scheme for trafﬁc surveillance systems using UAVs. The proposed communications scheme is straightforward to implement because of the typical availability of mobile broadband along highways. Their results show that the proposed scheme can transfer quality videos to the trafﬁc management center in realtime. They also implemented two types of data communications schemes to transmit captured videos through existing public mobile broadband networks: (1) Video stream delivered directly to clients. (2) Video stream delivered to clients through a server. In their experiments, they were able to transmit video signals with an image size of 320 × 280, at a rate of 112 Kbps, and 15 frames per second. In [181], a platform which operates autonomously and delivers highquality video imagery and sensor data in realtime is utilized. In their scenario, the authors employ a 10 lbs aircraft to ﬂy up to 6 hours with a telemetry range of 1 mile and payload capacity of 4 lbs. Their system consists of ﬁve components: (1) a GPS signal receiver (2) a radio control transmitter (3) a modem for ﬂight data (4) a PC to display the UAV on a map. (5) a realtime video downlink. Apeltauer et al. [182] present an approach for moving vehicle detection and tracking through the intersection of aerial images captured by UAVs. Overall, the system follows three steps: preprocessing, vehicle detection, and tracking. For preprocessing, images are undistorted georegistered against a userselected reference frame. For the detection step, the boosting technique is used to improve the training phase which employs Multiscale Block Local Binary Patterns (MBLBP). Finally for tracking, the system uses a set of Bootstrap particle ﬁlters, one per vehicle. An improved vehicle detection method based on Faster RCNNs is proposed in [183]. The overall vehicle detection method is illustrated in Figure 24. For training, the method crops the original largescale images into segments and augments the number of image segments with four angles (i.e., 0, 90, 180, and 270). Then, all the training image blocks that 
VOLUME 7, 2019 48595 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 24. Proposed vehicle detection framework in [183]. 
constitute the HRPN input are processed to produce candidate region boxes, scores and corresponding hyper features. Finally, the results of the HRPN are used to train a cascade of boosted classiﬁers, and a ﬁnal classiﬁer is obtained. For testing, a largescale testing image is cropped into image blocks. Then, HRPN takes these image blocks as input and generates potential outputs as well as hyper feature maps. The ﬁnal classiﬁer checks these boxes using hyper features. Finally, all the detection results of segments are gathered to integrate the original image. The authors tested their method on UAV images successfully. 
B. USE CASES The major applications of UAVs in transportation include security surveillance, trafﬁc monitoring, inspection of road construction projects, and survey of trafﬁc, rivers, coastlines, pipelines, etc. [176]. Some of the ITS applications that can be enabled by UAVs are as follows: 
• Flying Accident Report Agents: Rescue teams can use UAVs to quickly reach accident locations. Flying accident report agents can also be used to deliver ﬁrst aid kits to accident locations while waiting for rescue teams to arrive [169]. 
• Flying Police Eyes: UAVs can be used to ﬂy over different road segments in order to stop vehicle for trafﬁc violations. The UAV can change the trafﬁc light in front of the vehicle to stop it or relay a message to a speciﬁc vehicle to stop [169]. 
• Flying Roadside Unit: A UAV can be complemented with DSRC to enable a ﬂying RSU. The ﬂying RSU can ﬂy to a speciﬁc position to execute a speciﬁc application. For example, consider an accident on the highway at a speciﬁc segment that is not equipped with any RSU. 
Then the trafﬁc management center can activate a UAV to ﬂy to the accident location and land at the proper location to broadcast the information and warn all approaching vehicles about a speciﬁc incident [169]. 
• Behavior Recognition Method: UAVs can be used to recognize suspicious or abnormal behavior of ground vehicles moving along with the road trafﬁc [184]. 
• Monitor Pedestrian Trafﬁc: Sutheerakul et al. [185] used UAVs as an alternative data collection technique to monitor pedestrian trafﬁc and evaluate demand and supply characteristics. In fact, they classiﬁed their collected data into four areas: the measurement of pedestrian demand, pedestrian characteristics, trafﬁc ﬂow characteristics, and walking facilities and environment. 
• Flying Dynamic Trafﬁc Signals [169]. UAVs may also be employed for a wide range of transportation operations and planning applications such as following [186]: 
• Incident response. 
• Monitor freeway conditions. 
• Coordination among a network of trafﬁc signals. 
• Traveler information. 
• Emergency vehicle guidance. 
• Measurement of typical roadway usage. 
• Monitor parking lot utilization. 
• Estimate OriginDestination (OD) ﬂows. Figures 25, 26, and 27 illustrate different applications usecases of UAVs in smart cities. 
C. LEGISLATION The Federal Aviation Administration (FAA) approves the civil use of UAVs [187]. They can be utilized for public use provided that the UAVs are ﬂown at a certain altitude. 
48596 VOLUME 7, 2019 
TABLE 9. Summary of related literature. 
FIGURE 25. A UAV is used by police to catch traffic violators [169]. 
FIGURE 26. A UAV is used as a flying RSU that broadcasts a warning about road hazards that have been detected in an area not preequipped with an RSU (Flying Roadside Units) [169]. 
FIGURE 27. A UAV is used to provide the rescue team an advance report prior to reaching the incident scene [169]. 
For maintaining the safety of manned aircrafts and the public, the FAA in the United States has developed rules to regulate the use of small UAVs [188]. For example, the FAA requires small Unmanned Aircraft Systems (UAS) that weigh more than 0.55 lbs and below 55 lbs to be registered in their system. Regulations are broadly organized into two categories; namely, prescriptive regulations and 
TABLE 10. Rules for Operating a UAS. *These Rules are Subject to Waiver. 
performancebased regulations (PBRs). Prescriptive regulations deﬁne what must not be done whereas PBRs indicate what must be attained. 20% of FAA regulations are PBRs [189]. Table 10 describes the rules for operating UAS in the US [187]. When violating the FAA regulations, owners of drones can face civil and criminal penalties [188]. The FAA has also provided a smartphone application B4UFLY5 which provides drone users important information about the limitations that pertain to the location they where drone is being operated. The ‘‘Know Before You Fly’’ campaign by the FAA aims to educate the public about UAV safety and responsibilities. For civil operations, the FAA authorization can be received either through Section 333 Exemption (i.e., by issuing a COA), or through a Special Airworthiness Certiﬁcate (SAC) 
VOLUME 7, 2019 48597 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
in which applicants describe their design, software development, control, along with how and where they intend to ﬂy. The FAA also enforces their regulations along with Law Enforcement Agencies (LEAs) to deter, detect, investigate, and stop unauthorized and unsafe UAV operations [188]. Overall, the FAA regulations for small UAVs require ﬂying under 400 feet without obstacles in their vicinity, such that operators maintain a line of sight with the operated UAV at all times. It also requires UAVs not to ﬂy within 5 miles from an airport unless permission is received from the airport and the control tower; thus, avoiding the endangerment of people and aircrafts. Other FAA regulations require drones not to be operated over public infrastructure (e.g., stadiums) as that may pose dangers to the public. The Federal UAS regulations ﬁnal rule requires drone pilots to keep unmanned aircrafts within visual line of sight and operations are only allowed during daylight and during halflight if the drone is equipped with ‘‘anticollision lights.’’ The new regulations also initiate height and speed constraints and other operational limits, such as prohibiting ﬂights over unprotected people on the ground who are not directly participating in the UAS operation. There is a process through which users can apply to have some of these restrictions waived, while those users currently operating under section 333 exemptions (which allowed commercial use to take place prior to the new rule) are still able to operate depending on their exemptions [190]. In Section 107.51 of the FAA regulations [191], it is mentioned that a remote pilot in command and the person manipulating the ﬂight controls of the small unmanned aircraft system must consent with all of the following operating limitations when operating a small unmanned aircraft system: 
• The ground speed of a small unmanned aircraft may not exceed 87 knots (i.e., 100 miles per hour). 
• The altitude of the small unmanned aircraft cannot be higher than 400 feet above ground level, unless the small unmanned aircraft: (1) Is ﬂown within a 400-foot radius of a structure; and (2) Does not ﬂy higher than 400 feet above the structure’s immediate uppermost limit. 
• The minimum ﬂight visibility, as observed from the location of the control station must be no less than 3 statute miles. 
For critical infrastructure, a legislation has been developed to protect such infrastructure from rogue drone operators. UAVs should not be close to such places when these critical infrastructure exist. The classiﬁcation of critical infrastructure differs by state, but generally includes facilities such as petroleum reﬁneries, chemical manufacturing facilities, pipelines, waste water treatment facilities, power generation stations, electric utilities, chemical or rubber manufacturing facilities, and other similar facilities [192]. 
D. CHALLENGES AND FUTURE INSIGHTS There are several challenges and needed future extensions to facilitate the use of UAVs in support of ITS applications: 
• One of the important challenges is to preserve the privacy of sensitive information (e.g., location) from other vehicles and drones [169]. Since usually there is no encryption on UAVs onboard chips, they can be hijacked and subjected to maninmiddle attacks originating up to two kilometers away [188]. 
• countries should devise registration mechanisms for UAVs operated on their geographic areas. Commercial airplanes and their navigation might be affected by UAVs. So countries must implement rules and regulations for their proper use [193]. 
• Developing precise coordination algorithms is one of the challenges that need to be considered to enable ITS UAVs [169]. 
• Wireless sensors can be utilized to smooth the operations of UAVs. For example, surveillance and live feeds from wireless sensors can be developed for control trafﬁc systems [193]. 
• Data fusion of information from diverse sensors, automating image data compression, and stitching of aerial imagery are required techniques [193]. 
• Enabling teams of operators to control the UAV and retrieve imagery and sensor information in realtime. To achieve this goal, the development of networkcentric infrastructure is required [193]. 
• Limited energy, processing capabilities, and signal transmission range [169] are also some of the main issues in UAVs that require more development to contribute to the maturity of the UAV technology 
• UAVs have slower speeds compared to vehicles driving on highways. However, a possible solution might entail changing the regulations to allow UAVs to ﬂy at higher altitudes. Such regulations would allow UAVs to beneﬁt from high views to compensate the limitation in their speed. Finding the optimal altitude change in support of ITS applications is a research challenge [169]. 
• Battery technology that allow UAVs to achieve long operational times beyond half an hour is another challenge. Several UAVs can ﬂy together and form a swarm, by which they could overcome their individual limitations in terms of energy efﬁciency through optimal coordination algorithms [169]. Another alternative is for UAVs to use recharge stations. UAVs can be recharged while on the ground, or their depleted batteries can be replaced to minimize interruptions to their service. To this end, deployment of UAVs, recharge stations, and ground RSUs jointly becomes an interesting but complicated optimization problem [169]. 
• The detection of multiple vehicles at the same time is another challenge. In [194], Zhang et al. developed several computervision based algorithms and applied them to extract the background image from a video sequence, identify vehicles, detect and remove shadows, and compute pixelbased vehicle lengths for classiﬁcation. 
• Truly autonomous operations of UAV swarms are a big challenge, since they need to recognize other 
48598 VOLUME 7, 2019 
UAVs, humans and obstacles to avoid collisions. Therefore, the development of swarm intelligence algorithms that fuse data from diverse sources including location sensors, weather sensors, accelerometers, gyoscopes, RADARs, LIDARs, etc. are needed. 
X. SURVEILLANCE APPLICATIONS OF UAVs In this section, we ﬁrst present a detailed literature review of surveillance applications of UAVs. Then, based on the review, we summarize the advantages, disadvantages and important concerns of UAV uses in surveillance. Also, we discuss several takeaway lessons for researchers and practitioners in the area, which we believe will help guide the development of effective UAV surveillance applications. 
A. LITERATURE REVIEW The report in [195] discusses the advantages and disadvantages of employing UAVs along US borders for surveillance and introduces several important issues for the Congress. The advantages include: (1) The usage of UAVs for border surveillance improves the coverage along the remote border sections of the U.S.; and (2) The UAVs provide a much wider coverage than current approaches for border surveillance (e.g., border agents on patrol, stationary surveillance equipment, etc.). On the other hand, the disadvantages include: (1) The high accident rates of UAVs (e.g., inclement weather conditions); and (2) The signiﬁcant operating costs of a UAV, which are often more than double compared to the costs of a manned aircraft. The report also highlights other issues of UAVs that should be considered by the Congress including: UAV effectiveness, lack of information, coordination with USBP agents, safety concerns, and implementation details. This report provides a clear view of the advantages, disadvantages and important concerns for the border surveillance using UAVs. However, it lacks details/examples on each aspect discussed (i.e., only high level summaries are included). Also, other important aspects are missing, for example, the robustness beneﬁts (e.g., human error tolerance) of using the UAVs compared to manned aircraft. The authors in [196] present a multiUAV coordination system in the framework of the AWARE project (distributed decisionmaking architecture suitable for multiUAV coordination). Generally speaking, a set of tasks are generated and allocated to perform a given mission in an efﬁcient manner according to planning strategies. These tasks are subgoals that are necessary for achieving the overall goal of the system, and that can be achieved independent of other subgoals. The key issues include: 
• Task allocation: determines the place (i.e., in which UAV node) that each task should be executed to optimize the performance and to ensure appropriate cooperation among different UAVs. 
• Operative perception: generates and maintains a consistent view of all operating UAVs (e.g., number of sensors equipped). 
In addition, several types of ﬁeld experiments are conducted, including: (1) MultiUAV cooperative area surveillance; (2) Wireless sensor deployment; (3) Fire threat conﬁrmation and extinguishing; (4) Load transportation and deployment with single and multiple UAVs; and (5) People tracking. To sum up, this paper presents details of the proposed algorithm and shows the results of several real ﬁled experiments. However, it lacks detailed analysis or formal proof of the proposed approach to demonstrate its correctness and effectiveness. In [197], the authors perform analysis for UAVs deployments within warzones (i.e., Afghanistan, Iraq, and Pakistan), borderzones and urban areas in the USA. The analysis highlights the beneﬁts of UAVs in such scenarios which include: 
• Safety: Drones insulates operators and allies from direct harm and subjects targets to ‘precise’ attack. 
• Robustness: Drones reduce human errors (e.g., moral ambiguity) from political, cultural, and geographical contexts. Most importantly, the analysis uncovers a major limitation in UAV surveillance practices. The usage of drones for surveillance has difﬁculties in exact target identiﬁcation and control in risk societies. Potential blurred identities include: (1) insurgent and civilian; (2) criminal and undocumented migrant; and (3) remotely located pilot and frontline soldier. To sum up, this paper puts more focus on safety and robustness beneﬁts of using UAVs in battle ﬁelds and urban areas. Also, it presents the limitation of exact target identiﬁcation and control. However, it overlooks the coverage beneﬁts and the deployment/implementation limitations of using UAV systems for surveillance. In [198], the authors show the impact of UAVbased surveillance in civil applications on privacy and civil liberties. First, it states that current regulatory mechanisms (e.g., the US Fourth amendment, EU legislation and judicial decisions, and UK legislation) do not adequately address privacy and civil liberties concerns. This is mainly because UAVs are complex, multimodal surveillance systems that integrate a range of technologies and capabilities. Second, the inadequacy of current legislation mechanisms results in disproportionate impacts on civil liberties for already marginalized population. In conclusion, multilayered regulatory mechanisms that combine legislative protections with a bottomup process of privacy and ethical assessment offer the most comprehensive way to adequately address the complexity and heterogeneity of unmanned aircraft systems and their intended deployments. To sum up, this paper focuses on the law enforcement of privacy and civil liberty aspects of using UAVs for surveillance. However, the regulation recommendation provided is high level and lacks sufﬁcient details and convincing analysis. In [199], the authors introduce a cooperative perimeter surveillance problem and offer a decentralized solution that accounts for perimeter growth (expanding or contracting) and insertion/deletion of team members. The cooperative perimeter surveillance problem is deﬁned to gather information 
VOLUME 7, 2019 48599 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
TABLE 11. Summary of UAV applications for security surveillance. 
about the state of the perimeter and transmit that data back to a central base station with as little delay and at the highest rate possible. The proposed solution is presented in Algorithm 1. The proposed scheme is described comprehensively and a simple formal proof is provided. However, there is no related works or comparative evaluations provided to justify the effectiveness of the proposed scheme. 
Algorithm 1 Proposed Algorithm in [199] 
If There is an agreement with the neighbor: Then, (1) Calculate shared border position ; (2) Travel with neighbor to shared border ; (3) Set direction to monitor own segment; If Reached perimeter endpoint: Then Reverse direction. ENDIF ENDIF 
The authors of [200] present the results of using a UAV in two ﬁeld tests, the 2009 European Land Robot Trials (ELROB-2009) and the 2010 Response Robot Evaluation Exercises (RREE-2010), to investigate different realistic response scenarios. They transplant an improved photo mapping algorithm which is a variant of the Fourier Mellin Invariant (FMI) transform for image representation and processing. They used FMI with the following two modiﬁcations: 
• A logarithmic representation of the spectral magnitude of the FMI descriptor is used; 
• A ﬁlter on the frequency where the shift is supposed to appear is applied. To sum up, this report mainly discusses two ﬁeld tests with a description of a transplanted improved photo mapping algorithm. However, the proof/analysis of the improved algorithm is missing. In [202], the authors discuss a potential application of UAVs in delivering IoT services. A highlevel overview is presented and a use case is introduced to demonstrate how UAVs can be used for crowd surveillance based on face recognition. They also study the ofﬂoading of video data 
processing to an MEC (Mobile Edge Computing) node compared to the local processing of video data on board UAVs. The results demonstrate the efﬁciency of the MECbased ofﬂoading approach in energy consumption, processing time of recognition, and in promptly detecting suspicious persons. To sum up, this paper successfully demonstrates the beneﬁts of using a single UAV system with IoT devices for crowd surveillance with the introduced video data processing ofﬂoading technique. However, further analysis of the coordination of multiple UAVs is missing. In [201], the authors present the use of small UAVs for speciﬁc surveillance scenarios, such as assessment after a disaster. This type of surveillance applications utilizes a system that integrates a wide range of technologies involving: communication, control, sensing, image processing and networking. This article is a product technical report and the corresponding manufacturer successfully applied multiple practical techniques in a real UAV surveillance product. Table 11 summarizes the eight papers we have reviewed including their research focus and category. Fixed wing and VTOL UAVs are used in surveillance applications like LIDAR, Albatross UAV and EMT Luna X-2000 [32], [40]. 
B. DISCUSSION STATEOFTHEART RESEARCH Table 12 summarizes the advantages, disadvantages and important concerns for the UAV surveillance applications based on our literature review. We classify the UAV surveillance research works into six categories based on their focus and content: • MultiUAV cooperation: this line of research focuses on developing new orchestration algorithms or architectures in the presence of multiple UAVs. 
• Homeland/military security concerns: this line of research provides analysis and suggestions for the authorities with the considerations for Homeland/ military security. 
• Algorithm improvement: this line of research develops new algorithms for more efﬁcient postprocessing of the data (e.g., videos or photos captured by the UAVs). 
48600 VOLUME 7, 2019 
TABLE 12. Summary of advantages, disadvantages and important concerns for surveillance applications of UAVs. 
• New use case: this line of research integrates UAV system into new application scenarios for potential extra beneﬁts. 
• Product introduction: this line of research describes mature techniques leveraged by a real UAV surveillance product. 
C. RESEARCH TRENDS AND FUTURE INSIGHTS 1) RESEARCH TRENDS Based on the reviewed literature and our analysis, we identify that the use of research trends (e.g., machine learning algorithms, nanosensors [203], shortrange communications technologies, etc.) could bring further beneﬁts in the area of UAV surveillance applications: 
• There are several preferred features of UAV surveillance applications, especially for some military or homeland security use cases: (1) small size: this feature not only reduces the physical attack surface of the UAV but also achieves more secrete UAV patrolling. (2) high resolution photos: this feature helps the data post processing algorithms to draw more accurate conclusions or obtain more insight ﬁndings; (3) low energy consumption: this feature allows the UAV to operate for a longer time of period such that to achieve more complex tasks. Therefore, to capture more accurate data in a more efﬁcient and secret way, advanced sensor technologies should be considered. Such as nanosensors (small size), ultrahighresolution image sensors (accurate data) [204], and energy efﬁcient sensors (low energy consumption) [205]. 
• To develop more efﬁcient and accurate multiUAV cooperation algorithms or data postprocessing algorithms, advanced machine learning algorithms (e.g., deep learning [206]) could be utilized to achieve better performance and faster response. Speciﬁcally, multiUAV cooperation brings more beneﬁts than single UAV surveillance, such as more wider surveillance scope, higher error tolerance, and faster task completion time. However, multiUAV surveillance requires more advanced data collection, sharing and processing algorithms. Processing of huge amount data is time consuming and much more complex. Applying advanced machine learning algorithms (e.g., deep 
learning algorithm) could help the UAV system to draw better conclusions in a short period of time. For example, due to its improved data processing models, deep learning algorithms could help to obtain new ﬁndings from existing data and to get more concise and reliable analysis results. 
2) FUTURE INSIGHTS based on our literature review and analysis, to develop or deploy effective UAV surveillance applications, we suggest the following: 
• Select/design appropriate UAV system based on the state/federal laws and individual budget. For example, the design of UAVs surveillance systems should take the privacy and security requirements enforced by the local laws; 
• Develop efﬁcient customized algorithms (e.g., cooperation algorithm of multiUAVs, photo mapping algorithms, etc.) based on system requirements. MultiUAVs cooperation algorithm could achieve more efﬁ- cient surveillance (e.g., optimum patrolling routes with minimum power consumption). Also, advanced data postprocessing algorithms help the users to draw more accurate conclusions; 
• Conduct various ﬁeld experiments to verify newly proposed systems. Extensive ﬁeld experiments in various surveillance conditions (e.g., day time, night time, sunshine day, cloudy day, etc.) should be conducted to verify the effectiveness and robustness of the UAVs systems. 
XI. PROVIDING WIRELESS COVERAGE UAVs can be used to provide wireless coverage during emergency cases where each UAV serves as an aerial wireless base station when the cellular network goes down [207]. They can also be used to supplement the ground base station in order to provide better coverage and higher data rates for users [208]. In this section, we present the aerial wireless base stations use cases, CellularConnected UAV, UAV links and UAV channel characteristics. We also show the path loss models for UAVs, classify them based on environment, altitude and telecommunication link, and present some challenges facing these path loss models. Moreover, we present UAV deployment strategies that optimize several objective functions of interest. Then, we discuss the challenges facing UAV interference mitigation techniques. Finally, we present the research trends and future insights for aerial wireless base stations. 
A. AERIAL WIRELESS BASE STATIONS USE CASES The authors in [8], [209], [210] present the typical use cases of aerial wireless base stations which are discussed in the following: 
• UAVs for ubiquitous coverage: UAVs are utilized to assist wireless network in providing seamless wireless coverage within the serving area. Two example 
VOLUME 7, 2019 48601 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 28. Typical use cases of UAVAided wireless communications - inspired by [8]. (a) UAVs as network gateways. (b) UAVs as relay nodes. (c) UAVs for data collection. 
scenarios are rapid service recovery after disaster situations, and when the cellular network service is not available or it is unable to serve all ground users [211] as shown in Figure 28.a [8]. Fixed wing, Balloon and VTOL UAVs can be used for both aforementioned scenarios. Such as Viking aircraft [33]–[35], Tethered Helikite Balloon [30] and the FALCON Quadrotor [32]. 
• UAVs as network gateways: In remote geographic or disaster stricken areas, UAVs can be used as gateway nodes to provide connectivity to backbone networks, communication infrastructure, or the Internet. 
• UAVs as relay nodes: UAVs can be utilized as relay nodes to provide wireless connectivity between two or more distant wireless devices without reliable direct communication links as shown in Figure 28.b. Speciﬁcally, the authors in [212] proposed a new mobile relaying technique in which relay node is mounted on a UAV capable of moving at high speeds. As a result, the endtoend throughput was maximized by optimizing the relay trajectory and the sourcerelay power allocation. 
• UAVs for data collection: UAVs are utilized to gather delaytolerant information from a large number of distributed wireless devices. An example is wireless sensors in precision agriculture applications as shown in Figure 28.c. 
• UAVs for worldwide coverage: UAVsatellite communication is an essential component for building the 
integrated spaceairground network to provide high data rates anywhere, anytime, and towards the seamless wide area coverage. 
B. CELLULARCONNECTED UAV CellularConnected UAVs is a promising technology that can become a reality in the near future [213]. Enabling ultrareliability, low latency and high data rates are important issues to guarantee ubiquitous communications between UAVs and GCS/Users regardless of their locations. The main advantages of using cellularconnected UAVs compared to the traditional GroundtoUAV communications can be summarized as follows: 1- Ubiquitous accessibility. 2- Enhanced performance. 3- Ease of monitoring and management. 4- Robust navigaintion. 5- Costeffectiveness [213]. Furthermore, the authors [213] discussed the unique communications and spectrum requirements of cellularconnected UAV communication systems. they also proposed a new design to enable a future generation of heterogeneous cellularconnected UAV networks coexisting with terrestrial and aerial users in the 3D environment. 
C. UAV LINKS AND CHANNEL CHARACTERISTICS 1) CONTROL LINKS The control links are fundamental to guarantee the safe operation of all UAVs. These links must be lowlatency, highly reliable, and secure twoway communications, usually with low data rate requirement for safetycritical information exchange among UAVs, and additionally between the UAV and ground control stations (GCS). Few studies in the literature focus on control and CNPC links of UAVs. Most researchers studied the data and backhaul links, and the data exchange between UAVs and GCSs. Therefore, it is important to pay more attention to high reliability and low latency in CNPC links. The authors in [214], proposed a framework for enabling ultrareliable and lowlatency communications (URLLC) in CNPC links for UAV communication systems. They proposed the URLLC in the CNPC links of UAVs for downlink transmission (i.e. AirtoGround) and uplink transmission (ie GroundtoAir) for maximizing the available range of the GCS for URLLC UAVs communication. In this work, the available range refers to the maximum horizontal communications distance within which the delay of the round trip and the overall packet loss probability can be guaranteed with the required probability [214]. The control links information ﬂow can be classiﬁed into three types: 1) command and control from GCS to UAVs; 2) UAV status report from UAVs to ground; 3) senseandavoid information among UAVs. Also for autonomous UAV, which can fulﬁll missions depending on intelligent devices without realtime human control, the control links are important in case of emergency when human intervention is needed [8]. There are two frequency bands allocated for the control links, namely the Lband (960-977MHz) and the 
48602 VOLUME 7, 2019 
FIGURE 29. Applications of UAV links. 
TABLE 13. A comparison between data and control links of UAVs. 
Cband (5030-5091MHz) [215]. For delay reasons, we always prefer the primary control links between GCS to UAVs, but the secondary control links via satellite could also be utilized as a backup to enhance reliability and robustness. Another key necessity for the control links is the high security. Speciﬁcally, efﬁcient security mechanisms should be utilized to avoid the socalled ghost control scenario, a possibly disastrous circumstance in which the UAVs are controlled by unapproved operators via spoofed control or navigation signals. Accordingly, practical authentication techniques, perhaps supplemented by the emerging physical layer security techniques, should be applied for control links. Compared to data links, the control links usually have lower tolerance in terms of security and latency requirements [8]. 
2) DATA LINKS The purpose of using data links is to support taskrelated communications for the ground terminals which include ground base stations, mobile terminals, gateway nodes, wireless sensors, etc. The data links of UAVs need to provide the following communication services: 1) Direct mobileUAV communication; 2) UAVbase station and UAVgateway wireless backhaul; 3) UAVUAV wireless backhaul. The capacity requirement of these data links depends on the communication services, ranging from low capacity (kbps) in UAVsensor links to high speed (Gbps) in UAVgateway wireless backhaul. The UAV data links could utilize the existing band assigned for the particular communication services to enhance performance, e.g., using millimeter wave band [216] and free space optics [217] for high capacity UAVUAV wireless backhaul [8]. In Figure 29, we show the applications of UAV links. In Table 13, we make a comparison between control and data links. 
3) UAV CHANNEL CHARACTERISTICS Both control and data channels in UAV communication networks consist of two primary types of channels, UAVground 
FIGURE 30. Basic networking architecture of UAVAided wireless communications. 
and UAVUAV channels as shown in Figure 30. These channels have several unique characteristics compared with the characteristics of terrestrial communication channels. While line of sight links are expected for UAVground channels in most cases, they could also be occasionally blocked by obstacles such as buildings. For lowaltitude UAVs, the UAVground channels may also suffer a number of multipath components due to reﬂection, scattering, and diffraction by buildings, ground surface, etc. The UAVUAV wireless channels are line of sight dominated and thus the effect of multipath is minimal compared to that experienced in UAVground or groundground channels. Due to the continuous movements of UAVs with different velocities, the UAVtoUAV wireless channels will have high Doppler frequencies, especially the ﬁxed wing UAVs. On one hand, we can utilize the dominance of the line of sight channels to 
VOLUME 7, 2019 48603 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 31. Classification of path loss models for UAVs. 
achieve highcapacity for emerging mmWave communications. On the other hand, due to the continuous movements of UAVs with different velocities coupled with the higher carrier frequency in the mmWave band, the Doppler shift will increase [8]. 
D. PATH LOSS MODELS Path loss is an essential in the design and analysis of wireless communication channels and represents the amount of reduction in power density of a transmitted signal. The characteristics of aerial wireless channels are different than the terrestrial wireless channels due to the variations in the propagation environments and hence the path loss models for UAVs are also different than the traditional path loss models for terrestrial wireless channels. We classify the UAV path loss models based on environment, altitude and telecommunication link as shown in Figure 31. 
1) AIRTOGROUND PATH LOSS FOR LOW ALTITUDE PLATFORMS The authors in [24] present a statistical propagation model for predicting the path loss between a low altitude UAV and a Ground terminal. In this model, the authors assume that a UAV transmits data to more than 37000 ground receivers and use the Wireless InSite ray tracing software to model three types of rays (Direct, Reﬂected and Diffracted). Based on the simulation results, they divide the receivers to three groups. The ﬁrst group corresponds to receivers that have LineofSight or nearLineofSight conditions. The second group corresponds to receivers with non LineofSight condition, but still receiving coverage via strong reﬂection and refraction. The third group corresponds to receivers that have deep fading conditions resulting from consecutive reﬂections and diffractions, the third group only represents 3% of receivers. Therefore, based on the ﬁrst and second groups, the authors present the path loss model as a function of the altitude h and the coverage radius R as shown in Figure 34 and it is given as follows: 
L(h, R) = P(LOS) × LLOS + P(NLOS) × LNLOS (1) 
P(LOS) = 1 
1 + α.exp(−β[ 180 
π θ − α]) (2) 
LLOS(dB) = 20log(4πfcd 
c ) + ζLOS (3) 
FIGURE 32. Minimum required transmit power. 
FIGURE 33. Transmit power required to cover the building. 
LNLOS(dB) = 20log(4πfcd 
c ) + ζNLOS (4) 
where P(LOS) is the probability of having line of sight (LOS) connection at an elevation angle of θ, P(NLOS) is the probability of having non LOS connection and it equals (1- P(LOS)), LLOS and LNLOS are the average path loss for LOS and NLOS paths. In equations (2), (3) and (4), α and β are constant values which depend on the environment, fc is the carrier frequency, d is the distance between the UAV and the ground user, c is the speed of the light, ζLOS and ζNLOS are the average additional loss which depends on the environment. In this path loss model, the authors assume that all users are outdoor and the location of each user can be represented by an outdoor 2D point. These assumptions limit the applicability 
48604 VOLUME 7, 2019 
FIGURE 34. Coverage zone by a low altitude UAV. 
FIGURE 35. Providing wireless coverage for indoor users. 
of this model when one needs to consider indoor users. The authors of [218] describe the tradeoff in this model. At a low altitude, the path loss between the UAV and the ground user decreases, while the probability of line of sight links also decreases. On the other hand, at a high altitude line of sight connections exist with a high probability, while the path loss increases. In Figure 32,the authors show that as the radius of target area increases, both the optimal altitude and the minimum transmit power required to cover the area increase. 
2) OUTDOORTOINDOOR PATH LOSS MODEL The AirtoGround path loss model presented in [24] is not appropriate when we consider wireless coverage for indoor users, because this model assumes that all users are outdoor and located at 2D points. In [219], the authors adopt the OutdoorIndoor path loss model, certiﬁed by the International Telecommunication Union (ITU) [220] to provide wireless coverage for indoor users using UAV as shown in Figure 35. The path loss is given as follows: 
L = LF + LB + LI = (wlog10dout + wlog10f + g1) +(g2 + g3(1 − cosθi)2) + (g4din) (5) 
where LF is the free space path loss, LB is the building penetration loss, and LI is the indoor loss. Also, dout is the 
distance between the UAV and indoor user, θi is the incident angle, and din is the indoor distance of the user inside the building. In this model, we also have w = 20, g1=32.4, g2=14, g3=15, g4=0.5 and f is the carrier frequency. The authors of [219] describe the tradeoff in the above model when the horizontal distance between the UAV and a user changes. When this horizontal distance increases, the free space path loss (i.e.,LF) increases as dout increases, while the building penetration loss (i.e., LB) decreases as the incident angle (i.e., θi) decreases. In Figure 33,it is demonstrated that the transmit power required to cover the building as a function of incident angle. The ﬁgure clearly illustrates that the minimum transmit power required to cover the building is obtained at the optimal angle of 48 degrees. 
3) CELLULARTOUAV PATH LOSS MODEL In [221], the authors model the statistical behavior of the path loss from a cellular base station towards a ﬂying UAV. They present the path loss model based on extensive ﬁeld experiments that involve collecting both terrestrial and aerial coverage samples. They report the value of the path loss as a function of the depression angle and the terrestrial coverage beneath the UAV as shown in Figure 36. The path loss is given as follows: 
L(d, θ) = Lter(d) + η(θ) + Xuav(θ) = 10αlog(d) 
+A(θ − θo)exp(−θ − θo 
B ) + ηo + N(0, aθ + σo) (6) 
where Lter(d) is the mean terrestrial pathloss at a given terrestrial distance d from the cellular base station, η(θ) is the excess aerial pathloss, Xuav(θ) is a Gaussian random variable with an angledependent standard deviation σuav(θ) representing the shadowing component, θ is depression angle between the UAV and the cellular base station, α is the pathloss exponent. Also, A, B, θo and ηo are ﬁtting parameters. 
4) AIRTOGROUND PATH LOSS FOR HIGH ALTITUDE PLATFORMS The authors in [222] present an empirical propagation prediction model for mobile communications from high altitude platforms in builtup areas, where the frequency band is 2 − 6 GHz. The probability of LOS paths between the UAV and a ground user and the additional shadowing path loss for NLOS paths are presented as a function of the elevation angle (θ). The path loss model is deﬁned for four different types of builtup area (Suburban area, Urban area, Dense urban area and Urban highrise area). The path loss is given as follows: 
L = P(LOS) × LLOS + P(NLOS) × LNLOS (7) 
P(LOS) = a − a − b 
1 + ( θ−c 
d )e (8) 
LLOS(dB) = 20log(dkm) + 20log(fGHz) + 92.4 + ζLOS (9) LNLOS(dB) = 20log(dkm) + 20log(fGHz) + 92.4 + Ls + ζNLOS (10) 
VOLUME 7, 2019 48605 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
TABLE 14. Comparison among path loss models. 
In equation (7), P(LOS) is the probability of having line of sight (LOS) connection at an evaluation angle of θ, P(NLOS) is the probability of having non LOS connection and it equals (1- P(LOS)), LLOS and LNLOS are the average path loss for LOS and NLOS paths. In equation (8), a, b, c, d and e are the empirical parameters. In equations (9) and (10), dkm is the distance between the UAV and the ground user in km, fGHz is the frequency in GHz, Ls is a random shadowing in dB as a function of the elevation angle (θ), ζLOS and ζNLOS are the average additional losses which depend on the environment. 
5) UAV TO UAV PATH LOSS MODEL In general, the UAVtoUAV wireless channels are line of sight dominated, so that the free space path loss can be adopted for the aerial channels. Due to the continuous movements of UAVs with different velocities, the UAVtoUAV wireless channels will have high Doppler frequencies (especially the ﬁxed wing UAVs). In Table 14, we make a comparison among the path loss models based on operating frequency, altitude, environment, type of link, type of experiments and challenges. 
E. UAV DEPLOYMENT STRATEGIES UAVs deployment problem is gaining signiﬁcant importance in UAVbased wireless communications where the performance of the aerial wireless network depends on the deployment strategy and the 3D placements of UAVs. In this section, we classify the UAV deployment strategies based on the objective functions as shown in Figure 37. 
1) DEPLOYMENT STRATEGIES FOR MINIMIZING THE TRANSMIT POWER OF UAVs The authors in [223] propose an efﬁcient deployment framework for deploying the aerial base stations, where the goal is to minimize the total required transmit power of UAVs 
FIGURE 36. CellulartoUAV path loss parameters. 
while satisfying the users rate requirements. They apply the optimal transport theory to obtain the optimal cell association and derive the optimal UAV’s locations using the facility location framework. The authors in [218] investigate the downlink coverage performance of a UAV, where the objective is to ﬁnd the optimal UAV altitude which leads to the maximum ground coverage and the minimum transmit power. The authors in [219] propose using a single UAV to provide wireless coverage for indoor users inside a highrise building under disaster situations. They study the problem of efﬁcient UAV placement, where the objective is to minimize the total transmit power required to cover the entire highrise building. In [224], the authors propose a particle swarm optimization algorithm to ﬁnd an efﬁcient 3D placement of a UAV that minimizes the total transmit power required to cover the indoor users. The authors in [225] utilize UAVs to provide wireless coverage for indoor and outdoor users in massively crowded events, where the objective is to ﬁnd the optimal UAV placement which lead to the minimum transmit power. In [226], the authors propose an optimal placement algorithm for UAV that maximizes the number of covered users 
48606 VOLUME 7, 2019 
FIGURE 37. Classification of deployment strategies for UAVs. 
using the minimum transmit power. The algorithm decouple the UAV deployment problem in the vertical and horizontal dimensions without any loss of optimality. The authors in [227] consider two types of users in the network: the downlink users served by the UAV and devicetodevice users that communicate directly with one another. In the mobile UAV scenario, using the disk covering problem, the entire target geographical area can be completely covered by the UAV in a shortest time with a minimum required transmit power. They also derive the overall outage probability for devicetodevice users, and show that the outage probability increases as the number of stop points that the UAV needs to completely cover the area increases. 
2) DEPLOYMENT STRATEGIES FOR MAXIMIZING THE WIRELESS COVERAGE OF UAVs In [208], the authors highlight the properties of the UAV placement problem, and formulate it as a 3D placement problem with the objective of maximizing the revenue of the network, which is proportional to the number of users covered by a UAV. They formulate an equivalent problem which can be solved efﬁciently to ﬁnd the size of the coverage region and the altitude of a UAV. The authors in [228] study the optimal deployment of UAVs equipped with directional antennas, using circle packing theory. The 3D locations of the UAVs are determined in a way that the total coverage area is maximized. In [229], the authors introduce networkcentric and usercentric approaches, the optimal 3D backhaulaware placement of a UAV is found for each approach. In the networkcentric approach, the network tries to serve as many users as possible, regardless of their rate requirements. In the usercentric approach, the users are determined based on the priority. The total number of served users and sumrates are maximized in the networkcentric and usercentric frameworks. The authors in [230] study an efﬁcient 3D UAV placement that maximizes the number of covered users with different QualityofService requirements. They model the placement problem as a multiple circles placement problem and propose an optimal placement algorithm that utilizes an exhaustive search over a onedimensional parameter in a closed region. They propose a lowcomplexity algorithm, maximal weighted area algorithm, to tackle the placement problem. In [231], the authors aim to maximize the indoor 
wireless coverage using UAVs equipped with directional antennas. They present two methods to place the UAVs; providing wireless coverage from one building side and from two building sides. The authors in [232] utilize UAVshubs to provide connectivity to smallcell base stations with the core network. The goal is to ﬁnd the best possible association of the small cell base stations with the UAVshubs such that the sumrate of the overall system is maximized depending on a number of factors including maximum backhaul data rate of the link between the core network and motherUAVhub, maximum bandwidth of each UAVhub available for smallcell base stations, maximum number of links that every UAVhub can support and minimum signaltointerferenceplusnoise ratio. They present an efﬁcient and distributed solution of the designed problem, which performs a greedy search to maximize the sum rate of the overall network. In [233], the authors propose an efﬁcient framework for optimizing the performance of UAVbased wireless systems in terms of the average number of bits transmitted to users and UAVs’s hovering duration. They investigate two scenarios: UAV communication under hover time constraints and UAV communication under load constraints. In the ﬁrst scenario, given the maximum possible hover time of each UAV, the total data service under user fairness considerations is maximized. They utilize the framework of optimal transport theory and prove that the cell partitioning problem is equivalent to a convex optimization problem. Then, they propose a gradientbased algorithm for optimally partitioning the geographical area based on the users’s distribution, hover times, and locations of the UAVs. In the second scenario, given the load requirement of each user at a given location, they minimize the average hover time needed for completely serving the ground users by exploiting the optimal transport theory. 
3) DEPLOYMENT STRATEGIES FOR MINIMIZING THE NUMBER OF UAVs REQUIRED TO PERFORM TASK The authors in [234] propose a method to ﬁnd the placements of UAVs in an area with different user densities using the particle swarm optimization. The goal is to ﬁnd the minimum number of UAVs and their 3D placements so that all the users are served. In [235], the authors study the problem of minimizing the number of UAVs required for a continuous coverage of a given area, given the recharging requirement. They 
VOLUME 7, 2019 48607 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
prove that this problem is NPcomplete. Due to its intractability, they study partitioning the coverage graph into cycles that start at the charging station. Based on this analysis, they then develop an efﬁcient algorithm, the cycles with limited energy algorithm, that minimizes the number of UAVs required for a continuous coverage. The authors in [236] study the problem of minimizing the number of UAVs required to provide wireless coverage to indoor users and prove that this problem is NPcomplete. Due to the intractability of the problem, they use clustering to minimize the number of UAVs required to cover the indoor users. They assume that each cluster will be covered by only one UAV and apply the particle swarm optimization to ﬁnd the UAV 3D location and UAV transmit power needed to cover each cluster. In [237], the authors study the problem of deploying minimum number of UAVs to maintain the connectivity of ground MANETs under the condition that some UAVs have already been deployed in the ﬁeld. They formulate this problem as a minimum steiner tree problem with existing mobile steiner points under edge length bound constraints and prove that the problem is NPComplete. They propose an existing UAVs aware polynomial time approximate algorithm to solve the problem that uses a maximum match heuristic to compute new positions for existing UAVs. The authors in [238] aim to minimize the number of UAVs required to provide wireless coverage for a group of distributed ground terminals, ensuring that each ground terminal is within the communication range of at least one UAV. They propose a polynomialtime algorithm with successive UAV placement, where the UAVs are placed sequentially starting from the area perimeter of the uncovered ground terminals along a spiral path towards the center, until all ground terminals are covered. 
4) DEPLOYMENT STRATEGIES TO COLLECT DATA USING UAVs The authors in [239] propose an efﬁcient framework for deploying and moving UAVs to collect data from ground Internet of Things devices. They minimize the total transmit power of the devices by properly clustering the devices where each cluster being served by one UAV. The optimal trajectories of the UAVs are determined by exploiting the framework of optimal transport theory. In [240], the authors present a UAV enabled data collection system, where a UAV is dispatched to collect a given amount of data from ground terminals at ﬁxed location. They aim to ﬁnd the optimal ground terminal transmit power and UAV trajectory that achieve different Pareto optimal energy tradeoffs between the ground terminal and the UAV. The authors in [241] study the problem of trajectory planning for wireless sensor network data collecting deployed in remote areas with a cooperative system of UAVs. The missions are given by a set of ground points which deﬁne wireless sensor network gathering zones and each UAV should pass through them to gather the data while avoiding passing over forbidden areas and collisions between UAVs. The proposed UAV trajectory planners are based on Genetics Algorithm, Rapidlyexploring Random Trees 
and Optimal Rapidlyexploring Random Trees. The authors in [242] design a basic framework for aerial data collection, which includes the following ﬁve components: deployment of networks, nodes positioning, anchor points searching, fast path planning for UAV, and data collection from network. They identify the key challenges in each component and propose efﬁcient solutions. They propose a Fast Path Planning with Rules algorithm based on grid division, to increase the efﬁciency of path planning, while guaranteeing the length of the path to be relatively short. In [243], the authors jointly optimize the sensor nodes wakeup schedule and UAV’s trajectory to minimize the maximum energy consumption of all sensor nodes, while ensuring that the required amount of data is collected reliably from each sensor node. They formulate a mixedinteger nonconvex optimization problem and apply the successive convex optimization technique, an efﬁ- cient iterative algorithm is proposed to ﬁnd a suboptimal solution. UAVs can be classiﬁed into two types: ﬁxed wing and rotary wing, each with its own strengths and weaknesses. Fixedwing UAV usually has high speed and payload, but they need to maintain a continuous forward motion to remain aloft, thus are not suitable for stationary uses. In contrast, rotarywing UAV such as quadcopter, usually has limited mobility and payload, but they are able to move in any direction as well as to stay stationary in the air. Thus, the choice of UAVs critically depends on the uses [8]. In Table 15, we make a comparison among the research papers related to UAV deployment strategies based on the objective functions. 
F. INTERFERENCE MITIGATION One of the techniques to mitigate interference is the coordinated multipoint technique [244]. In downlink coordinated multipoint technique, the transmission aerial base stations cooperate in scheduling and transmission in order to strength the received signal and mitigate intercell interference [244]. On the other hand, the physical uplink shared channel (PUSCH) is received at aerial base stations in uplink coordinated multipoint technique. The scheduling decision is based on the coordination among UAVs [245]. The new challenge here is that UAVs receive interfering signals from more ground terminals in the downlink and their uplink transmitted signals are visible to more ground terminals due to the high probability of line of sight links. Therefore, the coordinated multipoint techniques must be applied across a larger set of cells to mitigate the interference and hence the coordination complexity will increase [246]. We can also mitigate interference by utilizing receiver techniques such as interference rejection combining and networkassisted interference cancellation and suppression. Compared to smart phones, we can equip UAVs with more antennas, which can be used to mitigate interference from more ground base stations. With MIMO antennas, UAVs can use beamforming to enables directional signal transmission or reception to achieve spatial selectivity which is also an efﬁcient interference mitigation technique [246]. 
48608 VOLUME 7, 2019 
TABLE 15. Comparison among research papers related to UAV deployment strategies based on objective functions. 
VOLUME 7, 2019 48609 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
Another interference mitigation technique is to partition radio resources so that ground trafﬁc and aerial trafﬁc are served with orthogonal radio resources. This simple technique may not be efﬁcient since the reserved radio resources for aerial trafﬁc may be not fully utilized. Therefore, UAV operators need to provide more information about the trajectories and 3D placements of UAVs to construct more dynamic radio resource management [246]. A powerful interference mitigation technique is the power control technique in which an optimized setting of power control parameters can be applied to reduce the interference generated by UAVs. This technique can minimize interference, increase spectral efﬁciency, and beneﬁt UAVs as well as ground terminals [246]. Dedicated cells for the UAVs is another option to mitigate interference, where the directional antennas are pointed towards the sky instead of downtilted. These dedicated cells will be a practical solution especially in UAV hotspots where frequent and dense UAV takeoffs and landings occur [246]. In Figure 38, we summarize the interference mitigation techniques and challenges. 
FIGURE 38. Interference mitigation techniques and challenges. 
G. RESEARCH TRENDS AND FUTURE INSIGHTS 1) CLOUD AND BIG DATA A cloud for UAVs contains data storage and highperformance computing, combined with big data analysis tools [260]. It can provide an economic and efﬁcient use of centralized resources for decision making and networkwide monitoring [260]–[262]. If UAVs are utilized by a traditional cellular network operator (CNO), the cloud is just the data center of the CNO (similar to a private cloud), where the CNO can choose to share its knowledge with some other CNOs or utilize it for its own business uses. On the other hand, if the UAVs are utilized by an infrastructure provider, the infrastructure provider can utilize the cloud to gather information from mobile virtual network operators and service providers. Under such scenario, it is important 
to guarantee security, privacy, and latency. To better exploit the beneﬁt of the cloud, we can use a programmable network allowing dynamic updates based on big data processing, for which network functions virtualization and software deﬁned networking can be research trends [260]. 
2) MACHINE LEARNING Nextgeneration aerial base stations are expected to learn the diverse characteristics of users’ behavior, in order to autonomously ﬁnd the optimal system settings. These intelligent UAVs have to use sophisticated learning and decisionmaking, one promising solution is to utilize machine learning. Machine learning algorithms can be simply classi- ﬁed as supervised, unsupervised and reinforcement learning as shown in Table 16. The family of supervised learning algorithms utilizes known models and labels to enable the estimation of unknown parameters. They can be used for spectrum sensing and white space detection in cognitive radio, massive MIMO channel estimation and data detection, as well as for adaptive ﬁltering in signal processing for 5G communications. They can also be utilized in higherlayer applications, such as estimating the mobile users’ locations and behaviors, which can help the UAV operators to enhance the quality of their services. The family of unsupervised learning algorithms utilizes the input data itself in a heuristic manner. They can be used for heterogeneous base station clustering in wireless networks, for access point association in ubiquitous WiFi networks, for cell clustering in cooperative ultradense smallcell networks, and for loadbalancing in heterogeneous networks. They can also be utilized in fault/intrusion detections and for the users’ behaviorclassiﬁcation. The family of reinforcement learning algorithms utilizes dynamic iterative learning and decisionmaking process. They can be used for estimating the mobile users’ decision making under unknown scenarios, such as channel access under unknown channel availability conditions in spectrum sharing, base station association under the unknown energy status of the base stations in energy harvesting networks, and distributed resource allocation under unknown resource quality conditions in femto/smallcell networks [263]. 
3) NETWORK FUNCTIONS VIRTUALIZATION Network functions virtualization (NFV) reduces the need of deploying speciﬁc network devices for the integration of UAVs [260], [261]. NFV allows a programmable network structure by virtualizing the network functions on storage devices, servers, and switches, which is practical for UAVs requiring seamless integration to the existing network. Moreover, virtualization of UAVs as shared resources among cellular virtual network operators can decrease OPEX for each party [264]. Here, the SDN can be useful for the complicated control and interconnection of virtual network functions (VNFs) [260], [261]. 
48610 VOLUME 7, 2019 
TABLE 16. UAV machine learning algorithms. 
4) SOFTWARE DEFINED NETWORKING For mobile networks, a centralized SDN controller can make a more efﬁcient allocation of radio resources, which is particularly important to exploit UAVs [260], [261]. For instance, SDNbased load balancing can be useful for multitier UAV networks, such that the load of each aerial base station and terrestrial base station is optimized precisely. A SDN controller can also update routing such that part of trafﬁc from the UAVs is carried through the network without any network congestions [260]–[262]. For further exploitation of the new degree of freedom provided by the mobility of UAVs, the 3D placements of UAVs can be adjusted to optimize paging and polling, and location management parameters can be updated dynamically via the uniﬁed protocols of SDN [260]. 
5) MILLIMETERWAVE Millimeterwave (mmWave) technology can be utilized to provide high data rate wireless communications for UAV networks [265]. The main difference between utilizing mmWave in UAV aerial networks and utilizing mmWave in terrestrial cellular networks is that a UAV aerial base station may move around. Hence, the challenges of mmWave in terrestrial cellular networks apply to the mmWave UAV cellular network as well, including rapid channel variation, blockage, range and directional communications, multiuser access, and others [265], [266]. Compared to mmWave communications for static stations, the time constraint for beamforming training is more critical due to UAV mobility. For fast beamforming training and tracking in mmWave UAV cellular networks, the hierarchical beamforming codebook is able to create highly directional beam patterns, and achieves excellent beam detection performance [265]. Although the UAV wireless channels have high Doppler frequencies due to the continuous movements of UAVs with different velocities, the major multipath components are only affected by slow variations due to high gain directional transmissions. In beam division 
multiple access (BDMA), multiple users with different beams may access the channel at the same time due to the highly directional transmissions of mmWave [265]–[267]. This technique improves the capacity signiﬁcantly, due to the large bandwidth of mmWave technology and the use of BDMA in the spatial domain. The blockage problem can be mitigated by utilizing intelligent cruising algorithms that enable UAVs to ﬂy out of a blockage zone and enhance the probability of line of sight links [265]. 
6) FREE SPACE OPTICAL Free space optical (FSO) technology can be used to provide wireless connectivity to remote places by utilizing UAVs, where physical access to 3G or 4G network is either minimal or never present [76]. It can be involved in the integration of ground and aerial networks with the help of UAVs by providing last mile wireless coverage to sensitive areas (e.g., battleﬁelds, disaster relief, etc.) where high bandwidth and accessibility are required. For instance, Facebook will provide wireless connectivity via FSO links to remote areas by utilizing solarpowered high altitude UAVs [76], [268], [269]. For areas where deployment of UAVs is impractical or uneconomical, geostationary earth orbit and low earth orbit satellites can be utilized to provide wireless connectivity to the ground users using the FSO links. The main challenge facing UAVfree space optics technology is the high blockage probability of the vertical FSO link due to weather conditions. The authors in [217] propose some methods to tackle this problem. The ﬁrst method is to use an adaptive algorithm that controls the transmit power according to weather conditions and it may also adjust other system parameters such as incident angle of the FSO transceiver to compensate the link degradation, e.g., under bad weather conditions, high power vertical FSO beams should be used while low power beams could be used under good weather conditions. The second method is to use a system optimization algorithm that can 
VOLUME 7, 2019 48611 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
optimize the UAV placement, e.g., hovering below clouds over negligible turbulence geographical area. 
7) UAVS AS RELAYS UAVs are becoming more attractive as aerial base stations or relays to provide network coverage [270]. In [271], the authors studied UAV assisted secure transmission for scalable videos in hyperdense networks via caching. They proposed a scheme to provide videos to mobile users in small cells via UAVs. In the proposed scheme, small cell base stations and UAVs were equipped with caches to store videos at offpeak time to reduce the pressure on wireless backhaul. The proposed scheme exploited the principle of interference alignment to manage interference. The authors in [272] studied UAV trajectory at the edges of three adjacent cells to ofﬂoad trafﬁc of ground base stations. They proposed a scheme that optimized UAV trajectory in each ﬂying cycle, where the objective was to maximize the sum rate of edge users subject to the rate requirements for all users. The optimization problem was a mixedinteger nonconvex problem. Thus, they transformed it into two convex problems, and iteratively solved it by optimizing the UAV trajectory and edge user scheduling, alternately. In [273], the authors studied the use of multiple UAVs in relaying. They considered two typical uses of multiple UAVs as relays that form either a single multihop link or multiple dualhop links. They maximized the endtoend SNR for three useful channel models and two common relaying protocols by optimizing the placement of the UAVs. The two relaying setups were then compared based on the optimal placement in terms of outage and bit error rate. 
8) FUTURE INSIGHTS Some of the future possible directions for this research are: 
• The majority of literature focuses on providing the wireless coverage for outdoor ground users, although 90% of the time people are indoor and 80% of the mobile Internet access trafﬁc also happens indoors [274]–[276]. Therefore, it is important to study the indoor wireless coverage problems by utilizing UAVs. 
• The majority of literature does not consider the limited energy capacity of UAV as a constraint when they study the wireless coverage of UAVs, where the energy consumption during data transmission and reception is much smaller than the energy consumption during the UAV hovering. It only constitutes 10%-20% of the UAV energy capacity [2]. 
• Some path loss models are based on simulations softwares such as AirtoGround path loss for low altitude platforms and AirtoGround path loss for high altitude platforms, therefore it is necessary to do real experiments to model the statistical behavior of the path loss. 
• To the best of our knowledge, no studies present the path loss models for uplink scenario and mmWave bands. 
• There are challenges facing the new technologies such 
as mmwave. The challenges facing UAVmmWave technology are fast beamforming training and tracking requirement, rapid channel variation, directional and range communications, blockage and multiuser access [265]. 
• We need more studies about the topology of UAV wireless networks where this topology remains ﬂuid during: a) Changing the number of UAVs; b) Changing the number of channels; c) The relative placements of the UAVs altering [2]. 
• There is a need to study UAV routing protocols where it is difﬁcult to construct a simple implementation for proactive or reactive schemes [2]. 
• There is a need for a seamless mechanism to transfer the users during the handoff process between UAVs, when out of service UAVs are replaced by active ones [2]. 
• Further research is required to determine which technology is right for UAV applications, where the common technologies utilized in UAV applications are the IEEE 802.11 due to wide availability in current wireless devices and their appropriateness for smallscale UAVs [1]. 
• D2D communications is an efﬁcient technique to improve the capacity in ground communications systems [277]. An important problem for future research is the joint optimization of the UAV path planning, coding, node clustering, as well as D2D ﬁle sharing [8]. 
• Utilizing UAVs in public safety communications needs further research, where topology formation, cooperation between UAVs in a multiUAV network, energy constraints, and mobility model are the challenges that are facing UAVs in public safety communications [9]. 
• In UAVsbased IoT services, it is difﬁcult to control and manage a high number of UAVs. The reason is that each UAV may host more than one IoT device, such as different types of cameras and aerial sensors. Moreover, conﬂict of interest between different devices likely to happen many times, e.g., taking two videos from two different angles from one ﬁxed placement. For future research, we need to propose efﬁcient algorithms to solve the conﬂict of interest among Internet of things devices onboard [3]. 
• For future research, it is important to propose efﬁcient techniques that manage and control the power consumption of Internet of things devices onboard [3]. 
• The security is one of the most critical issues to think about in UAVsbased Internet of things services, methods to avoid the aerial jammer on the communications are needed [3]. 
PART III: KEY CHALLENGES AND CONCLUSION XII. KEY CHALLENGES A. CHARGING CHALLENGES UAV missions necessitate an effective energy management for batterypowered UAVs. Reliable, continuous, and smart 
48612 VOLUME 7, 2019 
TABLE 17. Classification of autonomous battery swapping systems. 
management can help UAVs to achieve their missions and prevent loss and damage. The UAV’s battery capacity is a key factor for enabling persistent missions. But as the battery capacity increases, its weight increases, which cause the UAV to consume more energy for a given mission. The main directions in the literature to mitigate the limitations in UAV’s batteries are: (1) UAV battery management, (2) Wireless charging for UAVs, (3) Solar powered UAVs, and (4) Machine learning and communications techniques. 
1) BATTERY MANAGEMENT Battery management research in UAVs includes planning, scheduling, and replacement of battery so UAVs can accomplish their ﬂight missions. This has been studied in the literature from different perspectives. Saha et al. in [278] build a model to predict the end of battery charge for UAVs based on particle ﬁlter algorithm. They utilize a discharge curve for UAV LiPo battery to tune the particle ﬁlter. They have shown that the depletion of the battery is not only related to the initial Start of Charge (SOC) but also, load proﬁle and battery health conditions can be crucial factors. Park et al. in [279] investigate the battery assignment and scheduling for UAVs used in delivery business. Their objective is to minimize the deterioration in battery health. After splitting the problem into two parts; one for battery assignment and the other for battery scheduling. Heuristic algorithm and integer linear programming are used to solve the assignment and scheduling problems, respectively. The idle time between two successive charging cycles and the depth of the discharge are the main factors that control these algorithms. The use of UAVs in long time and enduring missions makes UAV battery swapping solutions necessary to accomplish these missions. An autonomous battery swapping system for UAVs is ﬁrst introduced in [280]. The battery swapping consists of landing platform, battery charger, battery storage compartment, and microcontroller. Similar concept to this autonomous battery swapping system was adopted and improved by different researchers in [281]–[284]. The hot swap term is adopted to refer to the continuous powering for the UAV during battery swapping. Figure 39 shows an illustration of hot swapping systems. First, the UAV is connected to an external power supply during the swapping process. This will prevent data loss during swapping as it usually happens in cold swapping. Second, the drained battery is removed and stored in multibattery compartment and charging station to be recharged. 
FIGURE 39. Illustration of battery hot swapping system. 
Third, a charged battery is installed in the UAV, and ﬁnally, the external power supply is disconnected. Another important aspect of the improvements presented in [281]–[284] is the several designs for the landing platform, which is an important part of the swapping system because it can compensate the error in the UAV positioning on the landing point. Table 17 shows the classiﬁcation of autonomous battery swapping systems. It can be seen from this table that the swapping time for most of these systems is around one minute. This is a short period comparing to the battery charging average time, which is between 45-60 minutes. 
2) WIRELESS CHARGING FOR UAV Simic et. Al in [285] study the feasibility of recharging the UAV from power lines while inspecting these power lines. Experimental tests show the possibility of energy harvesting from power lines. The authors design a circular antenna to harvest the required energy from power lines. Wang et al. in [286] propose an automatic charging system for the UAV. The system uses charging stations allocated along the path of UAV mission. Each charging station consists of wireless charging pad, solar panel, battery, and power converter. All these components are mounted on a pole. The UAV employs GPS module and wireless network module to navigate to the autonomous charging station. When the power level in the 
VOLUME 7, 2019 48613 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
UAV’s battery drops to a predeﬁned level. The UAV will communicate with the central control room, which will direct the UAV to the closest charging station. The GPS module will help the UAV to navigate to the assigned station. Different wireless power transfer technologies were used in literature to implement the automatic charging stations. In [287]–[289], the authors utilize the magnetic resonance coupling technology to implement an automatic drone charging station. Dunbar et al. in [290] use RF Far ﬁeld Wireless Power Transfer (WPT) to recharge micro UAV after landing. A ﬂexible rectenna was mounted on the body of the UAV to receive RF signals from power transmitter. Mostafa et al. in [291] develop a WPT charging system based on the capacitive coupling technology. One of the major challenges that face the deployment of autonomous wireless charging stations is the precise landing of the UAV on the charging pad. The imprecise landing will lead to imperfect alignment between the power transmitter and the power receiver, which means less efﬁciency and long charging time. Table 18 shows precise methodologies adopted in the literature. In [286], the authors use GPS to land the UAV on the charging pad. In [292], the authors corporate the GPS system with camera and image processing system to detect the charging station. The authors in [288] allow for imprecise landing on wide frame landing pad. Then a wireless charging transmitting coil is stationed perfectly in the bottom of the UAV by using a stepper motor and two laser distance sensors. 
TABLE 18. Classification of precise landing methodologies. 
3) SOLAR POWERED UAVs For long endurance and high altitude ﬂights, solarpowered UAVs (SUAVs) can be a great choice. These UAVs use solar power as a primary source for propulsion and battery as a secondary source to be used during night and sun absence conditions. The literature in ﬂight endurance and persistent missions for SUAVs can be classiﬁed into two main directions: path planning and optimization and hybrid Models. Path planning and optimization includes: (i) gravitational potential energy planning. This scheme optimizes the path of the UAV over three stages, ascending and battery charging stage during the day using solar power, descending stage during the night using gravitational potential energy, and level ﬂight stage using battery [293]–[295], (ii) optimal path planning while tracking ground object [296], [297], (iii) optimal path planning based on UAV kinematics and its energy loss model [298], and (iv) optimal path planning utilizing wind and metrological data [299], [300]. On the other hand, Hybrid models include: (i) UAVs with hybrid power sources such as solar, battery, and fuel cells [301] and (ii) UAVs with a hybrid propulsion system that can be transformed from ﬁxed wing to quadcopter UAV [302], [303]. 
4) MACHINE LEARNING AND COMMUNICATIONS TECHNIQUES Communication equipment and their status (transmit, receive, sleep, or idle) can affect drone ﬂight time. Utilizing the stateoftheart machine learning techniques can result in smarter energy management. These technologies and their role in UAV persistent missions are covered in the literature under the following themes: • Energy efﬁcient UAV networks: This area covers maximizing energy efﬁciency in the network layer, data link layer, physical layer, and crosslayer protocols in a bid to help UAVs to perform long missions. Gupta et al. in [2] list and compare several algorithms used in each layer. 
• Energybased UAV ﬂeet management: This includes modifying the mobility model of UAV ﬂeet to incorporate energy as decision criterion as in [304] to determine the next move of each UAV inside a ﬂeet. 
• Machine learning: This area covers research that utilizes machine learning techniques for path planning and optimization applications, taking into consideration energy limitations. Zhang et al. in [305] use deep reinforcement learning to determine the fastest path to a charging station. While Choi et al. in [306] use a densitybased spatial clustering algorithm to build a twolayer obstacle collision avoidance system. Several challenges still exist in the area of battery recharging and need to be appropriately addressed, including: 
• Advancements in wireless charging techniques such as magnetic resonant coupling and inductive coupling techniques are popular and suitable to be used because both of them have acceptable efﬁciency for small to midrange distances. The advancements in this technology are expected to have a positive impact on the use of UAVs. 
• Development of battery technologies will allow UAVs to extend ﬂight ranges. While LithiumIon batteries still dominant, PEM Fuel cells can be more convenient in UAVs because of their higher power density. 
• Motion planning of UAVs is an extremely important factor since it affects the wireless charging efﬁciency and the average power delivered. 
• The vast majority of the research, concerning UAV power management, focuses on multirotor UAVs. Other types of UAVs, such as ﬁxedwing UAVs, need more attention in future research. 
• Artiﬁcial intelligence, especially deep learning, can promote more advances in the ﬁeld of UAV power management. Many research frontiers still need to be explored in this ﬁeld such as; Deep reinforcement learning in the path planning and battery scheduling, convolutional neural networks in identifying charge stations and precise landing in a charging station, and recurrent neural networks in developing discharge models and precisely predicting the end of the charge. 
48614 VOLUME 7, 2019 
• Image processing techniques and smart sensors can also play an important role in identifying charging stations and facilitating precise positioning of UAVs on them. 
B. COLLISION AVOIDANCE AND SWARMING CHALLENGES One of the challenges facing UAVs is collision avoidance. UAV can collide with obstacles, which can either be moving or stationary objects in either indoor or outdoor environment. During UAV ﬂights, it is important to avoid accidents with these obstacles. Therefore, the development of UAV collision avoidance techniques has gained research interest [60], [307]. In this section, we ﬁrst present the major categories of collision avoidance methods. Then, the challenges associated with UAVs collision avoidance are presented. Moreover, we provide research trends and some future insights. 
1) COLLISION AVOIDANCE APPROACHES Many collision avoidance approaches have been proposed in order to avoid potential collisions by UAVs. The authors in [60] presented major categories of collision avoidance methods. These methods can be summarized as geometric approaches [308], path planning approaches [59], potential ﬁeld approaches [309], [310], and visionbased approaches [311]. Several collision avoidance techniques can be utilized for an indoor environment such as vision based methods (using cameras and optical sensors), onboard sensors based methods (using IR, ultrasonic, laser scanner, etc.) and vision based combined with sensor based methods [312]. The collision avoidance approaches are discussed in the following: 
• Geometric Approach is a method that utilizes the geometric analysis to avoid the collision. In [308], the geometric approach was utilized to ensure that the predeﬁned minimum separation distance was not violated. This was done by computing the distance between two UAVs and the time required for the collision to occur. Moreover, it was used for path planning to avoid UAV collisions with obstacles. Several different approaches utilize this method to avoid collision such as Point of Closest Approach [308], Collision Cone Approach [309], [313], and Dubins Paths Approach [314], [315]. 
• Path Planning Approach, which is also referred to as the optimized trajectory approach [316], is a grid based method that utilizes the path replanning algorithm with graph search algorithm to ﬁnd a collision free trajectory during the ﬂight. It uses geometric techniques to ﬁnd an efﬁcient and collision free trajectory, so it shares some similarities with the geometric approach. This method divides the map into a grid and represents the grid as a weighted graph [59]. The grid with graph search algorithm are used to ﬁnd collision free trajectory towards the desired target. 
• Potential Field Approach is proposed by [310] to be used as a collision avoidance method for ground robots. It has also been utilized for collision avoidance among UAVs and obstacles. This method uses the repulsive force ﬁelds which cause the UAV to be repelled by obstacles. The potential function is divided into attracting force ﬁeld which pulls the UAV towards the goal, and repulsive force ﬁeld which is assigned with the obstacle [309]. 
• Visionbased obstacle detection approaches utilize images from small cameras mounted on UAVs to tackle collision challenges. Advances in integrated circuits technology have enabled the design of small and low power sensors and cameras. Combined with advances in computer vision methods, such camera can be used in effective obstacle detection and collision avoidance [311]. Moreover, this approach can be used efﬁciently for collision avoidance in the indoor environment. Such cameras provide realtime information about walls and obstacles in this environment. Many researchers utilized this method to avoid a collision and to provide fully autonomous UAV ﬂights [317]–[320]. The researchers in [318] use a monocular camera with forward facing to generate collisionfree trajectory. In this method, all computations were performed offboard the UAV. To solve the offboard image processing problem, the authors in [321] proposed a collision avoidance system for UAVs with visualbased detection. The image processing operation was performed using two cameras and a small onboard computation unit. 
2) CHALLENGES There are several challenges in the area of collision avoidance approaches and need to be appropriately addressed, including: • The geometric approach utilizes the information such as location and velocity, which can be obtained using Automatic Dependent Surveillance Broadcast (ADSB) sensing method. Thus, it is not applicable to nonaircraft obstacles. Furthermore, input data from ADSB is sensitive to noise which hinders the exact calculation requirement of this approach. Moreover, ADSB requires cooperation from another aircraft, which can be an intruder, which is referred to as cooperative sensing. 
• In noncooperative sensing, the geometric approach requires UAVs to sense and extract information about the environment and obstacles such as position, speed and size of the obstacles. One of the possible solutions is to combine with a visionbased approach that uses a passive device to detect obstacles. However, this approach requires signiﬁcant data processing and only can be used when the objects are close enough. Therefore, hardware limitation for onboard processing should be taken into consideration. 
VOLUME 7, 2019 48615 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
• The diverse robotic control problem that allows UAV to perform complex maneuvers without collision can be solved using the standard control theory. However, each solution is limited to a speciﬁc case and not able to adapt to changes in the environments. This limitation can be overcome by learning from experience. This approach can be achieved using deep learning technique. More speciﬁcally, deep learning allows inferring complex behaviors from raw observation data. However, this approach has issues with samples efﬁ- ciency. For real time application, deep learning requires the usage of onboard GPUs. In [58], a review of deep learning methods and applications for UAVs were presented. 
• In the context of multiUAV systems, the collision avoidance among UAVs is a very complicated task, a technique known as formation control can be used for obstacle avoidance. More speciﬁcally, cooperative formation control algorithms were developed as a collisionavoidance strategy for the multiple UAVs [322], [323]. A recent advancement in this area employs Model Predictive Controllers (MPC) in order to reduce computation time for the optimization of UAV’s trajectory. 
• The limited available power and payload of UAVs are challenging issues, which restrict onboard sensors requirements such as sensor weight, size and required power. These sensors such as IR, Ultrasonic, laser scanner, LADAR and RADAR are typically heavy and large for sUAV [324]. 
• The high speed of UAVs is another challenge. With speed ranges between 35 and 70 Km/h, obstacle avoidance approach must be executed quickly to avoid the collision [324]. 
• The use of UAVs in an indoor environment is a challenging task and it needs higher requirements than outdoor environment. It is very difﬁcult to use GPS for avoiding collisions in an indoor environment, usually indoor is a GPS denied environment. Moreover, RF signals cannot be used in this environment, RF signals could be reﬂected, and degraded by the indoor obstacles and walls. 
• Visionbased collision avoidance methods using cameras suffer from heavy computational operations for image processing. Moreover, these cameras and optical sensors are light sensitive (need sufﬁcient lighting to work properly), so steam and smoke could cause the collision avoidance system to fail. Therefore, sensor based collision avoidance methods can be used to tackle these problems [325], [326]. 
• Some of the vision based collision avoidance approaches use a monocular camera with forward facing to generate collision free trajectory. In this method, all computations are performed offboard, with 30 frames per second which are sent in real time over WiFi network to perform image processing at a remote base station. This 
makes UAV sensing and avoiding obstacles a challenging task [318] 
3) FUTURE INSIGHTS Based on the reviewed literature of collision avoidance challenges, we suggest these possible future directions: 
• UAVs could be integrated with vision sensors, laser range ﬁnder sensors, IR, and/or ultrasound sensors to avoid collisions in all directions [325]. 
• UAV control algorithms can be developed for autonomous hovering without collision, and to ensure the completion of their mission successfully. 
• Standardization rules and regulations for UAVs are highly needed around the world to regulate their operations, to reduce the likelihood of collision among UAVs, and to guarantee safe hovering [327], [328]. 
• Under the deep learning techniques with Model Predictive Controllers (MPC), further work can be done on developing methods to generate guiding samples for more superior obstacle avoidance. The main concern in the deploying deep learning models is the processing requirement. Therefore, hardware limitation for onboard processing should be taken into consideration. 
• More studies are required to improve indoor and outdoor collision avoidance algorithms in order to compute smoother collision free paths, and to evaluate optimized trajectory in terms of aspects such as energy consumption [321]. 
• Onboard processing is required for many UAV operations, such as dynamic sense and avoid algorithms, path replanning algorithms and image processing. Design onboard powerful processor devices with low power consumption is an active area for future researches [58]. 
C. NETWORKING CHALLENGES Fluid topology and rapid changes of links among UAVs are the main characteristics of multiUAV networks or FANET [2]. A UAV in FANET is a node ﬂying in the sky with 3D mobility and speed between 30 to 460 km/h. This high speed causes a rapid change in the link quality between a UAV and other nodes, which introduces different design challenges to the communications protocols. Therefore, FANET needs new communication protocols to fulﬁll the communication requirements of MultiUAV systems [329]. 
1) FANET CHALLENGES 
• One of the challenging issues in FANET is to provide wireless communications for UAVs, (wireless communications between UAVGCS, UAVsatellite, and UAVUAV). More speciﬁcally, coordination, cooperation, routing and communication protocols for the UAVs are challenging tasks due to the frequent connections interruption, ﬂuid network topology, and limited energy resources of UAVs. Therefore, FANET requires some special hardware and a new networking model to address these challenges [3], [330]. 
48616 VOLUME 7, 2019 
• The UAV power constraints limit the computation, communication, and endurance capabilities of UAVs. Energyaware deployment of UAVs with Low power and Lossy Networks (LLT) approach can be efﬁciently used to handle this challenge [8], [331]. 
• FANET is a challenging environment for resources management in view of the special characteristics of this network. The challenge is the complexity of network management, such as the conﬁguration difﬁculties of the hardware components of this network [332]. SoftwareDeﬁned Networking (SDN) and Network Function Virtualization (NFV) are useful approaches to tackle this challenge [333]. 
• Setting up an adhoc network among nodes of UAVs is a challenging task for FANET, due to the high node mobility, large distance between nodes, ﬂuid network topology, link delays and high channel error rates. Therefore, transmitted data across these channels could be lost or delayed. Here the loss is usually caused by disconnections and rapid changes in links among UAVs. DelayTolerant Networking (DTN) architecture was introduced to be used in FANET to address this challenge [334], [335]. 
2) UAV NEW NETWORKING TRENDS A) DELAYTOLERANT NETWORKING (DTN) The DTN architecture was designed to handle challenges facing the dynamic environments as in FANET. [336]. In FANET, DTN approach based on storecarryforward model can be utilized to tackle the long delay for packets delivery. In this model, a UAV can store, carry and forward messages from source to destination with long term data storage and forwarding functions in order to compensate intermittent connectivity of links [337], [338]. DTN can be used with a set of protocols operating at MAC, transport and application layers to provide reliable data transport functions, such as Bundle Protocol (BP) [339], Licklider Transmission Protocol (LTP) [340] and Consultative Committee for Space Data Systems (CCSD) File Delivery Protocol (CFDP) [341]. These protocols use store, carry and forward model, so UAV node keeps a copy for each sent packet until it receives acknowledgment from the next node to conﬁrm that the packet has been received successfully. BP and LTP are protocols developed to cope with FANET challenges and solve the performance problems for FANET. The BP, forms a store, carry and forward overlay networks, to handle message transmissions, receptions and retransmissions using a convergence layer protocol (CLP) services [339], [342] with the underlying transport protocols such as TCPbased [343], UDPbased [344] or LTPbased [340]. The CFDP was designed for ﬁle transfer from source to destination based on store, carry and forward approach for DTN paradigm. It can be run over either reliable or unreliable service mode using transport layer protocols (TCP or UDP) [345]. 
A routing strategy that combines DTN routing protocols in the sky and the existing Adhoc Ondemand Distance Vector (AODV) on the ground for FANETs was proposed in [338]. In this work, they implement a DTN routing protocol on the top of traditional and unmodiﬁed AODV. They also use UAV to store, carry and forward the messages from source to destination. 
B) NETWORK FUNCTION VIRTUALIZATION (VFV) NFV is a new networking architecture concept for network softwarization and it is used to enable the networking infrastructure to be virtualized. More speciﬁcally, an important change in the network functions provisioning approach has been introduced using NFV, by leveraging existing IT virtualization technologies, therefore, network devices, hardware and underlying functions can be replaced with virtual appliances. NFV can provide programming capabilities in FANETs and reduces the network management complexity challenge [346], [347]. The research in [260], proposed dronecell management framework (DMF) using UAVs act as aerial base stations with a multitier dronecell network to complement the terrestrial cellular network based on NFV and SDN paradigms. In [348], the authors proposed a video monitoring platform as a service (VMPaaS) using swarm of UAVs that form a FANET in rural areas. This platform utilizes the recent NFV and SDN paradigms. Due to the complexity of the interconnections and controls in NFV, SDN can be consolidated with NFV as a useful approach to address this challenge. 
C) SOFTWAREDEFINED NETWORKING (SDN) SDN is a promising network architecture, which provides a separation between control plane and data plane. It can also provide a centralized network programmability with global view to control network. Beneﬁting from the centralized controller in SDN, the original network switches could be replaced by uniform SDN switches [349]. Therefore, the deployment and management of new applications and services become much easier. Moreover, the network management, programmability and reconﬁguration can be greatly simpliﬁed [350]. FANET can utilize SDN to address its environment’s challenges and performance issues, such as dynamic and rapid topology changes, link intermittent between nodes; when UAV goes out of service due to coverage problems or for battery recharging. SDN also can help to address the complexity of network management [2]. OpenFlow is one of the most common SDN protocols, used to implement SDN architecture and it separates the network control and data planes functionalities. OpenFlow switch consists of ﬂow Table, OpenFlow Protocol and a Secure Channel [351]. UAVs in FANETs can carry OpenFlow Switches. The SDN control plane could be centralized (one centralized SDN controller), decentralized (the SDN controller is distributed over all UAV nodes), or hybrid in 
VOLUME 7, 2019 48617 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
which the processing control of the forwarded packets can be performed locally on each UAV node and control trafﬁc also exists between the centralized SDN controller and all other SDN elements [2]. The controller collects network statistics from OpenFlow switches, and it also needs to know the latest topology of the UAVs network. Therefore, it is important to maintain the connectivity of the SDN controller with the UAV nodes. The centralized SDN controller has a global view of the network. OpenFlow switches contain software ﬂow tables and protocols to provide a communication between network and control planes, then the controller determines the path and tells network elements where to forward the packets [351]. 
FIGURE 40. Control and data planes in SDN platform. 
Figure 40 shows separation of control and data planes in SDN platform with OpenFlow interface. 
D) LOW POWER AND LOSSY NETWORKS (LLT) In FANETs, UAV nodes are typically characterized by limited resources, such as battery, memory and computational resources. LLN composed of many embedded devices such as UAVs with limited power, and it can be considered as a promising approach to be used in FANET to handle power challenge in UAVs [331], [352]. The Internet Engineering Task Force (IETF) has deﬁned routing protocol for LLN known as routing for lowpower and lossy network (RPL) [331], this protocol can utilize different low power communication technologies, such as low power WiFi, IEEE 802.15.4 [353], which is a standard for lowpower and low rate wireless communications and can be used for UAVUAV communications in FANET [354]. The IPv6 over Low Power Wireless Personal Area Networks (6LoWPAN) deﬁnes a set of protocols that can be used to integrate nodes with IPv6 datagrams over constrained networks such as FANET [355]. 
3) FUTURE INSIGHTS Based on the reviewed literature of the networking challenges and new networking trends of FANET, we suggest these future insights: 
• SDN services can be developed to support FANET functions, such as surveillance, safety and security services. 
• Further studies are needed to study the SDN deployment in FANET with high reliability, reachability and fast mobility of UAV nodes [356] 
• More research is needed to develop speciﬁc security protocols for FANETs based on the SDN paradigm. 
• More studies are needed to replace the conventional network devices by fully decoupled SDN switching devices, in which the control plane is completely decoupled from data plane and the routing protocols can be executed onboard from SDN switching devices [356]. 
• For NFV, more research is needed on optimal virtual network topology, customized endtoend protocols and dynamic network embedding [350]. 
• New routing protocols need to be tailored to FANETs to conserve energy, satisfy bandwidth requirement, and ensure the quality of service [330]. 
• Most of the existing MANET routing protocols partially fail to provide reliable communications among UAVs. So, there is a need to design and implement new routing protocols and networking models for FANETs [7]. 
• FANETs share the same wireless communications bands with other applications such as satellite communications and GSM networks. This leads to frequency congestion problems. Therefore, there is a need to standardize FANET communication bands to mitigate this problem [2]. 
• FANETs lack security support. Each UAV in FANET is required to exchange messages and routing information through wireless links. Therefore, FANETs are vulnerable to attacks. As a result, it is important to design and implement secure FANET routing protocols. 
• The design of congestion control algorithms become an important issue for FANETs. Current research efforts focus on modifying and improving protocols instead of developing new transport protocols that better suite FANETs [357]. 
D. SECURITY CHALLENGES Figure 41 illustrates the general architecture of UAV systems [358], which includes: (1) UAV units; (2) GCSs; (3) satellite (if necessary); and (4) communications links. Various components of the UAV systems provide a large attack surface for malicious intruders which brings huge cyber security challenges to the UAV systems. In this section, to present a comprehensive view of the cyber security challenges of UAV systems, we ﬁrst summarize the attack vectors of general UAV systems. Then, based on the attack vectors and the potential capabilities of attackers, we classify the cyber attacks/challenges against UAV systems into different categories. More importantly, we present a comprehensive literature review of the stateoftheart countermeasures for these security challenges. Finally, based on the reviewed literature, we summarize the security challenges and provide high level insights on how to approach them. 
48618 VOLUME 7, 2019 
FIGURE 41. General architecture of UAV systems. inspired by [358]. 
1) ATTACK VECTORS IN UAV SYSTEMS Based on the architecture illustrated in Figure 41, we identify four attack vectors: 
• Communications links: various attacks could be applied to the communications links between different entities of UAV systems, such as eavesdropping and session hijacking. 
• UAVs themselves: direct attacks on UAVs can cause serious damage to the system. Examples of such attacks are signal spooﬁng and identity hacking. 
• GCSs: attacks on GCSs are more fatal than others because they issue commands to actual devices and collect all the data from the UAVs they control. Such attacks normally involve malwares, viruses and/or key loggers. 
• Humans: indirect attacks on human operators can force wrong/malicious operating commands to be issued to the system. This type of attacks is usually enabled with social engineering techniques. 
2) TAXONOMY OF CYBER SECURITY ATTACKS/CHALLENGES AGAINST UAV SYSTEMS Based on the attack vectors and capabilities of attackers, as well as the work in [358], we classify the possible attacks against UAV systems into different categories. Figure 42 presents a graph that summarizes the proposed attack taxonomy. This attack model deﬁnes three general cyber security challenges for the UAV systems, namely, conﬁdentiality challenges, integrity challenges, and availability challenges. Conﬁdentiality refers to protecting information from being accessed by unauthorized parties. In other words, only the people who are authorized to do so can gain access to data. Attackers could compromise the conﬁdentiality of UAV systems by various approaches (e.g., malware, hijacking, social engineering, etc.) utilizing different attack vector. 
Integrity ensures the authenticity of information. Attackers could modify or fabricate information of UAV systems (e.g., data collected, commands issued, etc.) through communications links, GCSs or compromised UAVs. For example, GPS signal spooﬁng attack. Availability ensures that the services (and the relevant data) that UAV systems carry are running as expected and are accessible to authorized users. Attackers could perform DoS (Deny of Service) attacks on UAV systems by, for example, ﬂooding the communications links, overloading the processing units, or depleting the batteries. 
3) LITERATURE REVIEW OF THE STATEOFTHEART SECURITY ATTACKS/CHALLENGES AND COUNTERMEASURES The UAV related cyber security research can be classiﬁed into three main categories as shown in Figure 43, namely: 
• A: Speciﬁc attack discussion: This line of research focuses on one speciﬁc type of attack (e.g., signal spoofing) and proposes corresponding analysis or countermeasures. 
• B: General security analysis: This line of research presents the high level analysis, discussion or modeling of various attacks that exist in current UAV systems. 
• C: Security framework development: This line of research introduces new monitoring systems, simulation test beds or anomaly detection frameworks for the stateoftheart UAV applications. 
Table 19 summarizes the 15 relevant papers that we have reviewed including their attack vectors and categories. In addition, for the papers that discuss speciﬁc attacks/challenges, we summarize the proposed countermeasures, their limitations and propose high level countermeasures as guidelines for future enhancements. 
• Challenge of speciﬁc attack - GPS spooﬁng: GPS spoofing attack refers to the malicious attempt to manipulate the GPS signals in order to achieve the beneﬁts of the attackers. In [359], the authors provide an analysis of UAV hijacking attack (e.g., GPS signal spooﬁng) with an anatomical approach and outline a model to illustrate: (1) that such attacks can be observed to reveal their vulnerabilities; (2) how to exploit such attacks; (3) how to provide countermeasures and risk mitigation; and (4) details of the impact of such attacks. The use of anatomical investigation of a UAV hijacking aims to provide insights that help all the practitioners of the UAV systems. The article mainly focuses on the analysis of GPS signal spooﬁng attacks. However, the proposed countermeasures (i.e., cryptography based signal authentication and multiple receiver design) are not novel and are only described at high level without sufﬁcient details. 
• Challenge of speciﬁc attack - DDoS (Distributed Deny of Service) attack: DDoS attack is an attempt to make the UAV systems unavailable/unreachable by overwhelming it with trafﬁc from multiple sources. 
VOLUME 7, 2019 48619 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
FIGURE 42. Cyber attack/challenges taxonomy in UAV systems. 
FIGURE 43. Summary of the literature of the stateoftheart attacks/challenges and countermeasures in UAV systems. 
In [365], the authors present a platform to conduct a DDoS attack against UAV systems and layout the general mitigation method. Firstly, the authors present a DDoS attack modeling and speciﬁcation. Then, they analyze the effects caused by this kind of attack. However, the proposed attack scenario is too simple (i.e., UDP ﬂooding) and only a high level solution is said to be included in the future work. 
• Challenge of speciﬁc attack - sensor input spooﬁng attack: In [367], the authors introduce a new attack against UAV systems, named sensor input spooﬁng attack. If the attacker knows exactly how the sensor algorithms work, he can manipulate the victim’s environment to create a new control channel such that the entire UAV system is in dangerous. In addition, they provide suitable mitigation for the introduced attack. To perform the proposed sensor input spooﬁng attack, the attacker must meet three requirements: (1) environment inﬂuence requirement: the attacker must be able to modify the physical phenomenon that the sensor measures; (2) plausible input requirement: the attacker must be able to generate valid input that the sensor system can read; and (3) meaningful response requirement: the attacker must be able to generate meaningful UAV behavior corresponding to the spoofed input. This attack model is too demanding to be practical for many UAV applications and no convincing justiﬁcations were provided. 
• Challenge of speciﬁc attack - hijacking attacks: hijacking attack is a type of attack in which the attacker takes 
control of a communication between two parties and masquerades as one of them (e.g., inject extra commands). In [371], the authors demonstrate the capabilities of the attackers who target UAV systems to perform ManintheMiddle attacks (MitM) and control command injection attacks. They also propose corresponding countermeasures to help address these vulnerabilities. However, the introduced MitM attack requires fully compromised UAV system via reverseengineering techniques which largely limits the feasibility/practicability of this type of attacks. In addition, the proposed encryption scheme lacks the necessary details that warrant credible evaluation of robustness and security. The article of [369] proposes an anomaly detection scheme based on ﬂight patterns ﬁngerprint via statistical measurement of ﬂight data. Firstly, a baseline ﬂight proﬁle is generated. Then, simulated hijacking scenarios are compared to the baseline proﬁle to determine the detection result. The proposed scheme is able to detect all direct hijacking scenarios (i.e., assume a fully compromised UAV and the ﬂight plan has been altered to some random places.). However, temporary control instability (e.g., short time decreasing of amplitude) caused by the attacker cannot be detected. 
• Challenge of speciﬁc attack - Authentication/key management targeted attacks: The authors in [372] ﬁrst propose a communication architecture to integrate LTE technology into integrated CNPC (Control and NonPayload Communication) networks. Then, they deﬁne several security requirements of the proposed architecture. Also, they modify the authentication, the key agreement and the handover key management protocols to make them suitable to the integrated architecture. The proposed modiﬁed protocol is proved to outperform the LTE counterpart protocols via a comparative analysis. In addition, it introduces almost the same amount of communications overhead. 
• Challenge of security framework development: In [360], the authors introduce UAVSim, a simulation testbed for Unmanned Aerial Vehicle Networks cyber security 
48620 VOLUME 7, 2019 
TABLE 19. The summary of the literature of stateoftheart attacks and countermeasures for UAV. 
VOLUME 7, 2019 48621 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
analysis. The proposed test bed can be used to perform various experiments by adjusting different parameters of the networks, hosts and attacks. The UAVSim consists of ﬁve modules: (1) Attack library: generating various attacks (i.e., jamming attacks and DoS attacks); (2) UAV network module: forming different UAV networks; (3) UAV model library: providing different host functions (e.g., attack hosts or UAV hosts); (4) Graphical user interface; (5) Result analysis module; and (6) UAV Module browser. The proposed test bed is helpful in analyzing existing attacks against UAV systems by adjusting different parameters. However, the proposed attack library only contains two types of attacks for now (i.e., jamming attacks and DoS attacks). Therefore, UAVSim must be enriched with other types of attacks to improve its applicability and usability. The authors in [363] present R2U2, a novel framework for runtime monitoring of security threats and system behaviors for Unmanned Aerial Systems (UAS). Speciﬁcally, they extend previous R2U2 from only monitoring of hardware components to both hardware and software conﬁguration monitoring to achieve security threats diagnostic. Also, the extended version of R2U2 provides detection of attack patterns (i.e., illformatted and illegal commands, dangerous commands, nonsensical or repeated navigation commands and transients in GPS signals) rather than component failures. In addition, the FPGA implementation ensures independent monitoring and achieves software reconﬁgurable feature. The extended version of R2U2 is more advanced than the original prototype in terms of attack pattern recognition and secure & independent monitoring. However, the independent monitoring system introduces a new security hole (i.e., the monitoring system itself) without introducing appropriate security mechanism to mitigate it. In the work of [364], the authors present a UAV monitoring system that captures ﬂight data to perform realtime abnormal behavior detection. If an abnormal behavior is detected, the system will raise an alert. The proposed monitoring system includes the following features: – A speciﬁcation language for UAV in-ﬂight behavior modeling; – An algorithm to covert a given ﬂight plan into a behavioral ﬂight proﬁle; – A decision algorithm to determine if an actual UAV ﬂight behavior is normal or not based on the behavioral ﬂight proﬁle; – A visualization system for the operator to monitor multiple UAVs. The proposed monitoring system can detect abnormal behaviors based on the in ﬂight data. However, attacks which do not alter in-ﬂight data are not detectable (e.g., attacks that only collect ﬂight data). The authors of [366] propose and implement a cyber security system to protect UAVs from several dangerous 
attacks, such as attacks that target data integrity and network availability. The detection of data integrity attacks uses Mahalanobis distance method to recognize the malicious UAV that forwards erroneous data to the base station. For the availability attacks (i.e., wormhole attack in this paper), the detection process is summarized as follows: – The UAV relays a packet when it becomes within the range of the base station. The packet includes: (1) node’s type (source, relay or destination); and (2) its next (and previous) hops. – The base station collects the forwarded packets from the UAVs, veriﬁes whether the relay node forwards a packet or not and computes the Message Dropping Rate (MDR). – The base station will raise an alert if the MDR is higher than a false MDR assigned to a normal UAV. The proposed detection scheme for data integrity attacks and network availability attacks is proved to achieve high detection accuracy via simulations. The attack model assumes that every UAV node has the detection module and the node which is determined as malicious node would lose the ability to invoke the detection module. However, there is no cooperation detection algorithm provided and only single node detection procedure is introduced. In the work of [370], the authors present an improved mechanism for UAV security testing. The proposed approach uses a behavioral model, an attack model, and a mitigation model to build a security test suite. The proposed security testing goes as follows: – Model system behavior: associates behavior criteria (BC) with a behavioral model (BM) and build a behavioral test (BT). – Attack type deﬁnition: deﬁnes attack types (A) with attack criteria (AC) and determine attack applicability matrix (AM). – Determine security test requirements: determines at which point during the operation of a behavioral test an attack will occur. – Test generation: a required mitigation process is injected. This work provides a systematic approach to identify vulnerabilities in UAV systems and provides corresponding mitigation. However, the proposed mitigation methods are limited to state roll back or reexecuting. In other words, other prevention or detection methods (e.g., encryption or anomaly detection) cannot be included in the proposed scheme. 
• Challenges of security analysis works: In the work of [358], the authors perform an overall security threat analysis of a UAV system as described in Figure 41. A cyber security threat model is proposed and analyzed to show existing or possible attacks against UAV systems. This model helps designers and users of UAV systems across different aspects, including: (1) understanding 
48622 VOLUME 7, 2019 
threat proﬁles of different UAV systems; (2) addressing various system vulnerabilities; (3) identifying high priority threats; and (4) selecting appropriate mitigation techniques. In addition, a risk evaluation mechanism (i.e., a standard risk evaluation grid included in the ETSI threat assessment methodology ) is used to assess the risk of different threats in UAV systems. Even though this paper provides a comprehensive proﬁle of existing or potential attacks for UAV systems, the proposed attacks are general/high level threats that did not take into consideration hardware and software differences among different UAV systems. The survey paper of [188] reviews various aspects of drones (UAVs) in future smart cities, relating to cyber security, privacy, and public safety. In addition, it also provides representative results on cyber attacks using UAVs. The studied cyber attacks include deauthentication attack and GPS spooﬁng attack. However, no countermeasures are discussed. The authors in [373] introduce an implementation of an encrypted Radio Control (RC) link that can be used with a number of popular RC transmitters. The proposed design uses Galois Embedded Crypto library together with openLRSng opensource radio project. The key exchange algorithm is described below: – TX ﬁrst generates the ephemeral key Ke and, then encrypts Ke using permanent key Kp and IVrand. The encrypted result is mTX 1 and sent to RX. – RX decrypts mTX 1 with Kp to obtain Ke, then generate K ′ e and encrypts K ′ e with Ke and IV0 = 0. The encrypted result is mRX 2 and sent to TX. – TX decrypts mRX 2 with Ke and IV0 = 0 to verify that RX has a copy of Ke. Then TX sends ACK message encrypted with K ′ e and IV0 to RX as a conﬁrmation (denoted as mTX 3 – RX receives mTX). 3 it to verify that TX has a copy of K ′ e. Key exchange is then successful. The proposed scheme achieves secure communication link for open source UAV systems. However, the symmetric key is assumed to be generated in a thirdparty trusted computer and hardcoded in the source code, which degrades the security guarantees and limits the feasibility of the scheme. 
4) SUMMARIZING OF CYBER SECURITY CHALLENGES FOR UAV SYSTEMS Based on the reviewed literature, we summarize four key ﬁndings of the cyber security challenges on UAV applications: 
• DoS attacks and hijacking attacks (i.e., for signal spooﬁng) are the most prevailing threats in the UAV systems: Various DoS attacks have been found and proved to cause serious availability issues in UAV systems. In addition, signal spooﬁng via hijacking attacks is able to severely damage the behaviors of certain UAV systems. 
Possible solution: (1) Strong authentication (e.g., trust platform module, Kerberos, etc.); (2) Signal distortion detection; and (3) Directionofarrival sensing (i.e., transmitter antenna direction detection). 
• Existing countermeasure algorithms are limited to single UAV systems: most of the existing schemes are designed only for single UAV systems. A few of them discuss multiple UAV scenarios without providing any concrete solutions. Possible solution: develop cooperation countermeasure algorithms for multiple UAVs. For example, modify and adopt existing distributed security frameworks (e.g., Kerberos) to multiple UAVs systems; (2) 
• Most of the current security analysis of UAV systems overlook the hardware/software differences (e.g., different hardware platform, different communication protocols, etc.) among various UAV systems: On one hand, some of the attacks only exist in speciﬁc hardware or software conﬁguration. On the other hand, most of the proposed countermeasures are well studied solutions in other communication systems and there could be many deployment difﬁculties when applying them on different UAV systems. Possible solution: design uniﬁed/standard deployment interface or language for the diverse UAV systems. 
• Current UAV simulation test beds are still far from mature: Existing emulators for UAV security analysis are limited to few attack scenarios and speciﬁc hardware/software conﬁgurations. Possible solution: leveraging powerful simulation tools (e.g., Labview) or design customized simulation environments. 
XIII. CONCLUSION The use of UAVs has become ubiquitous in many civil applications. From rush hour delivery services to scanning inaccessible areas, UAVs are proving to be critical in situations where humans are unable to reach or cannot perform dangerous/risky tasks in a timely and efﬁcient manner. In this survey, we review UAV civil applications and their challenges. We also discuss current research trends and provide future insights for potential UAV uses. In SAR operations, UAVs can provide timely disaster warnings and assist in speeding up rescue and recovery operations. They can also carry medical supplies to areas that are classiﬁed as inaccessible. Moreover, they can quickly provide coverage of a large area without ever risking the security or safety of the personnel involved. Using UAVs in SAR operations reduces costs and human lives. In remote sensing, UAVs equipped with sensors can be used as an aerial sensor network for environmental monitoring and disaster management. They can provide numerous datasets to support research teams, serving a broad range of applications such as drought monitoring, water quality monitoring, tree species, disease detection, etc. In risk management, insurance companies can utilize UAVs to generate 
VOLUME 7, 2019 48623 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
NDVI maps in order to have an overview of the hail damage to crops, for instance. Civil infrastructure is expected to dominate the addressable market value of UAV that is forecast to reach $45 Billion in the next few years. In construction and infrastructure inspection applications, UAVs can be used to monitor realtime construction project sites. They can also be utilized in power line and gas pipeline inspections. Using UAVs in civil infrastructure applications can reduce workinjuries, high inspection costs and time involved with conventional inspection methods. In agriculture, UAVs can be efﬁciently used in irrigation scheduling, plant disease detection, soil texture mapping, residue cover and tillage mapping, ﬁeld tile mapping, crop maturity mapping and crop yield mapping. The next generation of UAV sensors can provide onboard image processing and in-ﬁeld analytic capabilities, which can give farmers instant insights in the ﬁeld, without the need for cellular connectivity and cloud connection. With the rapid demise of snail mail and the massive growth of eCommerce, postal companies have been forced to ﬁnd new methods to expand beyond their traditional mail delivery business models. Different postal companies have undertaken various UAV trials to test the feasibility and proﬁtability of UAV delivery services. To make UAV delivery practical, more research is required on UAVs design. UAVs design should cover creating aerial vehicles that can be used in a wide range of conditions and whose capability rivals that of commercial airliners. UAVs have been considered as a novel trafﬁc monitoring technology to collect information about realtime road trafﬁc conditions. Compared to the traditional monitoring devices, UAVs are costeffective and can monitor large continuous road segments or focus on a speciﬁc road segment. However, UAVs have slower speeds compared to vehicles driving on highways. A possible solution might entail changing the regulations to allow UAVs to ﬂy at higher altitudes. Such regulations would allow UAVs to beneﬁt from high views to compensate the limitation in their speed. One potential beneﬁt of UAVs is the capability to ﬁll the gaps in current border surveillance by improving coverage along remote sections of borders. MultiUAV cooperation brings more beneﬁts than single UAV surveillance, such as wider surveillance scope, higher error tolerance, and faster task completion time. However, multiUAV surveillance requires more advanced data collection, sharing, and processing algorithms. To develop more efﬁcient and accurate multiUAV cooperation algorithms, advanced machine learning algorithms could be utilized to achieve better performance and faster response. The use of UAVs is rapidly growing in a wide range of wireless networking applications. UAVs can be used to provide wireless coverage during emergency cases where each UAV serves as an aerial wireless base station when the cellular network goes down. They can also be used to supplement the ground base station in order to provide better 
coverage and higher data rates. Utilizing UAVs in wireless networks needs further research, where topology formation, cooperation between UAVs in a multiUAV network, energy constraints, and mobility models are challenges facing UAVs in wireless networks. Throughout the survey, we discuss the new technology trends in UAV applications such as mmWave, SDN, NFV, cloud computing and image processing. We thoroughly identify the key challenges for UAV civil applications such as charging challenges, collision avoidance and swarming challenges, and networking and security related challenges. In conclusion, a complete legal framework and institutions regulating the civil uses of UAVs are needed to spread UAV services globally. We hope that the key research challenges and opportunities described in this survey will help pave the way for researchers to improve UAV civil applications in the future. 
LIST OF ABBREVIATIONS The acronyms and abbreviations used throughout the survey and their deﬁnitions are listed below: 6LoWPAN IPv6 over Low Power Wireless Personal Area Networks. AC Attack Criteria. AODV Adhoc Ondemand Distance Vector. ATG AirtoGround. BC Behavior Criteria. BDMA Beam Division Multiple Access. BM Behavioral Model. BP Bundle Protocol. BT Behavioral Test. CC Capacitive Coupling. CCSD Consultative Committee for Space Data Systems. CFDP CCSD File Delivery Protocol. CLP Convergence Layer Protocol. CNN Convolutional Neural Network. CNO Cellular Network Operator. CNPC Control and NonPayload Communication. CoA Certiﬁcate of Authorization. DDoS Distributed Deny of Service. DMF Dronecell management framework. DoS Deny of Service. DTN Disruption Tolerant Networking. EVI Enhanced Vegetation Index. FAA Federal Aviation Administration. FANETs Flying AdHoc Networks. FLIR Forward Looking Infrared. FMI Fourier Mellin Invariant. FSO Free Space Optical. GCS Ground Control Station. GIS Geographic Information System. 
48624 VOLUME 7, 2019 
GPS Global Position System. GNDVI Green Normalized Difference Vegetation index. GVI Green Vegetation Index. HAP High Altitude Platform. IETF Internet Engineering Task Force. IMU Inertial Sensor. IoT Internet of Things. IR Infra Red. ITU International Telecommunication Union. KDOT Kansas Department of Transportation. LAP Low Altitude Platform. LLT Low power and Lossy Networks. LOS Line of Sight. LRF Laser Range Finder. LTP Licklider Transmission Protocol. MAC Medium Access Control. MANETs Mobile Adhoc Networks. MBLBP Multiscale Block Local Binary Patterns. MDR Message Dropping Rate. MEC Mobile Edge Computing. MitM ManintheMiddle attacks. mmWave MillimeterWave. MPC Model Predictive Controller. MRC Magnetic Resonance Coupling. NDVI Normalized difference vegetation index. NLOS NonLine of Sight. OBIA ObjectBased Image Analysis. OD OriginDestination. PA Precision Agriculture. PBR PerformanceBased Regulations. PCA Point of closest Approach. PG&E Paciﬁc Gas and Electric Company. PUSCH Physical Uplink Shared Channel. PVI Perpendicular Vegetation Index. RC Radio Control. RPL Routing for lowpower and Lossy network. RSU Roadside Unit. SAC Special Airworthiness Certiﬁcate. SAR Search and Rescue. SAVI Soil Adjusted Vegetation Index. SDN SoftwareDeﬁned Networking. SHM Structural Health Monitoring. SOC Start of Charge. SUAVs SolarPowered UAVs. sUAV Small Unmanned Aerial Vehicle. SVM Support Vector Machine. TIR Thermal Infrared. UAS Unmanned Aircraft Systems. UAV Unmanned Aerial Vehicle. 
VANETs Vehicular Adhoc Networks. NFV Network Function Virtualization. VIs Vegetation Indices. VMPaaS Video Monitoring Platform as a Service. VTOL Vertical TakeOff and Landing. WPT Wireless Power Transfer. WSN Wireless Sensor Networks. 
REFERENCES 
[1] S. Hayat, E. Yanmaz, and R. Muzaffar, ‘‘Survey on unmanned aerial vehicle networks for civil applications: A communications viewpoint,’’ IEEE Commun. Surveys Tuts., vol. 18, no. 4, pp. 2624–2661, 4th Quart., 2016. [2] L. Gupta, R. Jain, and G. Vaszkun, ‘‘Survey of important issues in UAV communication networks,’’ IEEE Commun. Surveys Tuts., vol. 18, no. 2, pp. 1123–1152, 2nd Quart., 2016. [3] N. H. Motlagh, T. Taleb, and O. Arouk, ‘‘Lowaltitude unmanned aerial vehiclesbased Internet of Things services: Comprehensive survey and future perspectives,’’ IEEE Internet Things J., vol. 3, no. 6, pp. 899–922, Dec. 2016. [4] M. Mozaffari, W. Saad, M. Bennis, Y.-H. Nam, and M. Debbah. (2018). ‘‘A tutorial on UAVs for wireless networks: Applications, challenges, and open problems.’’ [Online]. Available: https://arxiv.org/abs/1803.00680 [5] W. Khawaja, I. Guvenc, D. Matolak, U.-C. Fiebig, and N. Schneckenberger. (2018). ‘‘A survey of airtoground propagation channel modeling for unmanned aerial vehicles.’’ [Online]. Available: https://arxiv.org/abs/1801.01656 [6] A. A. Khuwaja, Y. Chen, N. Zhao, M.-S. Alouini, and P. Dobbins, ‘‘A survey of channel modeling for UAV communications,’’ IEEE Com- [7] I.mun. Surveys Tuts., vol. 20, no. 4, pp. 2804–2821, 4th Quart., 2018. Bekmezci, O. K. Sahingoz, and Ş. Temel, ‘‘Flying adhoc networks (FANETs): A survey,’’ Ad Hoc Netw., vol. 11, no. 3, pp. 1254–1270, 2013. [8] Y. Zeng, R. Zhang, and T. J. Lim, ‘‘Wireless communications with unmanned aerial vehicles: Opportunities and challenges,’’ IEEE Commun. Mag., vol. 54, no. 5, pp. 36–42, May 2016. [9] A. Kumbhar, F. Koohifar, I. Güvenç, and B. Mueller, ‘‘A survey on legacy and emerging technologies for public safety communications,’’ IEEE Commun. Surveys Tuts., vol. 19, no. 1, pp. 97–124, 1st Quart., 2017. [10] G. Chmaj and H. Selvaraj, ‘‘Distributed processing applications for UAV/drones: A survey,’’ in Progress in Systems Engineering. Springer, 2015, pp. 449–454. [11] M. Zuckerberg, ‘‘Connecting the world from the sky,’’ Facebook, Cambridge, MA, USA, Tech. Rep. 249, 2014. [12] Y. Sun, D. W. K. Ng, D. Xu, L. Dai, and R. Schober, ‘‘Resource allocation for solar powered UAV communication systems,’’ in Proc. IEEE 19th Int. Workshop Signal Process. Adv. Wireless Commun., Jun. 2018, pp. 1–5. [13] NASA Armstrong Fact Sheet. (2014). Beamed Laser Power for UAVs. [Online]. Available: http://www.nasa.gov/centers/armstrong/ news/FactSheets/FS-087-DFRC.html [14] PwC. Global Market for Commercial Applications of Drone Technology Valued at Over 127bn. Accessed: Feb. 2018. [Online]. Available: https://press.pwc.com/ [15] T. Kelly. The Booming Demand for Commercial Drone Pilots. Accessed: Feb. 2018. [Online]. Available: https://www.theatlantic.com/ technology/archive/2017/01/dronepilotschool/515022/ [16] S. D. Intelligence. The Global UAV Payload Market 2017– 2027. Accessed: Feb. 2018. [Online]. Available: researchandmarkets.com/research/nfpsbm/the_global_uavhttps://www. [17] Grand View Research. UAV Payload Market Analysis by Equipment. Accessed: Feb. 2018. [Online]. Available: https://www. grandviewresearch.com/industryanalysis/uavpayloadmarket [18] D. Joshi. Commercial Unmanned Aerial Vehicle (UAV) Market Analysis, Industry Trends, Companies and What You Should Know. Accessed: Feb. 2018. [Online]. Available: commercialuavmarketanalysis-2017-8http://www.businessinsider.com/ [19] D. Gettinger, ‘‘Drone spending in the ﬁscal year 2017 defense budget,’’ Center Study Drone, AnnandaleonHudson, NY, USA, Tech. Rep., 2016. 
VOLUME 7, 2019 48625 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
[20] PwC. Clarity From Above, PwC Global Report on the Commercial Applications of Drone Technology. Accessed: Feb. 2018. [Online]. Available: https://www.pwc.pl/pl/pdf/clarityfromabovepwc.pdf [21] Wikipedia. (2018). UncrewedVehicle Wikipedia, the Free Encyclopedia. Accessed: Feb. 22, 2018. [Online]. Available: https://en.wikipedia.org/wiki/Uncrewedvehicle [22] Y. B. Sebbane, Smart Autonomous Aircraft: Flight Control and Planning for UAV. Boca Raton, FL, USA: CRC Press, 2015. [23] A. G. Korchenko and O. S. Illyash, ‘‘The generalized classiﬁcation of unmanned air vehicles,’’ in Proc. IEEE 2nd Int. Conf. Actual Problems Unmanned Air Vehicles Develop. (APUAVD), Oct. 2013, pp. 28–34. [24] A. AlHourani, S. Kandeepan, and A. Jamalipour, ‘‘Modeling airtoground path loss for low altitude platforms in urban environments,’’ in Proc. Global Commun. Conf. (GLOBECOM), Dec. 2014, pp. 2898–2904. [25] A. Valcarce et al., ‘‘Airborne base stations for emergency and temporary events,’’ in Proc. Int. Conf. Pers. Satell. Services. Cham, Switzerland: Springer, 2013, pp. 13–25. [26] L. Reynaud and T. Rasheed, ‘‘Deployable aerial communication networks: Challenges for futuristic applications,’’ in Proc. 9th ACM Symp. Perform. Eval. Wireless Ad Hoc, Sensor, Ubiquitous Netw., 2012, pp. 9–16. [27] T. C. Tozer and D. Grace, ‘‘Highaltitude platforms for wireless communications,’’ Electron. Commun. Eng. J., vol. 13, no. 3, pp. 127–137, Jun. 2001. [28] J. Thornton, D. Grace, C. Spillard, T. Konefal, and T. Tozer, ‘‘Broadband communications from a highaltitude platform: The European HeliNet programme,’’ Electron. Commun. Eng. J., vol. 13, no. 3, pp. 138–144, 2001. [29] S. Karapantazis and F. Pavlidou, ‘‘Broadband communications via highaltitude platforms: A survey,’’ IEEE Commun. Surveys Tuts., vol. 7, no. 1, pp. 2–31, 1st Quart., 2005. [30] S. Chandrasekharan et al., ‘‘Designing and implementing future aerial communication networks,’’ IEEE Commun. Mag., vol. 54, no. 5, pp. 26–34, May 2016. [31] I. Bucaille, S. Héthuin, T. Rasheed, A. Munari, R. Hermenier, and S. Allsopp, ‘‘Rapidly deployable network for tactical applications: Aerial base station with opportunistic links for unattended and temporary events ABSOLUTE example,’’ in Proc. Mil. Commun. Conf. (MILCOM), Nov. 2013, pp. 1116–1120. [32] Airbornedrones. (2018). Sentinel+Drone. Accessed: Feb. 26, 2018. [Online]. Available: http://www.airbornedrones.co/ [33] D. W. Matolak and R. Sun, ‘‘Initial results for airground channel measurements & modeling for unmanned aircraft systems: Oversea,’’ in Proc. IEEE Aerosp. Conf., Mar. 2014, pp. 1–15. [34] R. Sun and D. W. Matolak, ‘‘Air–ground channel characterization for unmanned aircraft systems part II: Hilly and mountainous settings,’’ IEEE Trans. Veh. Technol., vol. 66, no. 3, pp. 1913–1925, Mar. 2017. [35] D. W. Matolak and R. Sun, ‘‘Air–ground channel characterization for unmanned aircraft systems—Part III: The suburban and nearurban environments,’’ IEEE Trans. Veh. Technol., vol. 66, no. 8, pp. 6607–6618, Aug. 2017. [36] D. Grace et al., ‘‘The european helinet broadband communications application—An update on progress,’’ in Proc. Jpn. Stratospheric Platforms Syst. Workshop (Invited Paper), 2003, pp. 1–8. [37] S. Katikala, ‘‘Google project loon,’’ InSight, Rivier Acad. J., vol. 10, no. 2, pp. 1–6, 2014. [38] S. Smith, M. Fortneberry, M. Lee, and R. Judy, ‘‘HiSentinel80: Flight of a high altitude airship,’’ in Proc. 11th AIAA Aviation Technol., Integr., Oper. (ATIO) Conf., 2011, p. 6973. [39] S. G. Gupta, M. M. Ghonge, and P. Jawandhiya, ‘‘Review of unmanned aircraft system (UAS),’’ Int. J. Adv. Res. Comput. Eng. Technol., vol. 2, no. 4, p. 1646, 2013. [40] EMT. (2018). Luna X-2000 UAV System. Accessed Feb. 26, 2018. [Online]. Available: http://www.emtpenzberg.de/en/ produkte/drohnensystem/luna.html [41] M. Silvagni, A. Tonoli, E. Zenerino, and M. Chiaberge, ‘‘ Multipurpose UAV for search and rescue operations in mountain avalanche events,’’ Geomatics, Natural Hazards Risk, vol. 8, no. 1, pp. 18–33, 2017. [42] P. Doherty and P. Rudol, ‘‘A UAV search and rescue scenario with human body detection and geolocalization,’’ in Proc. Austral. Conf. Artif. Intell., vol. 4830. Berlin, Germany: Springer, 2007, pp. 1–13. [43] J. Scherer et al., ‘‘An autonomous multiUAV system for search and rescue,’’ in Proc. 1st Workshop Micro Aerial Vehicle Netw., Syst., Appl. Civilian Use, 2015, pp. 33–38. 
[44] M. A. R. Estrada, ‘‘How unmanned aerial vehicles–UAV’s–(or Drones) can help in case of natural disasters response and humanitarian relief aid?’’ Tech. Rep., 2017. [45] (2018). Alcedo. Accessed: Feb. 28, 2018. [Online]. Available: http://www.alcedo.ethz.ch/ [46] J. Joern, ‘‘Examining the use of unmanned aerial systems and thermal infrared imaging for search and rescue efforts beneath snowpack,’’ Ph.D. dissertation, Univ. Denver, Denver, CO, USA, 2015. [47] D. Jo and Y. Kwon, ‘‘ Development of rescue material transport UAV (unmanned aerial vehicle),’’ World J. Eng. Technol., vol. 5, no. 4, p. 720, 2017. [48] M. L. Smith, ‘‘Regulating law enforcement’s use of drones: The need for state legislation,’’ Harvard J. Legislation, vol. 52, p. 423, Sep. 2015. [49] B. R. Jordan, ‘‘A bird’seye view of geology: The use of micro drones/UAVs in geologic ﬁeldwork and education,’’ GSA Today, vol. 25, no. 7, pp. 50–52, 2015. [50] B. Vergouw, H. Nagel, G. Bondt, and B. Custers, ‘‘Drone technology: Types, payloads, applications, frequency spectrum issues and future developments,’’ in The Future of Drone Use. The Hague, Netherlands: TMC Asser Press, 2016, pp. 21–45. [51] D. C. Macke, Jr., ‘‘Systems and image database resources for UAV search and rescue applications,’’ Missouri Univ. Sci. Technol., Rolla, MO, USA, Tech. Rep. T10848, 2013. [52] P. Rudol and P. Doherty, ‘‘Human body detection and geolocalization for UAV search and rescue missions using color and thermal imagery,’’ in Proc. IEEE Aerosp. Conf., Mar. 2008, pp. 1–8. [53] J.-J. HernándezLopez, A.-L. QuintanillaOlvera, J.-L. LópezRamírez, F.-J. RangelButanda, M.-A. IbarraManzano, and D.-L. AlmanzaOjeda, ‘‘Detecting objects using color and depth segmentation with Kinect sensor,’’ Procedia Technol., vol. 3, pp. 196–204, Jan. 2012. [54] K. Mikolajczyk, C. Schmid, and A. Zisserman, ‘‘Human detection based on a probabilistic assembly of robust part detectors,’’ in Proc. Int. Comput. Vis. (ECCV), 2004, pp. 69–82. [55] J. Sun, B. Li, Y. Jiang, and C.-Y. Wen, ‘‘A camerabased target detection and positioning UAV system for search and rescue (SAR) purposes,’’ Sensors, vol. 16, no. 11, p. 1778, 2016. [56] A. Giusti et al., ‘‘A machine learning approach to visual perception of forest trails for mobile robots,’’ IEEE Robot. Autom. Lett., vol. 1, no. 2, pp. 661–667, Jul. 2016. [57] M. B. Bejiga, A. Zeggada, A. Noufﬁdj, and F. Melgani, ‘‘A convolutional neural network approach for assisting avalanche search and rescue operations with UAV imagery,’’ Remote Sens., vol. 9, no. 2, p. 100, 2017. [58] A. Carrio, C. Sampedro, A. RodriguezRamos, and P. Campoy, ‘‘A review of deep learning methods and applications for unmanned aerial vehicles,’’ J. Sensors, vol. 2017, Aug. 2017, Art. no. 3296874. [59] A. Alexopoulos, A. Kandil, P. Orzechowski, and E. Badreddin, ‘‘A comparative study of collision avoidance techniques for unmanned aerial vehicles,’’ in Proc. Int. Conf. Syst., Man, Cybern. (SMC), Oct. 2013, pp. 1969–1974. [60] H. Pham, S. A. Smolka, S. D. Stoller, D. Phan, and J. Yang. (2015). ‘‘A survey on unmanned aerial vehicle collision avoidance systems.’’ [Online]. Available: https://arxiv.org/abs/1508.07723 [61] E. Tuyishimire, A. Bagula, S. Rekhis, and N. Boudriga, ‘‘Cooperative data muling from ground sensors to base stations using UAVs,’’ in Proc. IEEE Symp. Comput. Commun. (ISCC), Jul. 2017, pp. 35–41. [62] M. Quaritsch, K. Kruggl, D. WischounigStrucl, S. Bhattacharya, M. Shah, and B. Rinner, ‘‘Networked UAVs as aerial sensor network for disaster management applications,’’ E&I Elektrotechu. Informationstech., vol. 127, no. 3, pp. 56–63, 2010. [63] Vito. Datasets and Products for a Broad Range of Applications. Accessed: Dec. 2017. [Online]. Available: https://remotesensing. vito.be/dataproductsservices/usingremotesensingdrones [64] NASA. Remote Sensors. Accessed: Dec. 2017. [Online]. Available: https://earthdata.nasa.gov/userresources/remotesensors [65] C. H. Hugenholtz et al., ‘‘Geomorphological mapping with a small unmanned aircraft system (sUAS): Feature detection and accuracy assessment of a photogrammetricallyderived digital terrain model,’’ Geomorphology, vol. 194, pp. 16–24, Jul. 2013. [66] K. Whitehead, B. J. Moorman, and C. H. Hugenholtz, ‘‘Lowcost, ondemand aerial photogrammetry for glaciological measurement,’’ Cryosphere Discuss., vol. 7, no. 3, pp. 1879–1884, 2013. [67] K. Whitehead and C. H. Hugenholtz, ‘‘Remote sensing of the environment with small unmanned aircraft systems (uass), part 1: A review 
48626 VOLUME 7, 2019 
of progress and challenges,’’ J. Unmanned Vehicle Syst., vol. 2, no. 3, pp. 69–85, 2014. [68] R. Austin, Unmanned Aircraft Systems: UAVS Design, Development and Deployment, vol. 54. Hoboken, NJ, USA: Wiley, 2011. [69] T. F. Villa, F. Gonzalez, B. Miljievic, Z. D. Ristovski, and L. Morawska, ‘‘An overview of small unmanned aerial vehicles for air quality measurements: Present applications and future prospectives,’’ Sensors, vol. 16, no. 7, p. 1072, 2016. [70] J. Curry, J. Maslanik, G. Holland, and J. Pinto, ‘‘Applications of aerosondes in the arctic,’’ Bull. Amer. Meteorol. Soc., vol. 85, no. 12, pp. 1855–1861, 2004. [71] A. McGonigle, A. Aiuppa, G. Giudice, G. Tamburello, A. J. Hodson, and S. Gurrieri, ‘‘Unmanned aerial vehicle measurements of volcanic carbon dioxide ﬂuxes,’’ Geophys. Res. Lett., vol. 35, no. 6, Mar. 2008. [72] G. Saggiani et al., ‘‘A UAV system for observing volcanoes and natural hazards,’’ in Proc. AGU Fall Meeting Abstracts, 2007, Art. no. GC11B-05. [73] P.-H. Lin and C.-S. Lee, ‘‘The eyewallpenetration reconnaissance observation of typhoon longwang (2005) with unmanned aerial vehicle, aerosonde,’’ J. Atmos. Ocean. Technol., vol. 25, no. 1, pp. 15–25, 2008. [74] Python_Tips. Introduction to Machine Learning and Its Usage in Remote Sensing. Accessed: Dec. 2017. [Online]. Available: https://pythontips.com/2017/11/11/introductiontomachinelearninganditsusageinremotesensing/ [75] GisCloud. Combining Remote Sensing and Cloud Technology is the Future of Farming. Accessed: Feb. 2018. [Online]. Available: https://www.giscloud.com/blog/agricultureriskmanagementusecase/ [76] H. Kaushal and G. Kaddoum, ‘‘Optical communication in space: Challenges and mitigation techniques,’’ IEEE Commun. Surveys Tuts., vol. 19, no. 1, pp. 57–96, 1st Quart., 2017. [77] M. Madden, T. Jordan, D. Cotten, N. O’Hare, A. Pasqua, and S. Bernardes, ‘‘The future of unmanned aerial systems (UAS) for monitoring natural and cultural resources,’’ in Proc. Photogramm. Week, vol. 15, 2015, pp. 369–384. [78] W. W. Immerzeel et al., ‘‘Highresolution monitoring of Himalayan glacier dynamics using unmanned aerial vehicles,’’ Remote Sens. Environ., vol. 150, pp. 93–103, Jul. 2014. [79] A. Bhardwaj, L. Sam, Akanksha, F. J. MartínTorres, and R. Kumar, ‘‘UAVs as remote sensing platform in glaciology: Present applications and future prospects,’’ Remote Sens. Environ., vol. 175, pp. 196–204, Mar. 2016. [80] L. Sam, A. Bhardwaj, S. Singh, and R. Kumar, ‘‘Remote sensing ﬂow velocity of debriscovered glaciers using Landsat 8 data,’’ Progr. Phys. Geogr., vol. 40, no. 2, pp. 305–321, 2016. [81] G. Yang et al., ‘‘Unmanned aerial vehicle remote sensing for ﬁeldbased crop phenotyping: Current status and perspectives,’’ Frontiers plant Sci., vol. 8, p. 1111, Jun. 2017. [82] P. Liu et al., ‘‘A review of rotorcraft unmanned aerial vehicle (UAV) developments and applications in civil engineering,’’ Smart Struct. Syst, vol. 13, no. 6, pp. 1065–1094, 2014. [83] C. Deng, S. Wang, Z. Huang, Z. Tan, and J. Liu, ‘‘Unmanned aerial vehicles for power line inspection: A cooperative way in platforms and communications,’’ J. Commun., vol. 9, no. 9, pp. 687–692, 2014. [84] F. Mohamadi, ‘‘Vertical takeoff and landing (VTOL) small unmanned aerial system for monitoring oil and gas pipelines,’’ U.S. Patent 8 880 241, Nov. 4, 2014. [85] M. Gheisari, J. Irizarry, and B. N. Walker, ‘‘UAS4SAFETY: The potential of unmanned aerial systems for construction safety applications,’’ in Proc. Construct. Res. Congr., Construct. Global Netw., 2014, pp. 1801–1810. [86] D. Jones, ‘‘Power line inspection—A UAV concept,’’ in Proc. IEE Forum Auton. Syst., Nov. 2005, pp. 1–8. [87] L. F. LuqueVega, B. CastilloToledo, A. Loukianov, and L. E. GonzalezJimenez, ‘‘Power line inspection via an unmanned aerial system based on the quadrotor helicopter,’’ in Proc. 17th Medit. Electrotech. Conf. (MELECON), Apr. 2014, pp. 393–397. [88] C. Sampedro, C. Martinez, A. Chauhan, and P. Campoy, ‘‘A supervised approach to electric tower detection and classiﬁcation for power line inspection,’’ in Proc. Int. Joint Conf. Neural Netw. (IJCNN), Jul. 2014, pp. 1970–1977. [89] Z. Li, Y. Liu, R. Walker, R. Hayward, and J. Zhang, ‘‘Towards automatic power line detection for a UAV surveillance system using pulse coupled neural ﬁlter and an improved Hough transform,’’ Mach. Vis. Appl., vol. 21, no. 5, pp. 677–686, Aug. 2010. 
[90] J. I. Larrauri, G. Sorrosal, and M. González, ‘‘Automatic system for overhead power line inspection using an Unmanned Aerial Vehicle— RELIFO project,’’ in Proc. Int. Conf. Unmanned Aircr. Syst. (ICUAS), May 2013, pp. 244–252. [91] S. Sankarasrinivasan, E. Balasubramanian, K. Karthik, U. Chandrasekar, and R. Gupta, ‘‘Health monitoring of civil structures with integrated UAV and image processing system,’’ Procedia Comput. Sci., vol. 54, pp. 508–515, Jan. 2015. [92] MikroKopter. (Feb. 2018). MikrokopterL4-Me Quadcopter. Accessed: Feb. 2018. [Online]. Available: http://www.mikrokopter.de/en/home [93] I. Sa and P. Corke, ‘‘Vertical infrastructure inspection using a quadcopter and shared autonomy control,’’ in Field and Service Robotics. Berlin, Germany: Springer, 2014, pp. 219–232. [94] T. R. Bretschneider and K. Shetti, ‘‘UAVbased gas pipeline leak detection,’’ in Proc. ARCS, 2015, pp. 1–6. [95] PG&E. (2016). Testing Safety Drones to Inspect Electric and Gas Infrastructure. Accessed: Feb. 22, 2018. [Online]. Available: https://www.pge.com/ [96] Cyberhawk. (2008). Aerial Inspection and Survey Using UAVs. Accessed: Feb. 25, 2018. [Online]. Available: https://www.thecyberhawk.com [97] Industrial_SkyWorks. Drone Inspections Services. Accessed: Feb. 22, 2018. [Online]. Available: https://industrialskyworks.com/droneinspectionsservices [98] M. Gilbert. (2017). Drones & AI: The Next Phase of Automation. Accessed: Feb. 22, 2018. [Online]. Available: http://about.att.com/ innovationblog/drones_automation [99] Honeywell. (2017). Honeywell Launches UAV Industrial Inspection Service, Teams With Intel on Innovative Offering. Accessed: Feb. 22, 2018. [Online]. Available: https://www.honeywell.com/ newsroom/pressreleases/2017/09/ [100] Maverick Inspection Ltd. (1994). Maverick Industrial UAV Inspection. Accessed: Feb. 25, 2018. [Online]. Available: http://www. maverickinspection.com/services/industrialdroneinspection/ [101] Bluestream. Alongside UAV Inspection Services. Accessed: Feb. 25, 2018. [Online]. Available: http://www.bluestreamoffshore. com/site/services/uavinspection.html [102] Q. F. M. Dupont, D. K. H. Chua, A. Tashrif, and E. L. S. Abbott, ‘‘Potential applications of UAV along the construction’s value chain,’’ Procedia Eng., vol. 182, pp. 165–173, Jan. 2017. [103] Y. Ham, K. K. Han, J. J. Lin, and M. GolparvarFard, ‘‘Visual monitoring of civil infrastructure systems via cameraequipped Unmanned Aerial Vehicles (UAVs): A review of related works,’’ Vis. Eng., vol. 4, no. 1, p. 1, 2016. [104] A. Pagnano, M. Höpf, and R. Teti, ‘‘A roadmap for automated power line inspection. Maintenance and repair,’’ in Proc. CIRP, vol. 12, 2013, pp. 234–239. [105] Y. Huang, S. J. Thomson, W. C. Hoffmann, Y. Lan, and B. K. Fritz, ‘‘Development and prospect of unmanned aerial vehicle technologies for agricultural production management,’’ Int. J. Agricult. Biol. Eng., vol. 6, no. 3, pp. 1–10, 2013. [106] N. Muchiri and S. Kimathi, ‘‘A review of applications and potential applications of UAV,’’ in Proc. Sustain. Res. Innov. Conf., 2016, pp. 280–283. [107] W. Kazmi, M. Bisgaard, F. J. GarciaRuiz, K. D. Hansen, and A. la CourHarbo, ‘‘Adaptive surveying and early treatment of crops with a team of autonomous vehicles,’’ in Proc. ECMR, 2011, pp. 253–258. [108] V. GonzalezDugo, P. ZarcoTejada, E. Nicolás, P. Nortes, J. Alarcón, D. Intrigliolo, and E. Fereres, ‘‘Using high resolution UAV thermal imagery to assess the variability in the water status of ﬁve fruit tree species within a commercial orchard,’’ Precis. Agricult., vol. 14, no. 6, pp. 660–678, 2013. [109] F. GarciaRuiz, S. Sankaran, J. M. Maja, W. S. Lee, J. Rasmussen, and R. Ehsani, ‘‘Comparison of two aerial imaging platforms for identiﬁcation of Huanglongbinginfected citrus trees,’’ Comput. Electron. Agricult., vol. 91, pp. 106–115, Feb. 2013. [110] P. Mathur, R. H. Nielsen, N. R. Prasad, and R. Prasad, ‘‘Data collection using miniature aerial vehicles in wireless sensor networks,’’ IET Wireless Sensor Syst., vol. 6, no. 1, pp. 17–25, 2016. [111] J. Primicerio et al., ‘‘A ﬂexible unmanned aerial vehicle for precision agriculture,’’ Precis. Agricult., vol. 13, no. 4, pp. 517–523, 2012. [112] T. Jensen, A. Apan, F. R. Young, L. C. Zeller, and K. Cleminson, ‘‘Assessing grain crop attributes using digital imagery acquired from a lowaltitude remote controlled aircraft,’’ in Proc. Spatial Sci. Inst. Conf., Spatial Knowl. Without Boundaries (SSC). Los Angeles, CA, USA: Spatial Sciences Institute, 2003, pp. 1–11. 
VOLUME 7, 2019 48627 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
[113] E. R. Hunt, W. D. Hively, S. J. Fujikawa, D. S. Linden, C. S. T. Daughtry, and G. W. McCarty, ‘‘Acquisition of NIRgreenblue digital photographs from unmanned aircraft for crop monitoring,’’ Remote Sens., vol. 2, no. 1, pp. 290–305, Jan. 2010. [114] D. G. Sullivan, J. P. Fulton, J. N. Shaw, and G. Bland, ‘‘Evaluating the sensitivity of an unmanned thermal infrared aerial system to detect water stress in a cotton canopy,’’ Trans. ASABE, vol. 50, no. 6, pp. 1963–1969, 2007. [115] N. B. Akesson and W. E. Yates, The Use of Aircraft in Agriculture, vol. 94. Rome, Italy: Food & Agriculture Org., 1974. [116] B. C. Reed, J. F. Brown, D. VanderZee, T. R. Loveland, J. W. Merchant, and D. O. Ohlen, ‘‘Measuring phenological variability from satellite imagery,’’ J. Vegetation Sci., vol. 5, no. 5, pp. 703–714, 1994. [117] J. Baluja et al., ‘‘Assessment of vineyard water status variability by thermal and multispectral imagery using an unmanned aerial vehicle (UAV),’’ Irrigation Sci., vol. 30, no. 6, pp. 511–522, 2012. [118] S. Candiago, F. Remondino, M. De Giglio, M. Dubbini, and M. Gattelli, ‘‘Evaluating multispectral images and vegetation indices for precision farming applications from UAV images,’’ Remote Sens., vol. 7, no. 4, pp. 4026–4047, 2015. [119] S. Khanal, J. Fulton, and S. Shearer, ‘‘An overview of current and potential applications of thermal remote sensing in precision agriculture,’’ Comput. Electron. Agricult., vol. 139, pp. 22–32, Jun. 2017. [120] F. M. Rhoads and C. D. Yonts, ‘‘Irrigation scheduling for corn: Why and how,’’ in The National Corn Handbook (NCH). Madison, WI, USA: Univ. of Wisconson, 2000. [Online]. Available: http://corn.agronomy.wisc.edu/Management/NCH.aspx [121] L. HassanEsfahani, A. TorresRua, A. Jensen, and M. McKee, ‘‘Assessment of surface soil moisture using highresolution multispectral imagery and artiﬁcial neural networks,’’ Remote Sens., vol. 7, no. 3, pp. 2627–2646, 2015. [122] D. Pimentel, R. Zuniga, and D. Morrison, ‘‘Update on the environmental and economic costs associated with alieninvasive species in the United States,’’ Ecol. Econ., vol. 52, no. 3, pp. 273–288, Feb. 2005. [123] R. Calderón, J. A. NavasCortés, C. Lucena, and P. J. ZarcoTejada, ‘‘Highresolution airborne hyperspectral and thermal imagery for early detection of Verticillium wilt of olive using ﬂuorescence, temperature and narrowband spectral indices,’’ Remote Sens. Environ., vol. 139, pp. 231–245, Dec. 2013. [124] D.-C. Wang, G.-L. Zhang, X.-Z. Pan, Y.-G. Zhao, M.-S. Zhao, and G.-F. Wang, ‘‘Mapping soil texture of a plain area using fuzzyc-means clustering method based on land surface diurnal temperature difference,’’ Pedosphere, vol. 22, no. 3, pp. 394–403, 2012. [125] D.-C. Wang et al., ‘‘Retrieval and mapping of soil texture based on land surface diurnal temperature range data from MODIS,’’ PLoS ONE, vol. 10, no. 6, 2015, Art. no. e0129977. [126] U.S. Department of Interior. Mapping Crop Residue and Tillage Intensity on Chesapeake Bay Farmland. Accessed: Feb. 2018. [Online]. Available: https://eros.usgs.gov/doiremotesensingactivities/2015/mappingcropresidueandtillageintensitychesapeakebayfarmland [127] D. G. Sullivan et al., ‘‘Evaluation of multispectral data for rapid assessment of wheat straw residue cover,’’ Soil Sci. Soc. Amer. J., vol. 68, no. 6, pp. 2007–2013, 2004. [128] D. Hofstrand, ‘‘Economics of tile drainage,’’ Ag Decision Maker Newslett., vol. 14, no. 9, p. 3, 2015. [129] H. Steve and E. Kevin. Mapping Tile Drainage Systems. Accessed: Feb. 2018. [Online]. Available: https://fyi.uwex.edu/drainage/ ﬁles/2016/01/1603-HoffmanSystemforMappingTile.pdf [130] T. Jensen, A. Apan, and L. Zeller, ‘‘Crop maturity mapping using a lowcost lowaltitude remote sensing system,’’ in Proc. Surveying Spatial Sci. Inst. Biennial Int. Conf. (SSC). Nedlands, WA, Australia: Surveying and Spatial Sciences Institute, 2009, pp. 1231–1243. [131] K. C. Swain, S. J. Thomson, and H. P. W. Jayasuriya, ‘‘Adoption of an unmanned helicopter for lowaltitude remote sensing to estimate yield and total biomass of a rice crop,’’ Trans. ASABE, vol. 53, no. 1, pp. 21–27, 2010. [132] J. Geipel, J. Link, and W. Claupein, ‘‘Combined spectral and spatial modeling of corn yield based on aerial images and crop surface models acquired with an unmanned aircraft system,’’ Remote Sens., vol. 6, no. 11, pp. 10335–10355, 2014. [133] S. Sankaran et al., ‘‘Lowaltitude, highresolution aerial imaging systems for row and ﬁeld crop phenotyping: A review,’’ Eur. J. Agronomy, vol. 70, pp. 112–123, Oct. 2015. [134] K. Anderson and K. J. Gaston, ‘‘Lightweight unmanned aerial vehicles will revolutionize spatial ecology,’’ Frontiers Ecology Environ., vol. 11, no. 3, pp. 138–146, Apr. 2013. 
[135] Hummingbird Technologies. Advanced Crop Analytics and Artiﬁcial Intelligence for Farmers. Accessed: Feb. 2018. [Online]. Available: https://hummingbirdtech.com/ [136] A. Bannari, D. Morin, F. Bonn, and A. R. Huete, ‘‘ A review of vegetation indices,’’ Remote Sens. Rev., vol. 13, nos. 1–2, pp. 95–120, 1995. [137] E. P. Glenn, A. R. Huete, P. L. Nagler, and S. G. Nelson, ‘‘Relationship between remotelysensed vegetation indices, canopy attributes and plant physiological processes: What vegetation indices can and cannot tell us about the landscape,’’ Sensors, vol. 8, no. 4, pp. 2136–2160, 2008. [138] J. W. Rouse, Jr, R. H. Haas, J. A. Schell, and D. W. Deering, ‘‘Monitoring vegetation systems in the Great Plains with ERTS,’’ Texas A&M Univ., College Station, TX, USA, Tech. Rep., 1974. [139] A. A. Gitelson, Y. J. Kaufman, and M. N. Merzlyak, ‘‘Use of a green channel in remote sensing of global vegetation from EOSMODIS,’’ Remote Sens. Environ., vol. 58, no. 3, pp. 289–298, 1996. [140] A. R. Huete, ‘‘A soiladjusted vegetation index (SAVI),’’ Remote Sens. Environ., vol. 25, no. 3, pp. 295–309, 1988. [141] A. S. Laliberte, J. E. Herrick, and A. Rango, ‘‘Acquisition, orthorectiﬁ- cation, and classiﬁcation of unmanned aerial vehicle (UAV) imagery for rangeland monitoring,’’ Photogramm. Eng. Remote Sens., vol. 76, no. 6, pp. 661–672, 2010. [142] A. S. Laliberte, C. Winters, and A. Rango, ‘‘A procedure for orthorecti- ﬁcation of subdecimeter resolution imagery obtained with an unmanned aerial vehicle (UAV),’’ in Proc. ASPRS Annu. Conf., 2008, pp. 08–047. [143] Deﬁniens Developer 7 User Guide, document 75.968, 2007. [144] C. Zhang and J. M. Kovacs, ‘‘The application of small unmanned aerial systems for precision agriculture: A review,’’ Precis. Agricult., vol. 13, no. 6, pp. 693–712, Dec. 2012. [145] SLANTRANGE. The SlantRange 3p Multispectral Sensor. Accessed: Feb. 2018. [Online]. Available: http://www.slantrange.com/3pslantviewavailable/ [146] L. BurwoodTaylor. The Next Generation of Drone Technologies for Agriculture. Accessed: Feb. 2018. [Online]. Available: https://agfundernews.com/thenextgenerationofdronetechnologiesforagriculture.html [147] J. K. Patil and R. Kumar, ‘‘Advances in image processing for detection of plant diseases,’’ J. Adv. Bioinform. Appl. Res., vol. 2, no. 2, pp. 135–141, 2011. [148] G. N. Fandetti, ‘‘Method of drone delivery using aircraft,’’ U.S. Patent 14 817 356, Feb. 9, 2017. [149] PwC. Besides Blockchain, What’s Missing From IoT? Accessed: Dec. 2017. [Online]. Available: http://usblogs.pwc.com/emergingtechnology/ [150] RedStagfulﬁllment. The Future of Distribution. Accessed: Dec. 2017. [Online]. Available: https://redstagfulﬁllment.com/thefutureofdistribution/ [151] RedStagFULFILLMENT. The Future of Distribution—Part II: Product Distribution in Emerging Markets. Accessed: Dec. 2017. [Online]. Available: https://redstagfulﬁllment.com [152] Wikipedia. Delivery Drone. Accessed: Dec. 2017. [Online]. Available: https://en.wikipedia.org/wiki/Delivery_drone [153] C. T. Howell, III, et al., ‘‘The ﬁrst government sanctioned delivery of medical supplies by remotely controlled unmanned aerial system,’’ NASA Langley Res. Center, Hampton, VA, USA, Tech. Rep., 2016. [154] UnmannedCargo. Drones Going Postal a Summary of Postal Service Delivery Drone Trials. Accessed: Dec. 2017. [Online]. Available: http://unmannedcargo.org/dronesgoingpostalsummarypostalservicedeliverydronetrials/ [155] G. Hoareau, J. J. Liebenberg, J. G. Musial, and T. R. Whitman, ‘‘Package transport by unmanned aerial vehicles,’’ U.S. Patent 9 731 821, Aug. 15, 2017. [156] J. Gilbert. Tacocopter Aims to Deliver Tacos Using Unmanned Drone Helicopters. Hufﬁngton Post, New York, NY, USA. Accessed: Dec. 2017. [Online]. Available: https://www.huffpost.com/entry/tacocopterstartupdeliverstacosbyunmanneddronehelicopter_n_1375842 [157] Mashable. FAA Clariﬁes That Amazon Drones Are Illegal. Accessed: Dec. 2017. [Online]. Available: http://mashable.com/2014/06/24/faaamazondrones-2/ [158] Recode. A New Trump Policy Could Let Amazon and Google Test More Drones in U.S. Cities. Accessed: Dec. 2017. [Online]. Available: https://www.recode.net/2017/10/25/16542940/amazongoogledronesusgovernmenttrump [159] Law. Game of Drones: Liability and Insurance Coverage Issues Coming. Accessed: Dec. 2017. [Online]. Available: https://www.law.com/ ctlawtribune/almID/1202775117054 
48628 VOLUME 7, 2019 
[160] K. M. Fornace, C. J. Drakeley, T. William, F. Espino, and J. Cox, ‘‘Mapping infectious disease landscapes: Unmanned aerial vehicles and epidemiology,’’ Trends Parasitol., vol. 30, no. 11, pp. 514–519, 2014. [161] UnmannedAerial. The Importance of Advanced Weather Data for Commercial Drone Operations. Accessed: Dec. 2017. [Online]. Available: https://unmannedaerial.com/importanceadvancedweatherdatacommercialdroneoperations [162] Business Insider. Amazon Takes Critical Step Toward Drone Delivery. Accessed: Dec. 2017. [Online]. Available: http://www. businessinsider.com/amazontakescriticalsteptowarddronedelivery- 2017-5 [163] Revising the Airspace Model for the Safe Integration of Small Unmanned Aircraft Systems, Amazon Prime Air, Cambridge, U.K., 2015. [164] A. Glaser. Qualcomm’s Latest Technology Allows Drones to Learn About Their Environment as They Fly. Accessed: Feb. 25, 2018. [Online]. Available: https://www.recode.net/2017/1/7/14195076/qualcommdronesmachinelearning-ﬂightcontrolces-2017-snapdragon [165] B. Stark. What Drones May Come: The Future of Unmanned Flight Approaches. Accessed: Dec. 2017. [Online]. Available: https://theconversation.com/ [166] P. Ridden. Nvidia’s Autonomous Drone Keeps on Track Without GPS. Accessed: Dec. 2017. [Online]. Available: https://newatlas.com/nvidiacamerabasedlearningnavigation/50036/ [167] McKinsey. Commercial Drones Are Here: The Future of Unmanned Aerial Systems. Accessed: Dec. 2017. [Online]. Available: https://www.mckinsey.com/industries/ [168] R. D’Andrea, ‘‘Guest editorial can drones deliver?’’ IEEE Trans. Autom. Sci. Eng., vol. 11, no. 3, pp. 647–648, Jul. 2014. [169] H. Menouar, I. Guvenc, K. Akkaya, A. S. Uluagac, A. Kadri, and A. Tuncer, ‘‘UAVenabled intelligent transportation systems for the smart city: Applications and challenges,’’ IEEE Commun. Mag., vol. 55, no. 3, pp. 22–28, Mar. 2017. [170] R. Ke, Z. Li, S. Kim, J. Ash, Z. Cui, and Y. Wang, ‘‘Realtime bidirectional trafﬁc ﬂow parameter estimation from aerial videos,’’ IEEE Trans. Intell. Transp. Syst., vol. 18, no. 4, pp. 890–901, Apr. 2017. [171] G. Guido, V. Gallelli, D. Rogano, and A. Vitale, ‘‘Evaluating the accuracy of vehicle tracking data obtained from unmanned aerial vehicles,’’ Int. J. Transp. Sci. Technol., vol. 5, no. 3, pp. 136–151, Oct. 2016. [172] J. Leitloff, D. Rosenbaum, F. Kurz, O. Meynberg, and P. Reinartz, ‘‘An operational system for estimating road trafﬁc information from aerial images,’’ Remote Sens., vol. 6, no. 11, pp. 11315–11341, Nov. 2014. doi: 10.3390/rs61111315. 
[173] Kaust. (2016). Flood Detection System Employs UAVs Sensing Technology. [Online]. Available: https://innovation.kaust.edu.sa/ technologies/ﬂooddetectionsystememploysuavssensingtechnology/ [174] Y. Qu, L. Jiang, and X. Guo, ‘‘Moving vehicle detection with convolutional networks in UAV videos,’’ in Proc. 2nd Int. Conf. Control, Automat. Robot. (ICCAR), Apr. 2016, pp. 225–229. [175] L. Wang, F. Chen, and H. Yin, ‘‘Detecting and tracking vehicles in trafﬁc by unmanned aerial vehicles,’’ Automat. Construct., vol. 72, pp. 294–308, Dec. 2016. [176] H. Zhou, H. Kong, L. Wei, D. Creighton, and S. Nahavandi, ‘‘Efﬁcient road detection and tracking for unmanned aerial vehicle,’’ IEEE Trans. Intell. Transp. Syst., vol. 16, no. 1, pp. 297–309, Feb. 2015. [177] A. Puri, K. P. Valavanis, and M. Kontitsis, ‘‘Statistical proﬁle generation for trafﬁc monitoring using realtime UAV based video data,’’ in Proc. Medit. Conf. Control Autom. (MED), Jun. 2007, pp. 1–6. [178] R. Reshma, T. Ramesh, and P. Sathishkumar, ‘‘Security situational aware intelligent road trafﬁc monitoring using UAVs,’’ in Proc. Int. Conf. VLSI Syst., Archit., Technol. Appl. (VLSISATA), Jan. 2016, pp. 1–6. [179] P. A. R. Melissa McGuire and M. Rys, ‘‘A study of how unmanned aircraft systems can support the Kansas Department of Transportation’s efforts to improve efﬁciency, safety, and cost reduction,’’ Kansas State University Transportation Center, Manhattan, KS, USA, Tech. Rep., Aug. 2016. [180] Y. M. Chen, L. Dong, and J.-S. Oh, ‘‘Realtime video relay for UAV trafﬁc surveillance systems through available communication networks,’’ in Proc. IEEE Wireless Commun. Netw. Conf. (WCNC), Mar. 2007, pp. 2608–2612. [181] K. Ro, J.-S. Oh, and L. Dong, ‘‘Lessons learned: Application of small UAV for urban highway trafﬁc monitoring,’’ in Proc. 45th AIAA Aerosp. Sci. Meeting Exhibit, 2007, pp. 596–2007. 
[182] J. Apeltauer, A. Babinec, D. Herman, and T. Apeltauer, ‘‘Automatic vehicle trajectory extraction for trafﬁc analysis from aerial video data,’’ Int. Arch. Photogram., Remote Sens. Spatial Inf. Sci., vol. 40, no. 3, p. 9, 2015. [183] T. Tang, S. Zhou, Z. Deng, H. Zou, and L. Lei, ‘‘Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining,’’ Sensors, vol. 17, no. 2, p. 336, 2017. [184] H. Oh, S. Kim, H.-S. Shin, A. Tsourdos, and B. A. White, ‘‘Behaviour recognition of ground vehicle using airborne monitoring of unmanned aerial vehicles,’’ Int. J. Syst. Sci., vol. 45, no. 12, pp. 2499–2514, 2014. [185] C. Sutheerakul, N. Kronprasert, M. Kaewmoracharoen, and P. Pichayapan, ‘‘Application of unmanned aerial vehicles to pedestrian trafﬁc monitoring and management for shopping streets,’’ Transp. Res. Procedia, vol. 25, pp. 1717–1734, Jan. 2017. [186] A. Puri, ‘‘A survey of unmanned aerial vehicles (UAV) for trafﬁc surveillance,’’ Dept. Comput. Sci. Eng., Univ. South Florida, Tampa, FL, USA, 2008. [Online]. Available: http://www.csee.usf.edu/apuri/ techreport.pdf [187] (2016). Federal Aviation Administration. [Online]. Available: https://www.faa.gov/ [188] E. Vattapparamban, I. Güvenç, A. İ. Yurekli, K. Akkaya, and S. Uluağaç, ‘‘Drones for smart cities: Issues in cybersecurity, privacy, and public safety,’’ in Proc. Int. Wireless Commun. Mobile Comput. Conf. (IWCMC), Sep. 2016, pp. 216–221. [189] H. Nakamura and Y. Kajikawa, ‘‘Regulation and innovation: How should small unmanned aerial vehicles be regulated?’’ Technol. Forecasting Social Change, vol. 128, pp. 262–274, Mar. 2018. [190] (2016). NCLS. [Online]. Available: http://www.ncsl.org/research/ transportation/currentunmannedaircraftstatelawlandscape.aspx [191] (2016). FAA. [Online]. Available: https://www.federalregister.gov [192] Taking Off State Unmanned Aircraft Systems Policies, NCLS. [193] F. Mohammed, A. Idries, N. Mohamed, K. AlJaroodi, and I. Jawhar, ‘‘UAVs for smart cities: Opportunities and challenges,’’ Remote Sens. Environ., vol. 58, no. 3, pp. 289–298, Dec. 1996. [194] G. Zhang, R. P. Avery, and Y. Wang, ‘‘Videobased vehicle detection and classiﬁcation system for realtime trafﬁc data collection using uncalibrated video cameras,’’ Transp. Res. Rec., J. Transp. Res. Board, vol. 1993, no. 1, pp. 138–147, 2007. [195] C. C. Haddal and J. Gertler, ‘‘Homeland security: Unmanned aerial vehicles and border surveillance,’’ Library Congr., Washington, DC, USA, Tech. Rep., 2010. [196] I. Maza, F. Caballero, J. Capitán, J. R. MartínezdeDios, and A. Ollero, ‘‘Experimental results in multiUAV coordination for disaster management and civil security applications,’’ J. Intell. Robot. Syst., vol. 61, no. 1, pp. 563–585, 2011. [197] T. Wall and T. Monahan, ‘‘Surveillance and violence from afar: The politics of drones and liminal securityscapes,’’ Theor. Criminol., vol. 15, no. 3, pp. 239–254, 2011. [198] R. L. Finn and D. Wright, ‘‘Unmanned aircraft systems: Surveillance, ethics and privacy in civil applications,’’ Comput. Law Secur. Rev., vol. 28, no. 2, pp. 184–194, 2012. [199] D. Kingston, R. W. Beard, and R. S. Holt, ‘‘Decentralized perimeter surveillance using a team of UAVs,’’ IEEE Trans. Robot., vol. 24, no. 6, pp. 1394–1404, Dec. 2008. [200] A. Birk, B. Wiggerich, H. Bülow, M. Pﬁngsthorn, and S. Schwertfeger, ‘‘Safety, security, and rescue missions with an unmanned aerial vehicle (UAV),’’ J. Intell. Robot. Syst., vol. 64, no. 1, pp. 57–76, 2011. [201] A. Wada, T. Yamashita, M. Maruyama, T. Arai, H. Adachi, and H. Tsuji, ‘‘A surveillance system using small unmanned aerial vehicle (UAV) related technologies,’’ NEC Tech. J., vol. 8, no. 1, pp. 68–72, 2015. [202] N. H. Motlagh, M. Bagaa, and T. Taleb, ‘‘UAVbased IoT platform: A crowd surveillance use case,’’ IEEE Commun. Mag., vol. 55, no. 2, pp. 128–134, Feb. 2017. [203] H. Chen et al., ‘‘Infrared camera using a single nanophotodetector,’’ IEEE Sensors J., vol. 13, no. 3, pp. 949–958, Mar. 2013. [204] C. Jiang and J. Song, ‘‘An ultrahighresolution digital image sensor with pixel size of 50 nm by vertical nanorod arrays,’’ Adv. Mater., vol. 27, no. 30, pp. 4454–4460, 2015. [205] S. C. Folea and G. Mois, ‘‘A lowpower wireless sensor for online ambient monitoring,’’ IEEE Sensors J., vol. 15, no. 2, pp. 742–749, Feb. 2015. [206] Y. LeCun, Y. Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521, no. 7553, p. 436, 2015. 
VOLUME 7, 2019 48629 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
[207] P. Bupe, R. Haddad, and F. RiosGutierrez, ‘‘Relief and emergency communication network based on an autonomous decentralized UAV clustering network,’’ in Proc. SoutheastCon, Apr. 2015, pp. 1–8. [208] R. I. BorYaliniz, A. ElKeyi, and H. Yanikomeroglu, ‘‘Efﬁcient 3-D placement of an aerial base station in next generation cellular networks,’’ in Proc. IEEE Int. Conf. Commun. (ICC), May 2016, pp. 1–5. [209] I. Jawhar, N. Mohamed, J. AlJaroodi, D. P. Agrawal, and S. Zhang, ‘‘Communication and networking of UAVbased systems: Classiﬁcation and associated architectures,’’ J. Netw. Comput. Appl., vol. 84, pp. 93–108, Apr. 2017. [210] J. Zhao, F. Gao, Q. Wu, S. Jin, Y. Wu, and W. Jia, ‘‘Beam tracking for UAV mounted SatCom onthemove with massive antenna array,’’ IEEE J. Sel. Areas Commun., vol. 36, no. 2, pp. 363–375, Feb. 2018. [211] A. Sawalmeh, N. Othman, and H. Shakhatreh, ‘‘Efﬁcient deployment of multiUAVs in massively crowded events,’’ Sensors, vol. 18, no. 11, p. 3640, 2018. [212] Y. Zeng et al., ‘‘Throughput maximization for UAVenabled mobile relaying systems,’’ IEEE Trans. Commun., vol. 64, no. 12, pp. 4983–4996, Dec. 2016. [213] Y. Zeng, J. Lyu, and R. Zhang. (2018). ‘‘Cellularconnected UAV: Potentials, challenges and promising technologies.’’ [Online]. Available: https://arxiv.org/abs/1804.02217 [214] C. She, C. Liu, T. Q. S. Quek, C. Yang, and Y. Li, ‘‘Ultrareliable and lowlatency communications in unmanned aerial vehicle communication systems,’’ IEEE Trans. Commun., to be published. [215] D. W. Matolak, ‘‘Unmanned aerial vehicles: Communications challenges and future aerial networking,’’ in Proc. Int. Conf. Comput., Netw. Commun. (ICNC), Feb. 2015, pp. 567–572. [216] T. S. Rappaport, R. W. Heath, Jr., R. C. Daniels, and J. N. Murdock, Millimeter Wave Wireless Communications. London, U.K.: Pearson Education, 2014. [217] M. Alzenad, M. Z. Shakir, H. Yanikomeroglu, and M.-S. Alouini, ‘‘FSObased vertical backhaul/fronthaul framework for 5G+ wireless networks,’’ IEEE Commun. Mag., vol. 56, no. 1, pp. 218–224, Jan. 2018. [218] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, ‘‘Drone small cells in the clouds: Design, deployment and performance analysis,’’ in Proc. Global Commun. Conf. (GLOBECOM), Dec. 2015, pp. 1–6. [219] H. Shakhatreh, A. Khreishah, and B. Ji, ‘‘Providing wireless coverage to highrise buildings using UAVs,’’ in Proc. Int. Conf. Commun. (ICC), May 2017, pp. 1–6. [220] Guidelines for Evaluation of Radio Interface Technologies for IMTAdvanced, Standard ITU 2135-1, M Series, 2009. [221] A. AlHourani and K. Gomez, ‘‘Modeling cellulartoUAV pathloss for suburban environments,’’ IEEE Wireless Commun. Lett., vol. 7, no. 1, pp. 82–85, Feb. 2017. [222] J. Holis and P. Pechac, ‘‘Elevation dependent shadowing model for mobile communications via high altitude platforms in builtup areas,’’ IEEE Trans. Antennas Propag., vol. 56, no. 4, pp. 1078–1084, Apr. 2008. [223] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, ‘‘Optimal transport theory for powerefﬁcient deployment of unmanned aerial vehicles,’’ in Proc. Int. Conf. Commun. (ICC), May 2016, pp. 1–6. [224] H. Shakhatreh, A. Khreishah, A. Alsarhan, I. Khalil, A. Sawalmeh, and N. S. Othman, ‘‘Efﬁcient 3D placement of a UAV using particle swarm optimization,’’ in Proc. 8th Int. Conf. Inf. Commun. Syst. (ICICS), Apr. 2017, pp. 258–263. [225] A. Sawalmeh, N. S. Othman, H. Shakhatreh, and A. Khreishah, ‘‘Providing wireless coverage in massively crowded events using UAVs,’’ in Proc. 13th Malaysia Int. Conf. Commun. (MICC), Nov. 2017, pp. 158–163. [226] M. Alzenad, A. ElKeyi, F. Lagum, and H. Yanikomeroglu, ‘‘3-D placement of an unmanned aerial vehicle base station (UAVBS) for energyefﬁcient maximal coverage coverage,’’ IEEE Wireless Commun. Lett., vol. 6, no. 4, pp. 434–437, Aug. 2017. [227] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, ‘‘Unmanned aerial vehicle with underlaid devicetodevice communications: Performance and tradeoffs,’’ IEEE Trans. Wireless Commun., vol. 15, no. 6, pp. 3949–3963, Jun. 2016. [228] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, ‘‘Efﬁcient deployment of multiple unmanned aerial vehicles for optimal wireless coverage,’’ IEEE Commun. Lett., vol. 20, no. 8, pp. 1647–1650, Aug. 2016. [229] E. Kalantari, M. Z. Shakir, H. Yanikomeroglu, and A. Yongacoglu, ‘‘Backhaulaware robust 3D drone placement in 5G+ wireless networks,’’ in Proc. IEEE Int. Conf. Commun. Workshops (ICC), May 2017, pp. 109–114. 
[230] M. Alzenad, A. ElKeyi, and H. Yanikomeroglu, ‘‘3-D placement of an unmanned aerial vehicle base station for maximum coverage of users with different QoS requirements,’’ IEEE Wireless Commun. Lett., vol. 7, no. 1, pp. 38–41, Feb. 2018. [231] H. Shakhatreh, A. Khreishah, N. S. Othman, and A. Sawalmeh, ‘‘Maximizing indoor wireless coverage using uavs equipped with directional antennas,’’ in Proc. IEEE 13th Malaysia Int. Conf. Commun. (MICC), Nov. 2017, pp. 175–180. [232] S. A. W. Shah, T. Khattab, M. Z. Shakir, and M. O. Hasna, ‘‘A Distributed approach for networked ﬂying platform association with small cells in 5G+ networks,’’ in Proc. IEEE Global Commun. Conf., Dec. 2017, pp. 1–7. [233] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, ‘‘Wireless communication using unmanned aerial vehicles (UAVs): Optimal transport theory for hover time optimization,’’ IEEE Trans. Wireless Commun., vol. 16, no. 12, pp. 8052–8066, Dec. 2017. [234] E. Kalantari, H. Yanikomeroglu, and A. Yongacoglu, ‘‘On the number and 3D placement of drone base stations in wireless cellular networks,’’ in Proc. IEEE 84th Veh. Technol. Conf. (VTCFall), Sep. 2016, pp. 1–6. [235] H. Shakhatreh, A. Khreishah, J. Chakareski, H. B. Salameh, and I. Khalil, ‘‘On the continuous coverage problem for a swarm of UAVs,’’ in Proc. IEEE 37th Sarnoff Symp., Sep. 2016, pp. 130–135. [236] H. Shakhatreh, A. Khreishah, and I. Khalil, ‘‘Indoor mobile coverage problem using UAVs,’’ IEEE Syst. J., vol. 12, no. 4, pp. 3837–3848, Dec. 2018. [237] M. Zhu, Z. Cai, D. Zhao, J. Wang, and M. Xu, ‘‘Using multiple unmanned aerial vehicles to maintain connectivity of MANETs,’’ in Proc. 23rd Int. Conf. Comput. Commun. Netw. (ICCCN), Apr. 2014, pp. 1–7. [238] J. Lyu, Y. Zeng, R. Zhang, and T. J. Lim, ‘‘Placement optimization of UAVmounted mobile base stations,’’ IEEE Commun. Lett., vol. 21, no. 3, pp. 604–607, Mar. 2017. [239] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, ‘‘Mobile Internet of Things: Can UAVs provide an energyefﬁcient mobile architecture?’’ in Proc. IEEE Global Commun. Conf. (GLOBECOM), Dec. 2016, pp. 1–6. [240] D. Yang, Q. Wu, Y. Zeng, and R. Zhang, ‘‘Energy tradeoff in groundtoUAV communication via trajectory design,’’ IEEE Trans. Veh. Technol., vol. 67, no. 7, pp. 6721–6726, Jul. 2018. [241] D. Alejo, J. A. Cobano, G. Heredia, J. R. Martínezde Dios, and A. Ollero, ‘‘Efﬁcient trajectory planning for WSN data collection with multiple UAVs,’’ in Cooperative Robots and Sensor Networks. Cham, Switzerland: Springer, 2015, pp. 53–75. [242] C. Wang, F. Ma, J. Yan, D. De, and S. K. Das, ‘‘Efﬁcient aerial data collection with UAV in largescale wireless sensor networks,’’ Int. J. Distrib. Sensor Netw., vol. 11, no. 11, 2015, Art. no. 286080. [243] C. Zhan, Y. Zeng, and R. Zhang, ‘‘Energyefﬁcient data collection in UAV enabled wireless sensor network,’’ IEEE Wireless Commun. Lett., vol. 7, no. 3, pp. 328–331, Jun. 2018. [244] H.-L. Määttänen, K. Hämäläinen, J. Venäläinen, K. Schober, M. Enescu, and M. Valkama, ‘‘Systemlevel performance of LTEAdvanced with joint transmission and dynamic point selection schemes,’’ EURASIP J. Adv. Signal Process., vol. 2012, no. 1, p. 247, 2012. [245] M. Sawahashi, Y. Kishiyama, A. Morimoto, D. Nishikawa, and M. Tanno, ‘‘Coordinated multipoint transmission/reception techniques for LTEadvanced [coordinated and distributed MIMO],’’ IEEE Wireless Commun., vol. 17, no. 3, pp. 26–34, Jun. 2010. [246] X. Lin et al., ‘‘The sky is not the limit: LTE for unmanned aerial vehicles,’’ IEEE Commun. Mag., vol. 56, no. 4, pp. 204–210, Apr. 2018. [247] B. K. Donohoo, C. Ohlsen, S. Pasricha, Y. Xiang, and C. Anderson, ‘‘Contextaware energy enhancements for smart mobile devices,’’ IEEE Trans. Mobile Comput., vol. 13, no. 8, pp. 1720–1732, Aug. 2014. [248] V. S. Feng and S. Y. Chang, ‘‘Determination of wireless networks parameters through parallel hierarchical support vector machines,’’ IEEE Trans. Parallel Distrib. Syst., vol. 23, no. 3, pp. 505–512, Mar. 2012. [249] C.-K. Wen, S. Jin, K.-K. Wong, J.-C. Chen, and P. Ting, ‘‘Channel estimation for massive MIMO using Gaussianmixture Bayesian learning,’’ IEEE Trans. Wireless Commun., vol. 14, no. 3, pp. 1356–1368, Mar. 2015. [250] K. W. Choi and E. Hossain, ‘‘Estimation of primary user parameters in cognitive radio systems via hidden Markov model,’’ IEEE Trans. Signal Process., vol. 61, no. 3, pp. 782–795, Feb. 2013. [251] A. Assra, J. Yang, and B. Champagne, ‘‘An EM approach for cooperative spectrum sensing in multiantenna CR networks,’’ IEEE Trans. Veh. Technol., vol. 65, no. 3, pp. 1229–1243, Mar. 2016. 
48630 VOLUME 7, 2019 
[252] C.-K. Yu, K.-C. Chen, and S.-M. Cheng, ‘‘Cognitive radio network tomography,’’ IEEE Trans. Veh. Technol., vol. 59, no. 4, pp. 1980–1997, May 2010. [253] M. Xia, Y. Owada, M. Inoue, and H. Harai, ‘‘Optical and wireless hybrid access networks: Design and optimization,’’ J. Opt. Commun. Netw., vol. 4, no. 10, pp. 749–759, 2012. [254] H. Nguyen, G. Zheng, R. Zheng, and Z. Han, ‘‘Binary inference for primary user separation in cognitive radio networks,’’ IEEE Trans. Wireless Commun., vol. 12, no. 4, pp. 1532–1542, Apr. 2013. [255] A. Aprem, C. R. Murthy, and N. B. Mehta, ‘‘Transmit power control policies for energy harvesting sensors with retransmissions,’’ IEEE J. Sel. Topics Signal Process., vol. 7, no. 5, pp. 895–906, Oct. 2013. [256] G. Alnwaimi, S. Vahid, and K. Moessner, ‘‘Dynamic heterogeneous learning games for opportunistic access in LTEbased macro/femtocell deployments,’’ IEEE Trans. Wireless Commun., vol. 14, no. 4, pp. 2294–2308, Apr. 2015. [257] O. Onireti et al., ‘‘A cell outage management framework for dense heterogeneous networks,’’ IEEE Trans. Veh. Technol., vol. 65, no. 4, pp. 2097– 2113,Apr. 2016. [258] S. Maghsudi and S. Stańczak, ‘‘Channel selection for networkassisted D2D communication via noregret bandit learning with calibrated forecasting,’’ IEEE Trans. Wireless Commun., vol. 14, no. 3, pp. 1309–1322, Mar. 2015. [259] C. Wang, J. Wang, X. Zhang, and X. Zhang, ‘‘Autonomous navigation of uav in largescale unknown complex environment with deep reinforcement learning,’’ in Proc. IEEE Global Conf. Signal Inf. Process. (GlobalSIP), Nov. 2017, pp. 858–862. [260] I. BorYaliniz and H. Yanikomeroglu, ‘‘The new frontier in RAN heterogeneity: Multitier dronecells,’’ IEEE Commun. Mag., vol. 54, no. 11, pp. 48–55, Nov. 2016. [261] A. Bradai, K. Singh, T. Ahmed, and T. Rasheed, ‘‘Cellular software deﬁned networking: A framework,’’ IEEE Commun. Mag., vol. 53, no. 6, pp. 36–43, Jun. 2015. [262] X. Zhou et al., ‘‘Toward 5G: When explosive bursts meet soft cloud,’’ IEEE Netw., vol. 28, no. 6, pp. 12–17, Nov. 2014. [263] C. Jiang, H. Zhang, Y. Ren, Z. Han, K.-C. Chen, and L. Hanzo, ‘‘Machine learning paradigms for nextgeneration wireless networks,’’ IEEE Wireless Commun., vol. 24, no. 2, pp. 98–105, Apr. 2017. [264] C. Liang and F. Yu, ‘‘Wireless virtualization for next generation mobile cellular networks,’’ IEEE Wireless Commun., vol. 22, no. 1, pp. 61–69, Feb. 2015. [265] Z. Xiao, P. Xia, and X.-G. Xia, ‘‘Enabling UAV cellular with millimeterwave communication: Potentials and approaches,’’ IEEE Commun. Mag., vol. 54, no. 5, pp. 66–73, May 2016. [266] S. Rangan, T. S. Rappaport, and E. Erkip, ‘‘Millimeterwave cellular wireless networks: Potentials and challenges,’’ Proc. IEEE, vol. 102, no. 3, pp. 366–385, Mar. 2014. [267] C. Sun, X. Q. Gao, S. Jin, M. Matthaiou, Z. Ding, and C. Xiao, ‘‘Beam division multiple access transmission for massive MIMO communications,’’ IEEE Trans. Commun., vol. 63, no. 6, pp. 2170–2184, Jun. 2015. [268] D. Cohen. Facebook’s Connectivity Lab: Drones, Planes, Satellites, Lasers to Further Internet. Org Mission of Bringing Connectivity to the Whole World. Accessed: Jan. 2018. [Online]. Available: https://www.adweek.com/digital/connectivitylab/ [269] Mobileeurope. Facebook Targets Free Space Optical Tech Connect the Unconnected Accessed: Dec. 2017. [Online]. Available:to https://www.mobileeurope.co.uk/presswire/facebooktargetsfreespaceopticaltechtoconnecttheunconnected [270] Y. Chen, W. Feng, and G. Zheng, ‘‘Optimum placement of UAV as relays,’’ IEEE Commun. Lett., vol. 22, no. 2, pp. 248–251, Feb. 2018. [271] N. Zhao et al., ‘‘Caching UAV assisted secure transmission in hyperdense networks based on interference alignment,’’ IEEE Trans. Commun., vol. 66, no. 5, pp. 2281–2294, May 2018. [272] F. Cheng et al., ‘‘UAV trajectory optimization for data ofﬂoading at the edge of multiple cells,’’ IEEE Trans. Veh. Technol., vol. 37, no. 7, pp. 6732–6736, Jul. 2018. [273] Y. Chen, N. Zhao, Z. Ding, and M.-S. Alouini, ‘‘Multiple UAVs as relays: Multihop single link versus multiple dualhop links,’’ IEEE Trans. Wireless Commun., vol. 17, no. 9, pp. 6348–6359, Sep. 2018. [274] Ericsson. (2013). Ericsson Report Optimizing the Indoor Experience. [Online]. Available: http://www.ericsson.com/res/docs/2013/realperformanceindoors.pdf [275] Alcatel. InBuilding Wireless: One Size Does Not Fit All. Accessed: Jan. 2018. [Online]. Available: http://www.alcatellucent.com/solutions/inbuilding/inbuildinginfographic 
[276] Cisco, ‘‘Cisco service provider WiFi: A platform for business innovation and revenue generation’’ [Online]. Available: http://www.cisco.com/c/en/us/solutions/collateral/serviceprovider/ serviceproviderwi-ﬁ/solution_overview_c22_642482.html [277] A. Asadi, Q. Wang, and V. Mancuso, ‘‘A survey on devicetodevice communication in cellular networks,’’ IEEE Commun. Surveys Tuts., vol. 16, no. 4, pp. 1801–1819, Nov. 2014. [278] B. Saha et al., ‘‘Battery health management system for electric UAVs,’’ IEEE Aerosp. Conf., pp. 1–9, 2011. [279] S. Park, L. Zhang, and S. Chakraborty, ‘‘Battery assignment and scheduling for drone delivery businesses,’’ in Proc. IEEE/ACM Int. Symp. Low Power Electron. Design (ISLPED), Jul. 2017, pp. 1–6. [280] K. A. Swieringa et al., ‘‘Autonomous battery swapping system for smallscale helicopters,’’ Proc. IEEE Int. Conf. Robot. Automat., May 2010, pp. 3335–3340. [281] B. Michini et al., ‘‘Automated battery swap and recharge to enable persistent UAV missions,’’ in Proc. Infotech@Aerospace, Mar. 2011, pp. 1–10. doi: 10.2514/6.2011-1405. 
[282] K. A. O. Suzuki, P. K. Filho, and J. R. Morrison, ‘‘Automatic battery replacement system for UAVs: Analysis and design,’’ J. Intell. Robot. Syst., vol. 65, nos. 1–4, pp. 563–586, Jan. 2012. [283] D. Lee, J. Zhou, and W. T. Lin, ‘‘Autonomous battery swapping system for quadcopter,’’ in Proc. Int. Conf. Unmanned Aircr. Syst. (ICUAS), Jun. 2015, pp. 118–124. [284] N. K. Ure, G. Chowdhary, T. Toksoz, J. P. How, M. A. Vavrina, and J. Vian, ‘‘An automated battery management system to enable persistent missions with multiple aerial vehicles,’’ IEEE/ASME Trans. Mechatron., vol. 20, no. 1, pp. 275–286, Feb. 2015. [285] M. Simic, C. Bil, and V. Vojisavljevic, ‘‘Investigation in wireless power transmission for UAV charging,’’ Procedia Comput. Sci., vol. 60, no. 1, pp. 1846–1855, 2015. doi: 10.1016/j.procs.2015.08.295. 
[286] C. Wang and Z. Ma, ‘‘Design of wireless power transfer device for UAV,’’ in Proc. IEEE Int. Conf. Mechatron. Automat., Aug. 2016, pp. 2449–2454. [Online]. Available: http://ieeexplore.ieee.org/document/7558950/ [287] A. B. Junaid, Y. Lee, and Y. Kim, ‘‘Design and implementation of autonomous wireless charging station for rotarywing UAVs,’’ Aerosp. Sci. Technol., vol. 54, pp. 253–266, Jul. 2016. doi: 
10.1016/j.ast.2016.04.023. 
[288] C. H. Choi, H. J. Jang, S. G. Lim, H. C. Lim, S. H. Cho, and I. Gaponov, ‘‘Automatic wireless drone charging station creating essential environment for continuous drone operation,’’ in Proc. Int. Conf. Control, Autom. Inf. Sci. (ICCAIS), 2017, pp. pp. 132–136. [289] S. Aldhaher, P. D. Mitcheson, J. M. Arteaga, G. Kkelis, and D. C. Yates, ‘‘Lightweight wireless power transfer for midair charging of drones,’’ in Proc. 11th Eur. Conf. Antennas Propag. (EUCAP), Mar. 2017, pp. 336–340. [290] S. Dunbar et al., ‘‘Wireless far-ﬁeld charging of a microUAV,’’ in Proc. IEEE Wireless Power Transfer Conf. (WPTC), May 2015, pp. 7–10. [291] T. M. Mostafa, A. Muharam, and R. Hattori, ‘‘Wireless battery charging system for drones via capacitive power transfer,’’ in Proc. IEEE PELS Workshop Emerg. Technol., Wireless Power Transf. (WoW), May 2017, pp. 1–6. [292] A. B. Junaid, A. Konoiko, Y. Zweiri, M. N. Sahinkaya, and L. Seneviratne, ‘‘Autonomous wireless selfcharging for multirotor unmanned aerial vehicles,’’ Energies, vol. 10, no. 6, p. 803, 2017. [293] S. Hosseini and M. Mesbahi, ‘‘Energyaware aerial surveillance for a longendurance solarpowered unmanned aerial vehicles,’’ J. Guid., Control, Dyn., vol. 39, no. 9, pp. 1980–1993, 2016. [Online]. Available: http://dx.doi.org/10.2514/1.G001737%5Cnhttp://arc.aiaa.org/doi/abs/ 10.2514/1.G001737?journalCode=jgcd [294] J.-S. Lee and K.-H. Yu, ‘‘Optimal path planning of solarpowered UAV using gravitational potential energy,’’ IEEE Trans. Aerosp. Electron. Syst., vol. 53, no. 3, pp. 1442–1451, Jun. 2017. [Online]. Available: http://ieeexplore.ieee.org/document/7859311/ [295] X.-Z. Gao, Z.-X. Hou, Z. Guo, J.-X. Liu, and X.-Q. Chen, ‘‘Energy management strategy for solarpowered highaltitude longendurance aircraft,’’ Energy Convers. Manage., vol. 70, pp. 20–30, Jun. 2013. doi: 10.1016/j.enconman.2013.01.007. 
[296] Y. Huang, H. Wang, and P. Yao, ‘‘Energyoptimal path planning for Solarpowered UAV with tracking moving ground target,’’ Aerosp. Sci. Technol., vol. 53, pp. 241–251, Jun. 2016. doi: 10.1016/j.ast.2016.03.024. 
[297] S. C. Spangelo and E. G. Gilbert, ‘‘Power optimization of solarpowered aircraft with speciﬁed closed ground tracks,’’ J. Aircraft, vol. 50, no. 1, pp. 232–238, 2013. doi: 10.2514/1.C031757. 
VOLUME 7, 2019 48631 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
[298] A. T. Klesh and P. T. Kabamba, ‘‘Energyoptimal path planning for Solarpowered aircraft in level ﬂight,’’ in Proc. AIAA Guid., Navigat. Control Conf., vol. 3, Aug. 2007, pp. 2966–2982. [299] A. Chakrabarty and J. W. Langelaan, ‘‘Energybased longrange path planning for soaringcapable unmanned aerial vehicles,’’ J. Guid., Control, Dyn., vol. 34, no. 4, pp. 1002–1015, 2011. doi: 10.2514/1.52738. 
[300] Ph. Oettershagen, J. Förster, L. Wirth, J. Ambühl, and R. Siegwart. (2017). ‘‘Meteorologyaware multigoal path planning for largescale inspection missions with longendurance solarpowered aircraft.’’ [Online]. Available: https://arxiv.org/abs/1711.10328 [301] B. Lee, S. Kwon, P. Park, and K. Kim, ‘‘Active power management system for an unmanned aerial vehicle powered by solar cells, a fuel cell, and batteries,’’ IEEE Trans. Aerosp. Electron. Syst., vol. 50, no. 4, pp. 3167–3177, Oct. 2014. [302] R. D’Sa et al., ‘‘SUAV:Q—An improved design for a transformable solarpowered UAV,’’ in Proc. IEEE Int. Conf. Intell. Robots Syst., Oct. 2016, pp. 1609–1615. [303] R. D’Sa et al., ‘‘Design and experiments for a transformable solarUAV,’’ Proc. IEEE Int. Conf. Robot. Automat., May/Jun. 2017, pp. 3917–3923. [304] M. A. Messous, S. M. Senouci, and H. Sedjelmaci, ‘‘Network connectivity and area coverage for UAV ﬂeet mobility model with energy constraint,’’ in Proc. IEEE Wireless Commun. Netw. Conf. (WCNC), Sep. 2016, pp. 1–6. [305] B. Zhang, C. H. Liu, J. Tang, Z. Xu, J. Ma, and W. Wang, ‘‘Learningbased energyefﬁcient data collection by unmanned vehicles in smart cities,’’ IEEE Trans. Ind. Informat., vol. 14, no. 4, pp. 1666–1676, Apr. 2018. [Online]. Available: http://ieeexplore.ieee.org/document/8207610/ [306] Y. Choi, H. Jimenez, and D. N. Mavris, ‘‘Twolayer obstacle collision avoidance with machine learning for more energyefﬁcient unmanned aircraft trajectories,’’ Robot. Auto. Syst., vol. 98, pp. 158–173, Dec. 2017. doi: 10.1016/j.robot.2017.09.004. 
[307] A. R. Lacher et al., ‘‘Unmanned aircraft collision avoidance–technology assessment and evaluation methods,’’ in Proc. FAA EUROCONTROL ATM R&D Symp., Barcelona, Spain, 2007, pp. 1–6. [308] J.-W. Park, H.-D. Oh, and M.-J. Tahk, ‘‘UAV collision avoidance based on geometric approach,’’ in Proc. SICE Annu. Conf., 2008, pp. 2122–2126. [309] A. Mujumdar and R. Padhi, ‘‘Nonlinear geometric and differential geometric guidance of UAVs for reactive collision avoidance,’’ Indian Inst. Sci. Bangalore, India, Tech. Rep., 2009. [310] O. Khatib, ‘‘Realtime obstacle avoidance for manipulators and mobile robots,’’ Int. J. Robot. Res., vol. 5, no. 1, pp. 90–98, 1986. [311] J. B. Saunders, ‘‘Obstacle avoidance, visual automatic target tracking, and task allocation for small unmanned air vehicles,’’ Ph.D. dissertation, Brigham Young Univ., Provo, UT, USA, 2009. [312] C. Luo, S. I. McClean, G. Parr, L. Teacy, and R. De Nardi, ‘‘UAV position estimation and collision avoidance using the extended Kalman ﬁlter,’’ IEEE Trans. Veh. Technol., vol. 62, no. 6, pp. 2749–2762, Jul. 2013. [313] A. Chakravarthy and D. Ghose, ‘‘Obstacle avoidance in a dynamic environment: A collision cone approach,’’ IEEE Trans. Syst., Man, Cybern. A, Syst. Humans, vol. 28, no. 5, pp. 562–574, Sep. 1998. [314] L. E. Dubins, ‘‘On curves of minimal length with a constraint on average curvature, and with prescribed initial and terminal positions and tangents,’’ Amer. J. Math., vol. 79, no. 3, pp. 497–516, 1957. [315] M. Shanmugavel, A. Tsourdos, B. White, and R. Żbikowski, ‘‘Cooperative path planning of multiple UAVs using dubins paths with clothoid arcs,’’ Control Eng. Pract., vol. 18, no. 9, pp. 1084–1092, 2010. [316] M. Jun and R. D’Andrea, ‘‘Path planning for unmanned aerial vehicles in uncertain and adversarial environments,’’ in Cooperative Control: Models, Applications and Algorithms. Boston, MA, USA: Springer, 2003, pp. 95–110. [317] K. Schmid, T. Tomic, F. Ruess, and H. Hirschmüler, and M. Suppa, ‘‘Stereo vision based indoor/outdoor navigation for ﬂying robots,’’ in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), Nov. 2013, pp. 3955–3962. [318] H. Alvarez, L. M. Paz, J. Sturm, and D. Cremers, ‘‘Collision avoidance for quadrotors with a monocular camera,’’ in Experimental Robotics. Cham, Switzerland: Springer, 2016, pp. 195–209. [319] K. Schmid, P. Lutz, T. Tomić, E. Mair, and H. Hirschmüller, ‘‘Autonomous visionbased micro air vehicle for indoor and outdoor navigation,’’ J. Field Robot., vol. 31, no. 4, pp. 537–570, 2014. [320] Y. M. Mustafah, A. W. Azman, and F. Akbar, ‘‘Indoor UAV positioning using stereo vision sensor,’’ in Proc. Int. Symp. Robot. Intell. Sensors (IRIS), vol. 41. 2012, pp. 575–579. 
[321] S. Roelofsen, D. Gillet, and A. Martinoli, ‘‘Reciprocal collision avoidance for quadrotors using onboard visual detection,’’ in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), Sep. 2015, pp. 4810–4817. [322] Y. Kuriki and T. Namerikawa, ‘‘Consensusbased cooperative formation control with collision avoidance for a multiUAV system,’’ in Proc. Amer. Control Conf., Jun. 2014, pp. 2077–2082. [323] Y. Kuriki and T. Namerikawa, ‘‘Formation control with collision avoidance for a multiUAV system using decentralized mpc and consensusbased control,’’ in Proc. Eur. Control Conf. (ECC), Jul. 2015, pp. 3079–3084. [324] S. Grifﬁths, J. Saunders, A. Curtis, B. Barber, T. McLain, and R. Beard, ‘‘Obstacle and terrain avoidance for miniature aerial vehicles,’’ in Advances in Unmanned Aerial Vehicles. Dordrecht, The Netherlands: Springer, 2007, pp. 213–244. [325] K. Y. Chee and Z. W. Zhong, ‘‘Control, navigation and collision avoidance for an unmanned aerial vehicle,’’ Sens. Actuators A, Phys., vol. 190, pp. 66–76, Feb. 2013. [326] N. Gageik, T. Müller, and S. Montenegro, ‘‘Obstacle detection and collision avoidance using ultrasonic distance sensors for an autonomous quadrocopter,’’ in Proc. UAVveek Workshop Contrib., 2012. [327] R. Clarke and L. B. Moses, ‘‘The regulation of civilian drones’ impacts on public safety,’’ Comput. Law Secur. Rev., vol. 30, no. 3, pp. 263–285, 2014. [328] K. Archick, R. F. Grimmett, and S. Kan, ‘‘European union’s arms embargo on China: Implications and options for us policy,’’ Library Congr., Washington, DC, USA, CRS Rep. Congr. RL32870, 2005. [329] M. H. Tareque, M. S. Hossain, and M. Atiquzzaman, ‘‘On the routing in ﬂying ad hoc networks,’’ in Proc. Federated Conf. Comput. Sci. Inf. Syst. (FedCSIS), 2015, pp. 1–9. [330] O. K. Sahingoz, ‘‘Networking models in ﬂying adhoc networks (FANETs): Concepts and challenges,’’ J. Intell. Robot. Syst., vol. 74, nos. 1–2, p. 513, 2014. [331] T. Winter, RPL: IPV6 Routing Protocol for LowPower and Lossy Networks, document RFC 6550, 2012. [332] R. Kirichek, A. Vladyko, M. Zakharov, and A. Koucheryavy, ‘‘Model networks for Internet of Things and SDN,’’ in Proc. 18th Int. Conf. Adv. Commun. Technol. (CACT), Jan. 2016, pp. 76–79. [333] K. J. White, E. Denney, M. D. Knudson, A. K. Mamerides, and D. P. Pezaros, ‘‘A programmable SDN+NFVbased architecture for UAV telemetry monitoring,’’ in Proc. 14th IEEE Annu. Consum. Commun. Netw. Conf. (CCNC), Jan. 2017, pp. 522–527. [334] S. Burleigh et al., ‘‘Delaytolerant networking: An approach to interplanetary Internet,’’ IEEE Commun. Mag., vol. 41, no. 6, pp. 128–136, Jun. 2003. [335] J. Loo, J. L. Mauri, and J. H. Ortiz, Mobile Ad Hoc Networks: Current Status and Future Trends. Boca Raton, FL, USA: CRC Press, 2016. [336] F. Warthman et al., ‘‘Delayand disruptiontolerant networks (DTNs),’’ Interplanetary Internet Special Interest Group, Tech. Rep., 2012. [337] K. Fall et al., DelayTolerant Networking Architecture, document RFC 4838, 2007. [338] M. Le, J.-S. Park, and M. Gerla, ‘‘UAV assisted disruption tolerant routing,’’ in Proc. IEEE Military Commun. Conf. (MILCOM), Oct. 2006, pp. 1–5. [339] K. L. Scott and S. Burleigh, Bundle Protocol Speciﬁcation, document RFC 5050, 2007. [340] S. Burleigh, M. Ramadas, and S. Farrell, Licklider Transmission ProtocolMotivation, document RFC 5325, 2008. [341] Recommendation for Space Data System Standards, document CCSDS 133.0-B-1, Blue Book, 2003. [342] Z. Yang et al., ‘‘Analytical characterization of licklider transmission protocol (LTP) in cislunar communications,’’ IEEE Trans. Aerosp. Electron. Syst., vol. 50, no. 3, pp. 2019–2031, Jul. 2014. [343] M. Demmer, J. Ott, and S. Perreault, DelayTolerant Networking TCP ConvergenceLayer Protocol, document RFC 7242, 2014. [344] H. Kruse and S. Ostermann, ‘‘UDP convergence layers for the DTN bundle and LTP protocols,’’ IRTF Delay Tolerant Networking Research Group, no. 1, Nov. 2008. Accessed: Jan. 2018. [Online]. Available: https://tools.ietf.org/html/draftirtfdtnrgudpclayer-00 [345] R. Wang, T. Taleb, A. Jamalipour, and B. Sun, ‘‘Protocols for reliable data transport in space Internet,’’ IEEE Commun. Surveys Tuts., vol. 11, no. 2, pp. 21–32 2nd Quart., 2009. 
48632 VOLUME 7, 2019 
[346] B. Han, V. Gopalakrishnan, L. Ji, and S. Lee, ‘‘Network function virtualization: Challenges and opportunities for innovations,’’ IEEE Commun. Mag., vol. 53, no. 2, pp. 90–97, Feb. 2015. [347] R. Jain and S. Paul, ‘‘Network virtualization and software deﬁned networking for cloud computing: A survey,’’ IEEE Commun. Mag., vol. 51, no. 11, pp. 24–31, Nov. 2013. [348] C. Rametta and G. Schembra, ‘‘Designing a softwarized network deployed on a ﬂeet of drones for rural zone monitoring,’’ Future Internet, vol. 9, no. 1, p. 8, Mar. 2017. [349] D. Zhao, M. Zhu, and M. Xu, ‘‘Leveraging SDN and openﬂow to mitigate interference in enterprise WLAN,’’ J. Netw., vol. 9, no. 6, pp. 1526–1533, 2014. [350] N. Zhang, S. Zhang, P. Yang, O. Alhussein, W. Zhuang, and X. Shen, ‘‘Software deﬁned spaceairground integrated vehicular networks: Challenges and solutions,’’ IEEE Commun. Mag., vol. 55, no. 7, pp. 101–109, Jul. 2017. [351] N. McKeown et al., ‘‘OpenFlow: Enabling innovation in campus networks,’’ ACM SIGCOMM Comput. Commun. Rev., vol. 38, no. 2, pp. 69–74, Apr. 2008. [352] T. Rault, A. Bouabdallah, and Y. Challal, ‘‘Multihop wireless charging optimization in lowpower networks,’’ in Proc. IEEE Global Commun. Conf. (GLOBECOM), Dec. 2013, pp. 462–467. [353] M. C. Domingo, ‘‘An overview of the Internet of Things for people with disabilities,’’ J. Netw. Comput. Appl., vol. 35, no. 2, pp. 584–596, 2012. [354] W. Zafar and B. M. Khan, ‘‘A reliable, delay bounded and less complex communication protocol for multicluster FANETs,’’ Digit. Commun. Netw., vol. 3, no. 1, pp. 30–38, 2017. [355] L. Atzori, A. Iera, and G. Morabito, ‘‘The Internet of Things: A survey,’’ Comput. Netw., vol. 54, no. 15, pp. 2787–2805, Oct. 2010. [356] W. Xia, Y. Wen, C. H. Foh, D. Niyato, and H. Xie, ‘‘A survey on softwaredeﬁned networking,’ IEEE Commun. Surveys Tuts., vol. 17, no. 1, pp. 27–51, 1st Quart., 2014. [357] W. D. Ivancic, D. E. Stewart, D. V. Sullivan, and P. E. Finch, ‘‘An evaluation of protocols for UAV science applications,’’ NASA, Washington, DC, USA, Tech. Rep. NASA/TM-2012-217276, 2012. [358] A. Y. Javaid, W. Sun, V. K. Devabhaktuni, and M. Alam, ‘‘Cyber security threat analysis and modeling of an unmanned aerial vehicle system,’’ in Proc. IEEE Conf. Technol. Homeland Secur. (HST), Nov. 2012, pp. 585–590. [359] S. M. Giray, ‘‘Anatomy of unmanned aerial vehicle hijacking with signal spooﬁng,’’ in Proc. 6th Int. Conf. Recent Adv. Space Technol. (RAST), Jun. 2013, pp. 795–800. [360] A. Y. Javaid, W. Sun, and M. Alam, ‘‘UAVSim: A simulation testbed for unmanned aerial vehicle network cyber security analysis,’’ in Proc. IEEE Globecom Workshops (GC Wkshps), Dec. 2013, pp. 1432–1436. [361] J. Goppert, A. Shull, N. Sathyamoorthy, W. Liu, I. Hwang, and H. Aldridge, ‘‘Software/hardwareintheloop analysis of cyberattacks on unmanned aerial systems,’’ J. Aerosp. Inf. Syst., vol. 11, no. 5, pp. 337–343, 2014. [362] J.-A. Maxa, M. S. B. Mahmoud, and N. Larrieu, ‘‘Secure routing protocol design for UAV Ad hoc NETworks,’’ in Proc. IEEE/AIAA 34th Digit. Avionics Syst. Conf. (DASC), Sep. 2015, pp. 4A5-1–4A5-15. [363] J. Schumann, P. Moosbrugger, and K. Y. Rozier, ‘‘R2u2: Monitoring and diagnosis of security threats for unmanned aerial systems,’’ in Runtime Veriﬁcation. Cham, Switzerland: Springer, 2015, pp. 233–249. [364] Z. Birnbaum, A. Dolgikh, V. Skormin, E. O’Brien, D. Müller, and C. Stracquodaine, ‘‘Unmanned aerial vehicle security using behavioral proﬁling,’’ in Proc. Int. Conf. Unmanned Aircr. Syst. (ICUAS), Jun. 2015, pp. 1310–1319. [365] F. A. G. Muzzi, P. R. de Mello Cardoso, D. F. Pigatto, and K. R. L. J. C. Branco, ‘‘Using botnets to provide security for safety critical embedded systems—A case study focused on UAVs,’’ J. Phys., Conf. Ser., vol. 633, no. 1, 2015, Art. no. 012053. [366] H. Sedjelmaci, S. M. Senouci, and M.-A. Messous, ‘‘How to detect cyberattacks in unmanned aerial vehicles network?’’ in Proc. IEEE Global Commun. Conf. (GLOBECOM), Dec. 2016, pp. 1–6. [367] D. Davidson, H. Wu, R. Jellinek, V. Singh, and T. Ristenpart, ‘‘Controlling UAVs with sensor input spooﬁng attacks,’’ in Proc. WOOT, 2016, p. 16. [368] M. A. Fischler and R. Bolles, ‘‘Random sample consensus: A paradigm for model ﬁtting with applications to image analysis and automated cartography,’’ Commun. ACM, vol. 24, no. 6, pp. 381–395, 1981. 
[369] J. McNeely, M. Hatﬁeld, A. Hasan, and N. Jahan, ‘‘Detection of UAV hijacking and malfunctions via variations in ﬂight data statistics,’’ in Proc. IEEE Int. Carnahan Conf. Secur. Technol. (ICCST), Oct. 2016, pp. 1–8. [370] S. Hagerman, A. Andrews, and S. Oakes, ‘‘Security testing of an unmanned aerial vehicle (UAV),’’ in Proc. Cybersecur. Symp. (CYBERSEC), Apr. 2016, pp. 26–31. [371] N. M. Rodday, R. D. O. Schmidt, and A. Pras, ‘‘Exploring security vulnerabilities of unmanned aerial vehicles,’’ in Proc. IEEE/IFIP Netw. Oper. Manage. Symp. (NOMS), Apr. 2016, pp. 993–994. [372] G. Wang, B.-S. Lee, and J. Y. Ahn, ‘‘Authentication and key management in an LTEbased unmanned aerial system control and nonpayload communication network,’’ in Proc. IEEE 4th Int. Conf. Future Internet Things Cloud Workshops (FiCloudW), Aug. 2016, pp. 355–360. [373] M. Podhradsky, C. Coopmans, and N. Hoffer, ‘‘Improving communication security of open source UAVs: Encrypting radio control link,’’ in Proc. Int. Conf. Unmanned Aircr. Syst. (ICUAS), Jun. 2017, pp. 1153–1159. 
HAZIM SHAKHATREH received the B.S. and M.S. degrees (Hons.) in wireless communication engineering from Yarmouk University, Jordan, in 2008 and 2012, respectively, and the Ph.D. degree from the Electrical and Computer Engineering Department, New Jersey Institute of Technology, in 2018. He is currently an Assistant Professor with the Department of Telecommunications Engineering, Hijjawi Faculty for Engineering Technology, Yarmouk University. His research interests include wireless communications and emerging technologies with a focus on unmanned aerial vehicle (UAV) networks. 
AHMAD H. SAWALMEH received the B.S. degree and the M.S. degree in computer engineering from the Jordan University of Science and Technology, in 2003 and 2005, respectively. He is currently pursuing the Ph.D. degree with the Engineering College, Universiti Tenaga Nasional (UNITEN), Malaysia. He was a Lecturer with the Computer Department, Technical Vocational Training Corporation (TVTC), Saudi Arabia, from 2006 to 2016. His research interests include wireless communications, unmanned aerial vehicle (UAV) networks, and Flying Ad hoc Networks (FANETs). 
ALA ALFUQAHA (S’00–M’04–SM’09) received the Ph.D. degree in computer engineering and networking from the University of MissouriKansas City, Kansas City, MO, USA, in 2004. His research interests include the use of machine learning, in general, and deep learning, in particular, in support of the dataand selfdriven management of largescale deployments of the Internet of Things and smart city infrastructure and services, wireless vehicular networks (VANETs), cooperation and spectrum access etiquette in cognitive radio networks, and management and planning of softwaredeﬁned networks (SDNs). He is an ABET Program Evaluator (PEV). He serves on editorial boards and technical program committees of multiple international journals and conferences. 
VOLUME 7, 2019 48633 
H. Shakhatreh et al.: UAVs: A Survey on Civil Applications and Key Research Challenges 
ZUOCHAO DOU received the B.S. degree in electronics from the Beijing University of Technology, in 2009, the M.S. degree in embedded control systems from the University of Southern Denmark, in 2011, the M.S. degree in communications and signal processing from the University of Rochester, in 2013, and the Ph.D. degree in cloud computing security and network security, in 2018, under the supervision of Dr. A. Khreishah and Dr. I. Khalil. He is currently a Security Software Engineer with Novo Vivo, Inc., Palo Alto, CA, USA, working on the ﬁeld of secure cloud computing. 
EYAD ALMAITA received the B.Sc. degree from AlBalqa Applied University, Jordan, in 2000, the M.Sc. degree from Yarmouk University, Jordan, in 2006, and the Ph.D. degree from Western Michigan University, USA, in 2012. He is currently an Associate Professor with the Mechatronics and Power Engineering Department, Taﬁla Technical University, Jordan. His research interests include energy efﬁcient systems, smart systems, power quality, and artiﬁcial intelligence. 
ISSA KHALIL received the Ph.D. degree in computer engineering from Purdue University, USA, in 2007. He then joined the College of Information Technology (CIT), United Arab Emirates University (UAEU), where he served as an Associate Professor and the Department Head of the Information Security Department. In 2013, he joined the Cyber Security Group, Qatar Computing Research Institute (QCRI), where he has been a member of the Qatar Foundation, as a Senior Scientist, and a Principal Scientist, since 2016. His research interests include the areas of wireless and wireline network security and privacy. He is especially interested in security data analytics, network security, and private data sharing. His novel technique to discover malicious domains following the guiltbyassociation social principle attracts the attention of local media and stakeholders. He is a member of ACM. He received the Best Paper Award at the CODASPY 2018. He has served as an Organizer, a Technical Program Committee Member, and a Reviewer for many international conferences and journals. In 2011, he was granted the CIT Outstanding Professor Award for outstanding performance in research, teaching, and service. He delivers invited talks and keynotes in many local and international forums. 
NOOR SHAMSIAH OTHMAN received the B.Eng. degree in electronic and electrical engineering and the M.Sc. degree in microwave and optoelectronics from the University College London, London, U.K., in 1998 and 2000, respectively, and the Ph.D. degree in wireless communications from the University of Southampton, U.K., in 2008. She is currently a Senior Lecturer with Universiti Tenaga Nasional, Malaysia. Her research interests include audio and speech coding, joint source/channel coding, iterative decoding, unmanned aerial vehicle communications, and SNR estimator. 
ABDALLAH KHREISHAH received the B.S. degree in computer engineering from the Jordan University of Science and Technology, in 2004, and the M.S. and Ph.D. degrees in electrical and computer engineering from Purdue University, in 2006 and 2010, respectively. While pursuing his Ph.D. studies, he was with NEESCOM. He is currently an Associate Professor with the Department of Electrical and Computer Engineering, New Jersey Institute of Technology. His research interests include the areas of visiblelight communication, green networking, network coding, wireless networks, and network security. He is also the Chair of the IEEE North Jersey Section, EMBS Chapter. 
MOHSEN GUIZANI (S’85–M’89–SM’99–F’09) received the B.S. (Hons.) and M.S. degrees in electrical engineering and the M.S. and Ph.D. degrees in computer engineering from Syracuse University, New York, in 1984, 1986, 1987, and 1990, respectively. He was the Associate Vice President of the Graduate Studies and Research, Qatar University, and the Chair of the Computer Science Department, Western Michigan University, and the Computer Science Department, University of West Florida. He also served in academic positions at the University of MissouriKansas City, the University of Colorado Boulder, Syracuse University, and Kuwait University. He is currently a Professor and the ECE Department Chair with the University of Idaho. He is the author of nine books and more than 400 publications in refereed journals and conferences. His research interests include wireless communications and mobile computing, computer networks, mobile cloud computing, security, and smart grid. He has also served as a member, the Chair, and the General Chair for a number of international conferences. He was selected as the Best Teaching Assistant for two consecutive years at Syracuse University. He was the Chair of the IEEE Communications Society Wireless Technical Committee and the TAOS Technical Committee. He currently serves on the editorial boards of several international technical journals. He is also the Founder and the EditorinChief of Wireless Communications and Mobile Computing (Wiley). He has guest edited a number of special issues in IEEE journals and magazines. He has served as an IEEE Computer Society Distinguished Speaker, from 2003 to 2005. 
48634 VOLUME 7, 2019 
",Unmanned Aerial Vehicles (UAVs).pdf,7
"Available online 8 March 2022 1569-8432/© 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/). 
Investigating optimal unmanned aircraft systems flight plans for the  detection of marine ingress  
Ben Mcilwaine , M´onica Rivas Casado *, Toby Waine  
School of Water, Energy and Environment, Cranfield University, College Road, Cranfield MK43 0AL, United Kingdom    
A R T I C L E  I N F O    
Keywords:  Marine ingress  Optimisation  Flight search patterns  Remote sensing  VLOS EVLOS BVLOS  Unmanned aircraft systems  
A B S T R A C T    
From the shutting down of coastal tourism industries, the mass destruction of aquaculture, to the clogging of  power station water intakes, marine ingress events have the potential to cause widespread disruption along our  coastlines. To gain the ability to respond to such events, efforts are being made to advance the understanding of  bloom events which predominantly present as large aggregations of jellyfish, or detached aquatic macroalgaes in  the water column. This paper investigates the optimal flight search patterns with a focus on marine ingress bloom  detection from unmanned aircraft systems (UAS). The detection performance of four flight search patterns are  examined against five different bloom shapes. MonteCarlo simulations are deployed to assess probable performance of flight search pattern against variable bloom shapes. A total of 50,000 simulated flights were conducted, offering a maximum of 500 million marine ingress objects for possible detection. A two phased flight  approach is proposed, with first phase flights conducted as area search strategies, and second phase flights as  datum searches for scenarios where some information of possible bloom location is available. Parallel sweep was  found to be the best performing generalist flight search pattern, closely followed by the phase two search pattern  expanding square. Crossing barrier was found to be competitive but appeared to lend itself towards specific  detection scenarios with sector search being a consistently poor performing flight search pattern. This paper also  investigates the comparative performance of visual line of sight (VLOS), extended visual line of sight (EVLOS),  and beyond visual line of sight (BVLOS) operations. Increase of total survey area was found to increase bloom  detection frequency, with BVLOS operations the highest performer successfully increasing bloom detection by a  factor of 3.7. This paper exhibits the first assessment of flight search patterns within the context of dronebased  detection of marine ingress bloom events. This should facilitate the development of an early warning detection  system that can provide reliable warning to coastal industries prior to a marine ingress event occurring.    
1. Introduction  
Coastal industries regularly battle with the ‘marine ingress problem’  (Purcell, 2005; Flynn and Chapra, 2014). Marine ingress predominantly  forms as large aggregations of jellyfish and submerged aquatic vegetation (Lapointe and Bedford, 2007; Kim et al., 2012), although other less  conspicuous forms do exist as well (Maclsaac, 1996). Coastal industries  such as desalination plants and nuclear power stations often require  large volumes of water to operate; water intakes at these coastal locations can get clogged during marine ingress events, resulting in reduced  productivity (Kim et al., 2012). For example, losses of revenue for nuclear power stations can potentially reach up to US$ 1 million dollars per  day per reactor (Nuclear Energy Institute, 2015; Kim et al., 2016). This  leads to a conflict between the running of the coastal industry, and the  
source of the biological matter that contributes to recurrent ingress  events. This is of particular concern when considering the ecological  importance of seaweed and kelp bed habitats, and the relatively unknown ecological role played by many jellyfish species (Riascos et al.,  2018). However, other industries such as tourism, sporting events, and  finfish aquaculture are not immune from marine bloom events and can  be impacted by the same difficulties (Zoltan et al., 2005; Lippmann  et al., 2011; Haberlin et al., 2021). Marine ingress is essentially undesired biomass from the ocean that can potentially cause huge disruption  for coastal industries, and is critically important within the marine  environment due to this reality.  Barrier netting systems have proven to be useful for aquaculture and  coastal tourism (Vasslides et al., 2018), but have to be carefully  managed to prevent biofouling and damage (Klebert et al., 2013).  
* Corresponding author.  Email address: m.rivascasado@cranfield.ac.uk (M. Rivas Casado).   
Contents lists available at ScienceDirect  
International Journal of Applied Earth   Observations and Geoinformation  
journal homepage: www.elsevier.com/locate/jag  
https://doi.org/10.1016/j.jag.2022.102729  Received 6 December 2021; Received in revised form 5 February 2022; Accepted 17 February 2022    
2 
Nuclear power stations and desalination plants often utilise multistage  filtration procedures within their cooling water intake systems. However, with larger marine ingress events these systems can be overloaded  (Wei et al., 2018). With the provision of early warning of an impending  ingress event, coastal operators can initiate damage mitigation techniques. Water intake rates can be reduced to levels where ingress debris  can be processed by the inbuilt filtering systems. For more extreme  volumes of marine ingress, an increased workforce can be brought onto  site to assist the filtering systems by hosing down drum screens and  removing excess biomass. Without prior warning, severe productivity-  related delays can occur, leading to reduced site efficiency and potential damage to filtration systems (Kim et al., 2012).  Currently, there are minimal options to detect the source of marine  ingress events before their occurrence. Even if detection is possible, their  ability to provide a reliable warning of an impending event is not  currently available. Several studies have focused on the detection of  jellyfish using environmental DNA (eDNA), however difficulties in the  longevity of DNA in the marine environment are well documented  (Thomsen et al., 2012). eDNA is primarily used for the detection of  genetic material presence but not necessarily the current location of a  species individual which does not lend itself towards the application of  an early warning system (Minamoto et al., 2017). Ecological forecasting  models have found success in highlighting environmental conditions  that are significant predictors of medusae presence (Decker et al., 2007)  but are not intended to pinpoint an exact groundtruthed location as  would be required for an early warning system. Aerial remote sensing  attempts at jellyfish detection have taken place, providing variable  success rates across methods ranging from unmanned aircraft systems  (UAS) (Kim et al., 2015, 2016; Schaub et al., 2018), lightaircraft  (Houghton et al., 2006) to satellites (Becking et al., 2015). Remote  sensing efforts to date have predominantly been focused on the use of  RGB sensors, regardless of remote sensing platform. Contrary to the  statement by Jo et al. (2017) that monitoring jellyfish with imaging  satellites is not possible, there have been successful attempts at detecting  jellyfish blooms (Thorn and Lambert, 2016). We do, however, agree that  satellites on their own are not an appropriate jellyfish ingress monitoring platform. Predominantly due to the time taken to orbit and in turn  access data, their comparatively lowresolution images, and the remote  sensing challenges they face concerning atmospheric water vapour (Li  and Wu, 2021; Rogozovsky et al., 2021).  Marine ingress is frequently comprised of jellyfishbased events  during summer, and submerged marine vegetation during winter  months due to more stormy conditions leading to detachment of vegetation from their growing substate. Research focusing on the remote  detection of the majority of marine vegetation is well established  (Dierssen et al., 2015; Menu et al., 2021; Rowan and Kalacska, 2021).  Seagrasses have been successfully detected and researched remotely for  many years, with their sessile and highdensity characteristics large  contributors to the ease of detection. Despite not technically being  vegetation, the remote sensing of microalgae blooms is also well documented due to their conspicuous nature, the tendency to form large  blooms and their links to harmful algal blooms (HABs) (Weybright and  Kelly, 2016; Kwon et al., 2020; Mardones et al., 2021). The toxins  produced by microalgae blooms can cause reductions in habitat biodiversity by killing fish species and marine vertebrates through food chain  biomagnification (Bricelj et al., 2012), but can also indiscriminately  decimate groups of respiring animals due to their ability to induce  hypoxic conditions (Hu et al., 2006; MohdDin et al., 2020). However,  macroalgae blooms (MABs) are less well researched yet also have the  potential to cause severe disruption to coastal industries (Gansel et al.,  2017; Liu et al., 2020). MABs can form through largescale detachment  from their region of growth, or can also grow on the ocean surface;  resulting in the suspension of large quantities of biomass in the water  column (Marx et al., 2021). This biomass can often end up scattered  across intertidal coastal habitats (Fig. 1). This transition from a stationary growing location to one of mobility in the water column is often  
the most frequent cause of clogging of water intakes at nuclear power  stations.  The full economic, social and ecological impact of MABs in coastal  regions is not well understood, but pressure from commercial industries  has begun to change this (Thompson et al., 2020). Research into the  monitoring of MABs, and an early warning system, has been previously  called for (Mcilwaine et al., 2019) but due to their low density, non-  sessile and varied depth nature, advances have been slow. To deal  with the increasing global rates of MABs, efforts must focus on  improving the remote sensing techniques to detect and study them. This  will require a comprehensive understanding of all the aspects  comprising an early warning remote sensing system, from the characterisation of the spectral signature to the detection capability of blooms  in the marine environment. Recent work by Mcilwaine et al. (2019)  identified the wavebands most appropriate for macroalgae marine  ingress detection, and also presented the first multiocean, multisensor  jellyfish bloom detection capability (Mcilwaine and Rivas Casado,  2021). Without the ability to reliably detect, there is no possibility of  providing a warning before an ingress event occurs. This reliability of  detection must be inherent in whatever technology is used to provide  early warning of marine ingress, be it jellyfish or MAB based.  UAS have repeatedly been shown to have superior accessibility,  resolution and priceperformance balance compared to more traditional  remote sensing techniques (Colomina and Molina, 2014; Schaub et al.,  2018; Kwon et al., 2020). Especially as found by Mcilwaine and Rivas  Casado (2021) who showcase UAS based remote sensing, combined with  Artificial Intelligence (AI), as a highly successful method of jellyfish  detection (>90% classification accuracy) in the context of requiring  rapid data access. However, UAS based remote sensing technologies are  not without their own challenges. Rain, wind speed, and flight duration  limitations - in addition to local legal policy - all provide challenges to  UAS operations (AlbajesEizagirre et al., 2011; Nahirnick et al., 2019;  APA, 2020). Recent advancements in UAS technology have allowed new  research avenues to be explored, from the mapping and classification of  ecologically sensitive marine habitats (Ventura et al., 2018) to the  quantification of marine macro litter (Gonçalves et al., 2020) and Antarctic predators (Goebel et al., 2015). With a reliable platform for the  
Fig. 1. Example of a large section of macroalgae found in Cape Town, South  Africa (Nov 2019).  
B. Mcilwaine et al.                                                                                                                                                                                                                               
3 
remote sensing of ingress events, a move towards the production of an  early warning system becomes feasible.  To reliably detect and warn of inbound marine ingress events, UAS  are the current best hope for prospective early warning schemes (Mcilwaine et al., 2019). However, due to their relative infancy there is no  current framework, or structured guidance, in place for surveying the  marine environment. In this paper, a conceptual framework that considers optimal survey routes, and takes into account the most common  naturally occurring bloom shapes, is proposed. Bloom shape has the  potential to highly impact the rate of successful detection with respect to  the type of flight search pattern used, and it is vital to identify which  search pattern can cope with the natural variability of the marine  environment. Aircraftbased search and rescue efforts in the marine (and  terrestrial) environments are well documented and transferable lessons  can most certainly be applied to UAS based marine remote sensing.  Optimisation of surveying routes is critical for any proposed operational  framework to be successful (Auditorium and Washington, 1980). By  both optimising and standardising survey effort (the anecdotal sum of  survey duration, distance travelled and survey coverage), guidance for  the early detection of marine ingress events could help provide the  structure of a practical and workable early warning system. Many parameters can be altered to improve survey performance such as battery  endurance, the type of the flight search pattern, but also how the overall  mission is optimised for various shapes and sizes of marine ingress  events. In some instances, satellite imagery can be obtained to ascertain  bloom shape. However, if these data are not available, then detection  can be gained from UAS flights at a desired location. Another question  that would be beneficial to address; is there any advantage (to an early  warning system) in optimising visual line of sight (VLOS), extended line  of sight (EVLOS) and beyond visual line of sight (BVLOS) UAS operations. Through gaining the ability to fly further and even out of sight,  additional avenues of potentially increased performance improve  rapidly. This also in turn can allow reductions in the cost of flight operations. However, an increase in flying further is not a prerequisite for  BVLOS flight. BVLOS operations can in theory include smallarea surveys, with the defining factor being the pilot not having the ability to  directly view the UAS themselves (Table 1).  The type of UAS operation (VLOS, EVLOS, BVLOS) has the potential  to greatly impact ingress detection, assuming comparable flight parameters. Fig. 2 visualises the coverage between the three forms of  operation: visual line of sight (VLOS), extended visual line of sight  (EVLOS), and beyond visual line of sight (BVLOS). The key difference  between the three forms of operations are predominantly changes in  potential maximum survey area (‘as the crow flies’), with substantial  
operational differences regarding personnel for respective mission  types. As the wider usage of UAS expands, and not just for remote  sensing tasks, it is critical to maintain a grasp on the type of operation  (VLOS/EVLOS/BVLOS) due to rapidly changing legal policy (Davies  et al., 2018) that may enhance or limit usage. A robust early warning  framework should account for both jellyfish and MABs to have success  upon deployment.  The introduction of a framework for optimised UAS marine ingress  detection would benefit a wide range of coastal industries; potentially  prevent lasting damages and decrease marine ingress related reductions  in productivity. An output of an optimised flight plan for a given operation type (VLOS, EVLOS, BVLOS) would be a significant improvement  on current warning systems and operational procedures. Whilst also  gaining the awareness of whether to apply a generalist approach or a  situationspecific approach; this would be a huge benefit to the global  battle against marine ingress. This paper aims to provide a conceptual  framework that can provide the initial guidance for UAS marine ingress  detection in the marine environment. This work hopes to become a  practical tool that can be implemented by a wide range of coastal operators, not just one specific industry, and determine the most suitable  flight plan for a given combination of bloom shape, bloom size, and  flight search pattern. It should also complement the work in Mcilwaine  and Rivas Casado (2021) where algorithms to detect marine ingress  were developed using convolutional neural networks.  This study aims to contribute to the delivery of an optimised con- 
ceptual framework for the UAS detection of marine ingress events. This  will be achieved through the following objectives:.  Assess the performance of various VLOS search patterns across the  most commonly occurring bloom shapes.   
1. Assess the performance of various VLOS search patterns across the  most commonly occurring bloom shapes.   2. Quantify and explore the differences in detection performance of  both VLOS, EVLOS and BVLOS flights.   3. Develop a conceptual decision support chart to quickly ascertain the  optimum flight plan.  
2. Materials and methods  
2.1. Site selection  
The framework was developed for the area surrounding Torness  nuclear power station (East Lothian, UK) (Fig. 3). Torness nuclear power  station is regularly affected by marine ingress and has water intake  systems that are typical of other UK and global nuclear power stations  and desalination plants. Torness is powered by two advanced gascooled  reactors (AGR) with a water intake system consisting of large drum  screens for filtration of debris. Each drum screen draws in large amounts  
Table 1  Definitions of Civil Aviation Authority (CAA) regulated operation types (based  on CAP 722 8th edition (Civil Aviation Authority, 2020)).   
Type of operation  CAA definition  
Visual line of sight  (VLOS)  The remote pilot must maintain clear sight of the UAS  and surrounding area at all times during the operation.  Any form of “image enhancing” devices such as  binoculars are not permitted. The UAS must only be  flown within the pilot’s eyesight (maximum 500 m  horizontal distance from the pilot).  Extended visual line of  sight (EVLOS)  Technically a form of BVLOS operation. Requires  authorisation from the CAA but is not locationspecific.  Like with VLOS operations, collision avoidance is  conducted through “unaided visual observation”.  However, this is commonly mitigated by using  additional observers dedicated to assisting the mission.  Beyond visual line of  sight (BVLOS)  Any operations that are conducted at distances further  than the pilot can directly view the UAS, and respond to  and avoid other airspace users, with their own eyes.  There is a requirement for mitigation that can prevent  any threat of collision to aviation. CAA authorisation is  mandatory and locationspecific. Additional observers  are not compulsory to conduct operations.   
Fig. 2. Visual line of sight (VLOS), extended visual line of sight (EVLOS) and  beyond visual line of sight (BVLOS) operational illustration.  
B. Mcilwaine et al.                                                                                                                                                                                                                               
4 
of water from the ocean and has a direct impact on how much power the  station can generate at a time (Chae et al., 2008).  
Fig. 3 shows the zonation of the study area, with representative areas  showing the theoretical maximum extent of VLOS, and EVLOS, flights.  In turn, providing an easytounderstand visual description of survey  area coverage in relation to the water intake location.  
2.2. Generation of data  
Synthetic data were generated to simulate bloom shapes and flight  search patterns. Within this context ‘bloom shape’ refers to the spatial  composition of a generic marine ingress object, and flight ‘search  pattern’ refers to the flight pattern that the UAS takes to survey an area  when searching for a given marine ingress bloom. The analysis will be  focused on optimising UAS flight plans, and makes no attempt at being  able to discriminate between different types of marine ingress. Five  distinct shapes of bloom were simulated to cover the most commonly  occurring, natural bloom shapes; derived from imagery collected during  jellyfish data collection flights conducted between 2016 and 2018.  Despite being derived from jellyfish data, it is hoped that there are useful  levels of transferability to macroalgae blooms. The bloom shapes  investigated were as follows: ‘large coverage’, ‘clustered’, ’elongate’  ‘small coverage’ and ‘circular’ (Fig. 4). The small coverage bloom shape  looks unnatural in its representation, however is a common occurrence  during bloom events due to the presence of anthropogenic structures  (such as docks, jetties and piers). Each bloom, regardless of shape,  consisted of 10,000 marine ingress objects to ensure equivalence of  
comparison when investigating search pattern performance. Blooms  consisting of 10,000 objects were selected due to previous UAS imagery  (greater than4,000 images) collected giving an indication of bloom  extent; the same images were also used to develop JellyNet (Mcilwaine  and Rivas Casado, 2021).  All bloom shapes were created by individually hardcoding each  shape within individual bespoke ’R’ scripts. The basis of bloom shape  structure was built upon the principle of randomly filling matrix cells in  relation to a designated cell position. Each script used a random distribution function to randomly fill cells within the shape matrix using the  plot.matrix package. In order to maintain exactly 10,000 objects per  bloom, each iteration of bloom was coded to overfill each shape with  bloom objects, with a secondary bespoke ’hunting’ algorithm used to  eliminate single bloom objects when blooms exceeded 10,000 objects  (removal via the same random distribution function used to fill the  blooms). This ensured each bloom had exactly 10,000 objects to allow a  statistically robust and equal comparison across every iteration. This  process had to be applied to all bloom shapes, and was essential due to  the nature of the generation of randomised blooms, and in particular  clustered shaped blooms; where a randomised number of clusters were  generated, with a randomised number of bloom objects surrounding  each cluster. This meant that in some instances, this would create bloom  iterations of under or over 10,000 marine ingress objects. By ensuring  bloom sizes were initially created marginally larger than 10,000 objects,  and then trimmed down to a total bloom size of 10,000, equivalence was  maintained for fair comparison across all bloom simulations. During the  MonteCarlo simulations, each of these bloom shape scripts were called  
Fig. 3. Insitu depiction of Unmanned Aircraft Systems maximum operational capabilities using Torness nuclear power station, East Lothian, UK, as an example:  black square = nuclear power station, red circle = Visual Line of Sight (VLOS), blue circles = Extended Visual Line of Sight (EVLOS) extensions. Beyond Visual Line of  Sight (BVLOS) (not depicted) could theoretically expand indefinitely. Green crosses suggest possible positions of pilot/flight observers on boats, as is common  practice for marine surveys. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)  
B. Mcilwaine et al.                                                                                                                                                                                                                               
5 
into the main simulation script as and when required.  Blooms were simulated within 4,000 × 4,000 cell matrices, with  marine ingress objects represented by a binary value of presence or  absence within a given cell. Each cell represented a realworld size of a  common marine ingress object: an Aurelia aurita individual of 0.25 m  diameter (Tombs, 2015), in turn defining a survey area of 1,000 × 1,000  m. Once the matrices were generated, marine ingress objects were then  randomly populated within the desired bloom shape and iterated 1,000  times to complete their simulation cycle. Synthetic data generation for  flight search patterns was based within the context of using a single  generic UAS platform. This allowed representation of both fixedwing,  and multirotor platforms; the two most accessible and widely applicable UAS platforms (Simic Milas et al., 2018). Flight search pattern and  bloom simulations were processed using a highperformance computer  (HPC) with the following specifications: Dell Precision Tower 5810,  Intel Xeon E5-1650 v4 CPU, NVIDIA GeForce GTX 1080 GPU and 64 GB  of RAM.  A flight altitude of 100 m was assumed; providing the basis of image  collection for each iteration of every flight scenario. As a consequence,  the image field of view was 18 × 18 m (found to be a realistic image chip  size) based on the real world flights conducted by Mcilwaine and Rivas  Casado (2021). This would equate to a ground sampling distance of 1.63  cm/px if using a common imager like a Sony Alpha a6000 mirrorless  RGB sensor (sensor: 23.5 × 15.6 mm, resolution: 24.3 MP 6,000 × 4,000) (Sony, 2021). Simulations were run within the statistical software R 3.4.3 (R Foundation of Statistical Computing, Vienna, Austria) (R  Core Team, 2017) with the following packages: ‘ggplot2′ and ‘plot.matrix’. Flight simulations, for each unique combination of bloom shape  and flight search pattern (scenario), were generated and iterated 1,000  times (Fig. 5). This was conducted to provide a more realistic representation of uncertainty when working with regional risk assessment  simulations for the marine environment (Hayes and Landis, 2004).  Flight search pattern simulations were categorised into two phases.  Phase 1: no information is currently known on the location of the bloom.  
Phase 2: some level of information is currently known as to the location  of a bloom. These are sometimes referred to as an ‘area search’ (phase 1)  or ‘datum search’ (phase 2). The reference of two phases is due to the  context of investigating optimum flight search patterns for the detection  of marine ingress for an early warning system. When there is no current  information on bloom locality, then an area search (phase 1) should be  deployed. However, when a successful locating of marine ingress has  occurred during previous flights, subsequent flights should deploy a  datum search (phase 2) as a matter of priority. This split was conducted  to maintain alignment with wellestablished search techniques for  maritime search and rescue efforts (Royal National Lifeboat Institution,  2017). Phase 1 search patterns used were parallel sweep, and crossing  barrier; phase 2 search patterns consisted of expanding square search,  and sector search (Fig. 6). Phase 2 searches were initiated from a random  ingress presence location within the simulated bloom, per iteration.  Crossing barrier and sector search are both examples of repetitive flight  search patterns; all four flight search patterns captured the same number  of simulated images to ensure equivalence of survey effort across all  flight search patterns. The performance of phase 1 and phase 2 flight  search patterns were evaluated independentlyi.e., simulations for  phase 2 were not linked to phase 1 simulations.  
2.3. Investigation of optimised flight plans  
The primary output of the flight search pattern simulations were  frequency counts of successful marine ingress detection within the  simulated image capture area. The total counts of the simulations  resulted in 1,000 individual frequency tallies per scenario; each unique  combination of bloom shape and flight search pattern. The count of  ingress detection was tallied through the flight search pattern over the  bloom. With each iteration producing 10,000 marine ingress objects, a  total of 200 million marine ingress objects were available for detection  through the initial simulated flight search patterns. The median detection rate for each scenario is reported, across all iterations, along with  
Fig. 4. Examples of investigated bloom shapes: (a) Clustered; (b) Elongate; (c) Circular; (d) Small coverage; (e) Large coverage. Each cell represents an area of 0.25  m × 0.25 m, with black marks indicating presence of a marine ingress object. The survey area is composed of 4,000 × 4,000 cells, therefore a total area of 1,000 × 1,000 m.  
B. Mcilwaine et al.                                                                                                                                                                                                                               
6 
the respective standard deviation (Table 2). This was completed to  provide a more representative indication of central tendency across the  scenario distributions, and their respective simulation iterations. This  was conducted due to the output data being count data, and therefore an  increased likelihood of skewness.  The count frequency distributions were collated into multiple box- 
plots to show both the variance and nature of each of the 20 unique  scenarios with respect to each flight search pattern. This was conducted  to provide insight and context to the range of flight search pattern  performance (flights) across all 1,000 iterations per scenario. In turn,  
also providing information on the performance of each flight search  pattern across the bloom types that were available for survey. There is  currently no formal definition of what substantiates a bloom. The  threshold value (beta) used to initiate a response to detection of marine  ingress was selected at 50 counts per survey flight; with a field of view of  18 × 18 m, equating to 50 images containing a minimum of at least one  marine ingress object (and highly likely many more) in 900 linear metres of survey. In the situation of a marine ingress bloom being present,  this would certainly involve an amount of ingress that could cause  moderate to severe disruption to coastal industries. From this point on,  
Fig. 5. Overall workflow diagram summarising the methodological processes: using historic Unmanned Aircraft Systems flight images to provide information on  bloom characteristics; generation of data for the five assessed blooms and four flight search patterns, and associated flight simulations; operational investigation  using the best performing phase 1 and phase 2 flight search patterns and associated flight simulations; analysis of simulations outputs.  
B. Mcilwaine et al.                                                                                                                                                                                                                               
7 
this threshold will be referred to as the “beta threshold” value.  
2.4. Impact of operation type  
By building on the results from flight plan optimisation (Section 2.3),  we were able to explore and quantify the effect of changing operation  type (VLOS, EVLOS, BVLOS) on the rate of success of bloom detection.  Only the best performing flight search patterns identified in previous  sections (per phase) were included in the assessment of the operation  type. Flight search pattern and bloom simulations were conducted with  the same simulation constants as previously stated. However, the total  possible survey area increased with respect to operation. Maximum  possible survey areas were as follows: VLOS = 1,000 × 1,000 m, EVLOS  = 2,000 × 2,000 m, BVLOS = 9,000 × 9,000 m.  
3. Results  
3.1. Investigation of optimum flight plans  
The rate of detection for all configurations of flight search patterns  and bloom shapes (Table 2) are for VLOS operation flight bounds. For  every form of bloom shape, parallel sweep was the highest performing  phase 1 survey style, but also overall flight search pattern. Crossing  barrier was similar in performance to parallel sweep for both ‘elongate’  (88) and ‘circular’ (84) shaped blooms, but particularly struggled with  ‘large coverage’ (32) and ‘clustered’ (44) blooms. Expanding square was  
Fig. 6. Investigated flight search patterns: phase 1 = (a) Parallel sweep; (b) crossing barrier; phase 2 = (c) expanding square; (d) sector search. Red pin indicates  datum for phase 2 flights. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)  
Table 2  Median detection rate of marine ingress simulations per scenario, per 1,000 replicates. Frequency tally of detection with standard deviation in brackets.    
Bloom Shape  
‘Large coverage’  ‘Clustered’  ‘Elongate’  ‘Small coverage’  ‘Circular’  
Flight search pattern   Phase 1  (area searches)  Parallel sweep  85 (7.4)  95 (6.7)  96 (3.1)  107 (4.7)  108 (4.3)  Crossing barrier  32 (10.0)  44 (18.8)  88 (6.5)  76 (7.8)  84 (9.4)  Phase 2  (datum searches)  Expanding square  64 (6.6)  86 (7.2)  96 (3.1)  106 (4.6)  108 (4.4)  Sector search  8 (2.6)  10 (3.7)  15 (1.6)  17 (2.0)  19 (2.1)   
Fig. 7. The impact of bloom shape on detection performance, depicted using  the sum of median counts for each iterated flight search pattern scenario with  respect to bloom shape.  
B. Mcilwaine et al.                                                                                                                                                                                                                               
8 
the best performing phase 2 search style by a significant margin across  all bloom shapes. Much in the same way as crossing barrier, sector  search struggled most with ‘large coverage’ (8) and ‘clustered’ (10)  blooms, but unlike crossing barrier, it also struggled with ‘elongate’  blooms (15).  The best performing phase 1 search style for ‘circular’ shaped blooms  was parallel sweep (108), closely followed by crossing barrier (84).  Expanding square (phase 2) was an equally high performing search style  for ‘circular’ blooms (108), however sector search severely struggled to  reliably detect (19) in comparison to the other three survey styles.  Despite this differential, sector search also struggled to detect well on all  shapes of bloom, with it performing best on ‘circular’ blooms (19).  ‘Large coverage’ shaped blooms were the hardest to detect with respect  to flight search pattern, with parallel sweep (85) and expanding square  (64) being the clear leading styles respectively. This finding of best-  performing survey styles was the same case for ‘small coverage’  blooms however with more competitive performance for phase 1 with  crossing barrier (76) versus parallel sweep (107). Much like lowdensity  blooms, ‘elongate’ blooms were most easily detected using parallel  
sweep (96 (phase 1)) and expanding square (96 (phase 2)) with crossing  barrier also performing well (88).  Across all four forms of survey style combined, there was a clear  pattern in ease of detection with respect to bloom shape (Fig. 7). The  bloom shape ‘large coverage’ had the lowest total median detection  count of 189, with ‘circular’ having the highest with 319. Despite all  replicates and simulations of bloom shapes maintaining the same  number of marine ingress objects (10,000), bloom shape appears to  significantly impact the success rate of marine ingress object detection.  Fig. 8 shows the variability of counts for all 1,000 iterations of all 20  scenarios. Crossing barrier was hugely variable in detection performance, most notably for ‘clustered’ and ‘large coverage’ blooms. Parallel  sweep was the most consistent performer across all bloom shapes, whilst  simultaneously maintaining the highest detection performance across  all five bloom shapes. Sector search had notably low variability in  detection of marine ingress, however was also the worstperforming  flight search pattern by a significant amount. Sector search was well  below the designated performance threshold of 50 counts per flight, for  all bloom shapes. The highest performing sector search iteration, of all  
Fig. 8. Count data variance and structure across all 1,000 iterations for each unique pairing of flight search pattern and bloom shape. Beta threshold indicated by red  dashed line. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)  
B. Mcilwaine et al.                                                                                                                                                                                                                               
9 
5,000 iterations, occurred for a circular shaped bloom.  All four flight search patterns show the same general pattern in  detection performance versus bloom shape. From hardest to easiest  detection, this was ‘large coverage; ‘clustered’; ‘elongate’; ‘small  coverage’; and finally ‘circular’ being the easiest to detect. The single  exception to this trend occurred within crossing barrier simulations, in  which there was a marked improvement in performance on ‘elongate’  blooms, and uniquely (versus other flight search patterns) had the  highest median detection performance for ‘elongate’ blooms as well. The  highest levels of variability per combination of flight search pattern and  bloom shape occurred for crossing barrier; ‘large coverage’ and ‘clustered’ bloom shapes showed the largest ranges of performance across  iterations, with ‘clustered’ bloom shape iterations ranging from detecting zero evidence of marine ingress objects to 96 detection events in a  single flight. The median detection values for both of these scenarios fell  below the threshold value of 50 confirmed detection images. Crossing  barrier performed much better for the remaining three bloom shapes,  with over 90% of all iterations performing above the threshold beta  value. Despite this, crossing barrier remained the most inconsistent  performer of all four flight search patterns.  Parallel sweep was the highest performer of all four flight search  patterns, and the only flight search pattern with all 5,000 iterations  above the threshold beta value. Variability of performance across all  bloom shapes was only bettered by that of sector search, showing evidence of a consistently high performing generalist flight search pattern.  Expanding square was very competitive with respect to parallel sweep,  however performed worse on ‘large coverage’ blooms. Some iterations  for the expanding square/’large coverage’ scenario were not above the  threshold beta value, however only a very small proportion (<1%).  Within the context of survey phases, parallel sweep was by far the best  phase 1 performing flight search pattern, and expanding square beating  sector search for the best performing phase 2 flight search pattern.  ‘Clustered’ blooms were the most variable in detectability, with ‘elongate’ blooms the most consistent in detection across all four search flight  patterns. Despite not being a primary concern of the work, it is interesting to note the success of expanding square versus parallel sweep.  Phase 2 flight search patterns should (in theory) be inherently lower  performers due to the added variability of initiation of survey origin.  
3.2. Impact of operation type on flight plan performance  
Building on the findings of Section 3.1, parallel sweep and expanding  square search were selected as the chosen phase 1 and 2 survey styles  (Table 3). Operation type performance was assessed to include all five  shapes of bloom per operation type.  VLOS operations had a sum median detection rate of 882 across both  survey phases and all five bloom shapes. EVLOS operation flights had an  increased detection rate with 1,709 (by a factor of 1.94), with BVLOS  flights having the highest detection rate of all 3 operation types (3,253).  BVLOS operations had exactly nine times more potential theoretical  maximum survey area compared to VLOS and produced an increase in  detection rate by a factor of 3.69 times.  
4. Discussion  
Marine ingress blooms have the potential to devastate the produc- 
tivity of a diverse range of coastal industries (Matsumura et al., 2005;  Barath Kumar et al., 2017; Vaughan, 2018). Marine ingress mainly forms  as one of two types of biomass from the ocean: jellyfish or detached  macroalgaes. Due to their subsurface nature, relatively little is known  about what can initiate specific blooms, with jellyfish blooms being the  least well understood (Mcilwaine et al., 2019). Macroalgae blooms  (MABs) are easier to locate due to their more simplistic pathway to  suspension in the water column, but more research into their relationship with inclement weather systems would be hugely beneficial to the  sector to ascertain causes of largescale detachment from growing  
positions and vast increases in biomass growth. This research focused on  five distinct bloom shapes, that are relevant for both jellyfish and MABs,  and served as the spatial basis of variance within the conducted simulations. By iterating simulations over all five types of shapes simultaneously, any shape specific performance correlations were collected and  then collated within survey performance output. In the field, it is likely  that blooms are not strictly of one shape, but a blend of a few as imaged  by Schaub et al. (2018); Mcilwaine and Rivas Casado (2021). However,  for the purposes of quantifying the performance of different survey  patterns with respect to bloom shape, this level of shape complexity was  not required. This approach also allowed an indication of flight search  pattern performance for general deployment across a range of shapes.  The outputs of the investigation into optimal flight plans were  incredibly distinct with regard to the highest performing survey styles.  For phase 1 survey styles, parallel sweep was by far the best and most  consistent performer in detecting marine ingress objects. This is in  congruence with what is found by marine search and rescue teams  (Royal National Lifeboat Institution, 2017) and for the context of UAS  surveying in the marine environment, is beneficial to know. Parallel  sweep is an efficient and systematic area search, and for all shapes and  forms of bloom, it performed the best. An argument could be made to  deploy this form of survey route regardless of any other situational  knowledge, and detection performance would likely be of practical use.  Crossing barrier, in contrast, is structurally a very different search  pattern (slidetodoc, 2021) and is originally intended for more niche area  search demands. However, this does not mean it is limited to this usage.  Predominantly meant as a repetitive curtain style search pattern to  guard inlets and bays, the randomisation during the simulation process  allowed an accurate and reliable performance measurement in such a  way that ingress objects in the field would pass through the survey  pattern’s proverbial ‘curtain’. Nonetheless, it was somewhat surprising  to see just how well crossing barrier performed for both ‘circular’ and  ‘elongate’ shaped blooms. This is particularly enlightening concerning  flight mission planning, crossing barrier is the most accessible and  simplistic of all investigated. If a rapid response is required, there may  be some value in quickly deploying a crossing barrier survey style (slidetodoc, 2021). A caveat to this approach though, is that it was not a  good performer across all shapes and would likely require ground-  truthing before deployment; thus leading to the natural question that  if groundtruthing has already occurred, then a phase 2 (datum search)  would be a more logical approach. If crossing barrier was to be deployed  against ‘large coverage’ or ‘clustered’ bloom shapes, the source of  reduced performance is likely due to the more linear search area flown  unlike the greater area coverage of parallel sweep. It is hard to avoid the  broadspectrum highperformance output of parallel sweep as, not just a  phase 1 search pattern, but as a search pattern for any given bloom  shape or phase of survey for a prospective marine ingress warning system. Despite the presence of many 90◦ corners within the search patterns  (Fig. 6), it was assumed that UAS can follow the preplanned routes with  submetre accuracy, especially in the context of modern advancements  in flight control software allowing far superior precision (Benassi et al.,  2017). Any deviations from this path, more likely from fixedwing  platforms as opposed to rotary UAS, should be well overshadowed by  the much larger coverage of the imaging field of view.  Phase 2 search styles were equally as conclusive as the outputs for  phase 1; expanding square being the standout performer, and sector  search only getting slightly under 20% of the detection performance (of  expanding square search) for certain bloom shapes (‘circular’ & ‘small  coverage’). A note of particular resonance is how well expanding square  search did with respect to parallel sweep (phase 1). Both phase 2 search  styles began their data collection from randomised points within the  bloom. This was to simulate field scenarios in which some information is  known regarding bloom location, but not necessarily more than one field  of view’s worth of ingress accumulation coverage (18 × 18 m). Naturally, there will have been a portion of simulated phase 2 flight iterations  centred over the edge of blooms, spending a significant portion of data  
B. Mcilwaine et al.                                                                                                                                                                                                                               
10 
collection time ‘missing’ that given bloom. It is hoped that with performance reporting focused on median values, the reporting of these  situations were not overly influential, however it would not be unreasonable to assume it had some level of impact on the reduction of performance versus parallel sweep. This leads us to the next question, is it  even worth splitting the flights into two distinct phases? By initiating  phase 2 surveys on prior spatial location information, it is logically  sound to commence data collection at such locations. Despite the  decrease in overall detection performance, potentially by commencing  phase 2 flights in such a manner, phase 2 style flights would have an  underlying yet unquantifiable increase in the assurance of improving  current information on a given bloom. Within the context of an early  warning system, this is incredibly useful for any surveys conducted. In  light of the above, and with expanding square within 1% of the performance of parallel sweep for three out of five bloom shapes, it is hard  to deny the legitimacy of a double phased survey approach. On the other  hand, sector search was significantly poor with regard to detection  success, with it performing best on ‘circular’ shaped blooms (19), and  worst on ‘large coverage’ (8). With a total median count (across all  bloom shapes) of 69, equating to 15% of the capability of ‘expanding  square’, it is clear that the ‘sector search’ survey style does not translate  well to UAS surveying in the marine environment. This is likely due to  sector search surveys originally being developed for surveying craft with  a much larger field of view than a remote sensing UAS platform (Royal  National Lifeboat Institution, 2017), and requiring more refinement to  be translated as a UAS flight search pattern.  The beta threshold of 50 was developed in response to previous  knowledge of UAS surveying in the marine environment, and coming to  a threshold with a realistically low probability of a false positive in the  field. There are no widespread formal definitions of what constitutes a  bloom (Mcilwaine and Rivas Casado, 2021), however regardless of this,  the definition used in this work was used primarily for analytical reasons; particularly within a decision making context of an early warning  system. Further work is recommended to refine this threshold value,  however naturally as a comparative value it would be not necessarily  become more or less valid. The topic raises the broader question: at  which point should a given coastal industry decide to react from  warning provided via bloom detection. Does one single image detection  
of a bloom warrant an active response? Most likely not, but most  certainly the beta threshold of 50 used here would do so. The percentage  accuracy of the detection algorithm aspect of an early warning system  also plays an important role here in the overall system. For a highly  valuable coastal asset, would even one detection result be worth further  investigation? Despite not being the focus of this work, it is also worth  mentioning the pros and cons of false positives and false negatives of an  early warning system. A false positive would be far more acceptable for  all industries in comparison to a false negative, due to the huge potential  impact of a marine ingress event. Dependent on the sensitivity of a  coastal industry, they may wish to lower the beta threshold value in  order to reduce the risk of falsenegative action, or lack thereof.  Using a beta threshold of 50, seven of the 20 scenarios (Fig. 8) had a  large number of flight iterations that were either straddling, or below  the beta threshold value. These flight search patterns could not be recommended for use; especially in light of the consistently high performance of both parallel sweep and expanding square. Sector search for all  bloom shapes was a very low performer and is not of any particular  value in the context of an UAS early warning system. The uniquely  elevated performance of crossing barrier on ‘elongate’ blooms is likely  due to spatial compatibility of both the shape and the flight search  pattern. This suggests potential use of crossing barrier for areas of interest with narrow inlets, elongated manmade structures such as large  docks, or other narrow waterways. Unlike sector search, there may be  some value in further investigation of this finding due to the simplicity  of crossing barrier deployment. Despite not lending itself as a strong  generalist performer, crossing barrier does appear to have merit for  certain specific circumstances. Parallel sweep was the highest performer  of all four flight search patterns, without a single iteration of all 5,000  falling below the beta threshold. For reliability of a deployed flight  search pattern, consistency is most certainly a requirement. Despite  parallel sweep not being the most consistent flight search pattern, it was  only beaten in consistency of detection by the poorly performing sector  search flights. The phase 2 flight search pattern expanding square was  almost as good a generalist performer as parallel search. With the  naturally occurring additional randomisation of a phase 2 flight, this  result is worthy of recommendation as a practical flight search pattern.  With regard to performance relating to bloom shape, there should be  little surprise that ‘clustered’ blooms were the most variable to detect  compared to other shapes; purely from a spatial randomisation point of  view. Interestingly ‘elongate’ blooms were the most consistently  detectable blooms, which may be in part due to their stretched spatial  distribution. If a flight search pattern were to detect either end of an  ‘elongate’ bloom, it is highly likely it would continue on to confirm  detection of the rest of it.  BVLOS survey areas, unlike VLOS and EVLOS, are not precisely  determined distances through civil aviation authority (CAA) guidance  but are specifically granted for justifiable reasons on a case by case basis.  A BVLOS area of 9,000 × 9,000 m was used following the author’s  BVLOS application (CAA, 2021a) for other project work on the east coast  of Scotland, UK. However, a BVLOS maximum survey area could theoretically be smaller or larger than 9,000 × 9,000 m and is uniquely  specific for each BVLOS application. It is frequent, however, when going  to the administrative effort of making a BVLOS application, that a  sizeable flying area is to be obtained. In UK airspace, as governed by the  CAA, VLOS and EVLOS operations are of fixed maximum area coverage  and relatively easy to conduct. BVLOS, however, are bespoke designated  areas and can be viewed as more than just an extension of EVLOS operations. The first BVLOS authorisation was reported as being granted in  April 2021 (CAA, 2021b), and our survey size of 9,000 × 9,000 m is  based upon our own BVLOS application to fly on the east coast of  Scotland as part of the UK government pathfinder project (Catapult,  2020).  The rerunning of simulations to investigate the impact of operation  type (VLOS, EVLOS and BVLOS) was highly successful in deciphering  the broadspectrum impact of the flying operation. Median counts of  
Table 3  Median detection rate of marine ingress simulations per operation type, for  1,000 replicates per bloom shape. Using the highest two performing flight search  patterns. Sum of median frequency counts across all shapes of bloom per operation type.     
Operation type   
Bloom shape  VLOS  EVLOS  BVLOS  
Phase 1 (Parallel sweep)  Large  coverage  76 (7.1)  106 (9.0)  136  (11.3)  Clustered  90 (6.5)  185  (19.0)  425  (78.8)  Elongate  94 (3.0)  220 (8.9)  469  (19.2)  Small  coverage  85 (5.5)  175 (9.8)  346  (16.9)  Circular  111  (6.2)  178  (10.5)  256  (15.2)   Phase 1 total  456  864  1,632  Phase 2 (Expanding  square)  Large  coverage  59 (6.3)  94 (8.6)  132  (11.2)  Clustered  81 (6.7)  180  (19.9)  424  (79.1)  Elongate  93 (3.1)  219 (8.6)  463  (19.5)  Small  coverage  84 (5.6)  175 (9.7)  344  (17.0)  Circular  109  (6.0)  177  (10.3)  258  (15.4)   Phase 2 total  426  845  1,621   Grand total  882  1,709  3,253   
B. Mcilwaine et al.                                                                                                                                                                                                                               
11 
individual bloom shapes were collated across the 5,000 total iterations,  per phase, per operation type: a total of 30,000 blooms were analysed  and assessed for the two selected survey styles (parallel sweep &  expanding square – Table 3). The total median count for both survey  phases (combined) were lowest for VLOS (882), followed by EVLOS  (1,709) and then BVLOS (3,253). These results are to be expected in light  of each operation type having an increasingly larger area of survey, in  particular BVLOS with almost four times the detection rate of VLOS  operations. EVLOS operations had an increase in detection capability of  just under 93.7% compared to VLOS, as a result of doubling the theoretical total survey area to 2,000 × 2,000 m.  BVLOS operations are extremely rare in the UK and are at the cutting  edge of the progression of UAS operational technology both nationally,  and globally (Davies et al., 2018). The total number of civilian BVLOS  operations in the UK (as of July 2021) is expected to be below five.  Whilst guidance and operational protocol catch up with platform advancements, BVLOS operations remain practically inaccessible for the  vast majority of operators. However, if the aim for BVLOS is purely to  increase operational area, EVLOS could provide the solution. EVLOS  operations are not limited to 2,000 × 2,000 m as investigated here. An  EVLOS operation of this size assumes 2–4 dedicated flight spotters  assisting the mission, but in theory, this could be extended to greater  distances (and by virtue, area).  Truly distinct from both VLOS and EVLOS operations, BVLOS flights  can be conducted without any direct visual contact between the pilot  and the UAS (Civil Aviation Authority, 2020). BVLOS operations are not  solely limited to attempts to increase the maximum theoretical survey  area. In theory, a BVLOS operations area could be much smaller than  that of a VLOS or EVLOS operation, but at a further distance from the  pilot. BVLOS operations are generally more costeffective (Fig. 10) due  to being able to cover more ground, but also by being more efficient  through requiring less takeoff and landing manoeuvres. This reduces  the demand on both UAS flight time and administration efforts in pre-  flight planning. No spotters are required to conduct BVLOS operations,  which also vastly reduces cost due to less personnel needed on the  ground. In turn, these reasons have the potential to maximise surveying  efforts in the marine environment and thereby the probability of collecting useful data at the survey location. BVLOS operations present a  better survey area coverage per unit day cost in comparison to both  VLOS and EVLOS. With the increase in potential survey area of BVLOS  operations, aerodynamic optimisation of UAS platforms becomes much  more critical as a flight variable. The scale of BVLOS search areas often  means that multiple flights may have to be conducted, with battery  capacity of the chosen platform being the limiting factor in how long a  survey can be conducted for. This in turn leads to the rise in importance  of how aerodynamic a flight platform is, especially for certain UAS  platforms where battery capacity is fixed and can not be improved. The  aerodynamic efficiency of a flying craft is normally described by the lift  (L) to drag (D) ratio.  
AE = L D   
where:  
AE = Aerodynamic Efficiency  L = Lift  D = Drag  
Fixed wing UAS platforms often have vastly superior aerodynamic  efficiency in comparison to rotary platforms, and can cover much further  distances due to this. The optimisation of the aerodynamic efficiency is  commonly conducted by UAS manufacturers during production, however recent research is currently being conducted assessing this independent of private commercial industry (Di Luca et al., 2020;  Panagiotou and Yakinthos, 2020; Zhu et al., 2020).  
All early warning systems should at minimum consider the advan- 
tages and disadvantages of the nature of the deployed strategy. In  particular, whether a generalist or specialist approach is superior. The  best generalist performing phase 1 search pattern was found to be parallel sweep (total median detection count = 491 (Fig. 9), with expanding  square (460) for phase 2 searches. Due to how competitively ‘expanding  square’ performs compared to parallel search for ‘circular’, ‘small  coverage, and ‘elongate’; it would be advisable to strongly consider  expanding square as an alternative phase 1 survey style. If using  expanding square as a generalist approach, or even as a phase 1 survey,  consideration should be given to the starting point of the survey. Unlike  a datum search, one would be committed to initialising a survey from  the central point of the designated survey area in the absence of information of bloom location. There was no clear evidence of any benefit in  using a survey style other than parallel sweep as a generalist approach,  or in fact any other survey style, as a specialist approach for a specific  bloom shape (disregarding a phased approach). This suggests that a  repetitive flight programme would be most beneficial, repeating a phase  1 flight consisting of a parallel sweep area search, and then in the situation of locating a bloom, switching to a phase 2 datum survey consisting of expanding search.  A notable limitation of the simulations is the homogeneous nature of  the simulation cells; the location where a marine ingress object may, or  may not, have been located. All cells were of identical size and did not  account for the heterogeneous sizes of marine ingress objects in the field.  Despite this restriction, due to the comparative nature of this work, the  inclusion of this heterogeneity is not essential. Ingress object drift is  another notable exclusion within the simulated blooms and flights,  however in the context of the duration of UAS flights (commonly between 30 and 60 mins (MesasCarrascosa et al., 2015, 2016; Sherstjuk,  Zharikova and Sokol, 2018)), and the slow rate of surface and sub-  surface object drift (Li et al., 2019), it was deemed not constructive to  the overall simulation. When randomised over thousands of iterations,  whereby drift effects would result in an equivalent impact on detection  capability acrosstheboard or oppose each other and culminate in a  cancelling effect. This is a consequence of the symmetrical nature of the  investigated survey patterns and would not be the case for asymmetrical  search patterns. The simulations also follow the assumption of 100%  successful detection of marine ingress objects within the field of view of  
Fig. 9. Overall search style performance on all five forms of investigated bloom.  
B. Mcilwaine et al.                                                                                                                                                                                                                               
12 
the UAS imaging system. In the field this would not be the case, however  for the purpose of this work, the importance lies in consistency for  legitimate comparison. The discrimination of the different types of  marine ingress is an important feature by any early warning system.  Despite it not being a focus of this work, it is still worth mentioning that  any algorithms developed for the realworld detection of marine ingress  should be able to cope with multiple forms of ingress; be it jellyfish or  macroalgaes. Other minor forms of marine ingress that don’t cause  significant issues such as silverfish would not be worth attempting to  detect due to the lack of disruption they can cause to coastal operators.  The ability to discriminate between various forms of problematic macroalgaes was conducted by Mcilwaine et al. (2019) and it would be  worthwhile for further research to investigate jellyfish in the same  manner.  The production of a decision chart (Fig. 11) is a step towards a  functioning early warning marine ingress detection system. At this  
current time, there has been no work identifying optimal flight plans in  the marine environment for marine ingress using UAS remote sensing.  Coastal operators currently work off meteorological correlation risk-  based systems, and if moving to positively identified ingress blooms  using UAS remote sensing, they deserve clarity on what flight parameters to be using.  A field investigation that would be of interest would be to assess  which flight pattern (over 1,000 s of flights) detects a bloom quickest.  This was not a focus of this work but would be complementary to  investigate building on the findings of this paper. Further work to build  on this paper would be to investigate the impacts of various platform  features on bloom detection, such as: flight speed, flight heights and the  associated field of view changes, and the resolution of the onboard  sensor. A similar investigation into the effect of environmental conditions would be equally enlightening. Particularly, the variation in  detection performance due to change in wind direction/strength and the  reduction/gain of flight duration as a result. By moving towards a  standardised framework for marine ingress detection, the overall performance of detection and the ability to prevent losses of generation (or  revenue for nonpower station coastal industries) gradually increases.  With a deployable finished framework that can account for many more  flight parameters, the impact of problematic marine ingress bloom  events should reduce to a much more manageable level when used in  tandem with effective mitigation measures once a warning has been  provided. No realworld testing of the simulation findings was carried  out due to the immense difficulty of finding realworld blooms to test the  various strategies on. However, there are no indications from the simulations (or the logic therein) that would dramatically impact overall  flight performance in comparison to one another. To build on the work  described in this paper, the authors recommend further investigation  into encompassing more variables in their simulations. Investigation  into the effects of differing ground sampling distance, wind buffeting  and battery endurance would be worthy of consideration. The real-  world deployment of the strategies discussed in this article would be  of valuable contribution to knowledge as well, to groundtruth the  findings of the extensive simulations conducted. There are situations in  which marine ingress may occur in both forms of jellyfish and macroalgae. It would be prudent to build on the work conducted by Mcilwaine  and Rivas Casado (2021) in order to add macroalgae detection classes  into the detection algorithm so that both forms of marine ingress can be  detected using the same single discrimination algorithm.  This work delivers the assessment for quantifying the optimal flight  search patterns for marine ingress detection in the context of using UAS  as a remote sensing platform. Within the context of moving towards a  
Fig. 10. Indicative relationship between financial cost of operation and increase in maximum theoretical survey area. Maximum distance for BVLOS is  just indicative.  
Fig. 11. Decision chart for UAS deployment for an established operation type  (VLOS, EVLOS and BVLOS).  
B. Mcilwaine et al.                                                                                                                                                                                                                               
13 
functional early warning system for marine ingress, it is hoped that the  provision of reliable early warning can aid coastal industries in preventing unnecessary losses and downtime from disruptive blooms  (Hamner and Dawson, 2009). Examples of such preventative measures  are protective aquatic barrier curtains (Zielinski and Sorensen, 2016),  increased labour force for manual washing of filtration systems and pre-  emptive reductions in water intake to prevent longerterm disruption  (Nuclear Energy Institute, 2015; Wei et al., 2018). Through the introduction of this paper’s findings, losses due to the impacts of marine  ingress should hopefully be reduced. Future projects are recommended  to investigate the current state of UAS technology, and what final steps  are required to attain a fully functioning marine ingress early warning  system. This is particularly crucial due to how rapidly this field of  research is advancing on a yearly basis (Davies et al., 2018). The current  global trend is an increase in disruptive ingress events, in both the two  most common forms; jellyfish (Mills, 2001) and MABs (Spanakis et al.,  2014). For our global coastal industries not to suffer further, especially  in the postCOVID-19 era of extreme economic uncertainty (Baker et al.,  2020), it should be a key priority to react proactively to these hazards  and not reactionarily.  
5. Conclusion  
After completing a total of 50,000 simulations, the outputs showed a  clear result in which flight search pattern was both the highest  performer, but also most consistent for detecting marine ingress. Parallel  sweep was the best performing flight search pattern, closely followed by  expanding square. These were coincidentally a phase one and phase two  flight search pattern respectively. It was also found that type of flight  operation had a significant impact on marine ingress detection, with  performance improvements (over VLOS) of 94% for EVLOS and 269%  for BVLOS for a comparable density bloom but increased maximum  theoretical survey area. With regard to bloom shape, large coverage  blooms were the hardest to detect with circular blooms the easiest. The  field testing of the findings of these simulations is recommended, as well  as progressing the complexity of simulations if there is a demand to do  so. With the occurrence of marine ingress blooms notably increasing  around the globe, the effects of global changes are becoming more and  more apparent. Thus, it is vital to continue this line of research if we are  to maintain functionality and maximise the operational capacity of key  coastal industries; particularly aquaculture, desalination plants and  nuclear power stations. The impact of improving our understanding of  marine ingress events is wideranging and will likely become increasingly important if we are to achieve our COP26 carbon neutrality  agreements. Especially with nuclear power being the foundation of a  reliable lowcarbon energy economy.  
Funding  
This research was funded by the Engineering and Physical Sciences  Research Council (EPSRC), The Smith Institute and EDF Energy under  EPSRC Industrial Case Studentship Voucher 586 Number 16000001.  
CRediT authorship contribution statement  
Ben Mcilwaine: Methodology, Software, Validation, Investigation,  Data curation, Writing – original draft, Writing – review & editing.  M´onica Rivas Casado: Writing – review & editing, Supervision, Project  administration, Funding acquisition. Toby Waine: Writing – review &  editing, Supervision.  
Declaration of Competing Interest  
The authors declare the following financial interests/personal re- 
lationships which may be considered as potential competing interests:  [Monica Rivas Casado reports financial support was provided by  
Engineering and Physical Sciences Research Council. Monica Rivas  Casado reports financial support was provided by EDF Energy.]  
Acknowledgements  
We would like to thank the Engineering and Physical Sciences  Research Council (EPSRC), The Smith Institute and EDF Energy for  funding this project. We would also like to thank the reviewers for their  helpful comments and constructive criticism. The manuscript became  stronger thanks to their detailed contribution. The underlying data are  confidential and cannot be shared due to a permanent embargo.  
References  
AlbajesEizagirre, A., 2011. Jellyfish prediction of occurrence from remote sensing data  and a nonlinear pattern recognition approach. In: Neale, C.M.U., Maltese, A. (Eds.),  International Society for Optics and Photonic, p. 817418. https://doi.org/10.1117/  12.898162.  APA, 2020. Drones: Flying toward the future. Available at: APA Planning Advisory  Service Reports 2020 (597), 1–109 https://www.scopus.com/inward/record.url?ei  d=2-s2.0-85098713055&partnerID=40&md5=6940cfcc04d807f827d1bccb228c  8776.  Auditorium, S., Washington, D. C., 1980. BSÜE Visual Search Techniques Proceedings of  a Symposium Sponsored by the ARMED FORCESNRC COMMITTEE ON VISION.  Baker, S.R. et al., 2020. ‘COVIDInduced Economic Uncertainty’. Available at: http://  www.worlduncertaintyindex.com, (Accessed: 9 July 2021).  Barath Kumar, S., et al., 2017. Impingement of marine organisms in a tropical atomic  power plant cooling water system. Mar. Pollut. Bull. 124 (1), 555–562. https://doi.  org/10.1016/j.marpolbul.2017.07.067.  Becking, L.E., de Leeuw, C., Vogler, C., 2015. Newly discovered “jellyfish lakes” in  Misool, Raja Ampat, Papua, Indonesia. Mar. Biodivers. 45 (4), 597–598. https://doi.  org/10.1007/s12526-014-0268-6.  Benassi, F., et al., 2017. Testing Accuracy and Repeatability of UAV Blocks Oriented with  GNSSSupported Aerial Triangulation. Remote Sens. 9 (2), 172. https://doi.org/  10.3390/RS9020172.  Bricelj, V.M., et al., 2012. ‘Trophic transfer of brevetoxins to the benthic macrofaunal  community during a bloom of the harmful dinoflagellate Karenia brevis in Sarasota  Bay, Florida. Harmful Algae 16, 27–34. https://doi.org/10.1016/j.hal.2012.01.001.  CAA, 2021a. Airspace change proposal public view. Available at: https://airspacechange.  caa.co.uk/PublicProposalArea?pID=385 (Accessed: 3 December 2021).  CAA, 2021b. Step forward for the drone industry as Civil Aviation Authority authorises  trial of a concept for routine BVLOS operations | UK Civil Aviation Authority.  Available at: https://www.caa.co.uk/News/Stepforwardforthedroneindustryas–  CivilAviationAuthorityauthorisestrialofa-concept–forroutineBVLOSoperation  s/ (Accessed: 9 July 2021).  Catapult, 2020. UK Drones Pathfinder Programme has a new pathfinder announced.  Available at: https://cpold.catapult.org.uk/2020/07/22/ukdronespathfinderprog  rammefocusingonearlydetectionofmarineactivity/ (Accessed: 9 July 2021).  
Chae, J., et al., 2008. Distribution of a pelagic tunicate Salpa fusiformis in warm surface  current of the eastern Korean waters and its impingement on cooling water intakes of  Uljin nuclear power plant. J. Environ. Biol. 29 (4), 585–590.  Civil Aviation Authority, 2020. Unmanned Aircraft System Operations in UK Airspace –  Guidance. Crawley. Available at: https://publicapps.caa.co.uk/docs/33/CAP722  Edition8(p).pdf (Accessed: 4 March 2021).  Colomina, I., Molina, P., 2014. Unmanned aerial systems for photogrammetry and  remote sensing: A review. ISPRS J. Photogramm. Remote Sens. 79–97. https://doi.  org/10.1016/j.isprsjprs.2014.02.013.  Davies, L., et al., 2018. Review of Unmanned Aircraft System Technologies to Enable  beyond Visual Line of Sight (BVLOS) Operations. In: 2018 10th International  Conference on Electrical Power Drive Systems, ICEPDS 2018 - Conference  Proceedings, pp. 1–6. https://doi.org/10.1109/ICEPDS.2018.8571665.  Decker, M., et al., 2007. Predicting the distribution of the scyphomedusa Chrysaora  quinquecirrha in Chesapeake Bay. Mar. Ecol. Prog. Ser. 329, 99–113. https://doi.  org/10.3354/meps329099.  Dierssen, H.M., Chlus, A., Russell, B., 2015. Hyperspectral discrimination of floating mats  of seagrass wrack and the macroalgae Sargassum in coastal waters of Greater Florida  Bay using airborne remote sensing. Remote Sens. Environ. 167, 247–258. https://  doi.org/10.1016/j.rse.2015.01.027.  Flynn, K., Chapra, S., 2014. Remote Sensing of Submerged Aquatic Vegetation in a  Shallow NonTurbid River Using an Unmanned Aerial Vehicle. Remote Sens. (12),  12815–12836. https://doi.org/10.3390/rs61212815.  Gansel, L.C., et al., 2017. Drag on nets fouled with blue mussel (mytilus edulis) and sugar  kelp (saccharina latissima) and parameterization of fouling. In: Proceedings of the  International Conference on Offshore Mechanics and Arctic Engineering - OMAE.  American Society of Mechanical Engineers (ASME). https://doi.org/10.1115/  OMAE201762030.  Goebel, M.E., et al., 2015. A small unmanned aerial system for estimating abundance and  size of Antarctic predators. Polar Biol. 38 (5), 619–630. https://doi.org/10.1007/  s00300-014-1625-4.  Gonçalves, G., et al., 2020. Quantifying marine macro litter abundance on a sandy beach  using unmanned aerial systems and objectoriented machine learning methods.  Remote Sensing 12 (16). https://doi.org/10.3390/RS12162599.  
B. Mcilwaine et al.                                                                                                                                                                                                                               
14 
Haberlin, D., McAllen, R., Doyle, T.K., 2021. Field and flume tank experiments  investigating the efficacy of a bubble curtain to keep harmful jellyfish out of finfish  pens. Aquaculture 531. https://doi.org/10.1016/j.aquaculture.2020.735915.  Hamner, W., Dawson, M., 2009. A review and synthesis on the systematics and evolution  of jellyfish blooms: Advantageous aggregations and adaptive assemblages.  Hydrobiologia 616 (1), 161–191. https://doi.org/10.1007/s10750-008-9620-9.  Hayes, E.H., Landis, W.G., 2004. Regional ecological risk assessment of a near shore  marine environment: Cherry Point, WA. Hum. Ecol. Risk Assess. 10 (2), 299–325.  https://doi.org/10.1080/10807030490438256.  Houghton, J., et al., 2006. Developing a simple, rapid method for identifying and  monitoring jellyfish aggregations from the air. Mar. Ecol. Prog. Ser. 314, 159–170.  https://doi.org/10.3354/meps314159.  Hu, C., MullerKarger, F.E., Swarzenski, P.W., 2006. Hurricanes, submarine groundwater  discharge, and Florida’s red tides. Geophys. Res. Lett. 33 (11) https://doi.org/  10.1029/2005GL025449.  Jo, Y.-H., Bi, H., Lee, J., 2017. Potential Applications of Low Altitude Remote Sensing for  Monitoring Jellyfish. Korean J. Remote Sens. 33 (1), 15–24. https://doi.org/  10.7780/kjrs.2017.33.1.2.  Kim, D.H., et al., 2012. Estimating the economic damage caused by jellyfish to fisheries  in Korea. Fish. Sci. 78 (5), 1147–1152. https://doi.org/10.1007/s12562-012-0533-  1.  Kim, H., et al., 2015. Development of a UAVtype jellyfish monitoring system using deep  learning. In: 2015 12th International Conference on Ubiquitous Robots and Ambient  Intelligence (URAI). IEEE, pp. 495–497. https://doi.org/10.1109/  URAI.2015.7358813.  Kim, H., et al., 2016. Imagebased monitoring of Jellyfish using deep learning  architecture. IEEE Sensors J. 16 (8), 2215–2216. https://doi.org/10.1109/  JSEN.2016.2517823.  Klebert, P., et al., 2013. Hydrodynamic interactions on net panel and aquaculture fish  cages: A review. Ocean Eng. 260–274. https://doi.org/10.1016/j.  oceaneng.2012.11.006.  Kwon, Y.S., et al., 2020. Dronebased hyperspectral remote sensing of cyanobacteria  using vertical cumulative pigment concentration in a deep reservoir. Remote Sens.  Environ. 236 https://doi.org/10.1016/j.rse.2019.111517.  Lapointe, B.E., Bedford, B.J., 2007. Drift rhodophyte blooms emerge in Lee County,  Florida, USA: Evidence of escalating coastal eutrophication. Harmful Algae 6 (3),  421–437. https://doi.org/10.1016/J.HAL.2006.12.005.  Li, D., et al., 2019. ‘Characteristics and influence of green tide drift and dissipation in  Shandong Rongcheng coastal water based on remote sensing’, Estuarine. Coastal and  Shelf Science 227, 106335. https://doi.org/10.1016/J.ECSS.2019.106335.  Li, L., Wu, J., 2021. Spatiotemporal estimation of satelliteborne and groundlevel NO2  using full residual deep networks. Remote Sens. Environ. 254 https://doi.org/  10.1016/j.rse.2020.112257.  Lippmann, J.M., et al., 2011. Fatal and severe box jellyfish stings, including Irukandji  stings, in Malaysia, 2000–2010. J. Travel Med. 18 (4), 275–281. https://doi.org/  10.1111/j.1708-8305.2011.00531.x.  Liu, G., et al., 2020. A new biomimetic antifouling method based on water jet for marine  structures. Proc. Inst. Mech. Eng. Part M: J. Eng. Maritime Environ. 234 (2),  573–584. https://doi.org/10.1177/1475090219892420.  Di Luca, M., et al., 2020. A bioinspired Separated Flow wing provides turbulence  resilience and aerodynamic efficiency for miniature drones. Sci. Robot. 5 (38)  https://doi.org/10.1126/SCIROBOTICS.AAY8533/SUPPL_FILE/AAY8533_SM.PDF.  Maclsaac, H.J., 1996. Potential abiotic and biotic impacts of zebra mussels on the inland  waters of North America. Am. Zool. 36 (3), 287–299. https://doi.org/10.1093/icb/  36.3.287.  Mardones, J.I., et al., 2021. Disentangling the environmental processes responsible for  the world’s largest farmed fishkilling harmful algal bloom: Chile, 2016. Sci. Total  Environ. 766 https://doi.org/10.1016/j.scitotenv.2020.144383.  Marx, U.C., Roles, J., Hankamer, B., 2021. Sargassum blooms in the Atlantic Ocean –  From a burden to an asset. Algal Res. 54, 102188. https://doi.org/10.1016/j.  algal.2021.102188.  Matsumura, K., et al., 2005. Genetic polymorphism of the adult medusae invading an  electric power station and wild polyps of Aurelia aurita in Wakasa Bay, Japan.  J. Mar. Biol. Assoc. U. K. 85 (3), 563–568. https://doi.org/10.1017/  S0025315405011483.  Mcilwaine, B., Rivas Casado, M., 2021. JellyNet: The convolutional neural network  jellyfish bloom detector. Int. J. Appl. Earth Obser. Geoinformation 97, 102279.  https://doi.org/10.1016/j.jag.2020.102279.  Mcilwaine, B., Rivas Casado, M., Leinster, P., 2019. Using 1st Derivative Reflectance  Signatures within a Remote Sensing Framework to Identify Macroalgae in Marine  Environments. Remote Sensing 11 (6), 704. https://doi.org/10.3390/rs11060704.  Menu, M., et al., 2021. Towards a better understanding of grass bed dynamics using  remote sensing at high spatial and temporal resolutions. Estuarine Coastal Shelf Sci.  251, 107229. https://doi.org/10.1016/j.ecss.2021.107229.  MesasCarrascosa, F.-J., et al., 2015. (2015) Assessing Optimal Flight Parameters for  Generating Accurate Multispectral Orthomosaicks by UAV to Support SiteSpecific  Crop Management. Remote Sens. 7 (10), 12793–12814. https://doi.org/10.3390/  RS71012793.  MesasCarrascosa, F.-J., et al., 2016. An Analysis of the Influence of Flight Parameters in  the Generation of Unmanned Aerial Vehicle (UAV) Orthomosaicks to Survey  Archaeological Areas. Sensors 16 (11), 1838. https://doi.org/10.3390/S16111838.  Mills, C.E., 2001. ‘Jellyfish blooms: are populations increasing globally in response to  changing ocean conditions? Hydrobiologia 451 (1), 55–68. https://doi.org/  10.1023/A:1011888006302.  Minamoto, T., et al., 2017. Environmental DNA reflects spatial and temporal jellyfish  distribution. PLoS ONE. https://doi.org/10.1371/journal.pone.0173073.  
MohdDin, M., et al., 2020. Prolonged high biomass diatom blooms induced formation of  hypoxicanoxic zones in the inner part of Johor Strait. Environ. Sci. Pollut. Res. 27  (34), 42948–42959. https://doi.org/10.1007/s11356-020-10184-6.  Nahirnick, N.K., et al., 2019. Mapping with confidence; delineating seagrass habitats  using Unoccupied Aerial Systems (UAS). Remote Sens. Ecol. Conservat. 5 (2),  121–135. https://doi.org/10.1002/rse2.98.  Nuclear Energy Institute, 2015. Economic Impacts of The R.E. Ginna Nuclear Power  Plant An Analysis by the Nuclear Energy Institute. Available at: http://www.nei.org  (Accessed: 10 January 2019).  Panagiotou, P., Yakinthos, K., 2020. Aerodynamic efficiency and performance  enhancement of fixedwing UAVs. Aerosp. Sci. Technol. 99, 105575. https://doi.  org/10.1016/J.AST.2019.105575.  Purcell, J.E., 2005. Climate effects on formation of jellyfish and ctenophore blooms: a  review. J. Mar. Biol. Assoc. U. K. 85 (3), 461–476. https://doi.org/10.1017/  S0025315405011409.  R Core Team, 2017. R: A language and environment for statistical computing. Vienna,  Austria. Available at: https://www.rproject.org/.  Riascos, J.M., et al., 2018. Floating nurseries? Scyphozoan jellyfish, their food and their  rich symbiotic fauna in a tropical estuary. PeerJ. 2018 (6) https://doi.org/10.7717/  peerj.5057.  Rogozovsky, I., et al., 2021. Impact of aerosol layering, complex aerosol mixing, and  cloud coverage on highresolution MAIAC aerosol optical depth measurements:  Fusion of lidar, AERONET, satellite, and groundbased measurements. Atmos.  Environ. 247 https://doi.org/10.1016/j.atmosenv.2020.118163.  Rowan, G.S.L., Kalacska, M., 2021. A Review of Remote Sensing of Submerged Aquatic  Vegetation for NonSpecialists. Remote Sens. 13 (4), 623. https://doi.org/10.3390/  rs13040623.  Royal National Lifeboat Institution, 2017. Maritime Search and Rescue Manual. Poole.  Available at: https://rnli.org/-/media/rnli/downloads/maritimesar-2017.pdf.  Schaub, J., et al., 2018. Using unmanned aerial vehicles (UAVs) to measure jellyfish  aggregations. Marine Ecol. Progress Series 591, 29–36. https://doi.org/10.3354/  meps12414.  Sherstjuk, V., Zharikova, M., Sokol, I., 2018. Forest FireFighting Monitoring System  Based on UAV Team and Remote Sensing. In: 2018 IEEE 38th International  Conference on Electronics and Nanotechnology, ELNANO 2018 - Proceedings.  Institute of Electrical and Electronics Engineers Inc., pp. 663–668. https://doi.org/  10.1109/ELNANO.2018.8477527  
Simic Milas, A., et al., 2018. Unmanned Aerial Systems (UAS) for environmental  applications special issue preface. Int. J. Remote Sens. 4845–4851. https://doi.org/  10.1080/01431161.2018.1491518.  Slidetodoc, 2021. Search Patterns W S Objectives DEMONSTRATE search pattern.  Available at: https://slidetodoc.com/searchpatternsw-sobjectivesdemonstrate  -searchpattern/ (Accessed: 9 July 2021).  Sony, 2021. ILCE-6000 / ILCE-6000L / ILCE-6000Y / ILCE-6000Z Specifications |  Cameras | Sony UK. Available at: https://www.sony.co.uk/electronics/intercha  ngeablelenscameras/ilce-6000-bodykit/specifications (Accessed: 16 November  2021).  
Spanakis, N., et al., 2014. Modelling of seaweed ingress into a nuclear power station  cooling water intake. In: 21st TELEMAC MASCARET User Conference, pp. 125–131.  Thompson, T.M., Young, B.R., Baroutian, S., 2020. Pelagic Sargassum for energy and  fertiliser production in the Caribbean: A case study on Barbados. Renew. Sustain.  Energy Rev. https://doi.org/10.1016/j.rser.2019.109564.  Thomsen, P.F., et al., 2012. Detection of a Diverse Marine Fish Fauna Using  Environmental DNA from Seawater Samples. PLoS ONE 7 (8), e41732. https://doi.  org/10.1371/journal.pone.0041732.  Thorn, A., Lambert, N., 2016. Workshop: Managing Ocean risks (Satellites for managing  ocean risks). Available at: https://sa.catapult.org.uk/southcoast/wpcontent/upl  oads/sites/6/2016/09/MOR_23-Sep-2016_CombinedPresentationslessASV-3.pdf  (Accessed: 11 May 2020).  Tombs, R., Radford, U., 2015. ADW: Aurelia aurita: INFORMATION. Available at: htt  ps://animaldiversity.org/accounts/Aurelia_aurita/ (Accessed: 5 July 2021).  Vasslides, J.M., Sassano, N.L., Hales, L.S., 2018. Assessing the effects of a barrier net on  jellyfish and other local fauna at estuarine bathing beaches. Ocean Coastal Manage.  163, 364–371. https://doi.org/10.1016/j.ocecoaman.2018.07.012.  Vaughan, A., 2018. In a laver: seaweed shuts nuclear reactor again in bad weather |  Business | The Guardian, The Guardian. Available at: https://www.theguardian.  com/business/2018/mar/05/seaweedshutsnuclearreactoredftornessplant  (Accessed: 22 February 2019).  Ventura, D., et al., 2018. Mapping and classification of ecologically sensitive marine  habitats using unmanned aerial vehicle (UAV) imagery and ObjectBased Image  Analysis (OBIA). Remote Sens. 10 (9) https://doi.org/10.3390/rs10091331.  Wei, M., et al., 2018. Early warning model for marine organism detection in nuclear  power stations. In: Proceedings of the 30th Chinese Control and Decision  Conference, CCDC 2018. Institute of Electrical and Electronics Engineers Inc.,  pp. 1999–2004. https://doi.org/10.1109/CCDC.2018.8407454  
Weybright, S., Kelly, P., 2016. Algal bloom turns ocean red. In: OCEANS 2015 - MTS/  IEEE Washington. Institute of Electrical and Electronics Engineers Inc. https://doi.  org/10.23919/oceans.2015.7404622  
Zhu, H., et al., 2020. Design and assessment of octocopter drones with improved  aerodynamic efficiency and performance. Aerosp. Sci. Technol. 106, 106206.  https://doi.org/10.1016/J.AST.2020.106206.  Zielinski, D.P., Sorensen, P.W., 2016. Bubble Curtain Deflection Screen Diverts the  Movement of both Asian and Common Carp. North Am. J. Fish. Manag. 36 (2),  267–276. https://doi.org/10.1080/02755947.2015.1120834.  
Zoltan, T.B., Taylor, K.S., Achar, S.A., 2005. Health issues for surfers. Am. Fam.  Physician.  
B. Mcilwaine et al.                                                                                                                                                                                                                               
",Investigating optimal unmanned aircraft systems flight plans for the.pdf,8
"Article Improved Compact Cuckoo Search Algorithm Applied to Location of Drone Logistics Hub 
JengShyang Pan , PeiCheng Song , ShuChuan Chu * and YanJun Peng 
College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao 266590, China; jspan@cc.kuas.edu.tw (J.-S.P.); spacewe@outlook.com (P.-C.S.); *pengyanjuncn@163.com (Y.-J.P.) Correspondence: scchu0803@gmail.com 
Received: 31 January 2020; Accepted: 27 February 2020; Published: 3 March 2020 
���������� ������� 
Abstract: Drone logistics can play an important role in logistics at the end of the supply chain and special environmental logistics. At present, drone logistics is in the initial development stage, and the location of drone logistics hubs is an important issue in the optimization of logistics systems. This paper implements a compact cuckoo search algorithm with mixed uniform sampling technology, and, for the problem of weak search ability of the algorithm, this paper combines the method of recording the key positions of the search process and increasing the number of generated solutions to achieve further improvements, as well as implements the improved compact cuckoo search algorithm. Then, this paper uses 28 test functions to verify the algorithm. Aiming at the problem of the location of drone logistics hubs in remote areas or rural areas, this paper establishes a simple model that considers the trafﬁc around the village, the size of the village, and other factors. It is suitable for selecting the location of the logistics hub in advance, reducing the cost of drone logistics, and accelerating the largescale application of drone logistics. This paper uses the proposed algorithm for testing, and the test results indicate that the proposed algorithm has strong competitiveness in the proposed model. 
Keywords: improved compact cuckoo search algorithm; location of drone logistics hub; sampling technology; drone logistics 
1. Introduction 
There are many complex optimization scenarios in the ﬁelds of industry, ﬁnance, mathematics, etc. Some of them are difﬁcult to ﬁnd a true global optimal solution. The metaheuristic algorithm is suitable for dealing with problems that are not solved by speciﬁc effective methods [1–4]. The Cuckoo Search (CS) algorithm is a new heuristic algorithm that simulates cuckoo parasitic brooding and solves complex optimization problems [5,6]. The CS uses the nest position of the cuckoo bird to represent a possible solution in the solution space. The cuckoo bird’s parasitic brooding behavior is used to search the solution space of the complex optimization problem. The movement of the solution is realized by the cuckoo’s Levy ﬂight mechanism, and the potential better solution is found through continuous searching and updating. The Levy ﬂight mechanism used in the cuckoo algorithm can effectively jump out of the local optimal solution, and thus has better global search performance. It has also achieved better results in engineering optimization problems [6,7]. Since the cuckoo algorithm was proposed, various improved versions of the algorithm have been proposed for different uses, such as Modiﬁed Cuckoo Search (MCS) [8], Binary Cuckoo Search (BCS) [9], Multiobjective Cuckoo Search (MOCS) [10], Chaotic Cuckoo Search (CCS) [11], etc. This type of algorithm is usually used to solve complex optimization problems, thus it will set a population to obtain better solutions in a shorter time. Therefore, when dealing with complex optimization problems, or when it is applied to a device with 
Mathematics 2020, 8, 333; doi:10.3390/math8030333 www.mdpi.com/journal/mathematics 
limited memory, the heuristic algorithm needs to be improved to achieve the same or better solution in a shorter time or with less memory consumption. Compact is a technique that can reduce the memory usage of the metaheuristic algorithm. By using a probabilistic model to replace the population used in the algorithm from a macro perspective, it achieves less memory usage and shorter calculation time [12–18]. The compact method uses a probability model to represent the original population, and then uses the probability model to generate a new solution. By comparing the generated solutions, the probability model is updated, which is then used to replace the population update in the original algorithm [12]. Some related algorithm improvements using the compact method have been proposed, such as compact particle swarm optimization (cPSO) [12], compact genetic algorithm (cGA) [13], compact differential evolution (cDE) [14], compact bat algorithm (cBA) [15], etc. This article attempts to implement an improved version of the compact CS algorithm with a mixture of normal and uniform distributions. For the problem of weak search ability of the algorithm, this paper combines the method of recording the key positions of the search process and increasing the number of generated solutions to achieve further improvements and implements the improved compact cuckoo search algorithm (icCS). The algorithm was tested using 28 test functions of CEC2017. As a new logistics method in the supply chain, drone logistics can effectively improve the efﬁciency of the logistics system and solve the problem of express delivery in the last mile of the current logistics system [19,20]. Drone logistics, with its own advantages, can perform express delivery in rural, mountainous, or congested areas, as well as areas where ground trafﬁc is impassable [20]. It can also be used in special situations and applied to scenarios that require rapid delivery, such as medical rescue and blood product transportation [21–24]. To apply drones to logistics systems, there have been many related studies. In addition to optimizing the design of logistics systems and logistics drones [25], it is also necessary to design logistics models based on cost, efﬁciency, and other factors. Flight optimization in the process of logistics distribution of drones is also an issue that needs to be researched in the ﬁeld of drone logistics [26]. There are currently two main models of drone logistics: models for distribution centers and drones and those for delivery vehicles and drones. Many scholars have studied the logistics mode of combining drone and truck transportation [27,28]. For the model of using truck transportation and drone for distribution, the logistics problem is usually regarded as a path planning problem with the drone [29]. Then, usually the travelling salesman problem is used to solve it on the basis of adding drones [30]. Intelligent algorithms are also applied to such problems [31]. In addition, there are many studies using machine learning to deal with supply chain problems. Some machine learning methods, such as Bayesian optimization, can also effectively deal with optimization problems [32,33]. The logistics mode of distribution centers and drones usually focuses on the location of the logistics center, and, because of the low load of the drone itself and the limited battery energy, the logistics of the drone are limited [34]. In addition, other scholars have studied other inﬂuencing factors of drone logistics, including operating costs, differences between urban and rural areas, etc. [35,36]. Hu et al. [37] used CS to deal with the trajectory planning of micro aerial vehicles for express transportation in cities. Considering the wind ﬁeld, the obstacles of the building, and the characteristics of the goods, the cuckoo algorithm is used to plan the transportation path. This paper focuses on rural and remote areas, where surrounding villages are served by setting up a drone logistics hub. The path of the drone during transportation is a straight line between the logistics hub and the village. The main problem is the optimization of the location of the logistics hub. This paper aims at the logistics scenarios in rural and remote areas, using the logistics model of distribution centers and drones, assuming that future logistics drones can or have a stronger load capacity and longer dwell time. Then, the location of the drone logistics hub is simply modeled and tested using the algorithm proposed in this paper. 
2. Related Work 
This section brieﬂy introduces the cuckoo search algorithm and the drone logistics hub location model proposed in this paper. 
2.1. Metaheuristics Algorithm of Cuckoo 
The CS algorithm is a new metaheuristic algorithm that simulates the breeding strategy of cuckoo in nature [5]. It solves complex optimization problems by imitating the brooding and parasitic behavior of cuckoos in nature. The cuckoo search algorithm uses the position of the bird nest to represent a possible solution, and updates the solution by updating the position of the bird nest. The update method uses Lévy ﬂight to simulate the movement pattern of birds in nature. Lévy ﬂight consists of longrange ﬂights with occasional large steps and shortrange ﬂights with frequent small steps. The occasional longdistance ﬂight in Lévy ﬂight can expand the search range and prevent falling into local optimum. To simplify the implementation of this algorithm , three simple and idealized rules are set for the cuckoo search algorithm. (1) Each cuckoo produces only one egg at a time, and then randomly selects a location for hatching. (2) The nest with the best eggs will be preserved and passed on to the next generation. (3) The number of nests that can be used is ﬁxed, and the probability of the eggs in the nest being found is pa ∈ [0, 1] . When the egg is found, the owner of the nest will throwaway the egg or build a new nest. The cuckoo search algorithm uses the parameter pa to control local search and global exploration [38]. The formula for local search is written as 
x(t+1) i = x(t) i + α · St ⊗ H (pa − ϵ) ⊗ � x(t) j − x(t) k � (1) 
where x(t+1) represents the next generation solution, i is a cuckoo in the solution, St is the step size, 
H(u) is a Heaviside function, ϵ is a random number generated by a uniform distribution, and x(t) j and x(t) k represent two randomly selected different solutions from all current possible solutions. The implementation formula for global exploration is written as 
x(t+1) i = x(t) i + αL´evy(λ) (2) 
In Equation (2), α > 0 indicates the step size scaling factor, usually α = 1. The random step size in Lévy ﬂight is generated using the Lévy probability distribution. 
L´evy(δ) ∼ u = t−1−δ, (0 < δ ≤ 2) (3) 
The variance and mean of the distribution are inﬁnite. According to the original literature of the CS algorithm [5], the pseudocode of the algorithm is shown in Algorithm 1. Compared with PSO, cuckoo search algorithm can achieve global convergence [39–41]. Compared to algorithms using standard Gaussian processes, the cuckoo search algorithm is more efﬁcient by using Lévy ﬂights. 
Algorithm 1: Cuckoo search via Lévy ﬂights. 
Objective ﬁtness function f (x), x = (x1, . . . , xd)T; Generate initial n bird nests xi(i = 1, 2, 3, . . . , n); while (t< Max Generation ) or (stop criterion) do 
Generate a random solution using Lévy ﬂights; Calculate and store ﬁtness Fi; ifChoose a nest among n( say, j) randomly; � Fi > Fj � then 
endGenerate new solution and replace j; A fraction (pa) of worse nests are abandoned and generate a new solution; Keep the optimal solution in all solutions unchanged; Find the current optimal solution and save; end 
2.2. Location Model of Drone Logistics Hub 
At present, drone logistics is limited by the low load and weak endurance of drones. Moreover, drones have limited mobility and cannot perform longterm continuous delivery, thus the current more reasonable model is the collaborative model of delivery vehicles and drones. Then, path planning for delivery vehicles and drones is performed. However, after the drone picks up the goods from the delivery vehicle for delivery, it is necessary to consider that the drone returns to the delivery vehicle after the recipient receives it. The movement of the delivery vehicle and the uncertainty of the recipient’s pickup time will signiﬁcantly reduce the drone’s delivery efﬁciency. However, with the development of technology, drone equipment for logistics will solve the current problems, and, when the level of automation increases, the mode of combining small unmanned logistics centers with drones will become more competitive. This paper chooses the model of unmanned logistics center and drone, and applies it to the location of unmanned logistics hub in rural areas. The simulation diagram of the model in twodimensional space is shown in Figure 1. The premise assumptions and explanations of the model are as follows: 
(1) The frone only travels to and from one village at a time. 
(2) The drone’s endurance is able to meet the ﬂight requirements from the logistics hub to the farthest village that the logistics hub is responsible for. 
(3) Under ideal conditions, the drone distribution path is a straight line from the logistics hub to the corresponding village. 
(4) Each logistics hub is responsible for express delivery services in multiple villages, and each village chooses the nearest logistics hub to serve it. 
(5) The sizes of the villages are different, that is, the areas of the villages and the numbers of villagers are different. 
(6) Drones do not enter the village when delivering goods, but deliver goods to the edge of the village to ensure safety. 
(7) The land transportation distance from the logistics hub to each village is different and the degree of trafﬁc difﬁculty is measured by the distance. 
(8) The number of logistics hubs is artiﬁcially set according to the scope of application and artiﬁcially selected after calculating different solutions. 
𝑅 
𝐻 
𝐿1 
𝐿2 
Village and village edge 
Drone logistics hub 
Land transportroute 
Drone flight route 
Village center 
Village edge 
Figure 1. Location model of drone logistics hub. 
The circle in Figure 1 represents the village, and the size of the circle represents the radius of the village, which is expressed as R. The triangle represents the drone logistics station, and H represents the straight line distance between the logistics hub and the village center. The model established in this paper is relatively simple. This paper only considers the distribution distance, the size of the village, and the current village’s efﬁciency ratio of using drone logistics to land transportation, which is used to indicate the degree of difﬁculty of land transportation. The objective function of the model is written as min F = ∑ i∈N (Hi − Ri) · cp · ( L1 
L2) (4) 
In the formula, Hi represents the straight line distance from the center of the village labeled i to the nearest logistics hub. Hi − Ri is the distance between a village and a logistics hub minus the village radius, as drone delivery is not delivered to the precise location of the recipient, but is delivered to the edge of the village, which can ensure better security. N is the total number of villages to be considered. cp is the number of people living in the village. The larger is the population, the more frequently does the logistics center deliver to the village, thus the logistics hub needs to be closer to the village to reduce the overall cost. L1 is the distance for land transportation, L2 is the distance for linear delivery using drones, and L1 
L2 is usually a value greater than 1, thus the logistics hub needs to be closer to villages with high land distribution costs. The variables and parameters involved in the model can be obtained through actual measurement. There are no unnatural parameters, and the degree of trafﬁc difﬁculty is also obtained by using land transportation distance and straight ﬂight distance. All parameters can be calculated from the application environment data during actual application.The solution obtained after calculating the model is the relative positions of multiple logistics hubs, and different villages choose their nearest logistics hubs based on the distance. In the end, different solutions will be generated according to the number of logistics hubs. Because the proposed model does not take into account all the inﬂuencing factors, and the importance of the model’s constraints is different in different situations, it needs to be artiﬁcially selected according to actual conditions. 
3. Improved Compact Cuckoo Search Algorithm 
This section introduces the application of the compact scheme and improved compact scheme to cuckoo search algorithm. 
3.1. Compact Scheme 
The essence of the distribution estimation algorithm (EDA) is to use the probability model to represent the population in the metaheuristic algorithm, use the probability model to represent the population from a macro perspective, and implement the operation on the population in the metaheuristic algorithm by operating the probability model [42,43]. The compact method is an effective 
method to reduce the memory footprint of the metaheuristic algorithm. By updating the probability model instead of updating the entire population, the calculation amount is reduced and the algorithm running time is shortened. Firstly, the probabilistic model is constructed using the original population distribution, and then the population is updated by evaluating the probability model to ﬁnd the optimal solution. Since the probability model is used to represent the entire population, the characteristics of the original population are described from a macro perspective. Perturbation Vector (PV) is often used to represent the characteristics of the entire population. PV is constantly changing with the operation of the algorithm, which is deﬁned as : PVt = � µt, σt� , where µ is used to representing the mean value of the PV, σ is the standard deviation of the PV, and t is used to represent the number of current iterations. Each pair of mean and standard deviation corresponds to a probability density function (PDF), which is truncated at [−1, 1] and normalized to an area of amplitude of 1 [44]. Using the PV vector, the solution xi can be randomly generated by the inverse cumulative distribution function (CDF). After generating two solutions using PV, usually which is better is judged by comparing the ﬁtness function values of the two solutions; the better solution is the winner and the worse solution is the loser. Then, the PV is updated. The formula for updating each standard deviation and the average value in the PV using winner and loser is as follows: 
µt+1 i = µt i + 1 
Np (winneri − loseri) (5) 
where µt+1 i represents the newly generated average and Np is the virtual population. The update rules for σ are as follows: 
σt+1 i = 
� 
� σt i �2 + � µt i �2 − � µt+1 i �2 + 1 
Np 
� winner2 i − loser2 i � (6) 
The PV vector and the generated individual solution are stored during algorithm execution, instead of storing the location of the entire population solution and the motion vector, which achieves less runtime memory usage and is beneﬁcial for use on resourceconstrained devices. However, since the conventional compact algorithm only randomly generates one solution at a time, there are fewer possible solutions explored during each iteration, which will cause the problem of insufﬁcient convergence ability in the later iterations, and the method needs to be improved. 
3.2. Improved Compact Scheme 
The compact algorithm saves more memory resources, reduces the amount of calculation, and shortens the algorithm running time compared with the original algorithm. However, the compact algorithm generates two solutions per iteration and compares them. The solution generated during each iteration is less than the populationbased method in the original algorithm. Therefore, the number of overall searches is small, the algorithm will converge slowly in the later stages of iteration, and it is easy to fall into a local optimum. Because the compact algorithm uses PV to generate new solutions, with continuous iteration, PV will slowly converge to a certain area, but the number of solutions generated by PV during each iteration is small, thus it is difﬁcult to jump out of the local optimum. Thus, the method of sampling using the normal distribution in the compact mode is improved. Considering the above problems, this paper chooses to add the uniform distribution sampling method on the basis of using the original compact mode. As shown in Algorithm 2, a new solution is generated using PV during each iteration, and another new solution is generated using uniform sampling. Then, CS is used to update the two generated solutions. Using uniform sampling in the solution space can search for other regions to ﬁnd a better solution while the PV converges to the optimal region. Because there may be better solutions around the solution generated during the iteration, to get closer to the surrounding better 
solution during the iteration, a perturbation operation on the optimal value is added in this paper, as shown in Algorithm 2. 
Algorithm 2: Improved compact cuckoo search algorithm. 
Objective ﬁtness function f (x), x = (x1, . . . , xd)T; for i = 1 : d do 
endinitialize µ[i] = 0; σ[i] = λ = 10;//PV initialization//; Initializing nest location x randomly and gbest with the best location value:gbest = arg min f [x]; FL = 1;//Switch ﬂag//; while (t< Max Generation ) or (stop criterion) do 
if FL==1 then 
Generate x1, x2 using PV, uniformly distributed samples; x1, x2 randomly walk by Lévy ﬂights; [winner, loser] =compete(x1, x2, newx1, newx2); for i = 1 : d do 
endUpdate µ[i], σ[i]via Equations (5) and (6); gbestrd = gbest + rand · randn(1, d) //Perturbation//; [winner, loser] =compete(winner, gbest, gbestrd); gbest = winner; t = t + 1; endFL = 2 when caught in a local optimal; else 
nest randomly walk by Lévy ﬂights; evaluate nest’s quality/ﬁtness; A fraction(pa) of worse nests are abandoned and build a new one; Keep the optimal solution in all solutions unchanged; endFind the current optimal solution and save; if FL==1 then 
Use gbest to form nestpvi(i ≤ n/2) via Equations (7); endUse nestpv and uniform distribution to form the nest; end 
After improving the compact mode, the algorithm can achieve better results and convergence ability, but the global search ability and local search ability can still be further improved. Therefore, a switching mode is added in this paper. When the algorithm is trapped in a local optimal value, it switches to a populationbased search mode, as shown in Algorithm 2. There are many ways to judge when the algorithm is trapped in a local optimal value. The ﬁrst method can compare the recent iteration trend with the overall iteration trend. The second method can determine whether a better solution can be found within a certain number of iterations. This paper uses the second method to switch modes. The possible solutions when the mode is switched are divided into two parts, one is selected from the optimal solution obtained during the execution of the compact algorithm, and the other is generated using a uniform distribution, as shown in Algorithm 2, where n is the number of new solutions generated per iteration after switching. 
[best f it(t − m) − best f it(t − 1)] − [best f it(t − 1) − best f it(t)] < 0 (7) 
There are many ways to obtain the optimal solution from the operation of the compact algorithm. This article chooses the key solution of the optimal solution in the previous iterative process. 
The selection of the key solution needs to conform to Equation (7); the difference between the ﬁtness function value of the key solution and the previous solution is greater than the difference of the ﬁrst m optimal values of the key solution. m in this paper is 20. t represents the current number of iterations and best f it is used to store the optimal solution obtained during each iteration. By selecting the key solution from the optimal solution for each iteration, it is possible to use the previous search results for a more reﬁned search, which is a memorybased approach. Selecting those breakthrough solutions in the iterative process through Equation (7) can assist in ﬁne search after switching. Based on the above introduction, the ﬂow chart of the icCS algorithm in this paper is given in Figure 2. 
𝑆𝑡𝑎𝑟𝑡 
𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑒 x1, x2 𝑢𝑠𝑖𝑛𝑔 𝑃𝑉, 𝑢𝑛𝑖𝑓𝑜𝑟𝑚 𝑠𝑎𝑚𝑝𝑙𝑒𝑠 
x 1, x2 𝐿é𝑣𝑦 𝑓𝑖𝑔ℎ𝑡𝑠; 𝑈𝑝𝑑𝑎𝑡𝑒 𝜇, 𝜎 
𝐷𝑖𝑠𝑡𝑢𝑟𝑏 𝑔𝑏𝑒𝑠𝑡, 𝑈𝑝𝑑𝑎𝑡𝑒 𝑔𝑏𝑒𝑠𝑡 
𝑆𝑡𝑢𝑐𝑘 𝑖𝑛 𝑙𝑜𝑐𝑎𝑙 𝑜𝑝𝑡𝑖𝑚𝑢𝑚? 
𝐸𝑛𝑑 
𝑆𝑡𝑜𝑟𝑒 𝑘𝑒𝑦 𝑠𝑜𝑙𝑢𝑡𝑖𝑜𝑛𝑠; 𝑖 = 𝑖 + 1; 
𝐼𝑛𝑖𝑡𝑖𝑎𝑙𝑖𝑧𝑎𝑡𝑖𝑜𝑛, FL=1; 
𝑖 < 𝑀𝑎𝑥𝐺𝑒𝑛? 
𝐹𝐿 = 2; 𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑒 𝑛𝑒𝑠𝑡 
𝐹𝐿 = 1? 
𝑛𝑒𝑠𝑡 𝐿é𝑣𝑦 𝑓𝑖𝑔ℎ𝑡𝑠; 𝑅𝑒𝑠𝑒𝑡 𝑤𝑜𝑟𝑠𝑒 𝑛𝑒𝑠𝑡; 𝑈𝑝𝑑𝑎𝑡𝑒 𝑛𝑒𝑠𝑡 𝑓𝑖𝑡𝑛𝑒𝑠𝑠; 𝑈𝑝𝑑𝑎𝑡𝑒 𝑛𝑒𝑠𝑡; 𝑖=𝑖+1; 
𝑌𝑒𝑠 
𝑂𝑢𝑡𝑝𝑢𝑡 𝑜𝑝𝑡𝑖𝑚𝑎𝑙 𝑣𝑎𝑙𝑢𝑒 𝑁𝑜 
𝑁𝑜 
𝑌𝑒𝑠 
𝑌𝑒𝑠 
𝑁𝑜 
Figure 2. The overall ﬂow chart of icCS. 
4. Experimental Results 
The proposed algorithm was tested. The test function used CEC’17 benchmark suite [45]. The 28 test functions used in this study include unimodal functions, simple multimodal functions, mixed functions, and composition functions. All test functions used are minimization problems and are deﬁned as follows: min f (x), x = [x1, x2, x3, . . . , xD]T (8) 
where D is the number of dimensions and the search range is [−100, 100]D. According to the introduction of CEC’17 benchmark suite, f2 was excluded because it exhibits unstable behavior, especially for higher dimensions in test functions. Compared with the same algorithm implemented in Matlab, the performance of the one implemented in C is very different [45]. Thus, 28 test functions were used to the the algorithm in this paper. All tested algorithms maintained consistent parameter settings. The population size of all algorithms was 20 and the number of algorithm iterations was set to 3000. Each algorithm was tested ﬁve times on each function and the average value was retained. The parameter settings of each comparison algorithm are shown in Table 1. 
Table 1. Parameters setting of each algorithm. 
Algorithm Main Parameters Setting 
CS Population_number = 20, Max_iteration = 3000, Pa = 0.25 ACS Population_number = 20, Max_iteration = 3000, Pa = 0.25 PSO Population_number = 20, Max_iteration = 3000, ωmax = 0.9, ωmin = 0.4, c1 = 2, c2 = 2 DE Population_number = 20, Max_iteration = 3000, pCR = 0.2, Fmin = 0.2, Fmax = 0.8 SCA Population_number = 20, Max_iteration = 3000 icCS Population_number = 20, Max_iteration = 3000, Pa = 0.25 cPSO Population_number = 20, Max_iteration = 3000, φ1 = 1, φ2 = 1.5, φ3 = 2, c1 = 1, c2 = 1 
cBA Population_number = 20, Max_iteration = 3000, loudness = 0.5, pulse rate = 0.5, min/max frequency = [0,2] cABC Population_number = 20, Max_iteration = 3000, limit = 100 
4.1. Comparison with Common Optimization Algorithms 
The improved compact cuckoo search algorithm proposed in this paper was compared with common classical algorithms on the test functions: the original CS algorithm [5]; the Adaptive Cuckoo Search Algorithm (ACS) [46], in which the parameter pa was set to 0.25; common PSO [47]; DE [48]; and the sine cosine algorithm (SCA) proposed in 2016 [49]. The comparison results are shown in Table 2. 
Table 2. Comparison of means of ﬁtness functions on 30D optimization among CS, ACS, PSO, DE, SCA, and icCS is presented here. 
Functions icCS CS ACS PSO DE SCA 
f1(x) 1.20377 × 102 1.00000 × 1010 1.00000 × 1010 4.56091 × 103 3.18520 × 103 1.51701 × 1010 
f3(x) 3.05501 × 104 3.79292 × 104 3.22087 × 104 1.70774 × 104 8.66524 × 104 5.75645 × 104 
f4(x) 4.62444 × 102 4.73280 × 102 4.65491 × 102 5.40339 × 102 4.92410 × 102 1.93646 × 103 
f5(x) 6.76672 × 102 6.68839 × 102 6.34201 × 102 5.62882 × 102 6.36734 × 102 8.00897 × 102 
f6(x) 6.36546 × 102 6.44382 × 102 6.44924 × 102 6.00733 × 102 6.00000 × 102 6.53341 × 102 
f7(x) 9.50101 × 102 9.35106 × 102 9.64459 × 102 8.26945 × 102 8.72143 × 102 1.18146 × 103 
f8(x) 9.42849 × 102 9.40035 × 102 9.23222 × 102 8.65031 × 102 9.36081 × 102 1.07358 × 103 
f9(x) 5.07394 × 103 5.64605 × 103 9.30145 × 103 1.16805 × 103 9.00000 × 102 6.44206 × 103 
f10(x) 4.82195 × 103 5.05056 × 103 4.99931 × 103 5.12150 × 103 6.76785 × 103 8.60907 × 103 
f11(x) 1.22792 × 103 1.21607 × 103 1.20667 × 103 1.24541 × 103 1.20299 × 103 2.57068 × 103 
f12(x) 4.56905 × 104 6.00015 × 109 9.00368 × 109 6.21396 × 105 4.15010 × 106 1.61706 × 109 
f13(x) 1.81192 × 103 1.00001 × 109 1.00001 × 109 1.80866 × 104 1.77057 × 105 6.63131 × 108 
f14(x) 1.48212 × 103 1.47295 × 103 1.48013 × 103 4.06132 × 104 9.25870 × 104 2.99047 × 105 
f15(x) 1.62982 × 103 1.61110 × 103 1.63020 × 103 1.64717 × 104 5.03273 × 104 2.12505 × 107 
f16(x) 2.60602 × 103 2.65977 × 103 2.69191 × 103 2.22859 × 103 2.45510 × 103 3.82109 × 103 
f17(x) 2.04739 × 103 2.07413 × 103 2.01007 × 103 2.03341 × 103 1.93289 × 103 2.59000 × 103 
f18(x) 7.44420 × 103 7.67991 × 103 8.73722 × 103 1.25873 × 106 9.36373 × 105 4.69351 × 106 
f19(x) 1.94031 × 103 1.94203 × 103 1.95316 × 103 1.29213 × 104 2.77813 × 104 4.60719 × 107 
f20(x) 2.39270 × 103 2.43996 × 103 2.45306 × 103 2.23193 × 103 2.20637 × 103 2.84076 × 103 
f21(x) 2.44515 × 103 2.44476 × 103 2.41188 × 103 2.36641 × 103 2.44690 × 103 2.57886 × 103 
f22(x) 3.59559 × 103 4.00541 × 103 5.33379 × 103 5.83669 × 103 5.18629 × 103 9.64492 × 103 
f23(x) 2.81251 × 103 2.82287 × 103 2.77342 × 103 2.72533 × 103 2.78211 × 103 3.05061 × 103 
f24(x) 2.95136 × 103 2.99155 × 103 2.92443 × 103 2.92132 × 103 2.99210 × 103 3.21214 × 103 
f25(x) 2.88699 × 103 2.88668 × 103 2.88775 × 103 2.90573 × 103 2.88754 × 103 3.27954 × 103 
f26(x) 4.10369 × 103 4.71619 × 103 3.80533 × 103 4.42989 × 103 4.95267 × 103 7.52475 × 103 
f27(x) 3.22191 × 103 3.22988 × 103 3.22089 × 103 3.25043 × 103 3.21313 × 103 3.46664 × 103 
f28(x) 3.18150 × 103 3.17620 × 103 3.16887 × 103 3.27739 × 103 3.23058 × 103 4.11323 × 103 
f29(x) 3.82243 × 103 3.88243 × 103 3.93256 × 103 3.65520 × 103 3.71836 × 103 4.93266 × 103 
w - 19 17 15 16 28 
Table 2 shows the average value obtained by running the icCS algorithm and other algorithms on the test functions. The last row in the table summarizes the comparison results of icCS algorithm and other algorithms, where w indicates on how many test functions icCS has achieved better results than the algorithm results of the current column. Table 3 shows the standard deviation of the icCS algorithm and other algorithms on the test functions. 
According to the data in Tables 2 and 3, the algorithm proposed in this paper achieved better results than other algorithms on the test functions. Especially on speciﬁc functions, such as f1, f12, and f13, compared with CS and ACS algorithms, the proposed algorithm could obtain better and more stable results. At the same time, the overall performance of each algorithm compared with the icCS algorithm was measured at a signiﬁcant level α = 0.05 under the Wilcoxon’s sign rank test (Table 4) [50]. According to Table 2, compared with CS, icCS achieved better or similar results on 19 functions; compared with ACS, icCS achieved better or similar results on 17 functions; compared with the PSO algorithm, icCS obtained better or similar results on 15 functions, with better results than PSO on functions f1, f12, f13, f14, f15, f18, and f19; and compared with DE, icCS achieved better or similar results on 16 functions, obtaining better results on functions f1, f12, f13, f14, f15, f18, f19. However, DE could ﬁnd the global optimal value effectively on f6 and f9, which was better than other algorithms. Compared with SCA, icCS achieved better or similar results on all functions. For the comparison of standard deviations, Table 3 gives the corresponding data. Combined with the data in Table 2, the icCS algorithm proposed in this paper has similar stability compared with CS and ACS algorithms. However, the CS and ACS algorithms on f1, f12, and f13 did not achieve good results. Based on the above comparison, the overall performance of the algorithm icCS proposed in this paper is better on 28 test functions. 
Table 3. Comparison of standard deviation of Fitness Functions on 30D optimization among CS, ACS, PSO, DE, SCA, and icCS is presented here. 
Functions icCS CS ACS PSO DE SCA 
f1(x) 1.58954 × 101 0.00000 0.00000 6.14926 × 103 2.84671 × 103 1.02413 × 109 
f3(x) 6.59109 × 103 1.23039× 104 7.88398 × 103 8.30263 × 103 1.77116× 104 8.19113 × 103 
f4(x) 3.50377 × 101 3.05228 × 101 2.91856 × 101 4.49024 × 101 7.49639 4.73629 × 102 
f5(x) 1.82381 × 101 1.68876 × 101 2.20047 × 101 9.36649 9.54795 2.40221 × 101 
f6(x) 9.31378 4.77247 1.30530 × 101 6.83978 × 10−1 4.32832 × 10−8 6.94827 f7(x) 4.81329 × 101 4.62859 × 101 4.50709 × 101 2.98190 × 101 1.33903 × 101 4.31410 × 101 
f8(x) 1.93099 × 101 2.37403 × 101 2.19209 × 101 2.07230 × 101 1.45249 × 101 2.30497 × 101 
f9(x) 1.31639 × 103 1.92154 × 103 2.49578 × 103 1.23386 × 102 4.95811 × 10−6 1.51949 × 103 
f10(x) 4.30482 × 102 3.13624 × 102 5.02779 × 102 1.45196 × 103 1.62505 × 102 2.78873 × 102 
f11(x) 3.21566 × 101 1.69000 × 101 3.19376 × 101 6.68227 × 101 2.36883 × 101 5.02743 × 102 
f12(x) 1.44805× 104 5.16379 × 109 3.15063 × 109 5.45313 × 105 1.70294 × 106 4.08268 × 108 
f13(x) 1.64281 × 102 3.16227 × 109 3.16227 × 109 1.72784 × 104 1.73217 × 105 2.79337 × 108 
f14(x) 1.48525 × 101 1.37260 × 101 1.49070 × 101 1.72076 × 104 6.05874 × 104 1.67867 × 105 
f15(x) 4.30874 × 101 3.55231 × 101 3.23631 × 101 1.75888 × 104 4.37739 × 104 2.09254 × 107 
f16(x) 1.62357 × 102 2.19252 × 102 2.12095 × 102 4.70514 × 102 2.20864 × 102 1.97921 × 102 
f17(x) 1.07398 × 102 1.15661 × 102 1.20506 × 102 1.31330 × 102 6.21786 × 101 1.80726 × 102 
f18(x) 2.57856 × 103 2.16380 × 103 3.99360 × 103 2.08530 × 106 3.00827 × 105 2.25986 × 106 
f19(x) 8.58002 1.18938 × 101 1.24844 × 101 1.80333× 104 1.93855× 104 2.02455 × 107 
f20(x) 1.37076 × 102 9.92375 × 101 1.10117 × 102 8.55912 × 101 1.05365 × 102 1.36654 × 102 
f21(x) 3.40883 × 101 3.81789 × 101 1.45891 × 101 1.55532 × 101 8.60537 1.78525 × 101 
f22(x) 2.09828 × 103 2.20659 × 103 2.10756 × 103 2.64497 × 103 2.29411 × 103 5.89906 × 102 
f23(x) 3.75856 × 101 2.80960 × 101 2.37389 × 101 3.11146 × 101 8.48648 5.34084 × 101 
f24(x) 1.91721 × 101 2.94028 × 101 2.37271 × 101 5.32288 × 101 1.34512 × 101 4.98180 × 101 
f25(x) 2.06219 1.49136 3.20373 1.30168 × 101 3.47109 × 10−1 3.44086 × 101 
f26(x) 1.39171 × 103 7.69896 × 102 1.04089 × 103 1.88883 × 102 1.59418 × 102 5.10981 × 102 
f27(x) 1.21796 × 101 1.50593 × 101 6.43974 1.37635 × 101 3.20524 5.08883 × 101 
f28(x) 4.21625 × 101 4.16685 × 101 4.77688 × 101 4.34023 × 101 2.53171 × 101 1.56604 × 102 
f29(x) 1.61220 × 102 1.34654 × 102 1.73408 × 102 1.27860 × 102 1.62071 × 102 2.70478 × 102 
Table 4. Compared with the proposed icCS, the overall performance of each algorithm is measured at a signiﬁcant level α = 0.05 under the Wilcoxon’s signed rank test. 
Functions CS ACS PSO DE SCA 
f1(x) 6.386445 × 10−5 6.386445 × 10−5 7.685389 × 10−4 4.396388 × 10−4 1.826718 × 10−4 
f3(x) 2.413216 × 10−1 6.231762 × 10−1 2.827272 × 10−3 1.826718 × 10−4 1.826718 × 10−4 
f4(x) 3.074895 × 10−1 4.726756 × 10−1 1.314945 × 10−3 1.401928 × 10−2 1.826718 × 10−4 
f5(x) 4.273553 × 10−1 7.685389 × 10−4 1.826718 × 10−4 1.826718 × 10−4 1.826718 × 10−4 
f6(x) 3.120901 × 10−2 1.858767 × 10−1 1.826718 × 10−4 1.309295 × 10−4 1.314945 × 10−3 
f7(x) 7.913368 × 10−1 4.273553 × 10−1 1.826718 × 10−4 1.826718 × 10−4 1.826718 × 10−4 
f8(x) 9.097219 × 10−1 6.402210 × 10−2 2.461281 × 10−4 4.726756 × 10−1 1.826718 × 10−4 
f9(x) 5.205229 × 10−1 5.828399 × 10−4 1.826718 × 10−4 1.786145 × 10−4 1.212245 × 10−1 
f10(x) 7.566157 × 10−2 3.846731 × 10−1 7.337300 × 10−1 1.826718 × 10−4 1.826718 × 10−4 
f11(x) 8.501067 × 10−1 5.205229 × 10−1 7.913368 × 10−1 1.858767 × 10−1 1.826718 × 10−4 
f12(x) 6.519225 × 10−4 8.744987 × 10−5 2.827272 × 10−3 1.826718 × 10−4 1.826718 × 10−4 
f13(x) 1.826718 × 10−4 1.826718 × 10−4 1.725746 × 10−2 1.826718 × 10−4 1.826718 × 10−4 
f14(x) 7.566157 × 10−2 9.698500 × 10−1 1.826718 × 10−4 1.826718 × 10−4 1.826718 × 10−4 
f15(x) 2.122938 × 10−1 9.698500 × 10−1 1.826718 × 10−4 1.826718 × 10−4 1.826718 × 10−4 
f16(x) 6.231762 × 10−1 3.447042 × 10−1 1.725746 × 10−2 1.619724 × 10−1 1.826718 × 10−4 
f17(x) 6.775850 × 10−1 3.447042 × 10−1 9.097219 × 10−1 1.401928 × 10−2 1.826718 × 10−4 
f18(x) 7.337300 × 10−1 5.205229 × 10−1 1.826718 × 10−4 1.826718 × 10−4 1.826718 × 10−4 
f19(x) 9.698500 × 10−1 9.108496 × 10−3 1.826718 × 10−4 1.826718 × 10−4 1.826718 × 10−4 
f20(x) 4.273553 × 10−1 2.730363 × 10−1 1.725746 × 10−2 4.586392 × 10−3 2.461281 × 10−4 
f21(x) 7.913368 × 10−1 2.113393 × 10−2 2.461281 × 10−4 7.913368 × 10−1 1.826718 × 10−4 
f22(x) 7.566157 × 10−2 5.390256 × 10−2 1.041099 × 10−1 3.763531 × 10−2 1.826718 × 10−4 
f23(x) 3.846731 × 10−1 9.108496 × 10−3 3.298385 × 10−4 3.763531 × 10−2 1.826718 × 10−4 
f24(x) 1.706249 × 10−3 2.574808 × 10−2 1.212245 × 10−1 3.298385 × 10−4 1.826718 × 10−4 
f25(x) 7.913368 × 10−1 6.775850 × 10−1 1.826718 × 10−4 4.726756 × 10−1 1.826718 × 10−4 
f26(x) 3.447042 × 10−1 9.698500 × 10−1 4.726756 × 10−1 4.726756 × 10−1 1.826718 × 10−4 
f27(x) 2.122938 × 10−1 9.698500 × 10−1 5.828399 × 10−4 4.515457 × 10−2 1.826718 × 10−4 
f28(x) 9.097219 × 10−1 5.205229 × 10−1 2.461281 × 10−4 4.396388 × 10−4 1.826718 × 10−4 
f29(x) 5.707504 × 10−1 1.858767 × 10−1 4.515457 × 10−2 2.413216 × 10−1 1.826718 × 10−4 
4.2. Comparison with Compact Algorithms 
Tables 5 and 6 compare the proposed pcCS algorithm with other common algorithms using the compact method, including compact Particle Swarm Optimization (cPSO) [12], compact Bat Algorithm (cBA) [15], and compact Artiﬁcial Bee Colony algorithms (cABC) [51]. The number of virtual populations was set to 20 and the parameters of the algorithm remained the same as those in the original document. This paper compares the proposed icCS algorithm with other compact algorithms in 10D and 30D optimization. At the same time, the overall performance of each other algorithm was measured at a signiﬁcant level α = 0.05 under Wilcoxon sign rank test. According to the data in Tables 5 and 6, the algorithm proposed in this paper has better performance than the other three compact algorithms and can obtain better results. For the cBA algorithm in Table 5, icCS achieved better results on f1, f3, f4, f12, f13, f18, f19, and f29. Combined with the results of Wilcoxon sign rank test, the proposed icCS algorithm was signiﬁcantly better than the cBA algorithm on 28 test functions. For the cPSO and cABC algorithms, the cABC algorithm could still obtain better results when it was optimized in 10D, but, when it was optimized in 30D, as shown in Table 6, the cABC algorithm was not as stable as the proposed icCS algorithm. As shown in Table 5 for 10D optimization and Table 6 for 30D optimization, the performance of the proposed icCS algorithm in different dimensions is similar and does not ﬂuctuate too much. According to the results of the Wilcoxon sign rank test, the proposed icCS algorithm was signiﬁcantly better than other algorithms using only compact technology for the 28 test functions and both 10D and 30D optimization. 
Table 5. Comparison with the average ﬁtness function on 10D optimization among cBA, cPSO, and cABC algorithms on 28 test functions. The overall performance of each of the other algorithms was measured at a signiﬁcant level α = 0.05 under the Wilcoxon’s signed rank test. 
D = 10 Mean Wilcoxon’s Sign Rank Test 
Functions icCS cBA cPSO cABC cBA cPSO cABC 
f1(xi) 1.000 × 102 9.504 × 1010 1.050 × 1011 2.022 × 1010 1.391 × 10−20 7.188 × 10−21 1.391 × 10−20 
f3(xi) 3.000 × 102 5.782 × 109 3.178 × 1011 1.833 × 104 1.391 × 10−20 1.056 × 10−20 1.391 × 10−20 
f4(xi) 4.000 × 102 2.045 × 104 3.273 × 104 3.348 × 103 3.303 × 10−18 2.241 × 10−18 3.303 × 10−18 
f5(xi) 5.120 × 102 8.476 × 102 8.085 × 102 6.706 × 102 3.304 × 10−18 2.242 × 10−18 3.304 × 10−18 
f6(xi) 6.000 × 102 7.344 × 102 7.604 × 102 6.885 × 102 3.304 × 10−18 1.865 × 10−18 3.304 × 10−18 
f7(xi) 7.259 × 102 2.626 × 103 2.056 × 103 8.431 × 102 3.304 × 10−18 2.849 × 10−18 3.304 × 10−18 
f8(xi) 8.127 × 102 1.080 × 103 1.075 × 103 8.868 × 102 3.304 × 10−18 2.242 × 10−18 3.304 × 10−18 
f9(xi) 9.004 × 102 6.903 × 103 2.120 × 104 2.106 × 103 1.864 × 10−19 1.460 × 10−19 1.864 × 10−19 
f10(xi) 1.566 × 103 2.697 × 103 4.221 × 103 5.109 × 103 3.304 × 10−18 3.244 × 10−18 1.391 × 10−20 
f11(xi) 1.102 × 103 7.657 × 104 4.605 × 105 9.857 × 105 3.304 × 10−18 2.471 × 10−18 1.391 × 10−20 
f12(xi) 1.271 × 103 8.830 × 109 1.324 × 1010 2.340 × 109 3.304 × 10−18 2.576 × 10−18 3.304 × 10−18 
f13(xi) 1.307 × 103 5.387 × 109 1.077 × 1010 3.576 × 108 3.304 × 10−18 2.242 × 10−18 3.304 × 10−18 
f14(xi) 1.403 × 103 4.188 × 103 2.2266 × 108 4.0336 × 108 3.304 × 10−18 1.865 × 10−18 1.3916 × 10−20 
f15(xi) 1.501 × 103 4.065 × 104 3.248 × 109 5.095 × 106 3.304 × 10−18 2.675 × 10−18 3.304 × 10−18 
f16(xi) 1.610 × 103 5.251 × 103 1.548 × 104 2.653 × 103 3.304 × 10−18 2.242 × 10−18 3.304 × 10−18 
f17(xi) 1.720 × 103 2.948 × 103 1.1726 × 105 2.174 × 103 3.304 × 10−18 1.342 × 10−18 3.304 × 10−18 
f18(xi) 1.801 × 103 1.312 × 1010 3.134 × 1010 3.418 × 109 3.304 × 10−18 2.471 × 10−18 3.304 × 10−18 
f19(xi) 1.901 × 103 3.457 × 109 1.505 × 1010 3.4326 × 108 3.304 × 10−18 1.734 × 10−18 3.304 × 10−18 
f20(xi) 2.006 × 103 2.690 × 103 3.224 × 103 2.668 × 103 3.304 × 10−18 1.093 × 10−18 3.304 × 10−18 
f21(xi) 2.239 × 103 2.564 × 103 2.549 × 103 2.671 × 103 3.282 × 10−18 2.409 × 10−17 1.378 × 10−20 
f22(xi) 2.293 × 103 5.081 × 103 5.468 × 103 4.262 × 103 3.304 × 10−18 1.602 × 10−18 3.304 × 10−18 
f23(xi) 2.614 × 103 3.286 × 103 4.524 × 103 3.452 × 103 3.304 × 10−18 2.675 × 10−18 3.304 × 10−18 
f24(xi) 2.648 × 103 3.220 × 103 3.356 × 103 3.058 × 103 3.301 × 10−18 4.840 × 10−19 3.789 × 10−16 
f25(xi) 2.881 × 103 2.478 × 104 3.837 × 104 4.136 × 103 3.160 × 10−18 2.463 × 10−18 3.160 × 10−18 
f26(xi) 2.838 × 103 7.302 × 103 7.289 × 103 5.083 × 103 2.636 × 10−18 2.330 × 10−18 2.636 × 10−18 
f27(xi) 3.089 × 103 1.051 × 104 1.847 × 104 3.949 × 103 3.293 × 10−18 4.4716 × 10−16 3.293 × 10−18 
f28(xi) 3.109 × 103 3.714 × 103 3.423 × 103 4.172 × 103 2.602 × 10−18 3.203 × 10−17 1.018 × 10−20 
f29(xi) 3.171 × 103 7.190 × 106 7.6356 × 107 7.919 × 103 3.304 × 10−18 2.471 × 10−18 3.304 × 10−18 
Table 6. Comparison with the average ﬁtness function on 30D optimization among cBA, cPSO, and cABC algorithms on 28 test functions. The overall performance of each of the other algorithms was measured at a signiﬁcant level α = 0.05 under the Wilcoxon’s signed rank test. 
D = 30 Mean Wilcoxon’s Sign Rank Test 
Functions icCS cBA cPSO cABC cBA cPSO cABC 
f1(xi) 1.293 × 102 3.695 × 1011 4.358 × 1011 7.356 × 1010 3.304 × 10−18 1.602 × 10−18 3.304 × 10−18 
f3(xi) 3.107 × 104 1.374 × 1016 3.298 × 1016 9.454 × 104 3.304 × 10−18 2.242 × 10−18 3.304 × 10−18 
f4(xi) 4.608 × 102 1.451 × 105 1.302 × 105 2.740 × 104 3.304 × 10−18 2.471 × 10−18 3.304 × 10−18 
f5(xi) 6.719 × 102 2.119 × 103 2.082 × 103 1.021 × 103 3.304 × 10−18 1.472 × 10−18 3.304 × 10−18 
f6(xi) 6.432 × 102 7.472 × 102 8.101 × 102 7.132 × 102 3.304 × 10−18 1.342 × 10−18 3.304 × 10−18 
f7(xi) 9.618 × 102 9.068 × 103 7.007 × 103 1.495 × 103 3.304 × 10−18 2.120 × 10−18 3.304 × 10−18 
f8(xi) 9.354 × 102 1.811 × 103 1.908 × 103 1.222 × 103 3.304 × 10−18 1.865 × 10−18 3.304 × 10−18 
f9(xi) 5.221 × 103 2.989 × 104 1.312 × 105 1.420 × 104 8.947 × 10−18 1.734 × 10−18 3.304 × 10−18 
f10(xi) 4.784 × 103 6.173 × 103 1.259 × 104 9.609 × 103 6.339 × 10−15 3.105 × 10−18 3.304 × 10−18 
f11(xi) 1.224 × 103 6.970 × 1010 2.933 × 1011 2.072 × 105 3.304 × 10−18 1.865 × 10−18 3.304 × 10−18 
f12(xi) 9.720 × 104 8.191 × 1010 8.116 × 1010 2.227 × 1010 3.304 × 10−18 2.120 × 10−18 3.304 × 10−18 
f13(xi) 1.844 × 103 3.858 × 1010 6.393 × 1010 3.093 × 1010 3.304 × 10−18 1.472 × 10−18 3.304 × 10−18 
f14(xi) 1.481 × 103 3.739 × 109 1.643 × 1010 1.273 × 108 3.304 × 10−18 9.752 × 10−19 3.304 × 10−18 
f15(xi) 1.612 × 103 7.039 × 1010 7.402 × 1010 2.946 × 109 3.304 × 10−18 2.242 × 10−18 3.304 × 10−18 
f16(xi) 2.607 × 103 1.840 × 105 3.616 × 105 1.264 × 104 3.304 × 10−18 2.471 × 10−18 3.304 × 10−18 
f17(xi) 2.041 × 103 5.641 × 107 6.773 × 108 5.782 × 104 3.304 × 10−18 1.865 × 10−18 3.304 × 10−18 
f18(xi) 8.327 × 103 1.101 × 1010 2.532 × 1010 9.179 × 108 3.304 × 10−18 2.675 × 10−18 3.304 × 10−18 
f19(xi) 1.942 × 103 6.498 × 1010 7.511 × 1010 3.356 × 109 3.304 × 10−18 2.471 × 10−18 3.304 × 10−18 
f20(xi) 2.426 × 103 3.790 × 103 4.970 × 103 3.810 × 103 3.304 × 10−18 3.244 × 10−18 3.304 × 10−18 
f21(xi) 2.446 × 103 3.292 × 103 3.318 × 103 2.950 × 103 3.304 × 10−18 2.471 × 10−18 3.304 × 10−18 
f22(xi) 5.032 × 103 7.343 × 103 1.321 × 104 1.140 × 104 1.598 × 10−17 5.211 × 10−18 3.304 × 10−18 
f23(xi) 2.820 × 103 4.607 × 103 5.456 × 103 5.814 × 103 3.304 × 10−18 1.865 × 10−18 1.391 × 10−20 
f24(xi) 2.982 × 103 5.261 × 103 5.596 × 103 4.585 × 103 3.304 × 10−18 3.304 × 10−18 3.304 × 10−18 
f25(xi) 2.888 × 103 7.592 × 104 7.524 × 104 7.323 × 103 3.304 × 10−18 2.359 × 10−18 3.304 × 10−18 
f26(xi) 4.279 × 103 4.351 × 104 5.076 × 104 1.380 × 104 3.304 × 10−18 3.301 × 10−18 3.304 × 10−18 
f27(xi) 3.224 × 103 7.648 × 103 8.023 × 103 7.428 × 103 3.304 × 10−18 1.109 × 10−7 3.304 × 10−18 
f28(xi) 3.180 × 103 1.675 × 104 1.094 × 104 8.858 × 103 3.304 × 10−18 3.304 × 10−18 3.304 × 10−18 
f29(xi) 3.891 × 103 2.317 × 107 5.965 × 108 4.799 × 104 3.304 × 10−18 1.865 × 10−18 3.304 × 10−18 
4.3. Convergence Evaluation 
The optimal value obtained by the algorithm cannot completely deﬁne the validity of the algorithm’s search principle. It is also necessary to evaluate the convergence of the algorithm to measure the speed of the algorithm reaching the optimal value. In this study, two unimodal functions (f1, f3), two simple multimodal functions (f6, f10), two hybrid functions (f12, f13), and two composition functions (f22, f28) were selected as the test functions to compare the convergence of icCS and other common classic algorithms for evaluation of convergence. The comparison of the convergence performance of the icCS algorithm proposed in this paper and other common classical algorithms is shown in Figures 3 and 4. 
(a) f1 (b) f3 
(c) f6 (d) f10 
Figure 3. Convergence test results for functions f1,f3,f6 ,and f10 in 30 dimensions: (a) f1; (b) f3; (c) f6; and (d) f10. 
Because the algorithm proposed in this paper combines compactand populationbased technologies, the overall complexity of the algorithm is higher than that of algorithms using only compact. Based on the introduction of the algorithm in Section 3, the proposed icCS algorithm is divided into two phases. The ﬁrst stage uses compact technology. To increase the global search capability at this stage, a step of uniform sampling is added in this paper. Sn additional uniform distribution sampling is performed on the basis of compact using normal distribution sampling. In this way, a new solution is generated during each iteration and two new solutions are generated, which increases the global search capability and also increases the complexity of the algorithm. At the same time, because the compact method generates fewer solutions, as shown in Figures 3 and 4, the early convergence speed of the proposed algorithm is slow. After the algorithm switches to the second stage, the algorithm uses a populationbased method for enhanced search in order to jump out of the local optimum. The algorithm complexity at this time is the same as the original populationbased algorithm. In addition, during the execution of the ﬁrst 
phase of the algorithm, it is necessary to prepare for switching to the second phase. The key solutions in the ﬁrst phase need to be saved, and it is necessary to judge whether to switch to the second phase continuously. Therefore, the overall complexity of the algorithm is similar to or slightly higher than the original algorithm. 
(a) f12 (b) f13 
(c) f22 (d) f28 
Figure 4. Convergence test results for functions f12, f13, f22, and f28 in 30 dimensions: (a) f12; (b) f13; (c) f22; and (d) f28. 
5. Application to Drone Logistics Hub Location 
The location of the drone logistics hub is brieﬂy introduced in Section 2.2, which establishes a simple model based on three inﬂuencing factors, and then determines the ﬁtness function of the model. In this section, the proposed algorithm is applied to the model for testing. In fact, there are many studies on the way of drone logistics. The endurance time and load capacity of the drone itself also limit the development of drone logistics. However, there are studies on the innovative design of drone applications in the logistics industry [25]. With the development of technology, the restrictions on drone logistics due to the lack of performance of the drone itself will be gradually resolved. Thus, this paper focuses on the logistics issues in rural areas and remote mountain areas, and uses a model of logistics hubs and drones. A certain number of unmanned logistics hubs is used to provide logistics distribution services for surrounding villages and drones complete the end logistics tasks. According to the above model and the introduction in Section 2.2, this article considers three factors that affect the location of the logistics center: the distance from the village to its logistics hub, the rural population, and the degree of transportation difﬁculty from the logistics hub to the village. The signiﬁcance of the ﬁrst factor is that the total distance between the logistics hub and the villages it serves is the smallest, and its operating costs are also smaller. The signiﬁcance of the second factor is that, if the rural population is larger, the frequency of services required is higher, thus the proportion of the village in the whole is higher. Then, the logistics hub needs to be closer to the village, 
thus the cost is lower and the logistics efﬁciency will be higher. The signiﬁcance of the third factor lies in the advantage of the drone’s straight ﬂight. Traditional land logistics methods need to consider terrain factors, and roads are not straight, thus logistics in remote areas or mountain regions is more difﬁcult. By adding this factor, logistics hubs can provide better logistics services to areas with difﬁcult transportation. Based on the above and the introduction in Section 2.2, the objective function used in this paper is written as 
min F = ∑ k=1,k∈N (Hk − Rk) · cpk · ( L1k 
L2k ) (9) 
where k is the kth village, cpk is the population of villages k, N is the total number of villages, Rk is the radius of the village, and Hk is the straight line distance from the village to the nearest logistics hub to the village. The meaning of L2k is the same as Hk, and L1k is the land transportation distance from the current village to the nearest logistics hub. The goal of the intelligent algorithm is to ﬁnd the best logistics hub location so that the objective function is the smallest overall. That is, Hk is minimized in Equation (10) by an intelligent algorithm to achieve the overall minimum, where dj is the position of the drone logistics hub in a certain dimension. 
min Hk = 
� � � � 
∑2 j=1 (xj − dj)2 (10) 
A program was used to generate the original test data based on the proposed drone logistics hub location model. Thirty random village locations were generated in a twodimensional space of 50,000 m × 50,000 m. The radii of the villages were 200–900 m and their populations were 300–3000. The degree of trafﬁc difﬁculty was the land transportation distance divided by the drone’s straight ﬂight distance, which ranged from 1 to 3. Two generated models were used for testing. Table 7 shows the results of testing using Model 1. N is the number of logistics hubs for 30 villages. Each test was executed 50 times and the number of iterations was 3000. Table 8 shows the data results of the test using Model 2. N is the number of logistics hubs for 30 villages. Each test was performed 30 times and the number of iterations was 5000. 
Table 7. The results of the algorithm proposed in this paper and other algorithms on the location of the drone logistics hub in Model 1. 
N icCS PSO SCA DE 
5 5.785676 × 108 6.236103 × 108 7.100692 × 108 5.806862 × 108 
6 4.600239 × 108 5.084005 × 108 6.316990 × 108 4.675162 × 108 
7 3.784290 × 108 4.169794 × 108 5.960870 × 108 3.960494 × 108 
8 3.166692 × 108 3.559690 × 108 5.403769 × 108 3.376844 × 108 
9 2.581782 × 108 2.918791 × 108 5.234646 × 108 2.901538 × 108 
10 2.138288 × 108 2.376954 × 108 5.135606 × 108 2.624356 × 108 
Table 8. The results of the algorithm proposed in this paper and other algorithms on the location of the drone logistics hub in Model 2. 
N icCS PSO SCA DE 
5 6.217989 × 108 6.378356 × 108 7.707247 × 108 6.229052 × 108 
6 5.245071 × 108 5.475654 × 108 6.768210 × 108 5.313589 × 108 
7 4.296633 × 108 4.315215 × 108 6.820193 × 108 4.402420 × 108 
8 3.370972 × 108 3.631537 × 108 6.819135 × 108 3.623817 × 108 
9 2.546246 × 108 3.078089 × 108 6.418266 × 108 2.953324 × 108 
10 2.035772 × 108 2.984507 × 108 6.218386 × 108 2.722444 × 108 
Figure 5 shows the results of running different models using different algorithms, where circles represent the village location, squares represent the calculated logistics hub location, and circles of different sizes represent villages with different radii. According to the execution result data, setting more logistics hubs can obtain smaller ﬁtness function values. However, the construction of the logistics hub itself also requires costs. The larger is the number of logistics hubs, the higher is the overall cost, and the more dispersed are the goods, thus it is necessary to set an appropriate number of logistics hubs based on actual needs. It can be seen in Figure 5 that the location of some logistics hubs has been transferred to the village area after calculation, which means that the cost of logistics hubs will also be reduced, thus they can be selected based on actual conditions. 
(a) 5 logistics hubs, Model 1 (b) 10 logistics hubs, Model 1 
(c) 6 logistics hubs, Model 2 (d) 9 logistics hubs, Model 2 
Figure 5. Execution results of drone logistics hub location problem under different models and different numbers of hubs: (a) ﬁve logistics hubs, Model 1; (b) ten logistics hubs, Model 1; (c) six logistics hubs, Model 2; and (d) nine logistics hubs, Model 2. 
6. Conclusions and Discussion 
Drone logistics will play an increasingly important role in the logistics industry with the increase in the degree of automation of the supply chain. This paper presents a simple location model for a drone logistics hub. This model considers three factors that affect location selection and determines the ﬁtness function of the model. This paper is based on the original cuckoo search algorithm, which is improved by compact and other technology, and proposes the icCS algorithm. On the basis of sampling using the normal distribution, uniform distribution sampling and optimal solution perturbation are added, and, for the problem that is easy to fall into a local optimum, the global search ability is improved by increasing the number of generated solutions. Then, this paper uses the proposed algorithm to calculate the location for drone logistics hub. Compared with other algorithms, it can get better execution results. However, the model proposed in this paper is still inadequate, the inﬂuencing factors included are not comprehensive enough, and further improvements can be made, such as adding logistics hub cost, topographical inﬂuence, path planning between logistics hubs, and communication and control 
between logistics hub and drone. The proposed approach may be further improved by adopting some intelligent and efﬁcient algorithms. 
Author Contributions: Conceptualization, P.-C.S. and J.-S.P.; formal analysis, J.-S.P., P.-C.S., and S.-C.C.; Methodology, J.-S.P., P.-C.S., S.-C.C., and Y.-J.P.; validation, J.-S.P.; writing—original draft preparation, P.-C.S.; and writing—review and editing, J.-S.P., P.-C.S., S.-C.C., and Y.-J.P. All authors have read and agreed to the published version of the manuscript. 
Funding: This research received no external funding. 
Conﬂicts of Interest: The authors declare no conﬂict of interest. 
References 
1. BoussaïD, I.; Lepagnot, J.; Siarry, P. A survey on optimization metaheuristics. Inf. Sci. 2013, 237, 82–117. [CrossRef] 
2. Pan, J.S.; Hu, P.; Chu, S.C. Novel Parallel Heterogeneous MetaHeuristic and Its Communication Strategies for the Prediction of Wind Power. Processes 2019, 7, 845. [CrossRef] 
3. Du, Z.G.; Pan, J.S.; Chu, S.C.; Luo, H.J.; Hu, P. QuasiAfﬁne Transformation Evolutionary Algorithm With Communication Schemes for Application of RSSI in Wireless Sensor Networks. IEEE Access 2020, 8, 8583–8594. [CrossRef] 
4. Wang, J.; Ju, C.; Gao, Y.; Sangaiah, A.K.; Kim, G.J. A PSO based energy efﬁcient coverage control algorithm for wireless sensor networks. Comput. Mater. Contin. 2018, 56, 433–446. 
5. Yang, X.S.; Deb, S. Cuckoo Search via Lévy ﬂights. In Proceedings of the IEEE 2009 World Congress on Nature Biologically Inspired Computing (NaBIC), Coimbatore, India, 9–11 December 2009; pp. 210–214. [CrossRef] 
6. Yang, X.S.; Deb, S. Engineering Optimisation by Cuckoo Search. arXiv 2010, arXiv:1005.2908. 
7. Gandomi, A.H.; Yang, X.S.; Alavi, A.H. Cuckoo search algorithm: A metaheuristic approach to solve structural optimization problems. Eng. Comput. 2013, 29, 17–35. [CrossRef] 
8. Walton, S.; Hassan, O.; Morgan, K.; Brown, M. Modiﬁed cuckoo search: A new gradient free optimisation algorithm. Chaos Sol. Fract. 2011, 44, 710–718. [CrossRef] 
9. Rodrigues, D.; Pereira, L.A.; Almeida, T.; Papa, J.P.; Souza, A.; Ramos, C.C.; Yang, X.S. BCS: A binary cuckoo search algorithm for feature selection. In Proceedings of the IEEE 2013 IEEE International Symposium on Circuits and Systems (ISCAS), Beijing, China, 19–23 May 2013; pp. 465–468. 
10. Yang, X.S.; Deb, S. Multiobjective cuckoo search for design optimization. Comput. Oper. Res. 2013, 40, 1616–1624. [CrossRef] 
11. Wang, G.G.; Deb, S.; Gandomi, A.H.; Zhang, Z.; Alavi, A.H. Chaotic cuckoo search. Soft Comput. 2016, 20, 3349–3362. [CrossRef] 
12. Neri, F.; Mininno, E.; Iacca, G. Compact Particle Swarm Optimization. Inf. Sci. 2013, 239, 96–121. [CrossRef] 
13. Harik, G.R.; Lobo, F.G.; Goldberg, D.E. The compact genetic algorithm. IEEE Trans. Evol. Comput. 1999, 3, 287–297. [CrossRef] 
14. Mininno, E.; Neri, F.; Cupertino, F.; Naso, D. Compact differential evolution. IEEE Trans. Evol. Comput. 2010, 15, 32–54. [CrossRef] 
15. Nguyen, T.T.; Pan, J.S.; Dao, T.K. A Compact Bat Algorithm for Unequal Clustering in Wireless Sensor Networks. Appl. Sci. 2019, 9, 1973. [CrossRef] 
16. Xue, X.; Pan, J.S. A Compact CoEvolutionary Algorithm for sensor ontology metamatching. Knowl. Inf. Syst. 2018, 56, 335–353. [CrossRef] 
17. Nguyen, T.T.; Pan, J.S.; Dao, T.K. An Improved Flower Pollination Algorithm for Optimizing Layouts of Nodes in Wireless Sensor Network. IEEE Access 2019, 7, 75985–75998. [CrossRef] 
18. Tian, A.Q.; Chu, S.C.; Pan, J.S.; Cui, H.; Zheng, W.M. A Compact PigeonInspired Optimization for Maximum ShortTerm Generation Mode in Cascade Hydroelectric Power Station. Sustainability 2020, 12, 767. [CrossRef] 
19. Yildirim, Z.G. New Approaches in Supply Chains: A Research on the Use of Drone Technology in Logistics. J. Strateg. Res. Soc. Sci. 2016, 2, 133–142. 
20. Rabta, B.; Wankmüller, C.; Reiner, G. A drone ﬂeet model for lastmile distribution in disaster relief operations. Int. J. Disaster Risk Reduct. 2018, 28, 107–112. [CrossRef] 
21. Scott, J.; Scott, C. Drone delivery models for healthcare. Proc. Int. Conf. Syst. Sci. 2017, 3297–3304. [CrossRef] 
22. Amukele, T.; Ness, P.M.; Tobian, A.A.R.; Boyd, J.; Street, J. Drone transportation of blood products. Transfusion 2017, 57, 582–588. [CrossRef] 
23. Ling, G.; Draghic, N. Aerial drones for blood delivery. Transfusion 2019, 59, 1608–1611. [CrossRef] [PubMed] 
24. Amukele, T.K.; Hernandez, J.; Snozek, C.L.; Wyatt, R.G.; Douglas, M.; Amini, R.; Street, J. Drone transport of chemistry and hematology samples over long distances. Am. J. Clin. Pathol. 2017, 148, 427–435. [CrossRef] [PubMed] 
25. Kornatowski, P.M.; Bhaskaran, A.; Heitz, G.M.; Mintchev, S.; Floreano, D. Lastcentimeter personal drone delivery: Field deployment and user interaction. IEEE Robot. Autom. Lett. 2018, 3, 3813–3820. [CrossRef] 
26. Hartjes, S.; van Hellenberg Hubar, M.E.G.; Visser, H.G. Multiplephase trajectory optimization for formation ﬂight in civil aviation. CEAS Aeronaut. J. 2019, 10, 453–462. [CrossRef] 
27. Murray, C.C.; Chu, A.G. The ﬂying sidekick traveling salesman problem: Optimization of droneassisted parcel delivery. Transp. Res. Part C Emerg. Technol. 2015, 54, 86–109. [CrossRef] 
28. Carlsson, J.G.; Song, S. Coordinated logistics with a truck and a drone. Manag. Sci. 2018, 64, 4052–4069. [CrossRef] 
29. Wang, D.; Hu, P.; Du, J.; Zhou, P.; Deng, T.; Hu, M. Routing and scheduling for hybrid truckdrone collaborative parcel delivery with independent and truckcarried drones. IEEE Internet Things J. 2019, 6, 10483–10495. [CrossRef] 
30. Agatz, N.; Bouman, P.; Schmidt, M. Optimization approaches for the traveling salesman problem with drone. Transp. Sci. 2018, 52, 965–981. [CrossRef] 
31. Ha, Q.M.; Deville, Y.; Pham, Q.D.; Hà, M.H. A hybrid genetic algorithm for the traveling salesman problem with drone. J. Heurist. 2019, 26, 219–2479. [CrossRef] 
32. Imani, M.; Ghoreishi, S.F. Bayesian optimization objectivebased experimental design. In Proceedings of the IEEE 2020 American Control Conference (ACC 2020), Denver, CO, USA, 3 July 2020. 
33. Ghoreishi, S.F.; Imani, M. Bayesian optimization for efﬁcient design of uncertain coupled multidisciplinary systems. In Proceedings of the IEEE 2020 American Control Conference (ACC 2020), Denver, CO, USA, 3 July 2020. 
34. Kim, S.; Moon, I. Traveling salesman problem with a drone station. IEEE Trans. Syst. Man Cybern. Syst. 2018, 49, 42–52. [CrossRef] 
35. Sudbury, A.W.; Hutchinson, E.B. A cost analysis of amazon prime air (drone delivery). J. Econ. Educ. 2016, 16, 1–12. 
36. Park, J.; Kim, S.; Suh, K. A comparative analysis of the environmental beneﬁts of dronebased delivery services in urban and rural areas. Sustainability 2018, 10, 888. [CrossRef] 
37. Hu, H.; Wu, Y.; Xu, J.; Sun, Q. Cuckoo searchbased method for trajectory planning of quadrotor in an urban environment. Proc. Inst. Mech. Eng. Part G J. Aerosp. Eng. 2019, 233, 4571–4582. [CrossRef] 
38. Yang, X.S.; Deb, S. Cuckoo search: Recent advances and applications. Neural Comput. Appl. 2014, 24, 169–174. [CrossRef] 
39. Clerc, M.; Kennedy, J. The particle swarmexplosion, stability, and convergence in a multidimensional complex space. IEEE Trans. Evol. Comput. 2002, 6, 58–73. [CrossRef] 
40. Jiang, M.; Luo, Y.P.; Yang, S.Y. Stochastic convergence analysis and parameter selection of the standard particle swarm optimization algorithm. Inf. Process. Lett. 2007, 102, 8–16. [CrossRef] 
41. Wang, F.; He, X.; Wang, Y.; Yang, S. Markov model and convergence analysis based on cuckoo search algorithm. Comput. Eng. 2012, 38, 180–185. 
42. Larrañaga, P.; Lozano, J.A. Estimation of Distribution Algorithms: A New Tool For Evolutionary Computation; Springer Science & Business Media: Berlin/Heidelberg, Germany, 2001; Volume 2. 
43. Chu, S.C.; Xue, X.; Pan, J.S.; Wu, X. Optimizing ontology alignment in vector space. J. Internet Technol. 2020, 21, 15–22. 
44. Bronshtein, I.N.; Semendyayev, K.A. Handbook of Mathematics; Springer Science & Business Media: Berlin/Heidelberg, Germany, 2013. 
45. Awad, N.; Ali, M.; Liang, J.; Qu, B.; Suganthan, P. Problem Deﬁnitions and Evaluation Criteria for the Cec 2017 Special Session and Competition on Single Objective RealParameter; Nanyang Technological University, Singapore, Tech. Rep.: Nanyang, China, 2016. 
46. Naik, M.; Nath, M.R.; Wunnava, A.; Sahany, S.; Panda, R. A new adaptive Cuckoo search algorithm. In Proceedings of the IEEE 2015 IEEE 2nd International Conference on Recent Trends in Information Systems (ReTIS), Kolkata, India, 9–11 July 2015; pp. 1–5. 
47. Kennedy, J.; Eberhart, R. Particle swarm optimization. In Proceedings of the IEEE ICNN’95-International Conference on Neural Networks, Perth, WA, Australia, 27 November–1 December 1995; Volume 4, pp. 1942–1948. 
48. Price, K.; Storn, R.M.; Lampinen, J.A. Differential Evolution: A Practical Approach to Global Optimization; Springer Science & Business Media: Berlin/Heidelberg, Germany, 2006. 
49. Mirjalili, S. SCA: A Sine Cosine Algorithm for solving optimization problems. Knowl.-Based Syst. 2016, 96, 120–133. [CrossRef] 
50. Woolson, R.F. Wilcoxon SignedRank Test. Wiley Encycl. Clin. Trials 2008, 1–3. [CrossRef] 
51. Dao, T.K.; Pan, T.S.; Nguyen, T.T.; Chu, S.C. A Compact Articial Bee Colony Optimization for Topology Control Scheme in Wireless Sensor Networks. J. Netw. Intell. 2015, 6, 297–310. 
c⃝ 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/). 
",Improved_Compact_Cuckoo_Search.pdf,9
"Drone based 3D Area Localization 
Master Thesis 
Authors: 
Beda Berner 
Rahul Ravichandran 
3rd June 2021 
Attributions 
This report was typeset using LATEX 
Department of Electronic Systems Robotics 
Fredrik Bajers Vej 7 9000 Aalborg http://www.es.aau.dk 
Title: 
Drone based 3D Area 
Localization Project Period: 
Spring 2021 
Participants: 
Beda Berner Rahul Ravichandran 
Supervisor: 
Anders la Courharbo 
Number of Pages: 82 Date of completion: 03-06-2021 
In recent times, drones are used in a wide range of applications. Many of these, such as wildﬁre or ﬂood monitoring, include the classiﬁcation of certain areas. Delivering drone images with these speciﬁc areas highlighted can provide a lot of information but leaves the association between the 2D image and the 3d world to the user. It would therefore be preferable to have a method that connects the information gained from the drone images to 3d positions. The authors were confronted with this challenge during their internship at Robotto, where they developed an algorithm to autonomously detect and map wildﬁre. During this internship, a simple projection from the drone image to an assumed ground plane was used to map the found ﬁres. The accuracy of this method proved to be unsatisfactory. The aim of this master thesis is therefore to develop an improved method. For this purpose, an algorithm was developed that characterizes an area which is visible in many subsequent drone images as a 3d point cloud. It accomplishes this by tracking landmarks around the border of the target area and using Kalman ﬁltering to ﬁnd the 3d position of the individual landmarks. This algorithm was tested using state of the art simulation and showed promising results. It digests images subsequently and starts providing results as soon as possible. This means that, once optimized, it could run on the ﬂy and provide input to the path planning of a drone. This diﬀerentiates it from alternatives, such as photogrammetry, that require the whole dataset to start processing. 
Beda Berner Rahul Ravichandran 
iv 
List of Figures vii 
List of Tables x 
1 Introduction 1 
2 Problem Analysis 3 
2.1 Previous method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 
2.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 
3 Problem Formulation 10 
3.1 Research and Development Objectives . . . . . . . . . . . . . . . . 10 
3.2 Requirements speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . 11 
3.3 Delimitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 
4 Methodology 12 
4.1 Proposed Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 
4.2 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 
5 Implementation 30 
5.1 Data structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 
5.2 Process new Image . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 
5.3 Add Landmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 
v 
5.5 Purge Landmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 
5.6 Challenges encountered . . . . . . . . . . . . . . . . . . . . . . . . . 48 
6 Analysis 51 
6.1 Initial results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 
6.2 Outlier analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 
6.3 Variation of drone speed . . . . . . . . . . . . . . . . . . . . . . . . 58 
6.4 Variation of convergence threshold . . . . . . . . . . . . . . . . . . . 59 
6.5 Variation of AoI band width . . . . . . . . . . . . . . . . . . . . . . 60 
6.6 Variation of Mahalanobis distance . . . . . . . . . . . . . . . . . . . 61 
6.7 Variation of downscaling factor . . . . . . . . . . . . . . . . . . . . 62 
6.8 Best performing parameters . . . . . . . . . . . . . . . . . . . . . . 63 
6.9 Cliﬀ mission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 
6.10 Inﬂuence of static errors . . . . . . . . . . . . . . . . . . . . . . . . 67 
6.11 Previous method test . . . . . . . . . . . . . . . . . . . . . . . . . . 70 
7 Discussion 75 
8 Conclusion 78 
9 Future Works 79 
Bibliography 80 
vi 
2.1 Drone image of wildﬁre . . . . . . . . . . . . . . . . . . . . . . . . . 3 
2.2 Structure from Motion . . . . . . . . . . . . . . . . . . . . . . . . . 4 
2.3 Pin hole camera model . . . . . . . . . . . . . . . . . . . . . . . . . 5 
2.4 Limitation of the previous method . . . . . . . . . . . . . . . . . . . 8 
4.1 3D area localization process . . . . . . . . . . . . . . . . . . . . . . 12 
4.2 Landmarks found in the area of interest . . . . . . . . . . . . . . . . 13 
4.3 Kalman Estimation process . . . . . . . . . . . . . . . . . . . . . . 14 
4.4 Comparison of EKF and UKF . . . . . . . . . . . . . . . . . . . . . 15 
4.5 Visualization of a descriptor . . . . . . . . . . . . . . . . . . . . . . 20 
4.6 Gazebo and AirSim simulation environments . . . . . . . . . . . . . 22 
4.7 Data ﬂow diagram between simulation and codebase . . . . . . . . . 23 
4.8 Coordinate Frame . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 
4.9 ROS Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 
4.10 Coastline in Unreal Editor . . . . . . . . . . . . . . . . . . . . . . . 26 
4.11 Coastline ground truth . . . . . . . . . . . . . . . . . . . . . . . . . 27 
4.12 Mission plan around lake . . . . . . . . . . . . . . . . . . . . . . . . 28 
4.13 Cliﬀ in Unreal Editor . . . . . . . . . . . . . . . . . . . . . . . . . . 29 
4.14 Mission plan around cliﬀ . . . . . . . . . . . . . . . . . . . . . . . . 29 
5.1 Workﬂow of 3D area localization . . . . . . . . . . . . . . . . . . . . 30 
vii 
5.3 Measurement of drone to target pitch and yaw angles . . . . . . . . 33 
5.4 Process new Image ﬂowchart . . . . . . . . . . . . . . . . . . . . . . 35 
5.5 AirSim Segmented Image . . . . . . . . . . . . . . . . . . . . . . . . 36 
5.6 Dilation operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 
5.7 Process of how a band around the target area is generated . . . . . 38 
5.8 Comparison of feature detection with and without a mask . . . . . 39 
5.9 Flowchart of the Add Landmarks process. . . . . . . . . . . . . . . 40 
5.10 Flowchart of the Match and Estimate process. . . . . . . . . . . . . 41 
5.11 Feature matching procedure in MATCH block . . . . . . . . . . . . 43 
5.12 Calculation of yaw (ψimg) and pitch (θimg) from the pixel coordinates. 44 
5.13 Flowchart of the purging Landmarks process. . . . . . . . . . . . . . 46 
5.14 Unsynchronised Data collection . . . . . . . . . . . . . . . . . . . . 48 
5.15 Eﬀect of optimised synchronisation of data . . . . . . . . . . . . . . 49 
5.16 Synchronisation of data using ROS ApproximateTime ﬁlter . . . . . 49 
6.1 3D estimates of landmarks . . . . . . . . . . . . . . . . . . . . . . . 52 
6.2 Extended Library of Landmarks . . . . . . . . . . . . . . . . . . . . 53 
6.3 Outlier data plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 
6.4 Inliers data plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 
6.5 3D estimates of landmarks with base parameters . . . . . . . . . . . 56 
6.6 Histogram of distances achieved with base parameters . . . . . . . . 57 
6.7 Speed graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 
viii 
6.9 Area of Interest (AoI) band width graph . . . . . . . . . . . . . . . 60 
6.10 Mahalanobis distance graph . . . . . . . . . . . . . . . . . . . . . . 61 
6.11 Image resolution graph . . . . . . . . . . . . . . . . . . . . . . . . . 62 
6.12 3D estimates of landmarks with best performing parameters . . . . 63 
6.13 Histogram of distances achieved with base parameters . . . . . . . . 64 
6.14 Comparison of the histograms. yaxis in log scale. . . . . . . . . . . 65 
6.15 Comparison between initial and best parameters in the cliﬀ mission. 66 
6.16 Yaw oﬀset graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 
6.17 3D plot yaw oﬀset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 
6.18 Pitch oﬀset graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 
6.19 3D plot pitch oﬀset . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 
6.20 Location of ground reference points . . . . . . . . . . . . . . . . . . 71 
6.21 Camera positions at 10 m, 30m and 50m in NED frame. . . . . . . 71 
6.22 Data points generated from diﬀerent camera positions . . . . . . . . 71 
6.23 Prediction of marker locations at diﬀerent camera positions. . . . . 73 
7.1 Selection of AoI aﬀecting the ﬁnal boundary calculation . . . . . . . 77 
ix 
6.1 Baseline conﬁguration for the parameters that will be varied. . . . . 56 
6.2 Values of the parameters that will stay ﬁxed . . . . . . . . . . . . . 57 
6.3 Results achieved with the base parameters . . . . . . . . . . . . . . 57 
6.4 Conﬁguration with the best performing parameters. . . . . . . . . . 63 
6.5 Results achieved with the best performing parameters . . . . . . . . 64 
6.6 Results achieved with the best performing parameters . . . . . . . . 65 
6.7 Markers placed on the ﬁeld. . . . . . . . . . . . . . . . . . . . . . . 72 
6.8 Oﬀset corrections calculated by least squares optimisation. . . . . . 72 
6.9 Mean errors and standard deviation of original calculated data points and the optimised data points . . . . . . . . . . . . . . . . . . . . . 74 
x 
LoL Library of Landmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . viii 
AoI Area of Interest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix 
SfM Structure from Motion . . . . . . . . . . . . . . . . . . . . . . . . . . 4 
DEM Digital Elevation Model . . . . . . . . . . . . . . . . . . . . . . . . . 8 
UAV Unmanned Aerial Vehicle . . . . . . . . . . . . . . . . . . . . . . . . 9 
EKF Extended Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . 14 
UKF Unscented Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . 15 
SIFT Scale Invariant Feature Transform . . . . . . . . . . . . . . . . . . . 18 
ORB Oriented FAST and rotated BRIEF . . . . . . . . . . . . . . . . . . 19 
DoG Diﬀerence of Gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . 19 
SITL Software In the Loop . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 
HITL Hardware In the Loop . . . . . . . . . . . . . . . . . . . . . . . . . . 21 
ROS Robot Operating System . . . . . . . . . . . . . . . . . . . . . . . . . 21 
WGS 84 World Geodetic System 1984 . . . . . . . . . . . . . . . . . . . . 24 
NED NorthEastDown . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 
RANSAC Random Sample Consensus . . . . . . . . . . . . . . . . . . . . 42 
RTK Real Time Kinematic . . . . . . . . . . . . . . . . . . . . . . . . . . 70 
xi 
In this era of automation and aerial robots, some of the main tasks performed by drones are surveillance, mapping of areas and inspection. One of the big challenges in this context is connecting the information that is normally captured as a 2d image with the respective 3D position. While it is useful to see an image where wildﬁre is segmented and highlighted, this leaves the user to actually localize the area according to other features in the picture. Automating this process is not trivial. For realtime localization of single point targets, such as moving cars or persons, Kalman ﬁltering has proven to be a good solution. This is widely researched, especially in the context of dronebased surveillance. For mapping static points or areas, Structure from Motion based photogrammetry can be used to get a very precise localization. This is a very mature process and a wide variety of commercial and open source tools are available. It is however very computationally expensive and needs to be provided with a complete dataset before computation can start. It can therefore not be run in real time on a drone and interact with the path planning. During the authors’ internship at Robotto, they were introduced to the problem of ﬁre area localization and mapping. They developed a simple algorithm which projects the captured images into an assumed ground plane (described in section 2.1). This process is very lightweight and can be run in real time on a drone. It does however, provide unsatisfactory accuracy even when tested on ﬂat ground and would perform terrible in uneven areas. 
This thesis proposes a new method to 3D localize areas. It leverages data from multiple subsequent images to gain information about the 3D position of landmarks that surround the area of interest. This solution was developed for feature sparse areas, but is equally valid for any area which can be detected and segmented. The main idea proposed is to create a band around the target area and detect features inside it. These features are tracked over time and their 3D position is estimated using Kalman ﬁlters. 
This thesis is structured as follows. The theory of the algorithm developed at the internship and the projects done by other researchers in a similar ﬁeld is explained in the Problem Analysis Chapter 2. In Chapter 3 Problem formulation, the major objectives that have to be accomplished throughout the project and the assumptions that were made, are listed. This is followed by Chapter 4 Methodology, in which the theory behind the developed solution is explained. Furthermore, it includes a description on how the simulation environment was chosen. Chapter 5 Implementation describes the software architecture and Chapter 6 Analysis de- 
1 
2 
Drones are used in many applications such as wildﬁre monitoring, agriculture, or surveillance. They provide a topdown view of events on the ground that can greatly improve the user’s situational awareness. For example ﬁg. 2.1 shows aerial images of a wildﬁre. This can be very helpful for ﬁreﬁghters since ﬁre and remaining hot spots are clearly visible, especially in the thermal channel. For this information to be of any use, the user has to know where the image contents are located. If he is very familiar with the area he is working in, he might be able to identify the location based on landmarks in the image he recognizes. Especially in an urban area with a high density of unique buildings, this is a valid option. In settings such as the one displayed in ﬁg. 2.1, this approach is not viable since the landscape is too ambiguous. A method is needed to connect the information displayed in the image with an actual (ideally threedimensional) position. This problem is known as target localization and good solutions exist for individual targets that are easily trackable such as cars or people. There are however use cases, in which the object of interest is not a point target but an area. Examples could be ﬁres bigger than the one displayed in ﬁg. 2.1, ﬁelds, ﬂooded areas etc. In these cases, locating the area or the area borders is essential. 
Figure 2.1: Image of a wildﬁre taken by a drone (DJI Matrice 300 using a Zenmuse H20T). The left side shows the Thermal channel which clearly displays the hot spots. On the right side the color image shows ﬂames and smoke. While ﬁre is clearly visible, locating this ﬁre in the real world only based on this image would be very hard. Picture provided by Robotto. 
3 
Figure 2.2: Structure from Motion (SfM) takes information from overlapping images taken from diﬀernet points of view to create a 3D reconstruction of a subject. [1] 
One of the most popular approaches used to locate areas is Structure from Motion (SfM) based photogrammetry. This process requires a high number of overlapping images of the area of interest. Based on regions that can be seen in multiple images and the movement of the camera between the image captures, a 3D reconstruction of the area can be created. If the original pictures are georeferenced, the reconstruction can be as well. This method is very mature and a wide variety of commercial software is available. It is very accurate and widely used in surveying and agriculture. Since SfM is computationally expensive and needs a high number of images before it can start processing, it is not suited to be run during a drone mission. This means it can not be used to inﬂuence the behaviour of an autonomous drone on the ﬂy. Furthermore, it requires the area to stay consistent during the whole time that is needed to take the images. This makes this method unsuited for applications in dynamic environments such as wildﬁre. 
During their internship at Robotto, the authors were confronted with the requirement for a method that can perform target localization of wildﬁres during the ﬂight to inﬂuence the autonomous behaviour of the drone. They developed a simple method to tackle this problem, which is described in the following section. 
4 
2.1.1 Idea 
Figure 2.3: The pinhole camera model describes the image taking process. It allows the projection of a 3D point into the image plane given extrinsic and intrinsic camera parameters. This process is only reversible, if some information about the 3D point is known. For example if Zw is known. [2] 
The main idea of this approach is to invert the image taking process. The pinhole model [2] is a mathematical model of a camera. It describes how a 3D position is projected onto the image plane (a visual representation can be seen in ﬁg. 2.3). If it could simply be inverted, any pixel of the image could be localized in 3D space. However, as can be seen in ﬁg. 2.3, a pixel position on the image plane deﬁnes a ray of possible 3D positions. Since there is no depth information available, the 3D position associated with the pixel remains ambiguous. An assumption about the position has to be taken to solve the problem. If the world is assumed to be an entirely ﬂat plane at a certain altitude, the intersection between the ray and the ground plane can be found, which will deﬁne the 3D position associated with the pixel. This position will at ﬁrst be in relation to the camera frame. Since the camera’s location and rotation are known, it can be transformed into a local or global reference frame. 
5 
First, the transformations between the local coordinate system and the camera gimbal have to be deﬁned. Vector tL→G deﬁnes the translation from local frame to the gimbal position: 
tL→G = 
 
 Gx Gy Gz 
 
 (2.1) 
The rotation matrix RL→G deﬁnes the rotation from local frame to the current gimbal orientation. It can be separated into the static transformation from local to the gimbal origin pose RL→G0 and the dynamic transformation between gimbal origin and the actual gimbal orientation RG0→G. 
RL→G = RL→G0RG0→G (2.2) 
The rotation from local frame to gimbal origin position is equal to a rotation of 90° around zaxis followed by a rotation of 90° around xaxis: 
RL→G0 = 
 
 cos( π 
2) − sin( π 
2) 0 sin( π 
2) cos( π 
2) 0 0 0 1 
 
 
 
 1 0 0 0 cos( π 
2) sin( π 
2) 0 sin( π 
2) cos( π 
2) 
 
 (2.3) 
Assuming the kinematic chain of the gimbal is yaw->pitch->roll like it is on the zenmuse H20T: RG0→G = RyawRrollRpitch 
Ryaw = 
 
 cos(ψ) 0 sin(ψ) 0 1 0 − sin(ψ) 0 cos(ψ) 
 
 
Rroll = 
 
 cos(φ) − sin(φ) 0 sin(φ) cos(φ) 0 0 0 1 
 
 
Rpitch = 
 
 1 0 0 0 cos(θ) sin(θ) 0 sin(θ) cos(θ) 
 
 
(2.4) 
where ψ,φ and θ are the yaw, roll and pitch angles. 
6 
tG→L = −RG→LtL→G (2.6) 
Using these transformations, the intrinsic matrix of the camera A and the pixel coordinates u,v the local coordinates of the point can be calculated  
 
Xlocal Ylocal Zlocal 1 
 
 = � RG→L | tG→L 
0 0 0 1 
�−1  
 A−1s 
 
 vu 1 
 
 
1 
 
 (2.7) 
where s is a scaling variable which decides where on the ray the point is. As previously described the assumption is taken that Zlocal is known. 
A = 
 
 fx 0 cx 0 fy cy 0 0 1 
 
 (2.8) 
RG→L = 
 
 R11 R12 R13 R21 R22 R23 R31 R32 R33 
 
 (2.9) 
s can then be calculated as: 
s = (R13R22R31 − R12R23R31 − R13R21R32 + R11R23R32 + R12R21R33 − R11R22R33)fxfy(Gz − Zlocal) 
(R22R31 − R21R32)(u − cx)fy + fx(−R12R31v + R11R32v + (R12R31 − R11R32)cy + (R12R21 − R11R22)fy) (2.10) 
Applying s to eq. (2.7) now allows the determination of Xlocal and Ylocal. 
2.1.3 Limitations 
While this method allows the localization of any pixel in a given picture, it is limited by the assumptions taken. In real life applications, the identiﬁed area is often not ﬂat and its altitude is not known. The diﬀerence between the real altitude and the assumed altitude can lead to considerable errors as shown in ﬁg. 2.4. This is especially true, if the gimbal pitch is shallow. The performance of this method has been evaluated as a part of the thesis in Section 6.11. Furthermore, this method only uses the information of one single picture. This has its own advantages and disadvantages. It allows for an instant localization information but also means 
7 
Figure 2.4: Any deviation from the assumed ground plane (light green) will lead to considerable errors in the localization of the point of interest (errors are displayed in dotted red). [3] 
that no information from other pictures is used to minimize noise and improve localization accuracy. Merino et al. [4] use a similar method but get altitude information from a Digital Elevation Model (DEM) and calculate probabilities over multiple pictures to reduce these errors. 
2.1.4 Expectations for the new method 
The improvement of the previous method must overcome most of the limitations listed in the Section 2.1.3. The new method must not assume the location of the area to be in a ﬂat plane. It should be able to calculate the 3D position of the pixel instead of 2D position on a plane. The new method must also use information from multiple views and not just a single image. Although this will not give immediate results, it should provide a much more accurate 3D estimation of the pixel. 
2.1.5 Problems that may arise 
To keep track of a certain pixel on the contour of an area, it must have good features associated with it. These can then be matched between frames to keep track of it. In some cases, such as uniform areas, good consistent features are not available. In these cases, feature matching is not possible and hence it is not possible to keep track of a pixel through multiple frames. 
8 
A review of related literature showed that most of the authors only discuss the issue related to single point 3D target estimation and not an estimation of an area. Only Merino et al. [4] address the issue of 3D area localization. Although they follow a similar strategy to the one done in the internship, they improve the localization signiﬁcantly by using information from multiple drones and digital elevation maps. Moreover, they also generate a probability map of the area location. The drones are equipped with infrared cameras to detect the ﬁre contour without any hindrance from smoke. Each individual Unmanned Aerial Vehicle (UAV) estimates the ﬁre front pixels and ﬁre top pixels and sends it to a central decision system located in the ground control station. The position of the ﬁre pixels changes more slowly at the front compared to the top of the ﬁre. The characterisation of the top of the ﬁre from the front is done by applying a low pass ﬁlter over a sequence of consecutive segmented images. The data from the ﬁre area pixel points are then projected onto the ground plane by multiple drones and is used to create a probability map of ﬁre on the ground plane. The results shown in this paper look promising, however the method used requires additionaly information in form of a Digital Elevation Model and multiple UAVs 
The 3D Area localization can also be viewed as a series of 3D point target localization applied to static features in or around an area. Under this premise work on 3D point target localization will also give a lot of information on improving 3D area localization. A number of papers have discussed the 3D point target localization problem both for stationary[5][6] and moving[7][8] targets. Ponda et al. [9] and Wang et al. [10] discuss optimal trajectories during the drone ﬂight to ensure the fastest convergence. 
Especially interesting is ""Unscented Kalman Filter for VisionBased Target Localisation with a Quadrotor"" by Dena and Aouf [5]. This paper uses Unscented Kalman Filtering to get an estimate of the 3D position of the target using the drone location , camera gimbal angles, and the intrinsics of the camera. The authors also suggest that the estimates observed are prone to bias due to noise in the measurements. They consider the target to be stationary, which is similar to the problem addressed in this thesis proposal. This makes the state matrix an identity matrix which is linear, but the target position is calculated using the gimbal angles of the camera, which makes the measurement model a nonlinear function of elevation and azimuth from the target. Due to this nonlinearity, the unscented Kalman ﬁlter is used. This estimates the nonlinear function better than an extended Kalman ﬁlter using sigma points. 
9 
From the analysis done on the algorithm developed during the internship, it was discovered that the method has its limitations. The most severe one is, it assumes that the area is ﬂat. This leads to inaccurate 3D localization of uneven areas. Even when tested on ﬂat ground, small deviations in the measured gimbal pitch angle lead to big errors (see section 6.11). Therefore, to overcome the limitations of the previous method, it is proposed to use feature tracking and state estimation to provide more accurate results (explained in detail in section 4.1). This leads to the following problem statement: 
""Develop an algorithm which performs 3D area localization based on feature tracking and state estimation"" 
3.1 Research and Development Objectives 
To achieve the set problem statement, some basic goals were set to develop the base of the algorithm. 
1. Choose a suitable simulation environment where feature detection can be performed. 
2. Choose a suitable feature detector and matcher. 
3. Add a target area to the simulation and extract its segmented image output. 
4. Plan a drone mission to ﬂy around the target area to capture images and the necessary meta data. 
5. Develop a feature tracking algorithm which can track individual features. 
6. Develop a state estimation algorithm which is able to estimate the 3D position of the tracked features. 
10 
1. The region around the target area is feature rich. 
2. The segmented image of the target area is available beforehand. 
3.2 Requirements speciﬁcation 
To meet the standards of real world applications, the ﬁnal simulation results must meet the following requirements: 
1. The feature detection algorithm must be able to detect and track static and consistent features throughout the mission. 
2. The average distance of ﬁnal localised features to the ground truth of the boundary of target area, must be within 5m. 
3. The algorithm must be able to perform, even when noise is added to simulate real sensor data. 
4. The developed algorithm must be scalable for diﬀerent camera models, i.e. the algorithm should function equally well for reduced image resolution. 
3.3 Delimitation 
Due to the COVID-19 outbreak and the worldwide lockdown, most of this project has been limited to simulation. Access to the drone was possible only on limited days due to the restrictions active at the time in Denmark. The drone was therefore only used for early tests while later work focused on simulation. 
11 
4.1 Proposed Solution 
The objective of the thesis is to improve upon the 3D area localization that was developed as a part of the internship. As described in Chapter 2, the new method is expected to calculate the 3D area location and should fuse information gathered from multiple frames as seen in ﬁg. 4.1. 
Figure 4.1: The drone views the area from multiple perspectives.This allows the estimation of an accurate 3D location for the area. 
While exploring possible solutions from previous works on related ﬁelds as explained in Section 2.2, only one paper was found that tackles this speciﬁc problem. Merino et al. [4] solved the problem using a very similar approach to the one described in section 2.1. They further improved upon it by incorporating a Digital Elevation Model (DEM) and probability estimation. This relative lack of research motivated the authors of the thesis to propose a new idea for 3D area localization. 
12 
Figure 4.2: Based on the segmented target area (blue) a band around it is created (red). In this band notable landmarks are found (green) and tracked over subsequent images. 
The proposed idea is to localize a detected area by searching for static landmarks around the area of interest. This is accomplished by applying a standard feature detector like SIFT [11] in a band around the target (see ﬁg. 4.2). Tracking these landmarks over time and using state estimation such as Kalman ﬁltering, accurate 3D positions for each landmark can be found. These can then be joined, to generate a good 3D estimate of the area of interest. The use of state estimation to localize single point targets is well documented in literature for both static [5] [6] and moving targets [7] [8]. The nonlinear nature of the bearingonly localization problem requires the use of nonlinear state estimation such as Extended or Unscented Kalman ﬁltering. 
This proposed idea is implemented as a proof of concept in simulation software to evaluate its viability. 
13 
The information gathered about a landmark over multiple frames has to be combined to improve the estimation of its position. This can be solved using a Kalman ﬁlter which is a two step process. First, there is a prediction step where the ﬁnal mean and covariance of the state are predicted using a given model. Secondly, there is an update step where the state is measured using given sensors and their associated uncertainties. Combining predicted and measured state results in a new estimate with higher conﬁdence(see ﬁg. 4.3). 
0 2 4 6 8 10 
0.00 
0.05 
0.10 
0.15 
0.20 
0.25 
0.30 
0.35 
prior state estimate measurement posterior state estimate 
Figure 4.3: Kalman ﬁlter uses previous state estimate and the measurement to get a better estimate of the current state 
While the position of the landmark is stationary and therefore linear, the measurements (yaw and pitch) are nonlinear. Therefore, a nonlinear Kalman ﬁlter has to be used. The most widely used nonlinear Kalman ﬁlter is the Extended Kalman Filter (EKF). It models nonlinear behaviour by linearizing around the current state and estimating a new Gaussian distribution. 
14 
The Unscented Kalman Filter (UKF) [12] improves upon this concept by applying nonlinearity to a low number of particles, the socalled sigma points. These are chosen to represent the original Gaussian distribution. These points are passed through a nonlinear function and the output is then used to generate a closer Gaussian distribution estimate (see ﬁg. 4.4). 
Figure 4.4: The above plots show the propagation of mean and covariance through nonlinear functions using diﬀerent methods. The left plot shows the propagation of all the points of the state (Monte Carlo), the middle plot shows the propagation results after linearising the model (EKF) and the last plot shows propagation of only the sigma points to estimate the new mean and covariance (UKF). [12] 
This socalled unscented transform is performed in a series of steps: 
• Calculate the sigma points 
• Assign weights to the sigma points 
• Transform the points through a nonlinear function 
• Compute the Gaussian distribution from the transformed points 
15 
The sigma points and their weights are calculated based on Van der Merwe’s 2004 dissertation [13]. The sigma points are calculated as follows: 
U = � 
(n + λ)Σ (4.1) 
X 0 = µ X i = µ + Ui for i = 1, ...., n X i = µ − Ui−n for i = n + 1, ...., 2n (4.2) 
Where every X is one sigma point, µ is the mean of the Gaussian distribution, λ is the scaling factor which decides how far from mean the sigma points should be, and Σ is the covariance matrix which is a square matrix of size n. λ can be calculated as follows: λ = α2(n + κ) − n (4.3) 
where α and κ are tuning parameters. α is normally chosen as a small positive value while κ is usually set to zero [12]. 
Weights for the sigma points are chosen such that the sum of all weights is equal to one and more weight is given to the points which are closer to the mean. These weights are calculated as follows: 
W µ 0 = λ 
n + λ 
W Σ 0 = λ 
n + λ + (1 − α2 + β) 
W µ i = W Σ i = 1 
2(n + λ) for i = 1, ...., 2n 
(4.4) 
where β is a tuning parameter which is normally set to 2 [12]. 
The resulting Gaussian after passing through the nonlinear function is then calculated: 
µ′ = 
2n � 
i=0 W µ i g(Xi) 
Σ′ = 
2n � 
i=0 W Σ i g(Xi − µ′)(g(Xi) − µ′)T (4.5) 
Where, µ′ is the predicted mean, Σ′ is the predicted covariance and g : Rn → Rn 
is the state transition function. 
16 
A new state x[k + 1] is predicted based on the old state x[k] using the state transition model: x[k + 1] = Φ(x[k]) + w[k] (4.6) 
Where x[k] is the prior state, x[k + 1] is the current state, Φ(x[k]) is the state transition model between k and k + 1, w[k] is the process noise. 
Since Φ might be nonlinear, unscented transform as described in (4.1) to (4.5) is used to calculate the predicted mean (µ′) and covariance (Σ′) of state x[k + 1], where the function g is replaced with Φ. 
Update 
The measurement z[k] associated with state x[k] can be calculated using the measurement model: z[k] = h(x[k]) + v[k] (4.7) 
Where h : Rn → Rm is the measurement model, v[k] is the measurement noise and m is the dimension of the measurement. 
Since h might be nonlinear, unscented transform as described in (4.1) to (4.5) is used to calculate the mean ˆz and covariance S of the predicted measurement as shown below. Zi = h(Xi) 
ˆz = 
2n � 
i=0 W µ i Zi 
S = V + 
2n � 
i=0 W Σ i (Zi − ˆz)(Zi − ˆz)T 
(4.8) 
Where, Z is the transformed sigma points in the measurement space and V is a diagonal matrix which represents the measurement noise. 
17 
T = 
2n � 
i=0 W Σ i (X i − µ′)(Zi − ˆz)T 
K = T S−1 (4.9) 
Where T is the crosscorrelation matrix between state space and the measurement space, S is the predicted covariance matrix and K is the Kalman gain. The ﬁnal state and covariance are calculated incorporating the actual measurement z as follows: µ = µ′ + K(z − ˆz) 
Σ = Σ′ − KSKT (4.10) 
4.1.2 Feature tracking 
For the described Kalman ﬁlters to work, input in the form of pixel coordinates of the same landmark over several pictures has to be provided. For this purpose, a feature detection algorithm is used to ﬁnd and track notable features over multiple images. Numerous feature extraction algorithms such as SIFT [11], SURF [14], ORB [15] and KAZE [16] exist. They vary widely in computational cost and accuracy depending on the use case. In this application, the feature detection is going to be used to track individual features. For this reason, high conﬁdence in the matching is required since outlier removal with techniques like RANSAC is not possible. Tareen and Saleem [17] compared various feature detectors with multiple datasets. They show that Scale Invariant Feature Transform (SIFT) has generally the best matching performance and it is therefore used in this project. This comes at the cost of relatively high computational requirements. It is therefore important to limit the number of required features as much as possible. 
Feature Extraction 
The process of feature extraction can be split in 2 independent parts: 
First, distinctive points inside the picture have to be found. This usually involves ﬁnding corners or extrema of some sort. The resulting keypoint contains information about the features location and its strength. 
18 
Feature extractors usually consist of both keypoint detection and description. However, since the 2 processes are independent, they can also be mixed. Oriented FAST and rotated BRIEF (ORB) is as the name implies, a mixture of keypoint extractor and descriptor of 2 other algorithms. 
SIFT keypoint detection 
Based on an original greyscale image, Scale Invariant Feature Transform (SIFT) approximates a series of images of the same object taken from diﬀerent ranges, the socalled scale space. This is accomplished by applying increasing levels of Gaussian blur and downscaling. Neighbouring images in the scale space are then subtracted which results in the Diﬀerence of Gaussian (DoG). Like the scale space, the DoG space can be seen as a 3D space in which 2 dimensions are the pixel position of the image and the remaining one is the amount of blur applied (the approximated range the image is taken at). Keypoint candidates are now identiﬁed by ﬁnding local extrema in this 3D DoG space. 
These keypoint candidates are restricted to the pixel grid, which is a problem especially at higher levels of downscaling. Therefore, the position of the keypoints is therefore reﬁned to a subpixel level. This is done by creating a local quadratic model in the space around a given keypoint. The extrema of this local model is then identiﬁed to deﬁne the subpixel position of the keypoint. Unstable keypoints are eliminated by deleting low contrast candidates. This removescandidates with weak local minima which are not very well deﬁned. Keypoints that lie on edges are also purged, since they are normally not distinct. 
SIFT descriptor 
In the ﬁrst step, an orientation for the descriptor has to be generated to give it rotation invariance. This is done by ﬁnding the predominant direction of gradients in the area of the keypoint. First, the size of the to be evaluated area is chosen based on the scale of the keypoint (see green rectangle in.ﬁg. 4.5). The orientation of the gradient of every pixel in this area is then determined and added to a histogram of directions (normally with 36 bins for 360 degrees). The contribution 
19 
Figure 4.5: Visualization of the diﬀerent patches used during the calculation of the descriptor. σ is the scale of the keypoint.The tuning parameters λori and λdescr are parameters are normally set to 1.5 and 6 respectively. [18] 
In a second step, the actual descriptor is calculated. For this purpose, a square patch centred on the keypoint and oriented along the keypoints location is taken. Its size is once again dependant on the scale of the keypoint, but larger than in the previous step (see the red rectangle in ﬁg. 4.5). This patch is then subdivided into 16 subpatches. For every subpatch, the predominant orientation of gradients is calculated. This is done in a similar way to the ﬁrst step, however only 8 bins are used per histogram. The 16 resulting histograms with 8 bins each are then stored as the 128 values long feature vector of the keypoint. This vector characterizes the area around the keypoint and can now easily be compared to other descriptors. 
20 
4.2.1 Environment 
Simulation modelling is almost always used when developing robotic systems. It is used to speed up the overall software development cycle and to reduce the project’s overall cost. It allows to solve realworld problems in a safe and eﬃcient manner and provides the ability to analyse the outcome of an algorithm by simulating various scenarios in a physics simulation environment. The physical interaction between the robot and the environment can be safely analysed before deploying it into the real world. 
This situation is equally true for testing vision related algorithms, where the simulation environment provides access to a simulated camera. To make it more similar to that of a real camera, additional noise and distortion can also be easily introduced. Apart from having the ability to test the algorithm in a simulation , it is also necessary to be able to transfer the algorithm to a real world system with ease without modifying the structure of the algorithm. For this reason autopilot software such as PX4 or ardupilot is used to actually control the drone. They provide both Software In the Loop (SITL) and Hardware In the Loop (HITL) modes to test the solution in simulation before actually implementing it on a real drone. SITL is used to simulate the system entirely on a computer and does not require any hardware. Therefore, it is usually the ﬁrst step in the development process, since the hardware is not available initially. In a later stage, HITL runs the autopilot on the actual ﬂight controller hardware, while physics and control algorithms are still simulated on the computer. 
Both of these methods require a simulation environment which simulates a 3D world and the necessary physics. One of the most popular software to provide such an environment is Gazebo. It oﬀers the ability to accurately simulate the physics of any robotic system and its interaction with the environment. It is compatible with Robot Operating System (ROS) and it also has an active community support which is very helpful in debugging solutions. However, when dealing with vision algorithms, the simulation should not only posses realistic physics but also realistic graphics. One of the main problems in using Gazebo is its low visual ﬁdelity. Focused on physics and good performance, the visual details in the simulation are not very realistic. This provides a challenge when testing computer vision algorithms, because the simulation and the real world are not comparable. The identiﬁcation of features in the simple images provided by the simulation is 
21 
The unsuitability of widely used robot simulators for computer vision tasks has also been recognised by Microsoft. They developed and subsequently released AirSim [19] as an open source project. AirSim is based on Epic Games Unreal Engine 4, which allows for close to photorealistic visualization in real time. Mainly developed to help with the generation of machine learning datasets, AirSim allows the simulation of both quadcopters and cars. The simulation environment can be created using the Unreal Engine Editor which allows the creation of very detailed worlds. Moreover, various premade environments with high visual ﬁdelity are available online. These include realistic lighting and world geometry and should therefore be suitable for computer vision tasks. AirSim interfaces with the most commonly used drone autopilots PX4, Ardupilot and, using a wrapper, with ROS. 
When comparing Gazebo and Airsim visually, as it is illustrated in ﬁg. 4.6, the more realistic representation of the world is clearly visible. AirSim is therefore used as a simulation environment for this project. Building realistic worlds in Unreal Editor is a very complex and time consuming task. Therefore ""Landscape Mountains"" [20] provided by Epic Games is used as a base environment which can then be modiﬁed. Since the ﬂight model of the drone used is not relevant for this thesis, the default drone delivered by AirSim is used. 
(a) Gazebo environment 
(b) Airsim environment 
Figure 4.6: Comparison of Gazebo and AirSim shows the much higher visual ﬁ- delity of AirSim 
22 
ROS 
AirSim 
PX4 
AirSim ROS  
wrapper 
MAVROS 
ROS   
codebase 
Flight commands 
Position  
Images 
Gimbal 
Gimbal angle 
Gimbal target 
Gimbal angle 
Figure 4.7: This ﬁgure shows the data ﬂow between PX4 autopilot, AirSim environment and ROS codebase 
can be seen in ﬁg. 4.7. AirSim provides its own ROS wrapper which provides the camera data and gimbal control, but does not provide access to read gimbal orientation or reposition the gimbal. Hence, a gimbal node is introduced in between. It takes in the angle commands from the algorithm and publishes them to the AirSim ROS wrapper to actuate the gimbal. Furthermore, it publishes the current gimbal angles to a ros topic, simulating a sensor. For position data and ﬂight commands the process is easier since MAVROS can be used to interact between the ROS codebase and the autopilot. 
23 
N 
D 
E 
Local 
Z 
Y 
X 
Lat 
Alt 
Lon 
Gimbal 
GPS 
Figure 4.8: This ﬁgure displays the coordinate reference frame used for 3D area localization 
• GPS coordinate frame: GPS coordinates are handled in the World Geodetic System 1984 (WGS 84) reference frame [21]. In ﬁg. 4.8, the xaxis refers to latitude, the yaxis to longitude and the zaxis to altitude. 
• Local coordinate frame: Coordinate frame centred at the start location of the drone.This is a NorthEastDown (NED) coordinate system resulting in negative z values for positive altitudes. 
• Gimbal coordinate frame: The zero position of the gimbal is looking to the north, parallel to ground. Applying the pinhole camera model [2] results in a coordinate frame that is centred at the drone position with zaxis pointing north, xaxis pointing east and y pointing down. 
24 
To connect diﬀerent parts of the system such as the autopilot and the core algorithm, ROS is used. ROS is a ﬂexible open source framework for writing robotic software. It consists of libraries and tools which simplify the task of creating complex algorithms. ROS also supports communication between diﬀerent language codes, using the concept of nodes, topics, publishers and subscribers as seen in Figure 4.9. Nodes are individual programs which can send and receive data from a topic. Topics act as a middle ground to help exchange information from one node to another. Publishers and Subscribers respectively are the means to send and receive data through a node. ROS has its own data classes that can be used to exchange data with a topic. It also provides the freedom to create custom data classes. Each functionality of a robotic system is made as a ROS node which can send and receive data from topics. For example, in a mobile robot system, each of the sensors (like lidars, encoders, etc.) are individual nodes which publish appropriate data to their respective topics. These topics are then subscribed by the controller node which calculates the required motor input and then publishes the motors commands to another topic. Motor controller is another node which subscribes to these commands and sends it to the appropriate motor device. The advantage of this architecture is that the nodes are completely isolated and are only connected through topics. This allows the use of diﬀerent languages (C++ and python) for diﬀerent nodes based on the ease and the computation speed required. Apart from this, ROS has a vast community support to help debug any issues that may arise. 
ROS Topic B 
Python 
ROS Nodes 
Python 
ROS Nodes 
C++ 
ROS Nodes 
ROS Topic A 
Publishers 
Subscribers 
Figure 4.9: This ﬁgure describes how nodes communicate with each other using publisher and subscribers through topics. 
25 
A 3D area localization mission involves performing state estimation on the detected and matched features from the drone images. While the eventual aim is to perform this algorithm in real time, during the development, datasets where created ﬁrst and then the algorithm was run on these datasets. The mission plan for capturing such a dataset requires the drone to ﬂy around the target area and collect images of its boundaries. Apart from this, the information regarding the gimbal angles and drone position also has to be recorded for every image taken. The ﬁrst mission was planned around a lake in the unreal environment as shown in ﬁg. 4.10. 
Figure 4.10: Overview of the coastline that serves as a target for the tests. Taken in Unreal Editor. 
To achieve this, a few points along this coastline were extracted manually and the polyline they deﬁne is used as ground truth as shown in ﬁg. 4.11. This ground truth line is used in calculating the path to be followed by the drones during the mission. 
26 
Figure 4.11: Manually created ground truth consisting of 37 extracted points and the polyline they deﬁne. 
One of the important criteria for a good convergence of the 3D position of a feature is to view the feature from multiple perspectives. To introduce this variation in perspective, the paths must have a certain oﬀset to the ground truth along x, y and zaxis. 
The ﬁrst mission was performed around a lake, which is mostly ﬂat. The drone follows a set of waypoints that were oﬀset along all 3-axis from the ground truth line, at a constrained maximum velocity. The gimbal is pointed towards the closest waypoint on the ground truth line to capture images at regular intervals as shown in ﬁg. 4.12. This simulates either a computer vision algorithm or a human user focusing on the area border during the mission. 
27 
0 
100 
200 
300 100 
0 
100 
200 
300 
100 
0 
100 
200 
300 
Figure 4.12: The drone follows the path denoted by dashed blue line and focuses the camera onto the boundary of the lake which is denoted by the red line. At every waypoint, the camera is oriented a point on the ground truth line. This orientation is denoted by green arrows. 
The second mission is planned around a cliﬀ, which is more of a steeper terrain as shown in ﬁg. 4.13. Similar to the previous mission, a set of waypoints were calculated around the cliﬀ at an oﬀset. The gimbal is made to focus on the cliﬀ’s boundary, while the drone follows the waypoints around the area as shown in ﬁg. 4.14. Here, the drone’s path is represented by the blue line, the gimbal orientation by green arrows and the ground truth of the cliﬀ’s boundary is represented as a red line. 
28 
Figure 4.13: Overview of the cliﬀ that serves as a target for the tests.The area that should be localized is of slightly darker colour than the surrounding ground. Taken in Unreal Editor. 
250 
225 
200 
175 
150 
125 
100 
75 
50 0 
50 
100 
150 
200 
100 
50 
0 
50 
100 
150 
Figure 4.14: The drone follows the path denoted by dashed blue line and focuses the camera onto the boundary of the lake which is denoted by the red line. At every waypoint, the camera is oriented a point on the ground truth line. This orientation is denoted by green arrows. 
29 
Based on the ideas and principles described above, a working algorithm has been developed. It tracks notable landmarks around the target area over a large number of frames. By applying a Kalman ﬁlter to every single landmark, it eventually gets accurate estimates of numerous points in space around the target area. 
Match and Estimate 
Purge Landmarks 
Add Landmarks 
Process new Image 
Store Estimation 
Figure 5.1: This ﬁgure gives an overview of workﬂow of 3D area localization process. The features area detected in Process new Image and are added to LoL in Add landmarks and are matched with the previous landmarks in Match and Estimate. The worst landmarks are purged in Purge landmarks and the 3D estimate of landmarks are saved in Store Estimation. 
30 
1. Process new Image: Acquires new Image, detects the target area, obtains a mask around it and extracts relevant features. 
2. Match and Estimate: Matches the current features with the landmarks.When a match is found, updates the landmark and its UKF estimation based on this match. 
3. Purge Landmarks: Removes bad or old landmarks. If the to be removed landmarks satisfy the convergence criteria, their ﬁnal estimation is stored. 
4. Add Landmarks: Adds new landmarks to the LoL 
This is an indeﬁnite process and does not have a deﬁned end point. The process can be run during the entire duration of the drone’s ﬂight time or can be stopped at any moment. The exact functionality of each block is further explained in detail in section 5.2 - 5.5. 
5.1 Data structure 
5.1.1 Features 
To keep track of landmarks, their associated features and Kalman ﬁlters, the following data structure is used. The landmarks are assembled in a Library of Landmarks (LoL) which is displayed in ﬁg. 5.2. Each landmark contains all features associated with it. Every feature consists of keypoints and its corresponding descriptors which will eventually be matched to other features. Features are added to a landmark every time it is rediscovered in an image (see section 5.4). To keep track from which image a feature originated, the image index is also stored. This allows the calculation of metrics such as how often a landmark is seen or when it was last seen. 
31 
Landmark 1 
Landmark 2 
Landmark ... 
Descriptors 
Descriptor 1 
Descriptor 2 
... 
Image index 
Image index 1 
Image index 2 
... 
UKF 
ukf object 
Keypoints 
Keypoint 1 
Keypoint 2 
... 
Descriptors 
Descriptor 1 
Descriptor 2 
... 
Image index 
Image index 1 
Image index 2 
... 
UKF 
ukf object 
Keypoints 
Keypoint 1 
Keypoint 2 
... 
Descriptors 
Descriptor 1 
Descriptor 2 
... 
Image index 
Image index 1 
Image index 2 
... 
UKF 
ukf object 
Keypoints 
Keypoint 1 
Keypoint 2 
... 
Figure 5.2: Data structure of the LoL. Every Landmark contains information about the associated features, the frame in which a feature has been found and the current state of the UKF estimation. 
5.1.2 UKF object 
Every landmark also has its own UKF ﬁlter which estimates the 3D position of the landmark in the world. To make all information about the UKF ﬁlter easily accessible, it is stored in a so called UKF object class. Every landmark contains exactly one such object (see ﬁg. 5.2). While every landmark has its distinct ﬁlter, they are all modeled the same. A stationary target is assumed, therefore the state of the target only consists of its position and has dimension n = 3: 
x = 
 
 north east down 
 
 (5.1) 
When applied to the 3D localization of a static target, the state transition model in (4.6) is linear: 
Φ(x[k]) = x[k] (5.2) 
32 
x[k + 1] = x[k] (5.3) 
The measurements taken by the drone consist of the bearing from the drone to the target point expressed as yaw and pitch in the drone frame as shown in ﬁg. 5.3. The measurement space therefore has dimension m = 2. 
Z 
Y 
X 
Target 
Target bearing 
Figure 5.3: The bearing from drone to target can be expressed as yaw angle ψtarget and pitch angle θtarget. The angles are applied in order yaw → pitch. 
z = �ψtarget θtarget 
� (5.4) 
Since the measurement is expressed as the absolute bearing between the drone and the target, h is also dependent on drone position p: 
h(x) = f(x − p) 
f(r) = � arctan(ry/rx) − arctan(rz/�r2 x + r2 y) 
� (5.5) 
33 
V = �(0.5◦)2 0 0 (0.5◦)2 
� (5.6) 
To select the proper sigma points which deﬁne the variance of the state, it is necessary to choose the appropriate tuning parameters as described in (4.1) to (4.5). The tuning parameters chosen after experimentation were: 
α = 0.7 β = 2 κ = 0 (5.7) 
The abovedescribed implementation of the UKF ﬁlter is done with the help of the ﬁlterpy library [22]. 
34 
Process  new Image 
Acquire Image 
Detect Area of 
Interest 
Acquire Mask around the Area of Interest 
Extract N strongest features in masked 
area 
Match and Estimate 
Extracted features 
Add Landmarks 
Figure 5.4: The Process new Image block takes in the image, extracts features around the target area and sends it to Match and Estimate block. 
The algorithm starts by acquiring a new image from the camera. The image is segmented to get a binary mask of the target area. Segmenting an image depends on the type of area to be segmented and can be achieved by using either simple color segmentation or complex machine learning algorithms. The segmentation is an input for the developed algorithm and how it is created does not aﬀect the result. When the algorithm is applied to the simulation setup described in section 4.2, the segmentation is provided by the AirSim. It segments every object by its unreal engine object class (see ﬁg. 5.5). 
35 
Figure 5.5: The AirSim segmented image output assigns a unique color to object classes in the simulation environment. It is not a result of applying image processing techniques. 
The detected area is dilated by performing morphological operations as shown in ﬁg. 5.6. The dilated area is subtracted from the initial segmented area to get a band around the target area as shown in ﬁg. 5.7. This band is the Area of Interest (AoI), in which new features are detected and matched continuously. By selecting a small AoI around the target area, the performance of the feature matching algorithm is improved. This is because, it restricts the detection of features to a limited region as shown in ﬁg. 5.8. Furthermore, the AoI width is a tuning parameter, which determines the amount of features found and how close the 3D position estimates are to the boundary of the target area. Once the mask of the AoI around the target area is calculated, SIFT feature detection algorithm is performed to ﬁnd the best features. In the ﬁrst iteration the number of features detected is Lmax. Here, Lmax deﬁnes the size of landmark library and is user deﬁned. However, in subsequent steps a much larger number of features have to be extracted to match with the existing landmark library. The number of features should be high enough that a majority of the landmarks in the library can be matched and low enough so the matching process can be accomplished in a reasonable time. During the experiments, 10Lmax proved to be a good value. The extracted features are then passed to Match and Estimate for updating the position of the landmarks. The only exception is the case of the ﬁrst iteration.Since the landmark library is 
36 
Figure 5.6: In this ﬁgure, the white pixels represent the target area, black pixels represent the background and the grey area represents the dilated region. 
empty, the extracted features go directly to Add Landmarks without performing any action in Match and Estimate and Purge Landmarks. 
37 
(a) RGB Image 
(b) Segmented target area 
(c) Final output image 
Figure 5.7: The RGB image (a) is ﬁrst segmented to get a mask of the water area (b). This mask is subtracted from a dilated version of itself to get a band around the water (c). 
38 
(a) 116 features detected within the masked area 
(b) 500 features detected throughout the image 
Figure 5.8: In (a) the detection of features are restricted to the AoI and this in turn helps the feature matching process. Whereas in (b) the features are calculated all over the image which might lead to more false positives during feature matching. 
39 
The purged Library of Landmarks (LoL) received from Purge Landmarks is updated using the new features extracted from Process new Image. The LoL maximum size Lmax is given by the user. Using this and the current size L of the LoL, the number of new landmarks Lnew to add is calculated as: 
Lnew = Lmax − L (5.8) 
In the ﬁrst iteration the LoL is empty. Therefore, the best Lmax landmarks are selected from all the new features and are added as new landmarks into the library. Once a new landmark is added, the initial 3D position estimate is calculated using the method explained in section 2.1, with height assumed to be zero. This estimate is added into the UKF object of the landmark. The ﬂowchart of the adding landmarks process is displayed in ﬁg. 5.9. 
Add Landmarks 
Add initial position estimate 
assuming height=0m 
Process new Image 
Purge Landmarks 
Features extracted from New Image, Purged Library of Landmarks 
Select the strongest Features 
Add new landmarks into 
Landmarks library 
Determine the number of landmarks to be added 
Figure 5.9: Flowchart of the Add Landmarks process. 
40 
5.4.1 Match 
Purge Landmarks 
Match and Estimate 
Process new Image 
Features extracted from New Image (M) 
Library of Landmarks 
Have all landmarks in the LoL 
been matched with the new Image? 
Match next landmark Li 
to M no How many matches are 
there? 
Find the feature in M with most matches 
>1 
Remove the feature 
from M 
0 
yes 
1 
Update the Kalman 
estimation for Li 
Add the matched feature in M to Li 
Is the current  
UKF estimate of  
the landmark close to  
the found feature  
position? 
no 
yes 
Figure 5.10: Flowchart of the Match and Estimate process. 
Every landmark in the LoL is then individually matched with the features extracted in the previous step. Since every landmark holds at most a few hundred features, the extracted features from the Image are limited to a few thousand. Therefore the matching process is not computationally expensive. For this reason and because a fairly low amount of matches are expected, a brute force matcher is used in this process. This provides for every landmark feature descriptor l the two best matching image feature descriptors m1, m2. How well an image feature matches the given landmark feature can be expressed by calculating the distance s(l, m): 
s(l, m) = ∥l − m∥ (5.9) 
41 
r = s(l, m1) 
s(l, m2) (5.10) 
The lower r is, the higher the probability that l and m1 are a correct match. Values close to 1 indicate that the match between l and m1 is not unique, but due to randomness. In the original SIFT paper, Lowe [11] suggests using r < 0.8, as it eliminates 90% of false positives while only discarding 5% of positive matches. These are good values for comparing two images with 1000s of matches which allow the use of techniques such as Random Sample Consensus (RANSAC) to get rid of outliers. However, when comparing a landmark to an image, only a few matches are expected. These therefore need to be more reliable. For this reason, the threshold used in this algorithm is r < 0.5, which should discard approximately 97% of false matches [11]. 
Especially for a young landmark with a low number of stored features, there will often be only one match. With older landmarks however, there will often be multiple matches. If matching performance was perfect, all landmark features would match with the same image feature. In reality however, it is possible that the matching is not unanimous. In this case, the image feature with the most matches is declared the true match for that landmark (see ﬁg. 5.11). 
This whole process works very well as long as there is an actual match in the new image. If there is none, mismatches can not be eliminated by the majority of good matches and the system may produce false matches. To get rid of the most outrageous outliers, it is checked if the found feature is at least in the general direction of the estimate provided by the UKF ﬁlter. This is accomplished by calculating the Mahalanobis distance d [23] between the UKF estimate and the feature to be added. 
y = z − ˆz 
d = � 
yTS−1y (5.11) 
The Mahalanobis distance measures the distance between a point and a distribution in the measurement space and expresses it in terms of standard deviation. From eq. (5.11), z is the measurement of the feature which is represented as a point, ˆz and S represents the distribution of the current UKF estimated measurement. 
42 
l1 
l2 
l3 
l4 
l5 
Image feature descriptors 
i3 
i2 
i1 
i4 
m1 
m2 
Figure 5.11: For every landmark feature descriptor li the best (m1) and second best (m2, only displayed for l1) matching image feature descriptor is found. Depending on the quality ratio between those two, m1 is declared a real match (straight line) or a mismatch (dotted line). Since there is still the possibility of a false positive match (between l5 and i4), the number of real matches every image feature descriptor gets is counted and the most frequently matched one is declared the absolute match between the whole landmark and this image (in this case i3). 
Since d is expressed in standard deviations, a feature is almost guaranteed to be an outlier if |d| > 3 and can therefore be discarded. If a match has passed all checks, the image feature will be added to the landmark it matches. This means that the size of a landmark increases with every match found. 
43 
Once a new feature is added to a landmark, the information about its position must also be added to the landmarks UKF ﬁlter. Each feature contains information about its position in an image as pixel coordinates u, v. The UKF ﬁlter however requires information in its measurement space, which are bearings from the drone to the target position. First, u and v are reprojected into the focal plane:  
 uf vf 1 
 
 = A−1 
 
 vu 1 
 
 (5.12) 
Where A is the intrinsic matrix of the camera. 
v1 
v2 
v3 Feature Camera 
Focal Plane 
Figure 5.12: Calculation of yaw (ψimg) and pitch (θimg) from the pixel coordinates. 
The 3 vectors displayed in ﬁg. 5.12 can now be deﬁned: 
v1 = 
 
 00 1 
 
 v2 = 
 
 0uf 1 
 
 v3 = 
 
 uf vf 1 
 
 (5.13) 
44 
ψimg = 
� arccos( v1·v2 
∥v1∥·∥v2∥) uf ≥ cx − arccos( v1·v2 
∥v1∥·∥v2∥) uf < cx 
θimg = 
� arccos( v2·v3 
∥v2∥·∥v3∥) vf ≤ cy − arccos( v2·v3 
∥v2∥·∥v3∥) vf > cy 
(5.14) 
Where cx and cy are the principal point oﬀsets stored in the intrinsic matrix as can be seen in (2.8). This assumes the order of rotations to be yaw ﬁrst and pitch second. 
Applying these rotations after each other results in the rotation matrix from gimbal frame to the target RG→T: 
RG→T = 
 
 cos(ψimg) 0 sin(ψimg) 0 1 0 − sin(ψimg) 0 cos(ψimg) 
 
 
 
 1 0 0 0 cos(θimg) sin(θimg) 0 sin(θimg) cos(θimg) 
 
 (5.15) 
The total rotation matrix from local frame to the target RL→T can then be calculated: RL→T = RL→GRG→T (5.16) 
Where RL→G is the rotation matrix from local frame to gimbal frame. Its calculation can be seen in (2.2) to (2.4). Since (2.4) is dependant on the kinematic chain of the gimbal, it has to be modiﬁed depending on the gimbal used: 
RG0→G = 
� RyawRrollRpitch Zenmuse h20T RyawRpitchRroll AirSim (5.17) 
The measurement z can now be calculated: 
z = �ψtarget θtarget 
� = 
 
 arctan � RL→T 1,2 
RL→T 0,2 
� 
arctan � RL→T 2,2 
(RL→T 1,2 )2+(RL→T 0,2 )2 � 
 
 (5.18) 
z is then used to update the UKF ﬁlters estimation as described in (4.10). 
45 
Purge Landmarks 
Match and Estimate 
New library of landmarks 
Features extracted from New Image , Purged Library of Landmarks 
yes 
Is the size limit 
of the library of landmarks reached? 
Remove the landmarks 
no 
Has the UKF estimate converged for 
any of those landmarks ? yes 
no 
Store 
Store the final estimates of  those landmarks in a list. 
Select bad and old 
landmarks to be 
removed 
Add Landmarks 
Figure 5.13: Flowchart of the purging Landmarks process. 
This is the ﬁnal step of the algorithm. Its purpose is to avoid slowing down the algorithm with an ever increasing number of landmarks. As the drone moves through an area, landmarks are detected, matched and their positions are estimated. Most 
46 
Lpurge = Lmax − T (5.19) 
Where T is the turnover rate. This parameter is a userdeﬁned tuning parameter. It controls the amount of landmarks to be deleted in every iteration and should be selected based on the speed of the drone and the image acquisition frequency. Choosing the turnover rate too high, results in the removal of landmarks which are yet to converge. Choosing it too low would fail to remove all bad landmarks. When the number of landmarks exceeds the purging threshold, the excess L − (Lpurge) worst landmarks are selected. The worst landmarks are selected based on their quality Q, which is determined by the ratio of features it contains to the age of the landmark. Q = size(Li)/ age(Li) (5.20) 
where Li is the evaluated landmark. By this criteria, the landmarks that do not have frequent matches, also referred to as bad landmarks, will be queued up for purging. Furthermore, landmarks that have converged but are now out of frame, are eventually included for purging. Therefore,the purging process does not just remove bad landmarks, but also the good ones that have not been seen for a while. Before they are purged, the landmarks whose position estimates have converged are stored in a global list as shown in ﬁg. 5.13. This is done to ensure that the information contained in these converged landmarks is stored before they are deleted. Converged landmarks are selected and saved based on their low covariance value. This is evaluated using the covariance matrix given after the UKF state estimation. First, the eigenvalues e of the covariance matrix are calculated. The real parts of the eigenvalues describe the variance of the distribution along the three principle axes. The square roots of these variances are the standard deviations of the distribution along these axes. The value of the maximum standard deviation is used as the parameter to evaluate the convergence of the 3D position estimate of the landmark. η = max( � 
Re(e)) (5.21) 
η is the convergence criteria that is checked for every landmark that is to be deleted. If it falls below a chosen threshold, the landmark will be added to the ﬁnal results. After checking for convergence, all selected features are deleted. 
47 
5.6.1 Synchronization 
For the best results, position and angle data associated with a picture must be as accurate as possible. This can be a challenge since the data for these values follow diﬀerent paths. As illustrated in the simulation structure displayed in ﬁg. 4.7. Pictures are provided by AirSim directly, position data by PX4 and gimbal angles by a self written ROS script. This information is published at diﬀerent intervals. Furthermore, the picture data is heavily dependent on the performance of the simulation and might not always be published with the same frequency. In initial experiments, the Image topic was used as a trigger to assemble the information from the other topics (see ﬁg. 5.14). The Image topic was chosen because it is published at the lowest frequency and the resulting discrepancies with the other topics should therefore be the lowest. 
Gimbal angle 
Picture 
Position 
Figure 5.14: Information is collected whenever a picture is published. This can lead to sub optimal data association. 
There are 2 main problems with this approach: First, there are situations where a position or gimbal angle update is published shortly after the picture (see red data point in ﬁg. 5.14). This new information would ﬁt the actual state of the image way better than the old information that is used. Secondly, the topics are evaluated based on when they are received, not when they are created. A slowdown in one of the pipelines could therefore lead to mismatched data. This is especially true for the gimbal angle topic, which is calculated based on information depending on the picture topic. As can be seen in ﬁg. 5.15a, this can lead to bad measurements. Because of the time diﬀerences between measurement and acquisition of the image, the gimbal angles are trailing the image information by approximately one frame. While this is not noticeable when gimbal angles change very slowly, it leads to big spikes when the gimbal is slewed very fast. These big spikes do have an eﬀect on the 
48 
1.0 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 pitch 
measurement GT predicted 
(a) before optimizing synchronization 
0 50 100 150 200 
1.0 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 pitch 
measurement GT predicted 
(b) after optimizing synchronization 
Figure 5.15: Example of target pitch angle from the drone to a Landmark. The ground truth (GT) is calculated based on the drone position and the known Landmark position. The measurement combines information from drone position, gimbal angles and image. The spikes in the measurement are a result of mismatched image and gimbal data when the gimbal is moved sharply. 
UKF prediction and slow down the convergence. ROS provides some tools to solve these problems. Most topics include a timestamp of their creation. Based on these time stamps ROS ApproximateTime ﬁlter can associate data with the smallest ammount of time diﬀerence (see ﬁg. 5.16). Furthermore the timestamps can be manually set. This allows topics that are calculated based on information received from other topics to share the original timestamp. Applying these techniques 
Gimbal angle 
Picture 
Position 
Figure 5.16: ROS ApproximateTime ﬁlter is used to synchronize the data depending on their creation time stamp. While this will introduce some latency since the ﬁlter has to wait for some topics, the data will be synchronized much better. 
resulted in better results and faster convergence of the UKF prediction. This can be seen when comparing ﬁg. 5.15a and 5.15b. The diﬀerences between measurement and GT are generally lower and the UKF prediction converges more steadily. 
49 
One of the problems faced during UKF are numerical issues during the calculation of sigma points. This calculation involves computing the square root of a matrix (see (4.1)). This is done using the Cholskey decomposition, which decomposes a matrix B into a combination of a matrix L and its transpose LT. 
B = LLT (5.22) 
Applying the Cholskey decomposition presupposes a positive semideﬁnite matrix. A symmetric matrix B with real entries is said to be positive semideﬁnite if yByT 
is positive or zero for all nonzero real values of y. While covariance matrices generally should be positive semideﬁnite, this was not always the case with the matrices numerically calculated by the Kalman ﬁlter. This is a common problem faced by many programmers and discussed in several forums, but with no general solution. One proposed solution is to ensure matrix symmetry by applying the following ﬁx: B = (B + BT)/2 (5.23) 
This reduced the occurrence of nonpositive semideﬁnite matrices but did not completely solve the problem. Another possible solution is to calculate the closest positive semideﬁnite matrix as described by Higham [24]. Even though this worked in most cases, it also slowed down the process signiﬁcantly and could not eliminate all occurrences. During the tuning of the sigma point parameters, the authors noticed that the value of α had a signiﬁcant inﬂuence on the frequency of these errors. By choosing a suitable value of 0.7, the observed errors could be completely eliminated. 
50 
Simulation 
The tests are conducted in the AirSim simulation environment with water as the target area to be localized. The drone used in this simulation is a standard drone provided by AirSim, conﬁgured with a fullhd 3-axis gimbal camera. The images and data required are collected through the simulation and stored as a dataset. The algorithm is then run at a later time on the collected dataset. Since it is computationally expensive and not yet optimized, it is run at less than real time speed. 
To run the tests, a part of the coastline in the unreal environment is chosen as the target (see ﬁg. 4.10). 37 points along this coastline were extracted manually and the polyline they deﬁne serves as ground truth (see ﬁg. 4.11). 
To evaluate the quality of the resulting estimated landmark positions, the minimal distance from each landmark to the ground truth polyline is calculated. Low distances are desirable but because of the nature of the algorithm, a perfect 0 m is impossible. Since the landmarks are chosen from a band along the ground truth, they will have a certain distance even if the Kalman estimation is perfect. 
The landmarks are further classiﬁed into three groups: 
• Distance < 10 m: useful landmarks which can be used to estimate the contour of the target area. 
• 10 m < distance < 20 m: landmarks that might have converged to their real position but are too far away to be useful or landmarks that did not fully converge. 
• distance > 20 m: landmarks that converge to a wrong position (outliers) and require analysis. 
51 
The results obtained by an initial test of the 3D target localization algorithm are displayed in ﬁg. 6.1. Although this ﬁgure provides a good representation of the contour of the area, it also presents a lot of outliers (displayed in red). In order to reduce the number of outliers, the reason for them being present had to be identiﬁed. 
100 
0 
100 
200 
300 100 
0 
100 
200 
300 
100 
0 
100 
200 
300 
Figure 6.1: Displays the 3D estimates of the landmarks found around the lake. The red line denotes the ground truth, green points represent the inliers, yellow points are one which are considered not useful for 3D area localization and red point are the outliers. 
52 
In order to analyse the outliers, the information that was used to localize every landmark, needs to be stored at every frame. This is done by modifying the LoL to accommodate information about the measurement (target yaw and pitch), prediction (target yaw and pitch), mean of estimation (x,y,z), covariance of estimation and the Mahalanobis distance as shown in ﬁg. 6.2. This allows to track the history of every landmark and analyse every single update. 
Library of Landmarks (L) 
Landmark 1 
Measurement 
Measurement 1 
Measurement 2 
... 
Mahalanobis dist 
Distance 1 
Distance 2 
... 
Original Library 
Keypoints 
Descriptors 
... 
Covariance 
Covar 1 
Covar 2 
... 
Prediction 
Prediction 1 
Prediction 2 
... 
Mean 
Mean 1 
Mean 2 
... 
Landmark ... 
Measurement 
Measurement 1 
Measurement 2 
... 
Mahalanobis dist 
Distance 1 
Distance 2 
... 
Original Library 
Keypoints 
Descriptors 
... 
Covariance 
Covar 1 
Covar 2 
... 
Prediction 
Prediction 1 
Prediction 2 
... 
Mean 
Mean 1 
Mean 2 
... 
Figure 6.2: Additional elements are added to the library of landmarks, they are required to allow analysis of all updates of every landmark. 
The outliers are caused by landmarks that are not consistently matched. One such example can be seen in ﬁg. 6.3. This outlier data is compared with the data plots of one of the inlier points in ﬁg. 6.4 to ﬁnd the reason for its inaccuracy. 
From ﬁg. 6.3a and ﬁg. 6.3b it can be observed that, at around update cycle 150, there is jump in the measured pitch and yaw angles. This indicates that there was a false feature matching at that instant. The feature matcher then follows the new feature and the estimates converge to a new position as seen in ﬁg. 6.3e. In ﬁg. 6.3d it can be seen that the miss match leads to a very big mahalanobis distance since it is very unlikely that the new point is part of the current UKFestimate distribution. These kind of outliers can therefore be removed by applying a threshold criteria on mahalanobis distance as described in section 5.4.1 (this part of the algorithm was only added as a reaction to the initial tests described here). 
53 
Update cycle 
62.5 
60.0 
57.5 
55.0 
52.5 
50.0 
47.5 
45.0 
Pitch [deg] 
Measurement Predicted 
(a) Target pitch angle 
0 50 100 150 200 
Update cycle 
150 
100 
50 
0 
50 
100 
150 
Yaw [deg] 
Measurement Predicted 
(b) Target yaw angle 
0 50 100 150 200 
Update cycle 
0.1 
1.0 
10.0 
Standard deviation 
1st principle axis 
2nd principle axis 
3rd principle axis 
(c) Standard deviations (y in log. scale) 
0 50 100 150 200 
Update cycle 
0 
100 
200 
300 
400 
500 
600 
700 
Mahalanobis distance 
(d) Mahalanobis distance 
100 
90 
80 
70 
x [m] 
80 
100 
120 
y [m] 
0 50 100 150 200 
Update cycle 
20 
40 
60 
80 
100 
z [m] 
(e) Estimated target position 
Figure 6.3: Data of outlier which is caused due to false feature matching, where xaxis represents the iteration number. After an initial phase of convergence a substantial change in measured pitch and yaw occurs. This leads the estimation to change a lot and eventually converge to another position. The Mahalanobis distance of this update is very high which indicates a missmatch. 
54 
Update cycle 
62 
60 
58 
56 
54 
52 
50 
48 
Pitch [deg] 
Measurement Predicted 
(a) Target pitch angle 
0 20 40 60 80 
Update cycle 
130 
120 
110 
100 
90 
80 
70 
Yaw [deg] 
Measurement Predicted 
(b) Target yaw angle 
0 20 40 60 80 
Update cycle 
0.1 
1.0 
10.0 
Standard deviation 
1st principle axis 
2nd principle axis 
3rd principle axis 
(c) Standard deviations (y in log. scale) 
0 20 40 60 80 
Update cycle 
0 
1 
2 
3 
4 
5 
6 
7 
Mahalanobis distance 
(d) Mahalanobis distance 
70 
68 
66 
64 
62 
x [m] 
100 
110 
120 
130 
y [m] 
0 20 40 60 80 
Update cycle 
10 
20 
30 
40 
z [m] 
(e) Estimated target position 
Figure 6.4: Data of one of the inliers, where xaxis represents the iteration number. The estimatiod target converges nicely and no sudded changes in yaw or pitch can be observed. The mahalanobis distance of all updates stays below 10. 
55 
Figure 6.5: Results achieved with the base parameters shown in table 6.1. The red line denotes the ground truth, green points represent the inliers. 
6.2.1 Base parameters 
Based on the experience gathered during the development and the initial tests, a conﬁguration of base parameters is chosen, which can be seen in table 6.1. These will then be varied to ﬁnd ideal parameters. 
speed convergence band width mahalanobis dist. downsclaing factor 
3 m/s 0.25 100 px 10 2 
Table 6.1: Baseline conﬁguration for the parameters that will be varied. 
56 
Altitude (local) SD gimbal yaw SD gimbal pitch SD x SD y SD z 
30 m 0.5° 0.5° 0.5m 0.5 m 2 m 
Table 6.2: Values of the parameters that will stay ﬁxed 
errors with the indicated standard deviations are added to the measurements used in the algorithm. The values of standard deviation were chosen based on the estimated accuracy displayed by a DJI matrix 300 during a real test (see. section 6.10.2). 
These parameters lead to the results which can be seen in table 6.3, ﬁg. 6.5 and ﬁg. 6.6. 
Nr. of Landmarks dist < 10m 10m < dist < 20m dist > 20m avg. dist to GT 
809 805 4 0 3.43 m 
Table 6.3: Results achieved with the base parameters 
0 2 4 6 8 10 
Distance to ground truth [m] 
1.0 
10.0 
100.0 
Number of landmarks 
Figure 6.6: This histogram shows the distribution of the landmarks according to their minimal distance to ground truth. The red line indicates the average value. To make outliers visible, yaxis is in log scale. 
57 
The localization algorithm requires tracking of features throughout its ﬂight and requires a certain amount of time for the UKF estimates to converge. This process depends on the camera frequency and the speed of the drone. 
While capturing datasets, AirSim provides images at a frequency of approximately 6Hz. Varying the speed of the drone leads to variation in the overlap between images. To test the maximum drone velocity which can still provide satisfactory results, the performance of the algorithm is evaluated while limiting the drone maximum velocity to 1, 3, 5 or 10 m/s. These velocity limits are applied to each axis separately. The output of the algorithm is then evaluated based on the distance between the landmark and the ground truth line. 
6.3.1 Results 
1.5 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 
Speed [m/s] 
0 
200 
400 
600 
800 
1000 
1200 
1400 
Number of landmarks 
dist< 1m 1m<dist<2.5m 2.5m<dist<5m 5m<dist<10m 10m<dist<20m 20m<dist 
3.45 
3.50 
3.55 
3.60 
3.65 
Average distance to ground truth [m] 
Average distance to ground truth 
Figure 6.7: Displays the plot for various drone speeds vs the number of landmarks found. This plot also shows the average distance from the landmark to the ground truth for diﬀerent drone speeds. 
As can be seen in ﬁg. 6.7, lower drone speed leads to a higher number of landmarks. The small diﬀerence between 5 and 10 m/s might be because the drone does not 
58 
6.4 Variation of convergence threshold 
A landmark is considered good if it satisﬁes the convergence criteria (see (5.21) in section 5.5). It is therefore crucial to choose an optimal threshold to select the right landmarks. 
6.4.1 Results 
From ﬁg. 6.8, it can be inferred that at higher thresholds the number of converged landmarks is higher but their quality is reduced. Values below a threshold of 1 show good results and 0.5 is chosen as a good compromise between the number of landmarks and their quality. 
0.2 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 
Convergence threshold 
0 
1000 
2000 
3000 
4000 
5000 
Number of landmarks 
dist< 1m 1m<dist<2.5m 2.5m<dist<5m 5m<dist<10m 10m<dist<20m 20m<dist 
4 
6 
8 
10 
12 
14 
16 
Average distance to ground truth [m] 
Average distance to ground truth 
Figure 6.8: Displays the plot for convergence threshold vs the number of landmarks found. This plot also shows the average distance from the landmark to the ground truth for diﬀerent band widths. 
59 
The size of the Area of Interest (AoI) band is a crucial tuning parameter for the algorithm. If it is chosen too small, not a lot of features might be found. If it is chosen too big, some of the found landmarks might be very far away from the border and the information gained might therefore be of reduced value. To ﬁnd the optimal value, the drone mission is performed around the lake while varying the width of the band between 10 and 200 pixels. Since the downscaling factor of the base conﬁguration is 2, the actual band width used by the algorithm varies between 5 and 100 pixels (see explanation in section 6.7). 
6.5.1 Results 
10 25 50 75 100 125 150 175 200 
Band width [px] 
0 
200 
400 
600 
800 
1000 
Number of landmarks 
dist< 1m 1m<dist<2.5m 2.5m<dist<5m 5m<dist<10m 10m<dist<20m 20m<dist 
2.5 
3.0 
3.5 
4.0 
4.5 
5.0 
5.5 
Average distance to ground truth [m] 
Average distance to ground truth 
Figure 6.9: Displays the plot for various band widths vs the number of landmarks found. This plot also shows the average distance from the landmark to the ground truth for diﬀerent band widths. 
As expected, bigger band width leads to more but lower quality landmarks. The ideal value seems to be between 25 and 50 pixels where a high amount of good quality landmarks is present. It has to be said, that these results are related to the altitude the drone is ﬂying at. If the ﬂight height is changed signiﬁcantly, the ideal band width might change. 
60 
The Mahalanobis distance threshold is used during the matching process to determine whether a matched feature belongs to the same landmark or not (see section 5.4.1). The threshold is varied between 2, the lowest setting that actually produces some converged landmarks, and 1000 which is basically equal to no thresholding at all. 
6.6.1 Results 
As can be seen in ﬁg. 6.10, values between 3 and 4 seem to give the highest amount of good quality landmarks. Increasing the threshold allows more and more false matches to occur and therefore lowers the quality of the results. Lowering it below 3 results in the removal of many legitimate matches. 
1 2 10 100 1000 
Mahalanobis distance threshold 
0 
200 
400 
600 
800 
Number of landmarks 
dist< 1m 1m<dist<2.5m 2.5m<dist<5m 5m<dist<10m 10m<dist<20m 20m<dist 
3.4 
3.6 
3.8 
4.0 
4.2 
4.4 
Average distance to ground truth [m] 
Average distance to ground truth 
Figure 6.10: Displays the plot for various mahalanobis distance threshold vs the number of landmarks found. This plot also shows the average distance from the landmark to the ground truth for diﬀerent mahalanobis thresholds. xaxis is displayed in log scale. 
61 
The simulated drone is equipped with a full HD (1920x1080) camera. High Deﬁ- nition images provide more detail, which is advantageous when performing feature matching. In this test, the input full HD image is downscaled by factors of 2, 4 and 8 to evaluate the performance of the developed algorithm on images with lower resolution. Since the band width is expressed in pixels at full resolution, it is also reduced with the downscaling factor (e.g. band width 100 with downscaling factor 4 means that the algorithm uses a band of 25 pixels in the low resolution image). Choosing the factors as powers of 2 should lead to ideal downscaling results by avoiding any interpolation between pixels. 
6.7.1 Results 
1 2 3 4 5 6 7 8 
downscaling factor 
0 
200 
400 
600 
800 
1000 
Number of landmarks 
dist< 1m 1m<dist<2.5m 2.5m<dist<5m 5m<dist<10m 10m<dist<20m 20m<dist 
3.6 
3.8 
4.0 
4.2 
4.4 
4.6 
Average distance to ground truth [m] 
Average distance to ground truth 
Figure 6.11: Displays the plot for various downscaling factors. vs the number of landmarks found. This plot also shows the average distance from the landmark to the ground truth for diﬀerent image resolutions. 
As can be seen in ﬁg. 6.11, a downscaling factor of 2 gives not only the highest number of landmarks, but also the lowest average distance to ground truth. The only reason to choose another downscaling factor would be to improve computation speed which was not evaluated in this test. 
62 
Based on the tests described above, the set of best performing parameters is chosen, which can be seen in table 6.4. 
50 
0 
50 
100 
150 
200 
250 
50 
0 
50 
100 
150 
200 
250 
50 
0 
50 
100 
150 
200 
250 
Figure 6.12: Results achieved with the best performing parameters shown in table 6.4. The red line denotes the ground truth. 
speed convergence band width mahalanobis dist. downscaling factor 
3 m/s 0.5 50 px 3.75 2 
Table 6.4: Conﬁguration with the best performing parameters. 
63 
Nr. of Landmarks dist < 10m 10m < dist < 20m dist > 20m avg. dist to GT 
1631 1631 0 0 2.82 m 
Table 6.5: Results achieved with the best performing parameters 
0 2 4 6 8 
Distance to ground truth [m] 
1.0 
10.0 
100.0 
Number of landmarks 
Figure 6.13: This histogram shows the distribution of the landmarks according to their minimal distance to ground truth. The red line indicates the average value.To make outliers visible, yaxis is in log scale. 
64 
The tests described above have all been performed on the coastline mission as described in section 4.2.4. Since the area to be localized is water, all landmarks are located in the same plane. This does not demonstrate the 3D capability of the developed algorithm. Furthermore, all parameters have been tuned on that example. It is therefore possible, that the parameters do not ﬁt the algorithm in general, but the mission used to evaluate them. 
For this reason, the performance of the algorithm is evaluated using a second mission with the initial and the best performing parameters. The mission used is the cliﬀ mission described in section 4.2.4. As can be seen in ﬁg. 6.15, the 3D shape of the cliﬀ is captured well with both settings. The best performing parameters once again lead to a shorter average distance to the ground truth. This happens at the cost of a lower number of converged landmarks. It is therefore a tradeoﬀ between quality and quantity of the resulting data. 
Landmarks d < 10m 10m < d < 20m d > 20m avg. d to GT 
Initial 468 468 0 0 2.78 m Best performing 354 354 0 0 1.91 m 
Table 6.6: Results achieved with the best performing parameters 
0 1 2 3 4 5 6 7 8 
Distance to ground truth [m] 
1.0 
10.0 
100.0 
Number of landmarks 
(a) Initial parameters 
0 1 2 3 4 5 6 7 8 
Distance to ground truth [m] 
1.0 
10.0 
100.0 
Number of landmarks 
(b) Best performing parameters 
Figure 6.14: Comparison of the histograms. yaxis in log scale. 
65 
(a) Initial parameters 
(b) Best performing parameters 
Figure 6.15: Comparison between the results from initial and best performing parameters applied to the cliﬀ mission.The red line marks the manually generated ground truth. Both results capture the 3D shape of the are very well. 
66 
All tests so far have been performed with random, normally distributed errors added to both gimbal pitch and gimbal yaw measurements (see section 6.2.1). However, the normal distributions used were centred at the actual measurement. It is therefore not surprising, that the Kalman ﬁlters could easily deal with the introduced errors. As will be shown in section 6.11, static errors which move the center of the distribution away from the actual value also pose a real problem. Tests are therefore performed to evaluate the inﬂuence of static oﬀsets on both the measured gimbal pitch and gimbal yaw angles. The parameters used are the best performing parameters, mentioned in section 6.8. As in all tests before, the standard deviations for the distribution of gimbal yaw and gimbal pitch are 0.5◦each. 
6.10.1 Yaw oﬀset 
0 2 4 6 8 10 
Yaw offset [deg] 
0 
250 
500 
750 
1000 
1250 
1500 
1750 
2000 
Number of landmarks 
dist< 1m 1m<dist<2.5m 2.5m<dist<5m 5m<dist<10m 10m<dist<20m 20m<dist 
3.0 
3.5 
4.0 
4.5 
5.0 
5.5 
Average distance to ground truth [m] 
Average distance to ground truth 
Figure 6.16: With increasing yaw oﬀset the number of landmarks that fully converge diminishes. While the average distance to the ground truth increases steadily, it stays fairly low even with extreme oﬀsets. 
The eﬀects of applying a static yaw oﬀset can be seen in ﬁg. 6.16. While average distance from the ground truth increases steadily with increasing oﬀset, there seems to be little impact below 2 degrees. With higher values above 4 degrees, 
67 
The 3D plot shown in ﬁg. 6.16, displays the results achieved with an oﬀset of 5 degrees. It is visually almost indistinguishable from the one taken without any oﬀset (ﬁg. 6.12). This shows that the algorithm can compensate for yaw oﬀsets relatively well. In the case of extreme oﬀsets above 5 degrees, landmarks are rejected instead of being estimated in a wrong position. This reduces the amount of available data but still maintains the quality of the result. 
50 
0 
50 
100 
150 
200 
250 
50 
0 
50 
100 
150 
200 
250 
50 
0 
50 
100 
150 
200 
250 
Figure 6.17: A yaw oﬀset of 5 degrees does not aﬀect the results in a visible way (compare ﬁg. 6.12 for no oﬀset). 
6.10.2 Pitch oﬀset 
The eﬀects of applying a pitch oﬀset are much more severe. In ﬁg. 6.18 it can be seen that while the number of converging landmarks stays relatively high, the average distance to the ground truth greatly increases with increasing pitch oﬀset. When visually inspecting the results in ﬁg. 6.19, it can be observed that the major error generated is along the zaxis. The localization in xand yaxis is not aﬀected by the addition of pitch oﬀset. The average distance to the ground truth increases linearly with increasing pitch oﬀset. While the eﬀects are noticeable for all values tested, small oﬀsets below 2 degrees still produce acceptable results. 
68 
Pitch offset [deg] 
0 
250 
500 
750 
1000 
1250 
1500 
1750 
2000 
Number of landmarks 
dist< 1m 1m<dist<2.5m 2.5m<dist<5m 5m<dist<10m 10m<dist<20m 20m<dist 
4 
6 
8 
10 
12 
14 
Average distance to ground truth [m] 
Average distance to ground truth 
Figure 6.18: With increasing pitch oﬀset the average distance from ground truth increases greatly. As can be seen in ﬁg. 6.19, this increase is mostly along z axis. 
50 
0 
50 
100 
150 
200 
250 
50 
0 
50 
100 
150 
200 
250 
50 
0 
50 
100 
150 
200 
250 
Figure 6.19: With a pitch oﬀset of 5 degrees, it can be clearly seen, that the results are oﬀset along the zaxis. 
69 
6.10.3 Used Hardware 
The test was conducted with a DJI Matrice 300 provided by Robotto. A DJI DReal Time Kinematic (RTK) ground station was used to provide RTKGPS. The camera used was the wide RGB lens of a DJI Zenmuse H20T. 
6.11 Previous method test 
This test was performed to evaluate the accuracy of the previous method as described in 2.1. It predates most of the work in this thesis and played a vital role in the decision to ﬁnd a new method, instead of improving the old one. 
6.11.1 Setup 
The test was conducted on a soccer ﬁeld close to Aalborg University. The RTK base station and 2 additional markers are placed on the ﬁeld. They serve as ground reference points as shown in Figure 6.20. The RTK base station serves as the origin for the local coordinate system. The ground truth for the Center and Goal markers was established by using RTKGPS. These markers were then photographed from 8 predeﬁned positions as seen in Figure 6.21, at an altitude of 10 m, 30 m and 50 m resulting in 72 data points as seen in Figure 6.22. Furthermore, the area of the test was mapped using automatic mapping missions created with DJI tools. These images were then used to create a complete 3D reconstruction of the area using Pix4D. This includes a selfcalibration of the camera, which is performed during bundle adjustment. The intrinsic matrix gained from this calibration was used in further steps. 
70 
Figure 6.20: The ground reference points used during testing where, the RTK mast is denoted by red, marker in the center of the ﬁeld is denoted by green, marker near the goal is denoted as blue and common reference point on the goal post is denoted by grey. 
Figure 6.21: Camera positions at 10 m, 30m and 50m in NED frame. 
Figure 6.22: Data points generated from diﬀerent camera positions 
71 
The markers are placed at their respective locations and their GPS locations were recorded by measuring the drone GPS at that location, displayed in Table 6.7. A mission plan is developed to ﬂy the drone around the football ﬁeld at diﬀerent altitudes - 10 m, 30 m, and 50 m. At each height, the drone is stopped at 8 diﬀerent locations. The gimbal is then manually controlled to focus on each individual marker placed to capture images with metadata information. After acquiring the image dataset, the pixel locations of every marker visible in the images are recorded manually. They are stored along with the camera intrinsics and the drone location at the time of capture. 
Marker ID Lat Lon x y relative altitude 
RTK ground station R 57.0134748 9.9742011 0.00 0.00 0.00 Backpack ""Center"" M 57.0131517 9.9740606 -36.55 -8.45 0.00 Backpack ""Goal"" G 57.0133455 9.9736700 -14.97 -32.12 0.00 
Table 6.7: Markers placed on the ﬁeld. 
This data was then used to calculate the location of these markers through the method explained in section 6.11 and is compared with their respective ground truths recorded (see Figure 6.23a). This prediction was observed to be very inaccurate, especially at shallow pitch angles. It was hypothesised that, these might be static errors due to bad calibration of the gimbal angles. Because of the nonlinear eﬀect of the pitch angle on the results, errors would be disproportionally ampliﬁed at shallow angles. 
Roll Pitch Yaw 
Calculated oﬀset 0.60◦ -2.16◦ -0.34◦ 
Table 6.8: Oﬀset corrections calculated by least squares optimisation. 
In order to correct this, least square optimization was used to calculate the static errors in the gimbal angles by using the RTK position as the reference. The result of this optimisation produced static oﬀset corrections for each axis which can be seen in table 6.8. These can then be used to recalculate the positions of the other markers as shown in Figure 6.23b. 
72 
(a) Using previous method 
(b) After least square optimization 
Figure 6.23: Prediction of marker locations at diﬀerent camera positions. 
6.11.3 Results 
These original results along with the optimised results can be seen in Table 6.9. The initial results are bad, especially when considering the low altitude and ﬂat terrain. When least square optimization was used to adjust the gimbal angles, the results were greatly improved. This proves the initial hypothesis, that the errors generated were, at least partly, due to static errors in the gimbal angles. It is therefore necessary to perform an initial calibration around the RTK mast to calculate the static errors in the gimbal angles at the the beginning of a ﬂight mission. 
While the accuracy reached with least square optimization is satisfactory, it still has to be taken into account that this test was performed on a ﬂat soccer ﬁeld. Any kind of height deviation would negatively impact the results. This is one of the main reasons, why this approach was not followed up on. 
Static error will also occur in the new method developed later during this thesis. Even though it uses Kalman ﬁltering to eliminate normally distributed errors, the mean of this normal distribution still has to be accurate. 
73 
Marker mean error std deviation mean error std deviation 
RTK ground station 8.811 10.727 2.154 1.250 10 m 17.701 14.737 3.111 1.440 30 m 4.299 1.679 1.531 0.961 50 m 3.870 1.858 1.742 0.645 
Backpack ""Center"" 11.051 12.584 2.031 1.372 10 m 21.589 15.985 2.212 1.802 30 m 5.117 3.422 1.832 1.246 50 m 5.128 4.012 2.027 1.051 
Backpack ""Goal"" 6.901 5.651 1.693 0.919 10 m 14.211 4.043 1.866 1.086 30 m 3.591 1.492 1.422 1.065 50 m 3.400 1.215 1.779 0.661 
Goalpost ""Post"" 9.295 10.795 2.213 1.948 10 m 19.090 14.433 3.346 2.760 30 m 5.105 2.847 1.575 1.052 50 m 4.215 2.574 1.754 1.129 
Table 6.9: Mean errors and standard deviation of original calculated data points and the optimised data points 
74 
The idea proposed by the authors was successfully developed and it produced very promising results. The analysis of the algorithm helped the authors to calculate the best parameters for the algorithm. Using these parameters, a clear point cloud boundary of the area was produced with almost no outliers. To develop this algorithm, the authors made a few simpliﬁcations. These inﬂuenced on how the algorithm is to be developed and how it can be transferred to a realtime implementation. 
One of the major points to be addressed is how the algorithm has been implemented in the simulation environment. As previously mentioned in Section 4.2.4, the algorithm is not run directly on the simulation. The simulation was only run only to collect the necessary data to run the algorithm oﬄine. AirSim by itself takes a lot computation power, since it simulates high visual ﬁdelity. When added to the implementation of algorithm, the entire process slows down heavily. Therefore, a decision was made to run the two processes separately at a decent speed, rather than running both simultaneously at a slow speed. Moreover, it allows the authors to test the algorithm on the stored dataset again and again, without the need of rerunning the simulation. Thereby saving a lot of time. 
The second point to be discussed is the transfer of the algorithm to an actual drone. To facilitate the transfer, some of the basic requirements are: 
• Hardware Requirements - The processor available must be capable of running image segmentation, feature detection and matching simultaneously. 
• Area detection and segmentation algorithm - In the current implementation, the segmented image is received from simulation. Therefore, an additional area detection and segmentation algorithm is required based on the type of area selected. 
• Path planning - The main focus of the thesis is area localization. The path planned for the drone in the simulation was calculated assuming a prior knowledge of the ground truth. In an actual implementation, the path would have to be calculated dynamically based on the situation. 
• SIFT GPU implementation - The slowest part of the algorithm is the feature detection and matching. This part can be optimized by replacing it with 
75 
In Chapter 6, the results of the algorithm were tested for various boundary conditions, with addition of noise and the best parameters were calculated for optimal results. The algorithm is robust and provides satisfactory results even for unoptimised parameters. The algorithm performs well for downscaled images and is able to detect and match features consistently up to a downscaling factor of 8. While low drone speeds lead to better results, even when tested at the drone’s maximum speed, the algorithm performed well. Increasing the band width, also increases the amount of low quality landmarks detected. This is to be expected, as the region of feature matching increases, more landmarks will be detected. This indicates that, the algorithm is performing well in ﬁnding landmarks and localising them. However, not all the landmarks are useful in calculating the boundary of the area. By varying the size of the band width, the boundary calculated would be aﬀected signiﬁcantly as seen from ﬁg. 7.1. Therefore, a balance of the number of landmarks detected and the accuracy of the boundary is necessary while selecting the band width around the target area. 
The main parameter to remove outliers is the Mahalanobis distance. With an increase in the mahalanobis distance threshold, the number of outliers also increases. The total number of landmarks detected remains almost the same after a certain value. At high mahalanobis distance thresholds, the feature matcher adds outliers into the LoL. This leads to further mismatches and the estimation worsens. These outliers, would certainly be impactful during the calculation of the boundary of the target area. However,from a visual point of view, even with high mahalanobis distance the ﬁnal results can still be visualized satisfactorily. 
To have a better and faster convergence of the position estimate, the initial estimate chosen was taken as the 3D reprojection of the pixel values at an assumed ground plane. This improved the speed of convergence, although the eﬀect was not documented through tests. 
76 
Figure 7.1: The ﬁgure of the left shows the result with a high oﬀset mask and the ﬁgure on the right shows the result of lower oﬀset mask. 
Furthermore, due to the accurately segmented images acquired from the simulation, the eﬀect of an faulty or noisy segmentation on the ﬁnal results was also not tested. 
Boundaries of an area are best represented as a 3D line. Therefore, it would be desirable to calculate a 3D line from the generated pointcloud. 3D curve ﬁtting is still a topic of research and can be a separate project by itself. It is therefore not implemented. 
The ﬁnal tuned parameters provided a very accurate result for these speciﬁc landscapes. This might not be true for any area, and the parameters might have to be tuned again for diﬀerent circumstances. 
77 
The algorithm developed successfully localizes the borders of 3D areas in simulation. The results obtained were tuned and the parameters that produce the best result were calculated. The ﬁnal results are accurate and produce almost no outliers. While most of the tests were performed on an example mission without altitude changes, a second mission with big height diﬀerences was also tested and the algorithm performed equally well. 
The set research and development goals were followed to achieve the ﬁnal result. The algorithm performed well in all categories deﬁned in the initial requirements: 
1. The feature detector used for the project is SIFT, which is one the best feature detector algorithms. It is able to detect features consistently throughout the mission. 
2. The ﬁnal average distance obtained after using the tuned parameters was 2.82m, which satisﬁes the set requirement. 
3. All the tests described above were conducted with the addition of standard deviation in position and gimbal angles. The Kalman ﬁlter corrects for the noise in the measurement and provides a ﬁltered output. 
4. The algorithm was tested for multiple camera resolutions. Although the number of total landmarks decreased with reduced image resolution, the outliers still remained to a minimum. Moreover, the average distance was below 5m even when images downscaled by factor 8 were used. Hence, this requirement is also satisﬁed. 
The testing of the algorithm on a real dataset was delayed due to infrequent availability of the drone. The authors therefore directed their full attention on the simulation implementation. This resulted in an algorithm which performs exceptionally well in the simulation environment. 
78 
While the algorithm is working well in simulation, there is a further work to be done until it can be deployed on a drone: 
• The most important next step is to test the algorithm on a dataset captured on a real drone. This will show if the feature tracking also works as well on real images and if the data provided by the drone is accurate enough to provide good localization. 
• Using real life data, it has to be established for which use cases the algorithm is actually suitable. In regard to the use for detecting wildﬁres, which was the original inspiration, it would be interesting to conduct tests using thermal images. These show ﬁre very clearly but are not obscured by smoke. 
• If this gives promising results, the algorithm will have to be further optimized so it eventually can be run directly on a drone. This will require rewriting a lot of the code so it can be executed on a GPU. 
• The output of the algorithm is currently a point cloud. While humans can easily see the border of the area from the pointcloud, a solution should be found to extract the border as a 3D line. This would allow further processing by algorithms such as the path planner of the drone. 
79 
[1] S. Van Riel, “Exploring the use of 3d gis as an analytical tool in archaeological excavation practice,” Ph.D. dissertation, 2016, doi: 10.13140/RG.2.1.4738. 2643. 
[2] “Opencv reference manual,” OpenCV, accessed: 31.05.21. [Online]. Available: https://docs.opencv.org/4.4.0/d9/d0c/group__calib3d.html 
[3] B. Berner, “Internship at robotto - drone based autonomous ﬁre detection,” 2020, (Only accessible when logged in to AAU https://projekter.aau.dk/projekter/en/studentthesis/internshipatlibrary), robotto--dronebasedautonomous-ﬁredetection(17e3313b-7108-4785-8b04- d0c37616dee0).html). 
[4] L. Merino, F. Caballero, J. R. Martinezde Dios, I. Maza, and A. Ollero, “An unmanned aircraft system for automatic forest ﬁre monitoring and measurement,” Journal of Intelligent and Robotic Systems, vol. 65, pp. 533–548, 2012, doi: 10.1007/s10846-011-9560-x. 
[5] A. Dena and N. Aouf, “Unscented kalman ﬁlter for vision based target localisation with a quadrotor,” in 14th International Conference on Informatics in Control, Automation and Robotics (ICINCO), 2017, pp. 453–458, doi: 10.5220/0006474404530458. 
[6] H. Hosseinpoor, F. Samadzadegan, and F. Dadrass Javan, “PRICISE TARGET GEOLOCATION AND TRACKING BASED ON UAV VIDEO IMAGERY,” ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. XLIB6, pp. 243–249, 2016, doi: 10.5194/isprsarchivesXLIB6-243-2016. 
[7] J. Li, C. W. Chen, and T. Cheng, “Estimation and tracking of a moving target by unmanned aerial vehicles,” in 2019 American Control Conference (ACC), 2019, pp. 3944–3949, doi: 10.23919/ACC.2019.8815101. 
[8] X. Wang, J. Liu, and Q. Zhou, “Realtime multitarget localization from unmanned aerial vehicles,” Sensors, vol. 17, no. 1, p. 33, 2017, doi: 10.3390/ s17010033. 
[9] S. Ponda, R. Kolacinski, and E. Frazzoli, “Trajectory optimization for target localization using small unmanned aerial vehicles,” in In AIAA guidance, navigation, and control conference, p. 6015., 2009, doi: 10.2514/6.2009-6015. 
80 
[11] D. G. Lowe, “Distinctive image features from scaleinvariant keypoints,” Int. J. Comput. Vision, vol. 60, no. 2, pp. 91–110, Nov. 2004, doi: 10.1023/B: VISI.0000029664.99615.94. 
[12] E. A. Wan and R. Van Der Merwe, “The unscented kalman ﬁlter for nonlinear estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No.00EX373), 2000, pp. 153–158, doi: 10.1109/ASSPCC.2000.882463. 
[13] R. Van Der Merwe, “Sigmapoint kalman ﬁlters for probabilistic inference in dynamic statespace models,” Ph.D. dissertation, OGI School of Science & Engineering at OHSU, 2004. 
[14] H. Bay, T. Tuytelaars, and L. Van Gool, “SURF: Speeded up robust features,” in European conference on computer vision. Springer Berlin Heidelberg, 2006, vol. 3951, pp. 404–417, doi: 10.1007/11744023_32. 
[15] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “ORB: An eﬃcient alternative to SIFT or SURF,” in 2011 International Conference on Computer Vision, 2011, pp. 2564–2571, doi: 10.1109/ICCV.2011.6126544. 
[16] P. F. Alcantarilla, A. Bartoli, and A. J. Davison, “Kaze features,” in Computer Vision – ECCV 2012, A. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, and C. Schmid, Eds. Springer Berlin Heidelberg, 2012, pp. 214–227, doi: https: //doi.org/10.1007/978-3-642-33783-3_16. 
[17] S. A. K. Tareen and Z. Saleem, “A comparative analysis of SIFT, SURF, KAZE, AKAZE, ORB, and BRISK,” in 2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET), 2018, pp. 1–10, doi: 10.1109/ICOMET.2018.8346440. 
[18] I. Rey Otero and M. Delbracio, “Anatomy of the SIFT Method,” Image Processing On Line, vol. 4, pp. 370–396, 2014, doi: https://doi.org/10.5201/ipol. 2014.82. 
[19] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity visual and physical simulation for autonomous vehicles,” in Field and Service Robotics.Springer Proceedings in Advanced Robotics, M. Hutter and R. Siegwart, Eds., vol. 5. Cham: Springer, 2018, pp. 621–635, doi: https://doi.org/10.1007/978-3-319-67361-5_40. 
81 
[21] National Imagery and Mapping Agency, “Department of defense world geodetic system 1984: its deﬁnition and relationships with local geodetic systems,” National Imagery and Mapping Agency, Redmond, WA, USA, Tech. Rep. TR8350.2, Jan. 2000, accessed: 17.12.20. [Online]. Available: http://earthinfo.nga.mil/GandG/publications/tr8350.2/tr8350_2.html 
[22] R. Labbe, “Filterpy,” GitHub repository, 2014. [Online]. Available: https://github.com/rlabbe/ﬁlterpy 
[23] “Reprint of: Mahalanobis, p.c. (1936) ""on the generalised distance in statistics."",” Sankhya A, vol. 80, no. 1, pp. 1–7, 2018, doi: https://doi.org/10.1007/ s13171-019-00164-5. 
[24] N. J. Higham, “Computing a nearest symmetric positive semideﬁnite matrix,” Linear Algebra and its Applications, vol. 103, pp. 103–118, 1988, doi: https: //doi.org/10.1016/0024-3795(88)90223-6. 
[25] M. Björkman, N. Bergström, and D. Kragic, “Detecting, segmenting and tracking unknown objects using multilabel mrf inference,” Computer Vision and Image Understanding, vol. 118, pp. 111–127, 2014, doi: https: //doi.org/10.1016/j.cviu.2013.10.007. 
[26] C. Griwodz, L. Calvet, and P. Halvorsen, “Popsift: A faithful sift implementation for realtime applications,” in Proceedings of the 9th ACM Multimedia Systems Conference, ser. MMSys ’18. New York, NY, USA: Association for Computing Machinery, 2018, p. 415–420, doi: https://doi.org/10.1145/ 3204949.3208136. 
82 
",Master_Thesis_ROB10__Drone_based__3D_Area_Localization.pdf,10
"Twophase multiexpert knowledge approach by using fuzzy clustering and rulebased system for technology evaluation of unmanned aerial vehicles 
Murat Çolak1 • I˙hsan Kaya2 • Ali Karas¸an2 • Melike Erdog˘an3 
Received: 1 March 2021 / Accepted: 27 October 2021 / Published online: 3 January 2022 � The Author(s), under exclusive licence to SpringerVerlag London Ltd., part of Springer Nature 2021 
Abstract Unmanned aerial vehicles (UAVs) are utilized in many different areas for different aims such as the beneﬁt of humanity, safety control, trafﬁc control, crop monitoring, scientiﬁc research, and commercial applications. Moreover, the UAVs are also successfully utilized for military operations, such as surveillance of an area and counterterrorism actions. Evaluating them through the technological perspective is quite signiﬁcant and should be considered from multiple perspectives. In this context, it will be more beneﬁcial to construct a methodology for an efﬁcient evaluation process. The fuzzy set theory (FST) can also be integrated into this methodology to improve its sensitiveness and ﬂexibility. In this paper, a novel methodology integrating fuzzy cmeans (FCM) clustering and fuzzy inference system (FIS) has been suggested for the technical evaluation of UAVs. While the FCM clustering algorithm has been utilized to determine the clusters, rules have been created for the FIS through expert assessments, and alternative UAV technologies have been prioritized. For the evaluation procedure, the hierarchical structure of the technology evaluation features has been determined by fusing expert knowledge, literature review, and related ISO standards. Through the FCM clustering algorithm, alternative vehicles have been clustered based on the subfeatures of each main feature. Then, FIS has been conducted by using experts’ knowledge from the ﬁelds of military technologies in UAVs and armed UAVs to obtain the technology indices of the eight UAVs locally produced and used in Turkey. The results demonstrate that the proposed methodology can be successfully applied by the managers or research and development (R&D) engineers for evaluation of the UAV technologies to consider cardinal and linguistic data. Additionally, a comparative analysis based on selforganizing map (SOM) and fuzzy kmeans algorithms has also been applied for the proposed method, and their performances have been compared. 
Keywords Unmanned aerial vehicles � Decision making � Fuzzy sets � Fuzzy cmeans clustering � Fuzzy inference system � Technology evaluation 
1 Introduction 
Unmanned aerial vehicles (UAVs), also known as drones, which are aircraft operating without human pilots, have become popular in civilian and military ﬁelds because of their versatility and ﬂexibility. They are utilized for different applications such as trafﬁc monitoring, tracking and surveillance, remote sensing, crop monitoring, military operations, and search and rescue activities. The UAVs are operated with embedded sensors, cameras, and communication equipment and can be used for situations where human intervention is risky, dangerous, impossible, or expensive [1]. On the other hand, it is necessary to consider some technical and nontechnical challenges related to the 
& I˙hsan Kaya ihkaya@yildiz.edu.tr; iekaya@yahoo.com 
1 Department of Industrial Engineering, Kocaeli University, 41380 Izmit, Kocaeli, Turkey 
2 Department of Industrial Engineering, Yildiz Technical University, 34349 Besiktas, Istanbul, Turkey 
3 Department of Industrial Engineering, Du¨zce University, 81620 Konuralp, Duzce, Turkey 
123 
Neural Computing and Applications (2022) 34:5479–5495 https://doi.org/10.1007/s00521-021-06694-0 (0123456789().,-volV)(0123456789(). ,- volV) 
meaningful outputs for the applications since the output of the clustering algorithms is suitable for the rulebased systems. This methodology will be useful for the comprehensive assessment of military UAV alternatives in terms of critical features for both operational and strategic levels. Moreover, the proposed methodology offers an efﬁcient analytical tool for researchers and managers to evaluate critical and uptodate UAV technologies. In this context, this paper has performed a prioritization study for eight UAVs that are locally produced and used in Turkey. The FCM clustering method has been used to determine the prioritized clusters with the experts’ evaluations. Through experts’ evaluations, rules have also been created for the FIS, and all alternative UAV technologies have been prioritized. Additionally, a comparative analysis has been adopted to compare performance and obtain the results of the suggested methodology. For this aim, two methods, selforganizing map (SOM) and fuzzy kmeans algorithms, have been applied. Based on the obtained results, ﬁndings and possible managerial implications are discussed. The rest of this paper has been organized as follows: In Sect. 2, a literature analysis related to existing UAV applications has been summarized and it has been aimed to indicate the writing motivation and novelty of this paper. In Sect. 3, the proposed methodology consists of FCM clustering, and FIS methods have been introduced. In Sect. 4, an application for technology evaluation and prioritization of military UAV alternatives using the proposed methodology has been realized. Besides, comparative analyses, simulations for discussions, and managerial implications realized to check the robustness of the proposed methodology have been presented. The paper has been concluded with obtained results and future research suggestions into Sect. 5. 
2 A literature analysis 
In this study, it is aimed to propose a novel fuzzybased decisionmaking methodology integrating FCM clustering and FIS methods for technology evaluation and prioritization of UAV alternatives. For this aim, the application areas of UAVs both in civil and military manner and desired to present an efﬁcient approach for the evaluation of UAV technologies due to their importance in practical applications have been analyzed. At this point, it is necessary to examine UAVrelated studies in the literature and to reveal the novelty of both our methodology and our decisionmaking problem according to existing papers realized with respect to UAV technologies. Therefore, a comprehensive literature analysis has been conducted related to UAVs. 
5480 Neural Computing and Applications (2022) 34:5479–5495 
123 
such as TOPSIS, VIKOR, EDAS, CODAS, it has been observed that deviation in one indicator in the ideal solution powerfully affects the results. For the FISs, the most essential phase of the application is constructing a rulebased system and feeding the system with meaningful inputs. Based on the mentioned papers from the literature, it is observed that the inputs are mostly determined without a veriﬁed algorithm. Therefore, it is essential to feed the rulebased system with inputs gathered from the algorithms proven their high efﬁciency. Another analysis is carried out for the determination of the decisionmaking structure of the application. UAV technologies can be classiﬁed based on several features such as weight, altitude and range, wing and rotor, and application area. On the other hand, a classiﬁcation can be made according to their ﬁeld of use, which can be categorized as agricultural usage, civilian usage, and military usage. From a general perspective, there are many disadvantages and threats to the usage of UAVs for different applications. These challenges can be ensampled as regulations and legislation, training and test facilities, social acceptance, battery technologies, sensor accuracy, and security threats and authentication [2]. 
Table 1 Analyzed studies with respect to applied techniques and data type 
Paper ID FIS ANN FCA MCDM HOM Cardinal data Linguistic data Considering uncertainty 
[9] 4 4 4 4 
[10] 4 4 4 
[11] 4 4 4 [12] 4 4 4 
[13] 4 4 
[14] 4 4 4 4 4 
[15] 4 4 4 
[16] 4 4 4 4 4 
[17] 4 4 4 4 
[18] v 4 
[19] 4 4 4 
[20] 4 4 
[21] 4 4 4 
[22] 4 4 4 4 4 
[23] 4 4 4 
[24] 4 4 4 
[25] 4 4 4 
[26] 4 4 4 4 4 [27] 4 4 
[28] 4 4 
[29] 4 4 4 4 
[30] 4 4 4 4 
Neural Computing and Applications (2022) 34:5479–5495 5481 
123 
(1) What are the essential criteria and subcriteria playing a role towards the technology evaluation of the UAV alternatives? (2) How could involve both quantitative and qualitative data in a decisionmaking procedure considering the knowledge and expertbased evaluations? (3) What are the possible managerial implications based on the comparisons and discussions for the carried applications? Therefore, in this paper, differently from conventional prioritization and ranking methods, based on the constructed context, an integrated methodology consists of fuzzy cmeans clustering approach, and a fuzzy inference system has been proposed. The proposed methodology has also been compared with the neural network approach, selforganizing map (SOM), and clustering method, fuzzy kmeans clustering to check and discuss its advantages. Moreover, the implications of the results and the methodology dynamics have been presented and discussed. 
3 The proposed methodology 
In this section, the fuzzy clustering concept, FCM clustering algorithm, FIS, and the proposed methodology utilized for technology evaluation and prioritization of UAV alternatives have been introduced. 
3.1 Fuzzy clustering 
Before presenting the preliminaries of fuzzy clustering, a broad explanation for the ﬁeld of machine learning and its components will be useful. Since the machine learning techniques aim to improve the accuracy of the constructed model by using the eligible data set, many algorithms are introduced in the literature to do so. The performed algorithms are applied in many ﬁelds such as medical diagnosis, security applications, forecasting of the energy requirements, and pathing. Based on the data set and the aim of the decisionmakers, the topic is divided into three main areas, supervised, unsupervised, and reinforcement learning [32]. Unsupervised learning aims to distinguish a set of targets into structural groups, which can be classes or clusters, to identify their memberships with the determined criteria [33]. By contrast with supervised learning or reinforcement learning, there are no explicit target outputs or environmental evaluations associated with each input [34]. Since one of the most important phases of unsupervised learning algorithms is to determine the number of groups and their belongings, clustering techniques are remarkable to apply. In today’s world, a large amount of information has been processed for the decisionmaking procedure of humans based on their interaction level among themselves and the environment. Through these procedures, 
5482 Neural Computing and Applications (2022) 34:5479–5495 
123 
belong to a set or not; in optimization, a solution is either feasible or not; and in conventional Boolean logic, a statement can be true or false but nothing in between [41]. However, the states and the actions are mostly not precise and cannot be deﬁned in a deterministic environment with a single crisp number in reallife conditions [42]. Therefore, utilizing fuzzy logic in clustering algorithms can be an efﬁcient way to represent uncertainty while obtaining meaningful outputs. 
3.2 Fuzzy cmeans clustering 
FCM clustering is proposed for systems where objects can belong to more than one cluster. A membership function is utilized for each of the objects to indicate their degree of belongings to the determined clusters [43]. The mathematical modeling of the algorithm is presented as follows [43]: Partitioned set of input data points: X ¼ x1; x2; . . .; xn f g. 
Clusters : C ¼ C1; C2; . . .; Ck f g 
where k is the number of predeﬁned clusters and n is the number of samples. 
Table 2 Hierarchical structure of evaluation features for the UAV technologies 
Safety measures Cognitive functionality Endurance to environmental conditions Standard qualiﬁcations Nationallevel importance 
Availability of high risk rated components [64] Fault detection and vehicle health management [51, 52] 
Variation of mission performance with respect to altitude (wind, temperature, humidity, precipitation) [64] 
Wingspan [57–59] Software dependability [55] 
Acceptable level of risks to allow planned UAS operations [64] 
Situation awareness [51] The effects of clouds for visual observation [64] Length [57–59] Hardware dependability 
Availability of obstructions for a UAS operation [64] Communication [51, 60] Variation of battery/fuel performance with respect to altitude (wind, icing, humidity, precipitation) [64] 
Payload capacity [57] Availability of energy supply systems [8] 
Availability of safety management system [51] Payload management [51, 60] Ability to identify turbulence [64] Endurance [57–59, 61] International reputation 
Effectiveness of risk control systems [51] Guidance, navigation and control [8, 51, 62] Durability limits in case of air hazards [64] Cruise speed [51, 57–61, 63] Failure anticipation and reaction [51] Operational altitude [57] 
Flight planning and decision making [51] Range [57–59] 
Information/network management [51, 52, 54] 
Contingency management [51] 
Target coverage and detection performance [54] 
Neural Computing and Applications (2022) 34:5479–5495 5483 
123 
Jm ¼ X n 
i¼1 
X k 
j¼1 lm ji D xilj � � ð1Þ 
where m, greater than 1, is the fuzzy index (for the application, m is set to 2), xi is the ith data point, D is the Euclidian distance function, and lm ji is the membership degree of xi for the cluster j. XSubject to c 
i¼1 lij ¼ 1; 1 � j � n ð2Þ 
Jm ¼ X n 
i¼1 
X k 
j¼1 lm ji D xi; lj � � ð3Þ 
By using the Lagrange multiplier method, the objective function is updated as in Eq. (4) as follows: 
J ¼ X n 
i¼1 
X k 
j¼1 lm ji D xi; lj � � þ X k 
j¼1 kk X n 
i¼1 lji � 1 
  ! 
ð4Þ 
where k ¼ k1; k2; . . .; kn ½ �T is the Lagrange multiplier. Through that, Eqs. (2–3) are updated as follows: 
lik ¼ 1= X c 
j¼1 
dik djk 
� �2= m�1 ð Þ 1 � i � n; 1 � k � c ð5Þ 
where dik is the distance between the cluster k and element i, djk is the distance between the cluster k and element j. 
cj ¼ 
Pn k¼1 xk lik ð Þm Pn k¼1 lik ð Þm ; 1 � k � c ð6Þ 
where cj is the jth cluster center. The steps of the FCM clustering are presented in Algorithm 1 as follows: 
The process continues until the determined convergence condition is satisﬁed. 
3.3 Fuzzy inference system 
FISs are the expert knowledge systems based on if–then rules, in which premises and conclusions are expressed employing linguistic terms [44]. Zadeh introduced computing with words (CWWs), a methodology used for uncertain environments containing humanknowledge data deﬁned with linguistic variables [45]. By using the inputs in the form of linguistic variables, FISs enable the process of the data by employing the degree of membership functions. There are two types of FISs that use different procedures to compute the output data: Mamdani’s FIS and Takagi– Sugeno’s FIS [46, 47]. Both methods are useful to fulﬁll the aimed objectives during the decisionmaking processes, such as classiﬁcation of the samples, diagnoses, process controls, ranking the alternative solutions [48]. For evaluating the UAV technologies, Mamdani’s FIS has been utilized to apply the integrated methodology. The steps of Mamdani’s FIS are presented as in Algorithm 2: 
5484 Neural Computing and Applications (2022) 34:5479–5495 
123 
The proposed methodology is an integrated approach consisting of FCM clustering and FIS. For the ﬁrst phase of the proposed methodology, a hierarchical structure of the technology evaluation features is determined by combining expert knowledge, literature review, and ISO standards. There are two levels in the hierarchical structure. In the ﬁrst level, there are main features of the UAVs, which are the input parameters of the FIS. There are subfactors related to the corresponded main features in the second level, which are the evaluation criteria of the alternatives. For the second phase, a consensus of the experts evaluates the UAVs with respect to evaluation criteria to construct the dataset of the FCM clustering. For the third phase, Algorithm 1 is run concerning each main criterion to determine the clusters. Based on the clusters, each of the UAVs is clustered, and the corresponded membership degree is assigned. Therefore, for each of the UAV 
alternatives, there are different clusters and memberships for each of the ﬁrst level features as outputs. By the way, clusters are put in order from most desired one to least by considering current situations of the most efﬁcient technologies of the UAVs. For the next phase, these outputs are used as inputs of the FIS. The FIS procedure is carried out using experts’ knowledge from military technologies in UAVs and armed UAVs. The rules are constructed through their evaluations. After the FIS procedure presented in Algorithm 2 is run for each of the evaluated alternatives, the prioritization of the UAV technologies is completed, and the most appropriate UAV system is determined. The ﬂowchart of the proposed methodology is given in Fig. 1 for a visual representation and to summarize the decisionmaking system. 
Neural Computing and Applications (2022) 34:5479–5495 5485 
123 
In this section, the proposed methodology has been applied for technology evaluation and prioritization of the unmanned aerial vehicles (UAVs) that are utilized for military operations. Since UAVs have great potential in the defense industry for both defensive and attack aims, several vehicles are introduced to the market. Eight of them have been selected, and ﬁve main features have been determined, which are F1-Safety Measures [8, 52, 64], F2- Cognitive Functionality [51, 53–59], F3-Endurance to Environmental Conditions [50, 55], F4-Standard Qualiﬁ- cations, F5-Nationallevel Importance for the evaluation. Moreover, when the characteristics of UAVs are examined, the determined subfeatures can be considered for the evaluation of the UAV technologies presented in Table 2. Among them, F4-Standard Qualiﬁcations consist of six subfeatures, and the value of them can be obtained via company reports as in Table 3. A consensusbased decisionmaking procedure has been conducted for the evaluations through this structure. Since evaluating the subfactors of the F1-Safety Measures, F2- Cognitive Functionality, F3-Endurance to Environmental Conditions, and F5-Nationallevel Importance are subjective, computing with words concept has been considered. Based on that, the linguistic scales presented in Table 4 have been constructed for the evaluation process of the consensus. After gathering input data for the FCM clustering algorithm based on UAV evaluations with respect to the determined features, Algorithm 1 has been applied for each of the main features. The input data for the FCM clustering algorithm has been given in Table 11 in the Appendix Section. Through Algorithm 1, considered vehicles have been assigned to determined clusters. For the prioritization of the clusters, the average value of the distances to the clusters has been calculated. Since these vehicles are currently operational on the ground, the cluster that has the minimum average value has been considered as the accomplished one, which means it has the ﬁrst rank. Clusters have been prioritized based on these average values. After the calculations for each of the main features, the number of clusters has been determined based on the MAE values and Elbow method. The MAE values based on the cluster numbers with respect to main features are presented in Table 5 as follows: Elbow method has been used to determine the appropriate number of clusters which is presented in Fig. 2. 
Through the determined numbers, clusters and membership value of the vehicles to those clusters have been obtained as in Table 6. For the next phase, a rulebased system has been constructed based on the consensus evaluations. The constructed rulebased system has been given in Table 7. After that, steps of Algorithm 2 are applied to obtain the ﬁnal results. Through the calculations, the technology indices of the vehicles have been calculated as in Table 8. Through the calculations, V6 has been determined as the ﬁrst vehicle among the others. Six of the vehicles have values between ﬁve and six; therefore, they can be alternatives to each other in a costbased system. Vehicles V5 and V7 are out of the contest compared to index values with the other six alternative UAVs. 
4.1 A comparative analysis 
Selforganizing map (SOM) and fuzzy kmeans algorithms have been applied for the comparative analysis. To do that, each of the main feature datasets has been used as the inputs of the algorithms. Through the calculations, improvement rates on the errors of the algorithms have been calculated based on the number of clusters. The results obtained from the comparisons have been summarized in Table 9. Based on the results, Fuzzy cmeans has been determined as the most sensitive algorithm based on the improvement rate on the errors with respect to the number of clusters. By increasing the number of clusters, all algorithms performed the same pattern, which is decreasing the errors. Since the number of evaluated UAVs is equal to eight, overlapping values, especially starting from ﬁve clusters, are observed. This is another reason why the number of clusters is determined between the two and four for the features. Comparison between SOM and fuzzy cmeans algorithms yields that an increasing number of features increase the improvement rate on errors of the SOM algorithm better against the fuzzy cmeans algorithm. Moreover, since the SOM algorithm assigns the samples to the clusters as 0 or 1, it may not reﬂect the nature of the application since the UAVs have similarities among them and are not fully distinguished. Fuzzy cmeans algorithm outputs can perform clustering between not only 0 and 1 but also between the values of them. Moreover, this process is carried out by an objective function, which aims to minimize the distances of the samples to the cluster centers. Therefore, based on the distance, membership functions are calculated, and the clusters of the samples are determined. This is the most advantageous part of the fuzzy cmeans algorithm against the SOM when the nature of the problem is considered. 
5486 Neural Computing and Applications (2022) 34:5479–5495 
123 
MAE value than the fuzzy cmeans. When the number of clusters is increased, fuzzy cmeans is performed better than the fuzzy kmeans. This may yield to the importance of the initial solution. For further research, the initial solution for the algorithms can be obtained by using a heuristic algorithm to check its effects on the iterations. 
Expert Knowledge 
International Standards 
Literature Review 
Database 
Determination of the features  for the evaluation of the  UAVs’ Technology 
Gathering input data for the fuzzy cmeans  based on the consensus evaluations of UAVs’  with respect to determined features 
Application of fuzzy cmeans algorithm 
Determination of the  Clusters 
Validation of the number of  clusters by using Elbow Method 
Assigning membership degree to the UAVs with  respect to distance measurement 
Construction of rules for  fuzzy inference system 
x 
F1 F2 F3 ... Fn 
Fuzzy Inference System 
Defuzzification Prioritization of the UAV  technologies 
Inputs of the fuzzy  inference system 
PhaseI 
PhaseIIPhaseII 
PhaseIII PhaseIV 
Fig. 1 Flowchart of the proposed methodology 
Neural Computing and Applications (2022) 34:5479–5495 5487 
123 
Wingspan (meter) Length (meter) Payload capacity (kg) Endurance (min) Max cruise speed (knot) Operational altitude (feet) 
20 12.2 1350 1440 195 30,000 
12 6.5 150 1620 120 18,000 
2 1.2 10 80 40 2000 
24 12 750 1440 97.192 7600 
17 8 200 1920 117 300,000 
7 6 80 720 108 12,000 
1.5 2.3 10 60 400 15,000 
10.5 6.5 70 1200 80 22,000 
Table 4 Constructed linguistic scales for the subjective evaluations 
Linguistic term Abbreviation Corresponded value Linguistic term Abbreviation Corresponded value 
Scale for F1-safety measures Scale for F2-cognitive functionality Very reliable VRa 1 Masterpiece Mp 1 
Reliable Ra 0.8 Very Functional VFn 0.8 
Less reliable LRa 0.6 Functional Fn 0.6 
Hazardous Hd 0.4 Less Functional LFn 0.4 
Very hazardous VHd 0.2 Ineffective Iet 0.2 
Fatal results FRt 0 Inoperable Ioa 0 
Scale for F3-endurance to environmental conditions Scale for F5-nationallevel importance 
Very high endurability VHE 1 Outstanding Osd 1 
High endurability HE 0.8 Critical Crc 0.75 
Average endurability AE 0.6 Major Effect MaE 0.5 
Low endurability LE 0.4 Minor Effect MiE 0.25 
Very low endurability VLE 0.2 Negligible Ng 0 
Unendurable Uea 0 
Table 5 Statistical results of the application based on the MAE values 
Number of clusters Objective function (MAE) Number of clusters Objective function (MAE) Number of clusters Objective function (MAE) 
Safety measures feature Cognitive functionality feature Endurance to EC feature 
2 1.6735 2 3.7175 1 3.0567 3 0.9452 3 2.2202 2 1.9733 
4 0.5553 4 1.2397 3 1.2354 
5 0.2701 5 1.0409 4 0.7915 
Standard qualiﬁcations feature Nationallevel importance feature 
1 2.9823 2 1.4659 
2 0.3785 3 0.8736 
3 0.1198 4 0.4754 
4 0.0563 
Bold lines demonstrate the determined number of clusters for the related features 
5488 Neural Computing and Applications (2022) 34:5479–5495 
123 
Fig. 2 Elbow charts of the main features with respect to MAE values of the clusters 
Table 6 Determined clusters and membership values of the vehicles to these clusters 
C1: WQf C2: Qf C3: Acp C1: LD C2: D C3: MD C4: Acp 
wrt safety measures wrt cognitive functionality 
V1 0.305 0.260 0.435 V1 0.198 0.218 0.222 0.362 
V2 0.249 0.143 0.608 V2 0.162 0.159 0.502 0.177 
V3 0.375 0.245 0.380 V3 0.146 0.149 0.200 0.505 
V4 0.623 0.149 0.228 V4 0.113 0.728 0.079 0.080 
V5 0.076 0.855 0.069 V5 0.170 0.162 0.190 0.478 
V6 0.363 0.153 0.484 V6 0.119 0.118 0.600 0.163 V7 0.696 0.127 0.177 V7 0.790 0.086 0.063 0.061 
V8 0.313 0.236 0.451 V8 0.227 0.264 0.254 0.255 
C1: D C2: Md C3: Acp C1: Acc C2: Acp C1: Acc C2: Acp 
wrt endurance to environmental conditions wrt standard qualiﬁcations wrt nationallevel importance 
V1 0.224 0.346 0.430 V1 0.723 0.277 V1 0.353 0.647 
V2 0.230 0.530 0.240 V2 0.256 0.744 V2 0.814 0.186 
V3 0.145 0.678 0.177 V3 0.252 0.748 V3 0.462 0.538 
V4 0.166 0.162 0.672 V4 0.680 0.320 V4 0.500 0.500 
V5 0.588 0.208 0.204 V5 0.520 0.480 V5 0.568 0.432 
V6 0.266 0.422 0.312 V6 0.117 0.883 V6 0.295 0.705 
V7 0.412 0.268 0.320 V7 0.337 0.663 V7 0.384 0.616 V8 0.424 0.229 0.347 V8 0.177 0.823 V8 0.699 0.301 
wrt: with respect to; Qf: Qualiﬁed; WQf: wellqualiﬁed; LD: Least Desirable; D: Desirable; MD: Most Desirable; Acc: Acceptable; Acp: Accomplished 
Neural Computing and Applications (2022) 34:5479–5495 5489 
123 
Different simulations based on the different input values have been carried out for the system’s performance in terms of robustness of the results. The indices based on the simulations have been calculated as seen in Table 10. Based on the created artiﬁcial inputs, indices are gradually decreased except in one case, which is Simulation 2. The visual representation of the decrease can be observed in Fig. 3. Through the results, inconsistency in the decrease is discussed with the experts. Since it is a small part of the rulebased system, it is neglected. Therefore, it is revealed that the hesitancy of the experts is also an issue to deal with in the system. Another issue is the prioritization of the clusters. Since the prioritization is based on the minimum average value of the distances of the vehicles to the clusters, we conducted simulations to observe the variations. In some of the simulations, it is observed that average values are very close to each other. In that case, prioritization of them is not applicable. In other words, both of the clusters are equal values based on our assumptions. Therefore, they can be of equal importance while constructing the rules in the rulebased system. 
4.3 Managerial implications 
The proposed methodology can be effectively used by the managers or R&D engineers to evaluate the UAV technologies to consider cardinal data and linguistic data. Since the constructed decisionmaking framework has a complex hierarchical structure, the results of the proposed decisionmaking model can be an efﬁcient solution by combining different perspectives. Through the application, the below issues should be carefully investigated and decided. Firstly, characteristics, which are expertise in the autonomous vehicles (also consisting of ground and marine vehicles), assignment for designing/buying/construction projects for the UAVs (before/ongoing), education level, job description in the company, privacy policies of the company (willing to share the information) are considered for the construction of the consensus. Regarding a UAV company, information about the design of the UAVs and their components (both hardware and software components) are highly conﬁdential and prohibited to share. Therefore, the gathered information from the companies is quite limited. Based on that, in our application, the consensus has been constructed by involving academicians and researchers working in this area and specialists from the companies. We believe that, if the number of specialists from the company and the shared data will increase, the 
Table 7 Constructed rulebased system based on the evaluations of the consensus 
VL very low; L low; M medium; H high; VH very high 
5490 Neural Computing and Applications (2022) 34:5479–5495 
123 
the vehicle’s visibility and center of gravity. Therefore, constructing a rulebased system is mainly based on the expert knowledge and the requirements of the decisionmakers. In our system, the constructed context is for the military operations, which are surveillance of an area and counterterrorism actions. For example, standard qualiﬁ- cations can be more important for a supply chain logistic system in which the vehicle transports the goods to the determined destination. In such a case, we believe nationallevel importance can be considered as out of context since the software dependability, hardware dependability, and international reputation are negligible in such a mission. But in a military context, these are crucial for a vehicle for the sustainability of the system and the averseness against the terrorist attacks and inﬁltration from the borders. The last issue that needs to be explained is the absence of cost features in the constructed context. As a military operation, the most important issue is being killed or wounded in action. Through this, the vehicle’s value does not matter when compared to the soldiers’ or citizens’ lives. But, at a realistic level, purchasing power or construction power is also an important issue. Therefore, our system provides technology indices of the UAVs to prioritize them with no cost feature to enable companies/institutions to decide according to their budget. The limitations of this study can be summarized based on the abovementioned issues. The analysis is performed for eight UAVs produced in Turkey by four military companies. Through this concept, the consensus origin is selected as Turkey to reﬂect the study’s environment better. Through that, for a comparison with other military UAVs in the market, a new consensus should be organized by 
Table 8 The obtained results of the application 
VL L M H VH Index 
V1 0.224 0.260 0.346 0.346 0.362 5.707 
V2 0.230 0.249 0.256 0.502 0.240 5.532 
V3 0.200 0.245 0.375 0.380 0.177 5.258 
V4 0.166 0.166 0.500 0.228 0.228 5.464 
V5 0.479 0.208 0.204 0.204 0.069 3.646 
V6 0.153 0.266 0.363 0.422 0.312 5.830 
V7 0.384 0.412 0.320 0.177 0.086 3.855 
V8 0.236 0.264 0.264 0.264 0.264 5.290 
Table 9 Results of the comparisons based on the improvement rate on errors 
Cluster Improvement rate on errors Cluster Improvement rate on errors 
SOM Fuzzy cmeans Fuzzy kmeans SOM Fuzzy cmeans Fuzzy kmeans 
Results based on safety measures feature Results based on cognitive functionality feature 
From 2 to 3 26% 44% 12% From 2 to 3 32% 40% 5% 
From 3 to 4 21% 41% 6% From 3 to 4 53% 44% 8% 
From 4 to 5 33% 51% 31% From 4 to 5 31% 16% 26% 
Total improvement 79% 136% 49% Total improvement 116% 100% 39% 
Results based on cognitive functionality feature Results based on cognitive functionality feature 
From 2 to 3 29% 37% 4% From 2 to 3 52% 68% 3% 
From 3 to 4 36% 36% 4% From 3 to 4 47% 53% 18% 
Total improvement 65% 73% 8% Total improvement 99% 121% 21% 
Results based on cognitive functionality feature 
From 2 to 3 48% 40% 4% 
From 3 to 4 43% 46% 12% Total improvement 92% 86% 15% 
Neural Computing and Applications (2022) 34:5479–5495 5491 
123 
5 Conclusions 
UAVs are vehicles that can ﬂy on a path by directly programming or remotely operating. UAVs are divided into three categories in terms of their various uses: military, 
scientiﬁc research, and commercial applications. Popular applications include trafﬁc monitoring, monitoring and surveillance, remote sensing, crop monitoring, and search and rescue activities. In this study, the constructed framework concerns military applications by considering different perspectives based on the available cardinal data and knowledge of academicians and specialists from the ﬁeld. When addressing the growing uses of UAVs, some technical and nontechnical features of this technology need to be considered. UAV technologies are distinguished according to various features based on addressing problems. In this study, apart from the features assigned with cardinal data such as weight, altitude, endurance, and speed, the features that can be evaluated with a scale consisting of linguistic terms are considered. An integrated methodology involves FCM clustering, and FIS has been proposed to evaluate and prioritize the UAV alternatives with this perspective. Eight UAVs that are used for military applications produced in Turkey have been speciﬁed as alternatives. For the evaluation process, ﬁve main features have been determined as ‘‘Safety Measures’’, ‘‘Cognitive Functionality’’, ‘‘Endurance to Environmental Conditions’’, ‘‘Standard Qualiﬁcations’’, and ‘‘Nationallevel Importance’’ that are coded respectively as F1, F2, F3, F4, and F5. Besides, thirtyone subfeatures have been adopted under these main features to evaluate the UAV technologies. In light of this information, technology indices have been obtained to prioritize the alternative UAVs. Moreover, different scenarios have been simulated to check the robustness of the proposed methodology. Through the results and discussions, managers and researchers can make helpful inferences by using the proposed methodology for prioritizing UAVs by considering the constructed context. For further studies, the constructed system can be adapted to supply chain systems. In that case, the constructed feature hierarchy can be changed based on the 
Table 10 Input values and the calculated indexes based on the simulations 
SM CF EEC SQ NL Index 
Qf WQf Acp LD D MD Acp D Md Acp Acc Acp Acc Acp 
S1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 10.00 
S2 0 0.10 0.90 0 0 0.10 0.90 0 0.10 0.90 0.10 0.90 0.10 0.90 8.75 
S3 0.05 0.05 0.90 0 0.05 0.05 0.90 0.05 0.05 0.90 0.10 0.90 0.10 0.90 8.91 
S4 0.05 0.10 0.85 0 0.05 0.10 0.85 0.05 0.10 0.85 0.15 0.85 0.15 0.85 8.38 
S5 0.85 0.10 0.05 0.85 0.10 0.05 0 0.85 0.10 0.05 0.85 0.15 0.85 0.15 2.17 
S6 0.90 0.05 0.05 0.90 0.05 0.05 0 0.90 0.05 0.05 0.90 0.10 0.90 0.10 1.94 
S7 0.90 0.10 0 0.90 0.10 0 0 0.90 0.10 0 0.90 0.10 0.90 0.10 1.55 S8 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1.00 
10.00 8.75 8.91 8.38 
2.17 1.94 1.55 1.00 
0.00 
2.00 
4.00 
6.00 
8.00 
10.00 
12.00 
S1 S2 S3 S4 S5 S6 S7 S8 
Fig. 3 Visual representation of the variation in indexes for simulations 
5492 Neural Computing and Applications (2022) 34:5479–5495 
123 
Appendix 
See Table 11. 
Funding This study has not been funded. 
Declarations 
Conflict of interest All authors state that there is no conflict of interest. 
Ethical approval This article does not contain any studies with human participants or animals performed by any of the authors. 
References 
1. Saleem Y, Rehmani MH, Zeadally S (2015) Integration of cognitive radio technology with unmanned aerial vehicles: issues, 
Table 11 Evaluations of the consensus with respect to main features for the determination of the clusters for fuzzy cmeans clustering 
F21 F22 F23 F24 F25 F26 F27 F28 F29 F210 F51 F52 F53 F54 
V1 Iet VFn Mp LFn Iet Iet LFn LFn LFn VFn V1 Crc Ng MiE Crc 
V2 LFn LFn VFn LFn VFn VFn Mp Iet LFn LFn V2 Crc Osd MaE MaE 
V3 Iet VFn Mp LFn VFn Iet Ioa Ioa VFn Mp V3 Ng MaE MiE MiE 
V4 Mp Mp Fn Fn Ioa Iet Fn Fn VFn Iet V4 Osd MaE Ng Crc 
V5 Ioa Mp LFn VFn Fn LFn Iet Iet Fn Mp V5 Crc Crc Crc Ng 
V6 LFn Iet VFn Fn Mp LFn Fn Ioa Fn LFn V6 Crc Ng MaE Ng 
V7 VFn VFn Ioa Iet Ioa Iet Fn Ioa LFn LFn V7 Osd MiE Crc MiE 
V8 Mp Ioa Iet Mp VFn Ioa Iet Mp VFn Mp V8 Osd Osd Crc Crc 
F11 F12 F13 F14 F15 F31 F32 F33 F34 F35 
V1 LRa VHd VRa Hd FRt V1 HE LE Uea HE LE 
V2 VRa VHd Hd LRa LRa V2 AE VHE AE AE LE 
V3 LRa Hd LRa Hd Ra V3 VLE VHE VLE AE AE 
V4 VHd VHd FRt VHd LRa V4 AE VLE Uea VLE HE 
V5 Hd VRa VRa FRt VRa V5 VLE LE VHE VLE VHE 
V6 Ra VHd FRt LRa Hd V6 Uea AE VLE LE HE 
V7 Hd Hd VHd VHd Ra V7 Uea Uea AE Uea VLE 
V8 VRa LRa Hd FRt VHd V8 VHE Uea HE Uea HE 
F1 Feature for safety measures; F2 Feature for cognitive functionality; F3 Feature for endurance to environmental conditions; F5 Feature for nationallevel importance; V vehicle 
Neural Computing and Applications (2022) 34:5479–5495 5493 
123 
2. Aminifar F, Rahmatian F (2020) Unmanned aerial vehicles in modern power systems: technologies, use cases, outlooks, and challenges. IEEE Electrif Mag 8(4):107–116. https://doi.org/10. 1109/MELE.2020.3026505 
3. Sˇkrinjar JP, Sˇkorput P, Furdic´ M (2019) Application of unmanned aerial vehicles. In: Logistic processesin lecture notes in networks and systems, vol. 42. Springer, 2019, pp 359–366 4. Gao Y, Li D (2019) Consensus evaluation method of multigroundtarget threat for unmanned aerial vehicle swarm based on heterogeneous group decision making. Comput Electr Eng 74:223–232. https://doi.org/10.1016/j.compeleceng.2019.01.019 
5. Yu X, Li C, Yen GG (2020) A kneeguided differential evolution algorithm for unmanned aerial vehicle path planning in disaster management. Appl Soft Comput. https://doi.org/10.1016/j.asoc. 2020.106857 
6. Hang Wang B, Bo Wang D, Anwar Ali Z, Ting Ting B, Wang H (2019) An overview of various kinds of wind effects on unmanned aerial vehicle. Meas Control 52(8):731–739. https:// doi.org/10.1177/0020294019847688 
7. Yan F, Zhu X, Zhou Z, Tang Y (2019) Heterogeneous multiunmanned aerial vehicle task planning: simultaneous attacks on targets using the Pythagorean hodograph curve. Proc Inst Mech Eng Part G J Aerosp Eng 233(13):4735–4749. https://doi.org/10. 1177/0954410019829368 
8. Shakhatreh H et al (2019) Unmanned aerial vehicles (UAVs): a survey on civil applications and key research challenges. IEEE Access 7:48572–48634. https://doi.org/10.1109/ACCESS.2019. 2909530 
9. Kurnaz S, Cetin O, Kaynak O (2010) Adaptive neurofuzzy inference system based autonomous ﬂight control of unmanned air vehicles. Expert Syst Appl 37(2):1229–1234. https://doi.org/ 10.1016/j.eswa.2009.06.009 
10. Yang TW, et al (2012) Overhead power line detection from UAV video images. In: IEEE conference publication, in 2012 19th international conference on mechatronics and machine vision in practice (M2VIP), 2012 11. Fallahi K, Leung H, Chandana S (2009) An integrated ACOAHP approach for resource management optimization. In: Conference proceedings—IEEE international conference on systems, man and cybernetics, pp 4335–4340. https://doi.org/10.1109/ICSMC. 2009.5346794 
12. Sun G, Ma H, Zhao D, Zhang F, Jia L, Sun J (2015) Oil spill image segmentation based on fuzzy Cmeans algorithm, 2015, pp 406–409. https://doi.org/10.2991/csic-15.2015.98 
13. Zhao S, Wang X, Zhang D, Shen L (2017) modelfree fuzzy adaptive control of the heading angle of ﬁxedwing unmanned aerial vehicles. J Aerosp Eng 30(4):04017019. https://doi.org/10. 1061/(asce)as.1943-5525.0000730 
14. Chen L, Mantegh I, He T, Xie W (2020) Fuzzy kinodynamic RRT: a dynamic path planning and obstacle avoidance method. In: 2020 International conference on unmanned aircraft systems, ICUAS 2020, pp 188–195. https://doi.org/10.1109/ICUAS48674. 2020.9213964 
15. Sathyan A, Ernest ND, Cohen K (2016) An efﬁcient genetic fuzzy approach to UAV swarm routing. Unmanned Syst 04(02):117–127. https://doi.org/10.1142/s2301385016500011 
16. Woz´niak M, Połap D (2020) Intelligent home systems for ubiquitous user support by using neural networks and rulebased approach. IEEE Trans Ind Inf 16(4):2651–2658. https://doi.org/ 10.1109/TII.2019.2951089 
17. Goswami M, Arya R, Prateek (2021) UAV communication in FANETs with metaheuristic techniques. In: Advances in intelligent systems and computing, 2021, vol 1162, pp 1–11. https://doi. org/10.1007/978-981-15-4851-2_1 
18. Petkovics I, Simon J, Petkovics A, Covic Z (2017) Selection of unmanned aerial vehicle for precision agriculture with multicriteria decision making algorithm. In: SISY 2017—IEEE 15th international symposium on intelligent systems and informatics, proceedings, pp 151–155. https://doi.org/10.1109/SISY.2017. 8080543 
19. Korytkowski M, Scherer R, Szajerman D, Polap D, Wozniak M (2020) Efﬁcient visual classiﬁcation by fuzzy rules. In: IEEE international conference on fuzzy systems, 2020, vol. 2020-July. https://doi.org/10.1109/FUZZ48607.2020.9177777 
20. Ansari RI, Ashraf N, Politis C (2020) An energyaware distributed open market model for UAVassisted communications. In: IEEE vehicular technology conference, 2020, vol. 2020-May. https://doi.org/10.1109/VTC2020-Spring48590.2020.9128475 
21. Ferdaus MM, Anavatti SG, Garratt MA, Pratama M (2017) Fuzzy clustering based modelling and adaptive controlling of a ﬂapping wing micro air vehicle. In: 2017 IEEE symposium series on computational intelligence, SSCI 2017—proceedings, 2018, vol. 2018-January, pp 1–6, doi: https://doi.org/10.1109/SSCI.2017. 8280969 
22. Dorzhigulov A, Bissengaliuly B, Spencer BF, Kim J, James AP (2018) ANFIS based quadrotor drone altitude control implementation on Raspberry Pi platform. Analog Integr Circuits Signal Process 95(3):435–445. https://doi.org/10.1007/s10470- 018-1159-8 
23. Ercan C, Gencer C (2018) A decision support system for dynamic heterogeneous unmanned aerial system ﬂeets, Sep. 2018 24. Raj A, Sah B (2019) Analyzing critical success factors for implementation of drones in the logistics sector using greyDEMATEL based approach. Comput Ind Eng. https://doi.org/10. 1016/j.cie.2019.106118 
25. Yang Z, Pan C, Wang K, ShikhBahaei M (2019) Energy efﬁcient resource allocation in UAVenabled mobile edge computing networks. IEEE Trans Wirel Commun 18(9):4576–4589. https:// doi.org/10.1109/TWC.2019.2927313 
26. Pratama M, Anavatti SG, Garratt M, Lughofer E (2013) Online identiﬁcation of complex multiinputmultioutput system based on generic evolving neurofuzzy inference system. In: Proceedings of the 2013 IEEE conference on evolving and adaptive intelligent systems, EAIS 2013. 2013 IEEE symposium series on computational intelligence, SSCI 2013, pp 106–113. https://doi. org/10.1109/EAIS.2013.6604112 
27. Messous MA, Sedjelmaci H, Senouci SM (2017) Implementing an emerging mobility model for a ﬂeet of UAVs based on a fuzzy logic inference system. Pervasive Mob Comput 42:393–410. https://doi.org/10.1016/j.pmcj.2017.06.007 
28. Dovgal VA (2020) Decisionmaking for placing unmanned aerial vehicles to implementation of analyzing cloud computing cooperation applied to information processing. In: Proceedings—2020 international conference on industrial engineering, applications and manufacturing, ICIEAM 2020, 2020. https://doi.org/10.1109/ ICIEAM48468.2020.9111975 
29. Zhong Y, Yao P, Sun Y (2016) Decisionmaking allocation method in manned/unmanned combat aerial vehicle cooperative engagement, Xitong Gongcheng Lilun yu Shijian/System Eng. Theory Pract 36(11):2984–2992. https://doi.org/10.12011/1000- 6788(2016)11-2984-09 
30. Wozniak M, Zielonka A, Sikora A, Piran MJ, Alamri A (2020) 6Genabled IoT home environment control using fuzzy rules. IEEE Internet Things J. https://doi.org/10.1109/JIOT.2020. 3044940 
31. Nawaz H, Ali HM, Massan SUR (2019) Applications of unmanned aerial vehicles: a review. In: 3C Tecnologı´a. Glosas de innovacio´n aplicadas a la pyme. Special Issue, 2019, 85–105. https://doi.org/10.17993/3ctecno.2019.specialissue3.85-105 
5494 Neural Computing and Applications (2022) 34:5479–5495 
123 
37 Cherkassky V, Mulier FM (2007) Learning from data: concepts, theory, and methods, 2nd edn. Wiley, Hoboken 38. Gustafson DE, Kessel WC (1978) Fuzzy clustering with a fuzzy covariance matrix. In: Proceedings of the IEEE conference on decision and control, pp 761–766. https://doi.org/10.1109/cdc. 1978.268028 
39. Gath I, Geva AB (1989) Unsupervised optimal fuzzy clustering. IEEE Trans Pattern Anal Mach Intell 11(7):773–780. https://doi. org/10.1109/34.192473 
40. Zadeh LA (1965) Fuzzy sets. Inf Control 8(3):338–353. https:// doi.org/10.1016/S0019-9958(65)90241-X 
41. Zimmermann HJ (2001) Fuzzy set theory - and its applications. Springer, Amsterdam 42. Karas¸an A, Kahraman C (2018) Intervalvalued neutrosophic extension of EDAS method. Adv Intell Syst Comput 642:343–357. https://doi.org/10.1007/978-3-319-66824-6_31 
43. Bezdek JC (1981) Pattern recognition with fuzzy objective function algorithms. Springer, New York 44 Jouffe L (1998) Fuzzy inference system learning by reinforcement methods. IEEE Trans Syst Man Cybern Part C Appl Rev 28(3):338–355. https://doi.org/10.1109/5326.704563 
45. Zadeh LA (1996) Fuzzy logic = computing with words. IEEE Trans Fuzzy Syst 4(2):103–111. https://doi.org/10.1109/91. 493904 
46. Mamdani EH, Assilian S (1975) An experiment in linguistic synthesis with a fuzzy logic controller. Int J Man Mach Stud 7(1):1–13. https://doi.org/10.1016/S0020-7373(75)80002-2 
47 Takagi T, Sugeno M (1985) Fuzzy identiﬁcation of systems and its applications to modeling and control. IEEE Trans Syst Man Cybern 15(1):116–132. https://doi.org/10.1109/TSMC.1985. 6313399 
48. Guillaume S (2001) Designing fuzzy inference systems from data: an interpretabilityoriented review. IEEE Trans Fuzzy Syst 9(3):426–443. https://doi.org/10.1109/91.928739 
49. Zeng J, An M, Smith NJ (2007) Application of a fuzzy based decisionmaking methodology to construction project risk assessment. Int J Project Manag 25(6):589–600 50. DeBusk WM (2010) Unmanned aerial vehicle systems for disaster relief: Tornado alley. In: AIAA Infotech at aerospace. https://doi.org/10.2514/6.2010-3506 
51. ‘‘ISO - ISO 21384-3:2019 - Unmanned aircraft systems—Part 3: Operational procedures.’’ [Online]. Available: https://www.iso. org/standard/70853.html. [Accessed: 17-Feb-2021] 
52. Ferra˜o IG, et al (2020) STUART: ReSilient archiTecture to dynamically manage Unmanned aeriAl vehicle networks under attack. In: Proceedings—IEEE symposium on computers and communications, vol. 2020. https://doi.org/10.1109/ISCC50000. 2020.9219689 
53. FragaLamas P, Ramos L, Monde´jarGuerra V, Ferna´ndezCarame´s TM (2019) A review on IoT deep learning UAV systems for autonomous obstacle detection and collision avoidance. Remote Sens 11(18):2144. https://doi.org/10.3390/rs11182144 
54. Kim A, Kim M, Puchaty E, Sevcovic M, Delaurentis D (2010) A systemofsystems framework for the improved capability of insurgent tracking missions involving unmanned aerial vehicles. In: 2010 5th international conference on system of systems engineering, SoSE 2010. https://doi.org/10.1109/SYSOSE.2010. 5544076 
55. ANSI Unmanned Aircraft Systems Standardization Collaborative–UASSC. https://www.ansi.org/standardscoordination/colla borativesactivities/unmannedaircraftsystemscollaborative. Accessed: 17-Feb-2021 56 Liu Y, Dai HN, Wang Q, Shukla MK, Imran M (2020) Unmanned aerial vehicle for internet of everything: opportunities and challenges. Comput Commun 155:66–83. https://doi.org/10.1016/j. comcom.2020.03.017 
57 Hamurcu M, Eren T (2020) Selection of unmanned aerial vehicles by using multicriteria decisionmaking for defence. J Math. https://doi.org/10.1155/2020/4308756 
58. Hung KC, Yin M, Lin KP (2009) Enhancement of fuzzy weighted average and application to military UAV selected under group decision making. In: 6th international conference on fuzzy systems and knowledge discovery, FSKD 2009, vol. 7, pp 191–195. https://doi.org/10.1109/FSKD.2009.84 
59. Lin KP, Hung KC (2011) An efﬁcient fuzzy weighted average algorithm for the military UAV selecting under group decisionmaking. KnowledgeBased Syst 24(6):877–889. https://doi.org/ 10.1016/j.knosys.2011.04.002 
60. Zhang H, Xin B, Hua Dou L, Chen J, Hirota K (2020) A review of cooperative path planning of an unmanned aerial vehicle group. Front Inf Technol Electron Eng 21(12):1671–1694. https:// doi.org/10.1631/FITEE.2000228 
61. Saeed AS, Younes AB, Cai C, Cai G (2018) A survey of hybrid unmanned aerial vehicles. Progress in aerospace sciences, vol. 98. Elsevier Ltd, pp 91–105. https://doi.org/10.1016/j.paerosci.2018. 03.007 
62. ICAO Cir 328 (2011) Unmanned aircraft systems (UAS), Montre´al 63. Karasakal O, Karasakal E, Maras¸ G (2020) Multiobjective aerial surveillance over disjoint rectangles. Comput Ind Eng 148:106732. https://doi.org/10.1016/j.cie.2020.106732 
64. ISO - ISO 23665:2021 - Unmanned aircraft systems—training for personnel involved in UAS operations.’’ [Online]. https://www. iso.org/standard/76592.html. Accessed 17-Feb-2021 
Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations. 
Neural Computing and Applications (2022) 34:5479–5495 5495 
123 
",Two-phase multi-expert knowledge approach by using fuzzy clustering and rule-based system for technology evaluation of unmanned aerial vehicles.pdf,11
"Citation: Barbeau, M.; GarciaAlfaro, 
J.; Kranakis, E. Research Trends in 
Collaborative Drones. Sensors 2022, 
22, 3321. https://doi.org/ 
10.3390/s22093321 
Academic Editor: Andrey V. Savkin 
Received: 23 February 2022 
Accepted: 22 April 2022 
Published: 26 April 2022 
Publisher’s Note: MDPI stays neutral 
with regard to jurisdictional claims in 
published maps and institutional afﬁl- 
iations. 
Copyright: © 2022 by the authors. 
Licensee MDPI, Basel, Switzerland. 
This article is an open access article 
distributed under the terms and 
conditions of the Creative Commons 
Attribution (CC BY) license (https:// 
creativecommons.org/licenses/by/ 
4.0/). 
sensors 
Article Research Trends in Collaborative Drones 
Michel Barbeau 1,† , Joaquin GarciaAlfaro 2,* and Evangelos Kranakis 1,† 
1 School of Computer Science, Carleton University, Ottawa, ON K1S 5B6, Canada; barbeau@scs.carleton.ca (M.B.); kranakis@scs.carleton.ca (E.K.) 2 Télécom SudParis, Institut Polytechnique de Paris, 91120 Palaiseau, France * Correspondence: jgalfaro@ieee.org; Tel.: +33-160-76-47-22 † All authors contributed equally to this work. 
Abstract: The last decade has seen an explosion of interest in drones—introducing new networking technologies, such as 5G wireless connectivity and cloud computing. The resulting advancements in communication capabilities are already expanding the ubiquitous role of drones as primary solution enablers, from search and rescue missions to information gathering and parcel delivery. Their numerous applications encompass all aspects of everyday life. Our focus is on networked and collaborative drones. The available research literature on this topic is vast. No single survey article could do justice to all critical issues. Our goal in this article is not to cover everything and include everybody but rather to offer a personal perspective on a few selected research topics that might lead to fruitful future investigations that could play an essential role in developing drone technologies. The topics we address include distributed computing with drones for the management of anonymity, countering threats posed by drones, target recognition, navigation under uncertainty, risk avoidance, and cellular technologies. Our approach is selective. Every topic includes an explanation of the problem, a discussion of a potential research methodology, and ideas for future research. 
Keywords: drone; distributed computing; countering drone threats; target recognition; navigation; risk avoidance; cellular technologies; management of anonymity 
1. Introduction 
Drones have already demonstrated their potential. Their agility allows them to navigate and accomplish tasks in various indoor and outdoor environments. Application examples are the shipping and delivery of essential supplies to remote locations, safe contactless delivery or information gathering for disaster management, geographic mapping of inaccessible terrains, aid in thermal scanning for search and rescue operations, aid farmers for precision land monitoring, law enforcement and border control surveillance, storm tracking, forecasting hurricanes and tornadoes, military surveillance, parcel delivery, and sewer inspection [1–7]. Drone research is well established [8,9]. Key research areas include drone dynamics, navigation, path planning, and formation control. In this article, rather than cover everything, we offer a personal perspective on a few selected research topics that we envision might lead to fruitful future investigations. Each of the topics that we cover will play an essential role in developing future drone technologies. For every subject, we describe the main challenges, discuss potential approaches and methodology, and develop ideas for further research. More speciﬁcally, we explore the six speciﬁc problems listed in the sequel. The ﬁrst problem deals with collaborative drone applications, in which we assume services that are delegated to cloud services. We provide a representative scenario of distributed computing for the generation, management, and distribution of pseudonyms. The second problem relates to the issue of countering drone threats. Indeed, drones are a perfect example of dualuse technology. They can be used for beneﬁcial as well as malicious purposes. While the adoption of drones simpliﬁes unauthorized video and 
Sensors 2022, 22, 3321. https://doi.org/10.3390/s22093321 https://www.mdpi.com/journal/sensors 
photo recordings of protected areas, it can also lead to the release of dangerous materials in the proximity of critical infrastructures. Examples include attacks carried out in military missions and ﬂying drones in proximity to airport areas. According to [10], only in 2017, there were more than 300 reported incidents involving small drones. Similar issues are even more popular in the public media [11]. While speciﬁc drone applications have countless beneﬁts, others may be a real threat to safety and security. There is a need to develop a science for countering drone threats. The following three problems we address relate to speciﬁc algorithms for collaborative drone applications. First, we explore algorithms for target recognition from the air at a certain altitude. We establish the foundations of a probabilistic framework. Ideas for future work building on this framework are discussed. Second, we establish foundations for drone navigation in GPSdenied environments in the face of decisionmaking uncertainties. We discuss how these foundations can lead to future research. We complement the previous approach by exploring a formal framework for risk avoidance by ﬂying drones. Within this framework, we introduce ideas for further investigation. The ﬁnal problem that we address relates to cellular technologies in networked collaborative drone applications. While early generations of drones required lineofsight communications with a ground station to maintain control and push data, such as a video stream, the latest drone generations may beneﬁt from the use of Beyond Visual Line of Sight (BVLOS). BVLOS lifts the pilotdrone lineofsight constraint [12]. It expands the range of operation of drones and types of applications considerably. However, for safe operation, BVLOS requires reliable and realtime command and control. Connections may also serve other purposes, such as data communication and collaboration between drones. Various questions deserve attention, such as the use of cellular technology to network drones, making drones integral parts of the network infrastructure, such as base stationdrone or relay, drones pushing information to the cloud, such as locationrelated signal strength data, or drones pulling information from the cloud, such as ﬂight awareness data. Networking elevates drones’ capabilities to another level. The outline of this article is as follows. The management of anonymity in the context of distributed computing is explored in Section 2. Countering drone threats is investigated in Section 3. Target recognition, navigation under uncertainty, and risk avoidance problems are covered in Sections 4–6. Trends on networked drones using cellular technologies are discussed in detail in Section 7. Section 8 concludes the paper. 
2. Distributed Computing with Drones 
We start with collaborative drone applications in which services are delegated to edge nodes connected to cloud services. This aims at addressing situations in which cooperative drones may not have enough computational resources. They outsource part of their workload to remote resources using the distributed computing paradigm. We provide as a representative scenario the use of cloud services for the generation, management, and distribution of pseudonyms. The use case can be easily adapted to address other similar distributed computing problems. 
2.1. Problem Description 
Network operators may require that drones broadcast messages announcing their identity and location while executing a given mission. Reasons can include identifying and isolating faulty drones, e.g., drones misbehaving due to failures or errors. Privacy issues deriving from the broadcast of unprotected identiﬁers on a wireless channel have been extensively reported in the literature. This calls for the use of the distributed computing paradigm to delegate the management of pseudonyms while guaranteeing anonymity [13]. Similar privacypreserving authentication frameworks for Internetconnected vehicles have been proposed. Trends suggest that networked drones may interact with cloud services to generate, distribute, and report results. In such scenarios, edge cloud nodes connected to the cloud service and within the wireless range of collaborative drones 
assure the mapping of pseudonyms to real identities. These nodes also correlate the collected information and communicate to the cooperative drones new ﬁndings, e.g., lists of misbehaving faulty drones. 
2.2. Approach and Methodology 
As a representative example, we indicate how cloud services can be used to handle the anonymous reporting of faulty drones combining outsourced cloud services with machinetomachine (short range) communications. Figure 1 offers a centralized approach to generating and broadcasting lists of ephemeral identiﬁers (e.g., pseudonymous tokens) to collaborative drones using cloud services associated with 5G communications. Later, the drones would exchange those identiﬁers with other drones hovering nearby, e.g., using machinetomachine contacts, to report drones showing faulty behavior. The use of either edge or cloud services is assumed for both handling the generation and distribution of ephemeral identiﬁers as well as to help local drones to report faulty drones to other swarms of collaborative drones. 
Edge 
A B 
1. Ephemeral Identities (ei,A) 
2c. Neighbors (Mi,A) 
4. Status A 
Mi,A 
2a. ei,A into Mj,B 
2b. ej,B into Mi,A 
3. Report ej,B 
ei,A Mj,B ej,B 
Nodes Cloud 
Figure 1. Use of edge cloud nodes to report faulty drones using (anonymous) ephemeral identiﬁers. Step 1 represents the broadcasting of ephemeral identities to drones A and B. In steps 2a and 2b, the drone A’s data are stored in the memory of drone B. In Step 3, drone B reports a malfunction to the central service. In Step 4, the central service updates the neighboring drones (e.g., drone A, for instance). 
Step 1 in Figure 1 represents an operation in which a cloud service is broadcasting an ephemeral identity ei,A to drone A. This ephemeral identity becomes associated to drone A at time i. We assume that the cloud service has also generated and broadcasted an ephemeral identity, i.e., ej,B for drone B at time j. Notice that the ephemeral identities are timesensitive. Regularly, they expire, and drones need to request the cloud service to generate and broadcast their new identities to them. We also assume that drones A and B move around a given terrain in which 5G communications are possible to connect and use those cloud services required to create and distribute new ephemeral identities. Drones in this scenario are expected to periodically receive and store the ephemeral identities of nearby drones in their local memories. Each drone is expected to receive the identity of other drones via machinetomachine communications, e.g., using shortrange communications such as Bluetooth or IEEE 802.11. Step 2a in Figure 1 represents an operation in which drone B stores the ephemeral identity of drone A at time i in its local memory. Mj,B represents the action of storing those ephemeral identities received by user B at time j. Similarly, Step 2b represents drone A storing the ephemeral identity of drone B at time i into its local memory. Action Mi,A represents the action of storing those ephemeral identities received by drone A at time i. 
Periodically, drones report to a central cloud service the lists of ephemeral identities they received via machinetomachine communications, which they temporarily stored in their local memories. This is represented in Step 2c of Figure 1, in which drone A shares the list Mi,A with the central server. Now, let us assume that drone B is reported as faulty at time j to get blacklisted in the swarm. This situation is depicted in Figure 1 as Step 3, i.e., reporting the ephemeral identity of drone B at time j to the central server. The central server that holds all the information collected by all the drones can now warn other drones in the same swarm or other swarms in proximity to drone B. This is depicted in Figure 1 as Step 4. The central cloud service sends a new status to drone A, since this drone may be affected by the faulty behavior reported for drone B. 
2.3. Future Research 
The presented approach requires a dedicated infrastructure, which may not be possible in all drone scenarios and applications. For instance, cloud services and edge nodes to assure the mapping of pseudonyms to real identities may not ﬁt amateur drone applications, i.e., miniquadcopter drones without an Internet connection. Similar issues have been reported in the anonymity literature related to, e.g., Vehicular Adhoc Networks (VANET)s [14]). In such VANET scenarios, privacypreserving strategies leverage mixnetwork ideas that could also be implemented in collaborative drone applications. The use of group schemes [15,16] can be complemented with the help of privacy zones, in which entry and exit from a zone can force the update of pseudonyms. The approach can also be extended to crowd area scenarios [17], in which swap pseudonym strategies [18] and obfuscation techniques can help increase the level of anonymity between groups of drones. The main limitation of those approaches is the necessity of dedicated infrastructure and humanoperator assistance. We envision machinetomachine communication solutions without the necessity of deploying a cloud infrastructure to access edge computing services. Such future trends can beneﬁt from alternative schemes considered in avionic and maritime contexts. For instance, the authors in [19] propose secure pseudonym schemes for aircraft in which dedicated entities, referred to as trusted registration authorities, oversee assisting aircraft to generate pseudonyms that are evolving. The generation process guarantees irreversible procedures with respect to tracing back a pseudonym to its real identity. Continuous interaction with the operators associated with the trusted authorities may be an important limitation for drone applications. Moreover, data exﬁltration from such trusted authorities may reveal the correspondence between pseudonyms and real identities. Future trends can also be inspired by applications in the maritime domain, in which pseudonym strategies have been enforced over marine vessels via traditionally trusted authorities [20]. The main difference is using an infrastructurefree approach, although a persistent connection to the registration authority is still required. Methods such as those proposed in [21] for the secure and anonymous broadcast of identiﬁers assume a secure communication channel among mobile vehicles. Inspired by the previous ideas, we envision some alternative methodologies to conduct the anonymous collection and dissemination of information reported in this section, but limiting as much as possible the dependence of drones to cloud infrastructure services as those referred to in Figure 1. A decentralized version of the previous approach is depicted in Figure 2. In this new approach, drones create the list of ephemeral identities by themselves. This way, now both drones A and B manage their lists of ephemeral identities, broadcasting and expiring them at regular time intervals. When drones such as B are reported as faulty, they can be sensed and reported to other drones in the neighborhood by using the list of ephemeral identities previously exchanged when they were still behaving correctly. 
A B 
1a. ei,A into Mj,B 
1b. ej,B into Mi,A 
2. Report ej,B 
Additional Inputs 
Additional Outputs 
3. Status ej,B 
ei,A 
Status A 
ej,B 
Status B 
Mi,A Mj,B 
Machine 
to 
Machine 
Figure 2. Distributed architecture to report faulty drones using anonymous ephemeral identiﬁers. In contrast to the approach depicted in Figure 1, drones exchange lists of ephemeral identities and locally update their status without the necessity of central intermediary services. The main differences with respect to the centralized version are in Steps 2 and 3, which are conducted now in a machinetomachine mode. 
In contrast to the approach illustrated in Figure 1, drones can now request and obtain lists of ephemeral identities and locally check whether or not their neighbors are potentially faulty. Remote cloud and edge services can be used as a redundant resource to store, forward, and broadcast lists of potentially defective drones identiﬁed by their ephemeral identities. In some missionspeciﬁc applications, a delegation of distributed processing to edge nodes and cloud services leads to assumptions and issues, which are not always possible to solve—for example, in situations where the deployment of dedicated infrastructure and centralization is not possible. At the same time, the use of distributed computing with collaborative drones may lead to more complex scenarios when managing, e.g., fault diagnosis, which is a promising future research ﬁeld in collaborative drone scenarios with wellestablished foundations from cooperative multiagent systems [22,23]. We consider that further work remains to address such additional scenarios. 
3. Countering Drone Threats 
Drones can be used in illicit activities. Several cases have been documented lately. In Saudi Arabia, terrorists used drones to attack oil pumping stations [24]. A Turkish company has developed an autonomous suicide drone [25]. Drones ﬂying close to airports, business centers, and elsewhere in densely populated areas pose a direct danger to critical infrastructure and humans. There are numerous incidents involving rogue drones violating airspace. Drones have also been used to smuggle drugs into the walls of jails [10]. These incidents call for technology to protect infrastructures and populations from harmful activities perpetrated leveraging drones. 
3.1. Problem Description 
The problem is threefold: 
1. Spotting and Following. When do drones enter a given airspace? Are they friends or adversaries? What tasks are drones specially ﬁtted for? Are they armed? Loaded? What is their kind? How many rotors do they have? Are they small? Micro? Are they equipped with infrared cameras? Are drones autonomous or under the command and control of a human pilot? What are the whereabouts of spotted drones? 
2. Intention determination. What is the drones’ mission? Are they looking around infrastructures? Searching? Are they preparing for a strike? What is their target? Are they 
taking pictures? Shooting videos? What strategies or algorithms are they following to achieve their mission? 
3. Riposte. How to respond to the presence of adversarial drones? What are the effective countermeasures? 
The swarm dimension, drones acting in a collaborative group to achieve their goal, and the increasing degree of elaborateness of drones compound the problem. The environmental conditions are also challenging, since drones are equipped with sensors of various types and have onboard computing resources that enable embedding advanced algorithms and artiﬁcial intelligence, rendering them reactive and adaptable. 
3.2. Approach and Methodology 
The subproblems can be approached either passively or actively. Table 1 lists the countering drone threat tasks together with various ideas that exist and sometimes have been investigated in the research community. Conventional radar technology is not practical for spotting and following drones, especially for microdrones ﬂying at slow speed and low altitudes and for areas such as urban environments. Luckily, spotting and following can be achieved passively using audio, video, and radio frequency observations [26–28]. The use of artiﬁcial intelligence and Machine Learning (ML) for solving problems related to countering drone threats has grown considerably. Agents learn and exploit what they have learned to spot and follow drones. AlSa et al. have created a Data Base (DB) of drone Radio Frequency (RF) signals. Together with the use of neural networks, the database can be used to detect drone presence, their type, and kind of ﬂight [29]. Wit et al. use microDoppler signatures for drone rotarywing determination, i.e., helicopter, birotor, quadcopter, hexacopter, or octocopter [30]. Research has also been completed to determine the load carried by a drone. Using microDoppler signatures, Fioranelli developed an approach to determine whether a drone carries a load or not [31]. Traboulsi and Barbeau further investigated this question and developed a method to determine the weight of the payload of a drone [32]. Schumann et al. [33] use deep learning to build classiﬁers to discriminate drones from birds. The approaches considered have limited success due to the weaknesses of the classiﬁers to adversarial learning attacks, as demonstrated by Shamir et al. [34] and Eykholt et al. [35]. 
Table 1. Tackling the countering drone threats. The ﬁrst column lists the tasks. The second column provides passive solutions based solely on observations. The third column indicates active solutions, acting on threatening drone behavior. 
Passive Active 
Spotting and following Audio, video and RF Autonomy rev. Turing test observations and ML RF signal DB MicroDoppler signatures 
Intention determination Maneuvers recognition w. RL Formation prediction w. SL 
Riposte RF jamming Counterattack drones 
On the intention determination front, drone maneuvers recognition has also been investigated, such as the work of Bartak and Vemlelova using Reinforcement Learning (RL) [36]. Using telemetry data, a Supervised Learning (SL) model to predict the formation of a drone swarm aims has been proposed by Traboulsi and Barbeau [37]. Ideas for riposte in response to detecting malicious drone behavior have been quite limited. Attempts to jam the signals of autonomous drones may not be applicable. Shooting ﬂying drones may have their risk. 
3.3. Future Research 
In summary, attention has been given mainly to drone spotting and following. Intention determination and riposte have received little if any attention. More research is needed on all aspects of countering drone threats. Table 1 highlights the lack or absence of solutions to several subproblems related to the countering of drone threats. 
4. Target Recognition 
In this section, we focus on ways to recognize physical targets using one or more drones, possibly either cloud connected or wirelessly with each other, and which are traveling at a certain speed either away from or toward a (moving) target. 
4.1. Problem Description 
Drones are being used to explore terrestrial regions to discover speciﬁc or sought after features and characteristics, such as carrying out human or animal counts, diagnosing speciﬁc environmental situations, recognizing a given landmark or sign, or searching for a particular mammal, cf. [38,39]. Additional technical studies also include [40–42]. Tasks with low bandwidth requirements can be performed ofﬂine by participating ground stations interacting with drones using a cellular network. However, appropriate network bandwidth may not be available. It is more likely that a drone (or swarm of drones) would need to explore the environment, interpret the data collected, and make decisions based on its own online diagnosis. In this section, we ask the question: 
How can we accurately detect a (possibly) mobile target from a distance? 
We consider only altitudebased detection (see [43–46]). Considerations could also be extended to speedbased detection (e.g., speed of data sampling) or a combination of altitude and speed. 
4.2. Approach and Methodology 
Consider a drone ﬂying over a certain highway represented as a line segment L of length ℓ = |L|. During the ﬂight, the drone retains a given altitude x. The speed v(x) of the drone may be decided by prevailing regulations on the drone’s ﬂying territory and is a function of x (see Figure 3, left picture). 
M 
x 
σ τ(x) 
v(x) 
α 
σ 
τ(x) τ(x) 
A B 
L 
M 
Figure 3. Drone M is ﬂying over a line segment L of length ℓ and a certain object of length σ. At height x, the drone ﬂies with speed v(x) which depends on the altitude x. The picture in the righthand side indicates that the drone’s camera has a sliding window effect of width τ(x) within which it can detect an object of length σ. 
The drone’s camera has a ﬁeld of view (or visibility angle) α. When ﬂying at height x, the drone’s visibility angle α subtends a subsegment of the line of length τ(x); this is the portion of the line ℓ on which the drone’s camera can detect objects. Since the ﬁeld of view of the camera is α and the drone is ﬂying at altitude x, we must have that τ(x) = 2x tan(α/2) degrees. First, we determine the target object density as a fraction of the total length ℓ of the segment L. As the drone is moving, it is detecting objects located on the line segment. 
The drone’s camera at altitude x has a sliding window of width τ(x). This is moving with speed v(x), thus yielding a time of possible observation equal to τ(x)/v(x). The following analysis is inspired from [38,40]. An object of length σ is placed on the line L. As indicated in Figure 3 (right picture), assuming it is moving left to right, the drone starts observing the object at its leftmost endpoint A. It stops observing when at the point B, thus yielding a total distance of observation equal to σ + τ(x). The total duration of observation is equal to σ+τ(x) 
v(x) . It follows that the drone is observing the target object a fraction f (x), where 
f (x) = (σ + τ(x))/v(x) 
(ℓ − τ(x))/v(x) = σ + τ(x) 
ℓ − τ(x) , (1) 
of its total ﬂying time over the segment of length ℓ, assuming that the rightmost point of the target object is at a distance at most τ(x) from the rightmost point of the segment of length ℓ. In a way, the righthand side of Equation (1) represents the target object density. Next, we look at distributions for the probability that the object is detected by the drone. Using distance sampling statistics, one could use the exponential distribution 
p(x) = 1 
ln(1 + 1/k) · e−x 
k + e−x , (2) 
as the probability that a drone ﬂying at altitude x ≥ 0 detects the target object when its camera is directed toward it. The parameter k appearing in Equation (2) is chosen so as to indicate the probability that an autonomous ground vehicle detects the target when traveling at ground level (altitude x = 0), e.g., p(0) = 1 
k ln(1+1/k). The term 1 
ln(1+1/k) is the normalizing coefﬁcient 
and is derived as follows. Observe that � e−x 
e−x+kdx = x − ln(kex + 1) and for x ≥ 0, this yields the formula � ∞ 
0 e−x 
e−x + kdx = ln � 1 + 1 
k 
� 
Note that the (exponential) probability distribution above is one of several potential choices; others may be considered depending on the situation. Using the target object density calculations in the previous subsection, we conclude that the probability Pσ(x) that the drone detects a target object of length σ is equal to 
Pσ(x) := p(x) · f (x) = 1 
ln(1 + k) · e−x 
k + e−x · σ + τ(x) 
ℓ − τ(x) , 
where f (x) is the fraction of its total ﬂying time that the drone will be observing a ﬁxed object of length σ over the segment of length ℓ. 
4.3. Future Research 
There are several directions for additional research. In case of a searching swarm consisting of n drones acting independently of each other, the probability that the target object is detected by at least one of the drones is easily seen to be equal to 1 − (1 − Pσ(x))n. One could also consider more general settings by employing p(v, h) as the error probability of image interpretation, namely the probability that the image is interpreted wrongly given that the drone is moving with speed v and is at altitude h from the target. It should be noted that a similar analysis could be applied if the object itself is mobile. Moreover, one could also explore machine learning techniques to improve target recognition for swarms of cooperating drones. 
5. Navigating under Uncertainty 
This section considers drone navigation under conditions that may cause decision uncertainty in path selection. These may affect the signal’s quality as received or processed by the drone. To this end, we take into account recognition and advice errors. 
5.1. Problem Description 
When drones are navigating an unknown environment, they may have to ﬁnd a ﬂight path from a source to a destination in a GPSdenied environment using only clues obtained from landmarks visited. In particular, when hovering over an area, they can acquire data through their camera(s) and other sensors, which may be visual, acoustic, etc. Data collected are being used to interpret landmarks. A priori, drones may be given clues and speciﬁc characteristics about landmarks that they can use throughout their route discovery. For example, they may be seeking a green door or a tall building. Our main question here is the following: 
How can drones navigate through an environment of landmarks without using GPS information? 
5.2. Approach and Methodology 
In the sequel, we outline a technique notivated from the work of [47] and ﬁrst employed in [48,49] in order to navigate such a terrain. Consider a terrain with available landmarks, e.g., road signs, monuments, or even other terrestrial indicators. The landmarks encountered may provide information whose interpretation by the drone may be prone to various types of errors. One can distinguish two types of errors, namely, recognition and advice. 
• Recognition errors may be due to misinterpretation of sensed data or confusion of objects. For example, a drone has found a green door which in fact is not a door but rather a window leading to an incorrectly recognized object. We assume that for some real number p ∈ [0, 1], the value p is the probability that a drone performs recognition erroneously and 1 − p that it is correct. 
• Advice errors could be about landmarks because the information they provide is either not up to date or even outright wrong. For example, upon ﬁnding a landmark, a drone is advised to traverse a certain distance within the terrain in direction north where it will ﬁnd the next landmark, say a restaurant, but this information is wrong because the restaurant is no longer there. Again, we may assume that for some real number q ∈ [0, 1], the value q is the probability that the advice provided to a drone about a landmark is erroneously interpreted and 1 − q that it is correctly interpreted. 
The next step concerns mathematical assumptions of the model above. To facilitate calculations, one may assume that recognition errors are independent and identically distributed and advice errors are also independent and identically distributed. In other words, the drones are acting independently of each other. In addition, the outcome of the recognition process is random with a probability of success that depends on the parameter p. A similar observation applies to the advice process. We can use this methodology to our advantage so as to improve the recognition and advice mechanisms for swarms of drones. There is an underlying graph determined by the terrain of landmarks. Landmarks are vertices of a graph G = (V, E) whose edges are discovered online by the recognition and/or advice process. In a typical setting, we start with a source node s and end with a target node t. The drones are seeking a ﬂight path connecting m + 1 vertices s := v0, v1, . . . , vi, vi+1, . . . , vm := t, see Figure 4. 
s 
t 
vi vi+1 
Figure 4. Flight path from source s to destination t. Edge (vi, vi+1) is an intermediate segment connecting landmarks vi and vi+1. The vertical (thicker) arrows represent the landmarks encountered by the drones. 
A possible ﬂight path, denoted as P, consists of a number of vertices v0 := s, v1, . . . , vi, vi+1, . . . , vm := t from s to t. An edge {vi, vi+1} corresponds to a segment of the ﬂight path P. It is said to be correctly traversed if and only if the advice provided about the landmark associated with vertex vi is valid and correctly interpreted and the landmark associated with vertex vi+1 is correctly recognized. For i = 0, . . . , m − 1, the ﬂight path P is correctly traversed if and only if each of its segments deﬁned by edges {vi, vi+1} are correctly traversed. At the start, a drone is given a ﬂight plan. The ﬂight plan deﬁnes the ﬂight path P. For each vertex vi, i = 0, . . . , m − 1, the ﬂight plan comprises advice for searching for the next landmark, such as directional data. For each vertex vi+1, the ﬂight plan contains recognition data, such as landmark characteristics. A ﬂight plan is correctly performed solely if every single segment is correctly traversed. The next important step is for the swarm of drones to fuse information collected so to reach an optimal decision. The basic method is to employ error ampliﬁcation. Namely, we amplify errors by employing the majority rule and using statistical sampling in order to make decisions on how to navigate the environment (e.g., choose a direction along the ones recommended, accept a certain advice but discard others, and in general improve recognition, etc). Thus, at each landmark being explored, the drones ﬁrst collect information and make individual decisions on how to interpret the information. Second, they exchange their individual interpretations and decide based on the majority rule by communicating the common decision. Additional details of this can be found in [48,49]. Such an approach turns out to be useful and may lead to improved navigation that may supplement existing navigation techniques. 
5.3. Future Research 
The methodology proposed offers potential for additional research and testing more sophisticated majority strategies based on probability distributions that are realistic in that they are sensitive, for example to geographic location, proximity to base station, and/or landmarks and available energy of drones. Additionally, one could make use of a database (DB) of landmarks and employ machine learning approaches on drone swarms that could improve the overall decisionmaking process. For example, contextual bandits could be a natural framework for routing and decision making in such a terrestrial environment, cf. [50] for further details. 
6. Risk and Obstacle Avoidance 
This section considers approaches to risk and obstacle avoidance when drones are traveling through a possibly unknown environment. The drones themselves may be equipped with sensors to analyze collected data locally or in the cloud. 
6.1. Problem Description 
When a drone is traversing a given terrain, it is exposed to risks arising from the spatial density of a particular factor being taken into account, e.g., concentration of chemicals, disturbing weather phenomena and/or patterns, environmental pollution, population density, and even the viral load (for a given virus) in a constrained space. Our main question is the following: 
How can a drone navigate a risky terrain so as to minimize the impact of hazardous factors? 
6.2. Approach and Methodology 
There are many ways to measure the risk when a given region is traversed by the drone. For example, when the source of the risk is not known, it could be measured by the length of the portion of the path traversed by the drone that is inside the given region (see Figure 5), which we also refer to as the exposure. In general, the hazardous region will be enclosed by the perimeter of a complicated domain (represented as oval in Figure 5). 
However, if the source of the risk is also known (something we refer to as hotspot), then sometimes, it may be possible to measure the risk from either the distance from the hotspot or an area covered by the drone during the traversal (e.g., see Figure 6.) This is of course an idealized scenario, but it can be useful for making computational estimates on the complexity of the problem. 
M 
Figure 5. A drone M is traversing a straight line trajectory. Its exposure during the trajectory may be measured by the length of the path inside the risky region. 
More generally, consider regions in the plane representing risky areas, which were initiated from various sources (which may or may not be known) of pollution, viral infection, population density, etc. As depicted in Figure 6, a drone M is traversing a straightline trajectory in a given terrain. The goal of the drone is to navigate the terrain so as to ﬁnd a path that minimizes its risk of exposure. In many instances, one might be able to measure this quantity precisely. 
M 
Figure 6. A drone M is traversing a straight lime trajectory. Its exposure during the trajectory is measured by the sum of the exposures from each of the regions (depicted as ovals) within its range. 
Measurements can be done in various ways. For example, associated to each region, the authors of [51] consider only disks which are generated by hotspots located at the respective centers of the disks. The critical parameter is a distance r > 0 that represents how far the hotspots can reach. The value of r depends on the hotspot’s intensity to cause harm, so that the bigger the r, the higher the degree of harm that the hotspot can cause. Moreover, beyond r, no harm is possible. A drone in proximity to the hotspot may get harmed depending on its distance from it. The amount and intensity of harm depends on the distance between the drone and the hotspot (see [51] for additional details). More speciﬁcally, the metric introduced in [51] is deﬁned by the area of the circular sector delimited by the points 
A, E, B, D; it is easy to see that this satisﬁes Area(AEBD) = arccos � rh � · r2 − h · √ 
r2 − h2 
(see Figure 7). 
A B 
C 
D 
E 
r α 
h 
enter exit 
Figure 7. A drone M is traversing a straight line trajectory from A (entrance) to B (exit) at distance h from the point C. Its path inside the circle is depicted by the thick segment AB. 
For the metric of harm proposed here, one may consider the length of the portion of the trajectory of the mobile inside the disk. For this metric, we observe the following. Assume a drone is traversing a straight line trajectory at distance h from a hotspot (e.g., an infected station) with critical distance r, where r ≥ h ≥ 0. The total amount H(r, h) of harm that the drone M incurs for its entire trajectory within the hotspot’s range is equal to 
H(r, h) = 2r � 
1 − h2/r2 (3) 
(see Figure 7 where a drone M is traversing a straight lime trajectory at distance h from the source C entering at A and exiting at B). Similar estimates may also be possible to derive for other types of regions. There are various kinds of hotspots, for example, for the case of an infection, the function H(·, ·) may be the wellknown viral load which is studied in epidemics, but in general, it does not have to be limited to this. More generally, if a drone traverses a path P consisting of k + 1 vertices u0, u1, . . . , uk with respective sensitivity distances r0, r1, . . . , rk, then the total harm accumulated will be equal to 
∑k i=0 H(ri, hi), (4) 
where hi is the distance of the drone from node ui. The summands in Formula (4) as given by Equation (3) are for all the hotspots encountered. Moreover, a drone may be within more than one region at the same time and hence under the inﬂuence of more than one hotspot. More generally, different hotspots may cause different types of harm Hi(·, ·), for i = 0, . . . , k. We can develop navigation strategies using this metric that are similar to those developed in [51]. Similar obstacle avoidance navigation algorithms can also be found in [52,53]. 
6.3. Future Research 
In our discussion and analysis, we proposed a simple additive model for measuring the viral load accumulated thus leading to a technique for risk avoidance. Additional metrics for complex regions (e.g., polygonal, etc.) could also be explored using algorithmic and optimization techniques inspired from computational geometry. An interesting question would be to give algorithms to determine (or even prevent taking) a path given that a certain threshold cannot be exceeded. For example, if the critical value of harm is T, one might assume that a harm occurs if 
∑k i=0 H(r, hi) ≥ T, (5) 
i.e., the total amount of harm is at least T, a threshold value that may well depend on risk and safety considerations as well as sensitivity of the measuring equipment. The models proposed and studied so far are in 2D space; another important case would be to look at more realistic models in 3D space. It would also be interesting to explore other metrics as well as incorporate techniques from Bandit routing, cf. [50], and study machine learning approaches to improve risk avoidance and maintain a safe routing path. 
7. Networked Drones Using Cellular Technologies 
The classical pilottodrone connection is assured using Visual Line of Sight (VLOS) communications. A direct link is used between a pilot and a drone. The pilot–drone distance is limited by the capability of maintaining eye contact with a drone, such as 500 m. To go far off VLOS, it has been proposed that cellular network infrastructures be used for pilottodrone command and control and more. This concept is called BVLOS communications. Pilots maintain communications with drones across arbitrarily long distances over cellular networks. BVLOS enables the piloting of drones outside the line of sight of pilots. Drone traveling distances become limited solely by cellular network coverage. Drones may be guided in complex structures involving buildings, electric poles, and wiring. Because communications are maintained, pilots can always intervene if something goes wrong. Applications of BVLOS include inspection of infrastructures, such as pipelines, and delivery of parcels, food, and medical supplies in urban or rural areas. BVLOS communications are expected to be one of the important applications of upcoming generations of cellular networks. This realization of this concept enables drone applications requiring wide geographical area coverage. 
7.1. Problem Description 
Research issues that stem from BVLOS include: 
1. Low latency connections for drone commandandcontrol. 
2. Interference mitigation due to altitudes of drones. 
3. Cooperative drone group communications. 
7.2. Approach and Methodology 
The BVLOS concept has been explored with 4G cellular networks [12]. The 4G cellular networks made possible multimedia applications such as music and video streaming. The latency of 4G networks, in the order of 50 to 200 ms, does not address the realtime needs for drone command and control, which are in the order of a few milliseconds. Furthermore, 4G cellular networks have been designed for serving ground User Equipment (UE). In contrast, ﬂying drones operate at altitudes. It has been observed that 4G cellular communications of ﬂying drones are prone to high interference levels [12]. The resulting low Signal to Interference Ratio (SINR) values are causing degradation and loss of connectivity [2]. Low network latency is a requirement for safe drone command and control. The 5G networks are aiming at network latency in the order of a few milliseconds. The 5G technology being deployed addresses the needs of drone communications. BVLOS is a use case of 5G networks [54–56]. BVLOS communications require reliable connections between pilots and drones for aspects such as command and control, telemetry and ﬁrstperson camera video [57,58]. The 5G network BVLOS communications build upon the Ultra Reliable Low Latency Communication (URLLC) use case [59]. It aims at a latency of a few milliseconds. Another goal is reliability, that is, less than a 10−5 packet drop rate and less than 10−9 bit error rate. Pilots can safely operate drones unhindered within these parameters by network trafﬁc load and latency. 
7.3. Future Research 
Drone communications are different from traditional cellular communications due to the tight closedloop requirements of uplink and downlink communications and the mixed trafﬁc types communicated, such as command and control signals, sensor measurements, 
and bandwidthhungry highdeﬁnition video and liDAR. Future research should ideally be conducted in the 5G network environment, which aims to overcome the drawbacks of 4G networks concerning the support of drone communications. The 5G networks make drones capable of operating across long nonlineofsight distances. The wide 5G bandwidth enables more considerable data trafﬁc. The BVLOS concept paves the way for new applications. For example, drones can communicate with each other and ground stations to combine their efforts to achieve a search and rescue mission. The 5G BVLOS communications with drones have their challenges, including propagation channel modeling, 3D trajectory planning, energyefﬁcient design, and network and radio resource allocation. An exciting idea is the use of drones to physically carry core network elements [60–65]. They can play the role of relay or Base Station (BS). They can be untethered or tethered. The battery’s capacity limits the time of operation of an untethered drone, such as one hour. A tethered drone is powered and linked by a cable. Fly time is not limited by battery capacity. It can last for days. The wired link can provide wider bandwidth than a wireless link. However, being attached to the ground, the range of operation of a drone is limited to a particular zone, typically a truncated hemisphere. Beneﬁts of ﬂying relays or BSs include easy relocation and installation concerning ground relays and BSs. Their lifetime is, of course, limited, especially for untethered BS, which are dependent on the energy reserve of their battery. Research challenges include airtoground and airtoair channel modeling and optimal placement of ﬂying relays or BS. Past research has investigated the quality of ground user connections served by a ﬂying BS and achieving the best possible data rates using machine learning approaches to determine the best location for ﬂying relays. A challenge in the BVLOS connected drone use cases is ensuring drone connectivity and Quality of Service (QoS) support throughout a drone mission. Ensuring predictable coverage and low communication delays is a nontrivial task due to the phenomenon of scattered cell associations where different altitudes result in varying patterns of received signal power [66]. Classical drone navigation strategies tend to focus on reducing the travel time and energy consumption of drones. However, the optimal navigation path does not likely provide the uplink and downlink QoS requirements of a drone or may unnecessarily consume a large amount of network resources. Minimizing travel time and energy usage while maximizing connectivity and reducing cellular network resources are conﬂicting requirements that need novel algorithms and approaches to enable widespread connected drone usecase adoption. Another challenge within this domain is drone reconnectivity upon connection disruption. Even with connectivityaware path planning, a drone may often lose connectivity due to propagation errors or changes in network deployment and increased load. To mitigate this, novel algorithms that can reroute the drone to alternative routes with the required QoS connectivity are needed. 
8. Conclusions 
Collaborative drones use new networking technologies, such as 5G networks and cloud computing. The goal is to expand traditional applications and their horizon from search and rescue to more ambitious scenarios encompassing new aspects of everyday life. The available research literature on the topic is vast. No single survey article could ever cover all the relevant issues. We have presented a personal collection of research themes covering a few selected areas that we consider worth pursuing in future research. We chose a few research trends on networked, collaborative drones, highlighting target recognition, navigation, risk avoidance, use of cellular technologies, anonymity, and countering drone threats. We introduced each topic by including the problem deﬁnition, discussion of a potential research approach, and ideas for future research. Each topic represents just one promising solution w.r.t. the applications domains reported in our work. For example, the use of distributed computing with collaborative drones presents clear advantages for the management of communications in a decentralized manner, for instance, to deal with anonymity issues in situations where the deployment of dedicated infrastructure and centralization is not possible. At the same time, the use of distributed computing 
with collaborative drones may lead to more complex scenarios when managing, e.g., fault diagnosis in cooperative tasks. Further work remains to be done to address such issues. 
Author Contributions: All authors contributed equally to this work. All authors have read and agreed to the published version of the manuscript. 
Funding: Not applicable. 
Institutional Review Board Statement: Not applicable. 
Informed Consent Statement: Not applicable. 
Data Availability Statement: Not applicable. 
Acknowledgments: The authors gratefully acknowledge ﬁnancial support from the Natural Sciences and Engineering Research Council of Canada (NSERC) and the European Commission (H2020 SPARTA project), under grant agreement 830892. 
Conﬂicts of Interest: The authors declare no conﬂict of interest. 
References 
1. Clarke, R. Understanding the drone epidemic. Comput. Law Secur. Rev. 2014, 30, 230–246. [CrossRef] 
2. de Amorim, R.M.; Wigard, J.; Kovacs, I.Z.; Sorensen, T.B.; Mogensen, P.E. Enabling cellular communication for aerial vehicles: Providing reliability for future applications. IEEE Veh. Technol. Mag. 2020, 15, 129–135. [CrossRef] 
3. Zhang, J. Occlusionaware uav path planning for reconnaissance and surveillance in complex environments. In Proceedings of the 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), Dali, China, 6–8 December 2019; pp. 1435–1440. 
4. GuerreroSánchez, M.E.; Lozano, R.; Castillo, P.; HernándezGonzález, O.; GarcíaBeltrán, C.; ValenciaPalomo, G. Nonlinear control strategies for a UAV carrying a load with swing attenuation. Appl. Math. Model. 2021, 91, 709–722. [CrossRef] 
5. Shahmoradi, J.; Talebi, E.; Roghanchi, P.; Hassanalian, M. A comprehensive review of applications of drone technology in the mining industry. Drones 2020, 4, 34. [CrossRef] 
6. Burgués, J.; Marco, S. Environmental chemical sensing using small drones: A review. Sci. Total Environ. 2020, 748, 141172. [CrossRef] 
7. Chmaj, G.; Selvaraj, H. Distributed processing applications for UAV/drones: A survey. In Progress in Systems Engineering; Springer: Cham, Switzerland, 2015; pp. 449–454. 
8. Popescu, D.; Stoican, F.; Stamatescu, G.; Chenaru, O.; Ichim, L. A survey of collaborative UAV–WSN systems for efﬁcient monitoring. Sensors 2019, 19, 4690. [CrossRef] 
9. Li, X.; Savkin, A.V. Networked Unmanned Aerial Vehicles for Surveillance and Monitoring: A Survey. Future Internet 2021, 13, 174. [CrossRef] 
10. Di Pietro, R.; Raponi, S.; Caprolu, M.; Cresci, S. New Dimensions of Information Warfare; Advances in Information Security; Springer International Publishing: Cham, Switzerland, 2020. 
11. SeedScientiﬁc. Electrifying Drone Statistic; Technical Report. 2021. Available online: https://seedscientiﬁc.com/dronestatistics/ (accessed on 19 January 2022). 
12. Ivancic, W.D.; Kerczewski, R.J.; Murawski, R.W.; Matheou, K.; Downey, A.N. Flying drones beyond visual line of sight using 4G LTE: Issues and concerns. In Proceedings of the 2019 Integrated Communications, Navigation and Surveillance Conference (ICNS), Herndon, VA, USA, 9–11 April 2019; pp. 1–13. 
13. Tian, Y.; Yuan, J.; Song, H. Efﬁcient privacypreserving authentication framework for edgeassisted Internet of Drones. J. Inf. Secur. Appl. 2019, 48, 102354. [CrossRef] 
14. Calandriello, G.; Papadimitratos, P.; Hubaux, J.P.; Lioy, A. Efﬁcient and robust pseudonymous authentication in VANET. In Proceedings of the Fourth ACM International Workshop on Vehicular Ad Hoc Networks, Montreal, QC, Canada, 10 September 2007; pp. 19–28. 
15. Boualouache, A.; Moussaoui, S. S2si: A practical pseudonym changing strategy for location privacy in vanets. In Proceedings of the 2014 International Conference on Advanced Networking Distributed Systems and Applications, Bejaia, Algeria, 17–19 June 2014; pp. 70–75. 
16. Boualouache, A.; Senouci, S.M.; Moussaoui, S. PRIVANET: An efﬁcient pseudonym changing and management framework for vehicular adhoc networks. IEEE Trans. Intell. Transp. Syst. 2019, 21, 3209–3218. [CrossRef] 
17. Benarous, L.; Kadri, B.; Boudjit, S. Alloyed pseudonym change strategy for location privacy in vanets. In Proceedings of the 2020 IEEE 17th Annual Consumer Communications & Networking Conference (CCNC), Las Vegas, NV, USA, 10–13 January 2020; pp. 1–6. 
18. Li, X.; Zhang, H.; Ren, Y.; Ma, S.; Luo, B.; Weng, J.; Ma, J.; Huang, X. PAPU: Pseudonym Swap With Provable Unlinkability Based on Differential Privacy in VANETs. IEEE Internet Things J. 2020, 7, 11789–11802. [CrossRef] 
19. Asari, A.; Alagheband, M.R.; Bayat, M.; Asaar, M.R. A new provable hierarchical anonymous certiﬁcateless authentication protocol with aggregate veriﬁcation in ADSB systems. Comput. Netw. 2021, 185, 107599. [CrossRef] 
20. Goudossis, A.; Katsikas, S.K. Towards a secure automatic identiﬁcation system (AIS). J. Mar. Sci. Technol. 2019, 24, 410–423. [CrossRef] 
21. Zamani, M.; Saia, J.; Movahedi, M.; Khoury, J. Towards provablysecure scalable anonymous broadcast. In Proceedings of the 3rd USENIX Workshop on Free and Open Communications on the Internet (FOCI 13), Washington, DC, USA, 13 August 2013. 
22. Niu, G.; Han, T.; Yang, B.S.; Tan, A.C.C. Multiagent decision fusion for motor fault diagnosis. Mech. Syst. Signal Process. 2007, 21, 1285–1299. [CrossRef] 
23. Jennings, N. Cooperation in Industrial MultiAgent Systems; World Scientiﬁc: Singapore, 1994; Volume 43. 
24. Raghavan, S. Saudis say oil pipeline was attacked by drones, possibly from Yemen. The Washington Post, May 15, 2019. 
25. Zitser, J. A rogue killer drone ’hunted down’ a human target without being instructed to, UN report says. Bus. Insid. 2021. Available online: https://www.businessinsider.com/killerdronehunteddownhumantargetwithoutbeingtoldun-2021-5 (accessed on 19 January 2022). 
26. Shi, X.; Yang, C.; Xie, W.; Liang, C.; Shi, Z.; Chen, J. Antidrone system with multiple surveillance technologies: Architecture, implementation, and challenges. IEEE Commun. Mag. 2018, 56, 68–74. [CrossRef] 
27. Shi, L.; Ahmad, I.; He, Y.; Chang, K. Hidden Markov model based drone sound recognition using MFCC technique in practical noisy environments. J. Commun. Netw. 2018, 20, 509–518. [CrossRef] 
28. Anwar, M.Z.; Kaleem, Z.; Jamalipour, A. Machine learning inspired soundbased amateur drone detection for public safety applications. IEEE Trans. Veh. Technol. 2019, 68, 2526–2534. [CrossRef] 
29. AlSa’d, M.F.; AlAli, A.; Mohamed, A.; Khattab, T.; Erbad, A. RFbased drone detection and identiﬁcation using deep learning approaches: An initiative towards a large open source drone database. Future Gener. Comput. Syst. 2019, 100, 86–97. [CrossRef] 
30. de Wit, J.M.; Harmanny, R.; PremelCabic, G. MicroDoppler analysis of small UAVs. In Proceedings of the 2012 9th European Radar Conference, Amsterdam, The Netherlands, 31 October–2 November 2012; pp. 210–213. 
31. Fioranelli, F.; Ritchie, M.; Grifﬁths, H.; Borrion, H. Classiﬁcation of loaded/unloaded microdrones using multistatic radar. Electron. Lett. 2015, 51, 1813–1815. [CrossRef] 
32. Traboulsi, A.; Barbeau, M. Identiﬁcation of Drone Payload Using MelFrequency Cepstral Coefﬁcients and LSTM Neural Networks. In Proceedings of the Future Technologies Conference; Springer: Cham, Switzerland, 2020; pp. 402–412. 
33. Schumann, A.; Sommer, L.; Müller, T.; Voth, S. An image processing pipeline for long range UAV detection. In Emerging Imaging and Sensing Technologies for Security and Defence III; and Unmanned Sensors, Systems, and Countermeasures; International Society for Optics and Photonics: Bellingham, WA, USA, 2018; Volume 10799, p. 107990T. 
34. Shamir, A.; Safran, I.; Ronen, E.; Dunkelman, O. A simple explanation for the existence of adversarial examples with small hamming distance. arXiv 2019, arXiv:1901.10861. 
35. Eykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati, A.; Xiao, C.; Prakash, A.; Kohno, T.; Song, D. Robust physicalworld attacks on deep learning visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18–22 June 2018; pp. 1625–1634. 
36. Barták, R.; Vomlelová, M. Using machine learning to identify activities of a ﬂying drone from sensor readings. In Proceedings of the Thirtieth International Flairs Conference, Marco Island, FL, USA, 22–24 May 2017. 
37. Traboulsi, A.; Barbeau, M. Recognition of drone formation intentions using supervised machine learning. In Proceedings of the 2019 International Conference on Computational Science and Computational Intelligence (CSCI), Las Vegas, NV, USA, 5–7 December 2019; pp. 408–411. 
38. McLaren, I.A. Methods of determining the numbers and availability of ringed seals in the eastern Canadian Arctic. Arctic 1961, 14, 162–175. [CrossRef] 
39. van Gemert, J.C.; Verschoor, C.R.; Mettes, P.; Epema, K.; Koh, L.P.; Wich, S. Nature conservation drones for automatic localization and counting of animals. In European Conference on Computer Vision; Springer: Cham, Switzerland, 2014; pp. 255–270. 
40. Laake, J.L.; Calambokidis, J.; Osmek, S.D.; Rugh, D.J. Probability of detecting harbor porpoise from aerial surveys: Estimating g (0). J. Wildl. Manag. 1997, 61, 63–75. [CrossRef] 
41. Hodgson, A.; Kelly, N.; Peel, D. Unmanned aerial vehicles (UAVs) for surveying marine fauna: A dugong case study. PLoS ONE 2013, 8, e79556. [CrossRef] [PubMed] 
42. Chittka, L.; Dyer, A.G.; Bock, F.; Dornhaus, A. Psychophysics: Bees trade off foraging speed for accuracy. Nature 2003, 424, 388. [CrossRef] [PubMed] 
43. Chen, B.; Perona, P. Speed versus accuracy in visual search: Optimal performance and neural architecture. J. Vis. 2015, 15, 9. [CrossRef] 
44. Chen, B.; Perona, P. Speed Versus Accuracy in Visual Search: Optimal Performance and Neural Implementations. In Computational and Cognitive Neuroscience of Vision; Springer: Singapore, 2017; pp. 105–140. 
45. Ha, I.K.; Cho, Y.Z. A probabilistic target search algorithm based on hierarchical collaboration for improving rapidity of drones. Sensors 2018, 18, 2535. [CrossRef] 
46. Yonas, A.; Pittenger, J. Searching for many targets: An analysis of speed and accuracy. Percept. Psychophys. 1973, 13, 513–516. [CrossRef] 
47. Kranakis, E.; Krizanc, D. Searching with Uncertainty. In Proceedings of the SIROCCO’99, 6th International Colloquium on Structural Information & Communication Complexity, LacanauOcean, France, 1–3 July 1999; pp. 194–203. 
48. Barbeau, M.; GarciaAlfaro, J.; Kranakis, E. GeocachingInspired Navigation for Micro Aerial Vehicles with Fallible Place Recognition. In Proceedings of the International Conference on AdHoc Networks and Wireless, Bari, Italy, 19–21 October 2020; Springer: Cham, Switzerland, 2020; pp. 55–70. 
49. Barbeau, M.; GarciaAlfaro, J.; Kranakis, E.; Santos, F. GPSFree, Error Tolerant Path Planning for Swarms of Micro Aerial Vehicles with Quality Ampliﬁcation. Sensors 2021, 21, 4731. [CrossRef] 
50. Lattimore, T.; Szepesvári, C. Bandit Algorithms; Cambridge University Press: Cambridge, UK, 2020. 
51. Barbeau, M.; GarciaAlfaro, J.; Kranakis, E. Risky Zone Avoidance Strategies for Drones. In Proceedings of the 2021 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE), Virtually, 12–17 September 2021; pp. 1–6. 
52. OlfatiSaber, R.; Murray, R.M. Distributed cooperative control of multiple vehicle formations using structural potential functions. IFAC Proc. Vol. 2002, 35, 495–500. [CrossRef] 
53. Vásárhelyi, G.; Virágh, C.; Somorjai, G.; Nepusz, T.; Eiben, A.E.; Vicsek, T. Optimized ﬂocking of autonomous drones in conﬁned environments. Sci. Robot. 2018, 3, eaat3536. [CrossRef] 
54. The 3rd Generation Partnership Project (3GPP). 3GPP TR 36.777, Enhanced LTE Support for Aerial Vehicles. 2017. Available online: ftp://www.3gpp.org/specs/archive/36_series/36.777 (accessed on 18 January 2022). 
55. The 3rd Generation Partnership Project (3GPP). UASUAV. 2019. Available online: https://www.3gpp.org/uasuav (accessed on 18 January 2022). 
56. The 3rd Generation Partnership Project (3GPP). Unmanned Aerial System (UAS) Support in 3GPP, Technical Speciﬁcation (TS). 2020. Available online: https://www.3gpp.org/ftp//Specs/archive/22_series/22.125 (accessed on 18 January 2022). 
57. Ullah, H.; Nair, N.G.; Moore, A.; Nugent, C.; Muschamp, P.; Cuevas, M. 5G communication: An overview of vehicletoeverything, drones, and healthcare usecases. IEEE Access 2019, 7, 37251–37268. [CrossRef] 
58. Mozaffari, M.; Kasgari, A.T.Z.; Saad, W.; Bennis, M.; Debbah, M. 3D cellular network architecture with drones for beyond 5G. In Proceedings of the 2018 IEEE Global Communications Conference (GLOBECOM), Abu Dhabi, United Arab Emirates, 9–13 December 2018; pp. 1–6. 
59. Bennis, M.; Debbah, M.; Poor, H.V. Ultrareliable and lowlatency wireless communication: Tail, risk, and scale. Proc. IEEE 2018, 106, 1834–1853. [CrossRef] 
60. Kishk, M.; Bader, A.; Alouini, M.S. Aerial base station deployment in 6G cellular networks using tethered drones: The mobility and endurance tradeoff. IEEE Veh. Technol. Mag. 2020, 15, 103–111. [CrossRef] 
61. Kishk, M.A.; Bader, A.; Alouini, M.S. On the 3-D placement of airborne base stations using tethered UAVs. IEEE Trans. Commun. 2020, 68, 5202–5215. [CrossRef] 
62. Pourbaba, P.; Ali, S.; Manosha, K.S.; Rajatheva, N. MultiArmed Bandit Learning for FullDuplex UAV Relay Positioning for Vehicular Communications. In Proceedings of the 2019 16th International Symposium on Wireless Communication Systems (ISWCS), Oulu, Finland, 27–30 August 2019; pp. 188–192. 
63. Fotouhi, A.; Ding, M.; Hassan, M. DroneCells: Improving spectral efﬁciency using dronemounted ﬂying base stations. J. Netw. Comput. Appl. 2021, 174, 102895. [CrossRef] 
64. Fotouhi, A.; Ding, M.; Hassan, M. Flying Drone Base Stations for Macro Hotspots. IEEE Access 2018, 6, 19530–19539. [CrossRef] 
65. Fotouhi, A.; Ding, M.; Hassan, M. Dynamic base station repositioning to improve spectral efﬁciency of drone small cells. In Proceedings of the 2017 IEEE 18th International Symposium on A World of Wireless, Mobile and Multimedia Networks (WoWMoM), Macau, China, 12–15 June 2017; pp. 1–9. 
66. Lin, X.; Wiren, R.; Euler, S.; Sadam, A.; Määttänen, H.L.; Muruganathan, S.; Gao, S.; Wang, Y.P.E.; Kauppi, J.; Zou, Z.; et al. Mobile networkconnected drones: Field trials, simulations, and design insights. IEEE Veh. Technol. Mag. 2019, 14, 115–125. [CrossRef] 
",Research Trends in Collaborative Drones.pdf,12
"Olga Burukina1,2 and Inna Khavanova2,3(&) 
1 The University of Vaasa, Wolfﬁntie 34, 65200 Vaasa, Finland obur@mail.ru 2 The Institute of Legislation and Comparative Law under the Government of the Russian Federation, B. Cheremushkinskaya Street 34, 117218 Moscow, Russia 3 The Financial University under the Government of the Russian Federation, Leningradsky Prospect 49, 125993 Moscow, Russia ahavanov@mail.ru 
Abstract. The emergence and use of advanced technologies in today’s commerce has gradually grown into habitual practice, and the introduction of more modern weapons including UAVs to military operations is hardly a new challenge in the history of armed conﬂicts. The interim research results concerning attitudes to drone usage have highlighted a number of contradictions in national and international law and policies and revealed a certain inconsistency in the respondents’ attitudes partially caused by the different width of Overton windows devoted to drone expansion in the two countries, as well as by the use of the sociocognitive tools currently changing the national attitudes and value systems as part of the national mentalities. The research has highlighted a number of contradictions that proved to be more professionspeciﬁc, age and genderspeciﬁc. 
Keywords: UAVs � Commercial use � Warfare � Human factors � Overton windows � International legislation � National regulations 
1 Introduction 
Twenty-ﬁve years ago nobody could imagine that UAVs or drones would enter our lives so tightly. Of course, they are still far from the popularity of mobile phones, but there is no doubt – a real technological revolution is taking place right before our eyes: small and large, ﬂying and crawling, radiocontrolled and practically autonomous – all sorts of drones have been entering and inﬂuencing human lives. Today drones have found application in many areas: ﬁlmmakers shoot videos from a bird’seye view, emergency services investigate dangerous terrains, online stores have been gradually replacing couriers with drones, and drone competitions spreading all over the world – e.g. on 28 July 2018 more than 3,000 guests attended Drone Racing League event at BMW Welt, with the entire Drone Racing League series broadcasted every Thursday from 13 September 2018 until midDecember, and the 
© Springer Nature Switzerland AG 2020 J. Chen (Ed.): AHFE 2019, AISC 962, pp. 217–229, 2020. https://doi.org/10.1007/978-3-030-20467-9_20 
2 Rise of the Drones 
By 2025 the development of UAVs can create 100,000 jobs for the US economy and give an economic effect of $82 billion. The research service of Business Insider predicts annual growth rates of 19% in this industry between 2015 and 2020, with the growth of the military expenditure on drones by 5% only. Amazon, Just Eat, Flytrex, UPS, DHL have been practicing cargo delivery by drones, reducing delivery time drastically. In 2017 Dubai tested a Volocopter drone for passenger transportation. 
2.1 Commercial Use 
The global market for commercial use of unmanned aerial vehicles has exceeded $127 billion, according to the results of a study by PwC. Currently, the most promising industry for the introduction of drone is infrastructure, with the market valued at $45.2 bn, next come agriculture ($32.4 bn) and transport ($13 bn) [2]. However, even here drones raise enormous privacy concerns as they can be easily abused. Therefore, before acquiring drones, two threshold questions should be asked – (1) whether the local community really needs them and whether it has rigid safeguards and accountability mechanisms in place, so that no one uses drones for warrantless mass surveillance. 
2.2 Warfare 
The increased use of drones for civilian applications has presented many countries with regulatory challenges. Such challenges include the need to ensure that drones are operated safely, without harming public and national security [3]. However, some 
218 O. Burukina and I. Khavanova 
3 The Use and Abuse of Drones 
Drones are the ﬁrst cast of the key that opens the Pandora’s Box, and if the stakeholders do not take action today, tomorrow the uncontrolled contents of this box, breaking free from it, can destroy humanity. Today is high time to take the necessary steps. 
3.1 Both Parties Easily Abused 
The military doctrine of the Russian Federation refers the massive use of unmanned aerial and autonomous naval vehicles, guided robotic models of weapons and military equipment to the typical features of modern military conﬂicts [6]. The US Department of Defense in Unmanned Systems Integrated Roadmap FY2017-2042 states, “Advances in autonomy and robotics have the potential to revolutionize warﬁghting concepts as a signiﬁcant force multiplier” [7]. Besides, the new technology now allows perpetrators to stalk and harass victims, as well as avoid restrictions imposed by restraining orders [8]. Yet, in the absence of sound regulations, both parties (not only the military and civilians or criminals and victims) but humans and robotized selflearning items will be increasingly abused. 
3.2 Human Responsibility 
Human responsibility for drone use is threefold. First, it is strategic – people (both the decision makers and the general public representatives as in the future everyone will be concerned) should work out a global strategy for UAVs and other robotized systems development taking into account the need to preserve the present alignment of forces – humans’ rule, not robots. Second, it is humanity’s selfpreservation – not yet the standoff against robots but primarily the protection against the privileged ones making decisions to use drones and discarding possible poorly justiﬁed civilian casualties, with 
Human–Drone Interaction: Virtues and Vices 219 
4 Research Outline 
The undertaken pilot research was aimed at both identifying the ongoing processes in the national legislations of the United States and Russia, which seem to have been experiencing a new round of heightened tension in international relations, and revealing the attitudes of future decision makers – today’s Russian university students and their supporters and educators – Russian academics and university managers. The survey was supposed to be conducted simultaneously in Russian and US universities, but the American colleague addressed did not support the idea or even replied to the request. The research methodology is based upon comparative and systemic approaches to the US and Russian national legal systems in their application to drone usage in such drastically contradicting spheres as commerce and war conﬂicts and includes analysis of national juridical norms and legal practices, as well as a pilot survey aimed at revealing the attitudes of US and Russian university students to the achievements, failures and prospects of drone application in national and international commerce, as well as the past and ongoing military conﬂicts, taking into account the previously unseen economic and humanitarian beneﬁts yielded by drone usage, along with the virtualization of drone operators’ responsibilities for any possible wrongs. The research consisted of two parts – one was based on thorough investigation of the current US and Russian legislation and legal discussions, both national and international; while the other was a pilot qualitative research based on a multiplechoice questionnaire. 
4.1 Research Goal and Hypotheses 
The undertaken pilot research was aimed at both identifying the ongoing processes in the national legislations of the United States and Russia, which seem to have been experiencing a new round of heightened tension in international relations, and revealing the attitudes of the future stakeholders and decision makers as regards to the future development of the Russian Federation – those of todays’ students of pedagogy, economics and management. The survey based on two hypotheses – 
Hypothesis 1 – students (people aged under 25) feel freer with innovations and are less cautious concerning their use (UAVs included). Hypothesis 2 – students (people aged under 25) are more sensitive to decisions taken by single persons (even experts) and not so trustful when it comes to issues that might concern them directly. 
The research analysis of the Russian and US legislation and legal and political discourse have identiﬁed ﬁve challenges facing the growing use of UAVs/drones in the world as both a prominent logistics and warfare tool: 
220 O. Burukina and I. Khavanova 
Challenge 2 – Economic: the economic motives aimed at beneﬁts (proﬁts included) from using UAVs instead of soldiers (who are evidently more vulnerable and quite costly taking into account their training, equipment, supplies, transportation, accommodation, salaries, insurance, hospital treating, pensions, etc.) can surpass and prevail in decisions taken by core stakeholders of leading economies. 
Challenge 3 – Political: with UAVs opening new prospects and opportunities in armed conﬂicts, as well as in all sorts of political conﬂicts and collisions as both a new means and method of their resolution, they simultaneously turn into both an effective and efﬁcient means of strong political inﬂuence, with no country in the world feeling safe enough. 
Challenge 4 – Social: practically every society in the world will face a dilemma of looking down on the possibility of unlawful and unjustiﬁed civilian casualties in other countries or looking up hoping that drones will never be used to harm them, their families and friends. 
Challenge 5 – Organisational: with strong political lobbies, great proﬁts and economic interests of drive manufacturers and the governments of leading economies, the international legislation regulating drones production and application will be kept unchanged as long as possible and organizational difﬁculties will be used to avoid possible fast solution of the current legal turmoil in regulating the widening use of drones in all spheres including targeted killings inside and outside military conﬂicts. 
4.2 Research Method and Processes 
The research methodology was based upon comparative and systemic approaches to the US and Russian national legal systems in their application to drone usage in such drastically contradicting spheres as commerce and war conﬂicts and includes analysis of national juridical norms and legal practices, as well as a pilot survey aimed at revealing the attitudes of Russian university students, academics and managers to the achievements, failures and prospects of drone application in national and international commerce, as well as in the past and ongoing military conﬂicts, taking into account the previously unseen economic and humanitarian beneﬁts produced by drone usage, along with the virtualization of drone operators’ responsibilities for any possible wrongs. The designed questionnaire contained 3 background questions and 13 thematic multiplechoice questions intended to identify the general attitude of Russian undergraduate and graduate students and university employees to the drone’s employment both in civil and military operations, both inside and outside armed conﬂicts, as well as opportunities and risks following their predicted and unpredicted potentials. The participation in the survey was voluntary, which was a delimitation of the pilot research, corresponding to its goal and conditions: the questionnaire was printed out on paper and distributed among students and employees to be collected 20 min later. 
Human–Drone Interaction: Virtues and Vices 221 
Aimed at a pilot research, the sample was limited to 100 respondents – 75 Russian undergraduate and graduate students from one Russian university – Lipetsk State Pedagogical University – and 25 Russian academics and managers from the same university. The random sampling covered over 70 third and fourthyear Bachelor students and ﬁrstyear Master students, with 57 of them providing valid responses in the questionnaires, and 25 employees of the same university, with 22 of them providing valid responses. Both groups of the respondents took part in the survey in similar research situations. The students returned 71 questionnaires, with 14 of them containing insufﬁcient information for further analysis and the employees returned all the 25 questionnaires, with three of them lacking part of the required information to the extent necessary for further analysis. The described situation has revealed a problem of either a lack of interest to the research topic or insufﬁcient time length due to the novelty of the topic to both groups of the respondents. As 20% of the respondents have practically fallen out of the research (with their responses being only partly valid and thus discarded by the researchers), the authors intend to interview several representatives from both groups in order to reveal the cause of the problem so that it could be avoided in the future. 
5 Research Results 
The students’ and the employees’ responses were not unanimous, which reﬂects the respondents’ voluntary and free expression of their opinions, without any prior preparation or instructions given but the request not to skip questions, express their true attitudes and not exceed the 20 min’ time limit. The responses have provided a variety and inconsistency of the Russian students’ knowledge of UAVs and their usage, though some of the results have proved to be quite opposite to the expected attitudes of university students and university employees. 
5.1 Analysis of the Responses 
The survey intended to reveal the primary attitudes, which could be analysed and generalized to serve as basis for further research and did not aim at identifying deep personal reasons of the students’ and employees’ reactions. Therefore, the questionnaire contained 16 questions – 3 background questions and 13 thematic multiplechoice questions, which yielded the following results considered below. The background questions aimed at revealing the respondents’ age, gender and occupation as this information has revealed their maturity and responsibility for these particular responses, partly explained their attitudes and gave ground for further research of the topic. It is evident from the histograms that most of the respondents are students – under 25 years of age with the standard deviation of about 5 years. The range of the age was found to be 38 years starting from 19 and up to 57 years of age. The data related to the respondents’ age are presented in Fig. 1. 
222 O. Burukina and I. Khavanova 
regular basis, their positions are not listed as researchers, so they did not indicate themselves as such, while researchers having no teaching workloads did not take part in the survey. In the absence of big enough separate samples of academics and managers, their responses were combined and analysed in one united sampling. The data are presented in Fig. 3. Question 1 was intended to identify the general attitude of the respondents to the emergence of UAVs/drones. The responses revealed that 29% of the respondents approved and 8% fully approved the emergence of drones, 6% disapproved and 5% completely disapproved their emergence, with a majority of the respondents (52%) remaining neutral. The neutral majority indicates the respondents’ absence of experience in this issue (they have never used or dealt with drones in any way) – see Fig. 4. 
Fig. 1. Age groups 
Fig. 2. Respondents’ gender 
Fig. 3. Occupation 
Human–Drone Interaction: Virtues and Vices 223 
Question 3 was supposed to identify the respondents’ perception of the safety/danger category. The results have revealed a majority of neutral attitudes, which again must have proved the absence of the respondents’ experience and their unwillingness to be involved in any issues concerning drones. The respondents considering drones fully safe (3%) proved to be students, with only 1% of the students ﬁnding drones quite dangerous against 4% of employees. 32% of all the respondents are conﬁdent about potential danger of drones, with only 14% thinking that they are safe (Fig. 6). 
Fig. 4. General attitude to the emergence of drones 
Fig. 5. Assessment of drones as useful/harmful 
Fig. 6. Assessment of drones as safe/dangerous 
224 O. Burukina and I. Khavanova 
Question 5 was intended to ﬁnd out the respondents’ attitudes to the use of UAVs/drones in armed conﬂicts. The responses showed similar extremes – 8% fully approved and 8% completely disapproved the use of drones in armed conﬂicts, with 15% of the students and only 9% of the employees fully approving the use of drones in armed conﬂicts, and 12% of the students and 0% of the employees completely disapproving it. 28% of the students approved the application of drones in armed conﬂicts against 42% of the employees, which may reﬂect the employees’ deeper emotional involvement in the recent and ongoing armed conﬂicts (Fig. 8). 
Fig. 7. Dis/Approval of drones applicability in commerce 
Fig. 8. Dis/Approval of drones use in armed conﬂicts 
Fig. 9. Probability of drones abuse 
Human–Drone Interaction: Virtues and Vices 225 
5.2 Comparative Results 
The responses to Questions 6–7 and 9–13 obtained results, which were worth comparing as they revealed the ongoing political and legal discussions concerning the use of drones inside and outside hostilities and provided deeper understanding of the attitudes of two generations – academics and university managers in charge for the education of tomorrow’s decision makers in Russia. Question 6 was expected to clarify the respondents’ attitude to the use of drones in military operations by national armed forces (the Armed Forces of the Russian Federation included). The employees proved to be more involved in the issue (18% neutral only against 33% of the students) and much more positive: 65% approving (against 40% of the students) and 13% fully approving (with 12% of the students), with 4% of disapproving employees with zero complete disapproval (8% of disapproving and 7% of completely disapproving students, which might express their disapproval of Russia’s involvement in any armed conﬂicts). Question 7 aspired to reveal the respondents’ attitudes to the possibility of drones use in peacetime outside armed conﬂicts by security forces (FSS, CIA, FBI), both Russian and American taken as common examples. The results received were a bit unexpected – 34% of the respondents did not mind, with 32% approving security forces’ use of drones and 8% fully approved it, with 19% of disapproval and 8% of complete disapproval. However, the comparative analysis has revealed a higher level of the Russian employees’ approval – 51% (against 24% of approving students), with 12% of students completely disapproving against the employees’ zero disapproval in this option (see Fig. 10). 
Fig. 10. Attitude to drones use by security forces 
226 O. Burukina and I. Khavanova 
Human–Drone Interaction: Virtues and Vices 227 
6 Conclusions and Discussion 
The results of the legislation and discourse analysis supported by the survey ﬁndings have highlighted a scope of problems proving the correctness of the previously formulated economic, social, political, legal and organisational challenges facing the drone use and possible abuse of both UAVs and humans. Hypothesis 1 has been proved partly as the students participating in the survey have expressed a high enough level of cautiousness – very probably because they were mainly females. Hypothesis 2 has been proved, though the employees have also showed a high level of sensitivity to decisions taken by single persons (even experts) and a high enough level of distrust, which again might be explained by a majority of females in the employees’ group. The ﬁve challenges revealed above, facing the growing use of drones as both a prominent means of commerce and warfare and the results of the survey involving Russian students and university employees as stakeholders of Russia’s policies future development, have prompted ﬁve priority issues highlighting the primary steps to be taken to resolve the existing problems in legal, political and organisational regulations of drone use in the present and future both in peacetime and armed conﬂicts. 
Priority issue 1: The need for a comprehensive review of the current legal basis (both national and international) for drone application in commerce and warfare in order to pinpoint contradictions and inconsistencies and reveal possibilities for misuse and abuse of drones as means and methods of war and commerce (e.g. logistics). 
Priority issue 2: The need for national and international changes to be introduced to the legal basis for the development of economic use of drones – at the level of WTO and other international organisations (ITC, ECO, WHO, AITIC, FAO, etc.) for the economic development and prosperity of the world to the fullest. 
Priority issue 3: The need for deeper political involvement of all the drone stakeholders (80 countries) in order to prevent future clashes of economic and political interests and the use (and abuse) of drones as means and method of their resolution. 
228 O. Burukina and I. Khavanova 
Priority issue 5: The need for International Law, IHRL and IHL reconsidered and amended to the necessary extent under the new conditions, to regulate the core terms of drone’s production, sale and application at the global level and take the necessary organizational measures in order to reach global consensus as soon as possible. 
References 
1. Hetzenecker, J.: More than 3,000 Guests Attend Drone Racing League Event at BMW Welt. BMW Group. https://www.press.bmwgroup.com/global/article/detail/T0283592EN/morethan-3-000-guestsattenddroneracingleagueeventatbmwwelt?language = en (2018) 2. Clarity from Above. PwC Global Report on the Commercial Application of Drone Technology. https://www.pwc.pl/en/publikacje/2016/clarityfromabove.html (2016) 3. Regulation of Drones. The Law Library of Congress, Global Legal Research Center. https:// www.loc.gov/law/help/regulationofdrones/index.php (2016). Accessed 21 Dec 2018 4. Qureshi, A.W.: The Legality and Conduct of Drone Attacks. Notre Dame J. Int. Comp. Law 7 (2), Article 5 (2017). https://scholarship.law.nd.edu/ndjicl/vol7/iss2/5 
5. Karyakin, V.V.: UAVs – a new war reality. Prob. Natl. Strat. 3(30) (2015). Available in Russian at https://riss.ru/images/pdf/journal/2015/3/10_.pdf 
6. The Military Doctrine of the Russian Federation. Approved by the President of the Russian Federation on 25.12.2014 г. (# Pr-2976). http://base.garant.ru/70830556/ (2014) 7. Unmanned Systems Integrated Roadmap FY2017-2042. http://www.defensedaily.com/wpcontent/uploads/post_attachment/206477.pdf (2017) 8. Branley, A., Armitage, R.: Perpetrators using drones to stalk victims in new age of technology fuelled harassment. https://www.abc.net.au/news/2018-10-01/dronesusedtostalkwomeninnewageofharassment/10297906 (2018) 
Human–Drone Interaction: Virtues and Vices 229 
",Human–Drone Interaction  Virtues and Vices.pdf,13
"Andreas Neil Jensen Aalborg University Aalborg, Denmark anje15@student.aau.dk 
Matias Jensen Aalborg University Aalborg, Denmark matjen15@student.aau.dk 
Abstract This paper, through the use of a HoloLens Head Mounted Display (HMD) and a Tello EDU Drone, presents a research tool as well as two expert reviews, aimed at exploring path visualisation using augmented reality for firefighters in their efforts to find and rescue people in danger. Drones have seen some usage in the field of firefighting, especially in the context of highrise fires and plantation fires. So too has Augmented Reality (AR) devices historically been used in the context of providing firefighters with thermal visioning information, how ever, not much research has gone into combing the two technologies in a firefighter/rescue worker centric context. We discuss the implications of our findings both during the development of the research tool and as a result of our expert reviews in the future work and discussion section respectively. Lastly a short conclusionary take on the work will be presented to the reader, summarising the work described in the paper. 
Keywords 
AAU, Aalborg, University 
ACM Reference Format: Andreas Neil Jensen and Matias Jensen. 2020. A Research Platform for Drone Assisted Firefighting with AR Path Visualisations. In . ACM, Aalborg, Denmark, DK, 11 pages. 
1 Introduction 
It lies within the nature of working as a firefighter or a rescue worker, that one also has to work during periods of intense stress [12], time is, in other words, of the essence and time saved, is time the firefighter in question does not have to spend in impending danger. As such an interesting problem domain arises in trying to minimise the amount of time spent, for example, inside a burning building in order to save 
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact , ,the owner/author(s). © 2020 Copyright held by the owner/author(s). 
survivors, this problem domain can be summarised by the following problem statement: 
""How do we best minimise the amount of time firefighters have to spent in hazardous situations?"". 
While there have been development of AR solutions usable for firefighters serving during fires as well as no small amount of drone solutions serving to stake out an ongoing fire for possible Points Of Interest (POIs), there have yet to be many solutions combining the two technologies. Beyond that, the solutions developed thus far, with regards to drones, have yet to fully utilise the possibly autonomous capabilities offered by drones. 
This paper will describe the development of a research platform using AR as well as autonomous drones in order to aid research in the area of firefighting in house fires. The paper will furthermore also feature expert reviews of said research platform by professionals. With regards to the structure of the paper, related work will follow in the following section. The related work section is divided into areas of importance to the research platform, namely: ’Drones in Firefighting’, ’Mixed Reality in Firefighting’, ’Drone Visualisation’ and ’Drone Pathfinding’. Next a section covering the implementations of the research platform as well as any considerations to be had in the context of implementation. Lastly the paper will cover a discussion section, allowing the reader to get a indepth look into the consequences of choices made, during the development of the research platform as well as a conclusion tying the work together, and summarising our work. 
2 Related Work In creating systems for use in stressful situations, clear and obvious directions are often needed, in fact, in the paper ’Vigilance Performance Modelled as a Complex Adaptive System with Listener Event Graph Objects (LEGOS)’, stress is shown to have a direct impact on factors such as reaction time and vigilance, we in this paper stipulate that this must necessarily be especially true in lifeordeath situations where failure to understand the correct usage of any given system in time could potentially lead to the loss of human lives. As such, an important area of focus for the project 
should be to ensure that the usability of the system is up to par with the users requirements, even during times of heavy stress and confusion[10]. 
Drones in firefighting Using drones for hazardous assignments has already been thought of and is widely used in many areas, for example the usage of drones has been used in order to relay important information to rescue personnel during wildfires as well as highrise fires[14]. 3D Robotics’ have developed an API, “DroneKit"", which features both piloted and some autonomous flight capabilities, such as GPS and camera control[14]. Most fully developed systems, how ever, does not feature autonomous path finding and exploration, something which would indubitably serve to free up resources in the context of, for example, fire rescue work. In the paper ’Autonomous Drones for Assisting Rescue Services within the context of Natural Disasters’, Histogram of Gradients (HOG) is used in order to detect survivors during disasters[11]. The very same system also used a ’RaoBlackwelissed particle filter’, in order to extrapolate movement direction of the detected survivors[11]. Lastly the paper very explicitly remarks on the importance of the manipulation of such systems not requiring any special skills, seeing as such requirements would be detrimental to the successful integration of the system into rescue teams. 
Mixed Reality in firefighting The Qwake’s CThru Headsup display allows firefighters to substitute the use of handheld thermal vision cameras with a single AR display device allowing for seamless navigation through dark and smoke filled rooms as well as thermal visioning helping firefighters in seeking out survivors of a fire in real time. The Qwake’s CThru device also allows for live streaming of anything the wearer is watching providing whoever is responsible at the scene of the fire with valuable onscene intel[13]. The paper ’Using Serious Games and Virtual Simulation for Training in the Fire Service: A Review’ reviews the development of serious games and virtual simulation applications with the purpose of being utilised for training in the fire service. The paper refers to such applications as ’serious games’ and claims that as of the the publishing of said paper (2014), gaming technology was not capable of completely and faithfully encompassing the very dynamic and varied real world scenarios that firefighters could possibly be facing while performing their work[6]. In the paper ’Finding Our Way In The Dark: A Firefighters Augmented Reality HeadsUpDevice’ Google Glasses as well as edgedetection technology is used in order to develop a Glassware application which enables firefighters to have an augmented view during lowvisibility environments. The 
paper claims that the most contributing factors to low visibility is heavy smoke, heated air, and transient flames[5]. The paper ’Handheld Augmented Reality Indoor Navigation with ActivityBased Instructions’ presents a novel design of AR interfacing in order to support indoor navigation through the use of arrows in order to infer directional information[3]. In order to convey a path, the paper ’Human pacman: a wide area sociophysical interactive entertainment system in mixed reality’ uses little yellow spheres, inspired by the target objects one is tasked with collecting in the well known computer game ’Pacman’[2]. It is possible that using similar spheres as a metaphor for those aforementioned target objects might infer direction. 
Drone Visualisations The paper ’Multiview Augmented Reality with a drone’ presents an exploratory research effort into AR applications where the users of said systems are able to control alternative viewing positions based on a drone, the paper then presents the resulting system specifications of a system which offers multiview AR as well as a demo application in which the user can switch between first and third person view. The results of the paper serves as evidence to the applicability of a system offering alternate viewing experiences[1]. In the conference paper ’Augmented Reality Based Thermal Drones for Victim Detection’ a system such as the one presented in the introduction to this section is presented. The paper proposes the use of thermal imaging, drones as well as AR in order to provide aid rescue workers in locating and rescuing victims. The results of the paper show that the described approach enhances the efficacy of rescue workers in terms of time and amount of humans located[4]. 
Drone Pathfinding Autonomous movement capabilities necessitates pathfinding algorithms. In the paper ’An algorithm for onthefly K shortest paths finding in multistorey buildings using a hierarchical topology model’ the exploration of buildings with complex vertical and horizontal features are presented. The approach is based on computing several shortest paths using building information models implemented using directed multigraphs[8]. The paper ’Distributed path planning for building evacuation guidance’ presents a distributed path planning approach meant to provide guidance for reliable and efficient building evacuation in emergency situations. The approach involves networked sensors, information sharing and distributed computation[15]. 
An interesting concept seems to arise when one combines 
Figure 1: A visual representation of a drone assisting in finding survivors 
the fully autonomous capabilities of drones with the handsfree operations of AR in that the resulting piece of technology would allow for a safe, automated, handsfree, search and rescue tool which could potentially help save human lives during a disaster. And while research into such systems has already been conducted, considerations such as User Interface (UI), Human Computer Interaction (HCI) and path finding remains interesting avenue of research which remains in its infancy. 
3 System 
It is dangerous for firemen to enter burning buildings, in such cases the firemen in question are forced to make difficult choices. Do they enter the burning building and hope to find survivors? and if so, where are the survivors most likely to be found? Due to the nature of burning buildings, rescuing people from fires is as much a matter of time issue as it is of weighing the risks of entering the burning building. Therefore employing the usage of drones in order to scout a burning environment would ensure that the risk to human lives are minimised, and depending on the amount of drones employed the time necessary to find survivors could even be lessened. 
Vision The base idea is to have a building explored by autonomous drones reporting the position of survivors to a AR HMD or application, enabling the real time visualisation of paths leading to survivors for firefighters. As visualised in Figure 1 one or more drones are sent in to search the building providing realtime instructions to firefighters such that they can get to a survivor as fast as possible while avoiding dead 
Figure 2: The different components used in order to construct the system. 
Figure 3: The flow of information in the system. 
ends. We propose a system that can track the drone’s position, in our example we suggest utilising the Vicon system with Vicon cameras as well as the Vicon Tracker software as can be seen in Figure 2. The flow of the system is shown in Figure 3. As we get to the point where it is possible to get very precise positional information in small spaces the Vicon system could be omitted which would result in less equipment being required and therefore less setup. The goal is to make the system as intuitive and fast to use as it should be competitive with the speed of which current operations are done. 
Figure 4: Storyboard of a demonstrational video made for the purpose of showing our vision for the system 
Consider the following scenario Based on our storyboard in Figure 4, a potential scenario can be derived. A fireman at the place of a burning construction puts down a drone in front of him, the drone takes off into the air and heads inside the burning building. Inside the building, the drone navigates through the rooms of the building searching for people that may be trapped inside by the flames. Upon detecting a person the drone will alert the fireman observing the situation from the outside that a person has been found. The drone supplies a path that will become visible using the fireman’s AR HMD and the fireman can enter the building, going straight for the person in distress and lead them out of the building. Meanwhile, the drone continues to look for additional people and the fireman spends as little time inside the building as possible in order to stay out of potentially dangerous situations. 
Technologies 
Augmented Reality AR is a concept in which you augment the real world, usually through the use of holograms but any change to the real world. 
Compared to Virtual Reality (VR), in which you create a virtual world that may not correspond to the real world, AR does rely on the real world environment in which the user experiences it. In addition to VR and AR there also exists a third concept, Mixed Reality (MR). MR can be seen as a branch of AR as, generally, all MR is also AR, however, not all AR is MR. In order for a AR application to be considered MR it has to have the ability to interact with the physical objects in the real world environment, while a pure AR environment does not. 
HoloLens The HoloLens is a MR HMD developed by Microsoft which utilises the Windows Mixed Reality platform. Here we will focus on the AR capabilities of the HoloLens as that will be the focus of this project. The AR capabilities of the HoloLens makes it able to show visual objects, called holograms, which the user can see, and interact with, in the real world. The HoloLens contains a custom built Microsoft Holographic Processing Unit, based on Intel 32-bit architecture. The specifications does not state clock speed of the device, but it can be assumed to be on the level of a Windows Phone. As for memory, the HoloLens comes with 64 GB flash memory for apps as well as 2 GB of RAM. There is support for both WiFi and Bluetooth LE on the device1. 
Vicon Tracker In order to perform extremely precise object tracking, Vicon has created a tool aptly called Tracker which provides realtime data with minimal latency. Vicon furthermore also offers a Unity plugin allowing for direct data transfer between their own Tracker software and Unity, enabling real time translation between real time coordinate systems and virtual ones. Tracker itself is claimed to be able to process data in as little as 1.5ms, at more than 500 frames per second using their own proprietary cameras2. Vicon offers a variety of different cameras3, the cameras are optical motion capture cameras made specifically for observing objects and movement in a space covered by a set of cameras. The objects are tracked using indicators on the tracked objects in a certain pattern unique to a given object such that they are distinguishable from each other. Before using the Vicon system there is a calibration step in which a special wand is used together with the cameras in order to define the center of the area in which objects are tracked. 
1https://docs.microsoft.com/enus/hololens/hololens1-hardware 2https://www.vicon.com/software/tracker/ 3https://www.vicon.com/hardware/cameras/ 
Unity 3D Unity3D is a powerful tool for many use cases. While being initially developed for the purpose of game development, it has, since then, been used in areas, not necessarily game related, such as AR and VR. Unity 3D is, as the time of writing, the preferred, and suggested, way of developing for the Microsoft HoloLens. During the development of our system, Unity underwent a transition period of their AR and VR implementations where they are separating the features from the engine itself. This means we had to make a choice whether we wanted to use the new system or the legacy system. In order to make a decision we looked into both options. The benefit of using the new system would be the fact that it would be compatible with future versions of Unity and therefore easier to maintain going forward should the system be continued in the future. However, we discovered that, at the time of writing, the new system lacked the sufficient documentation to set it up and thus we would have been required to invest a lot of time into figuring the system out for ourselves. As none of us had previous experience with AR and the workflow hereof we decided to setup the legacy system in the current implementation of the system until the new system is more mature. Using the legacy system means that we will have the resources we need to set it up as well as getting help if we encounter issues. The downside is the fact that it is a legacy system and therefore will need to be updated in later versions of Unity if the project is continued at a later time. 
Method 
For the purpose of improving the platform we will be proposing a heuristic inspection that could be performed on iterations of the system as it is being developed. This is aimed to help guide the direction of the project while ensuring the system will perform as expected. We will also be conducting a study in the form of an expert review in order to get a better understanding of the problem domain and get new insights into potential issues from experts in the field of drones and firefighting. 
Inspection The heuristic inspection will be focused on the evaluation of essential features of the platform that we have implemented as well as an evaluation of how those features could be improved by conceptual features that were not implemented in this version of the platform but is assumed to provide additional value to it. In our case we will utilise a modified set of heuristic inspection criteria formulated by one Gerhardt Powal whom developed the criteria specifically for human computer interaction, anchored in the cognitive sciences. The following is a list of those criteria[7]. 
• Automate unwanted workload. • Reduce uncertainty. • Fuse data. • Present new information with meaningful aids to interpretation. 
• Include in the displays only that information needed by the operator at a given time. 
• Practice judicious redundancy. • Reduce danger to the individual firefighter. 
The first criteria aims to reduce the amount of ’lowlevel’ cognitive thinking required to complete a task, thus freeing up cognitive resources for ’high level tasks’. Reducing uncertainty refers to the way information is displayed to the user and ensuring that the data shown is clear and obvious in order to reduce decision time and the chance of failure. The criteria of fusing data refers to, where applicable, fusing multiple low level data sets into higher level summation data sets, reducing the cognitive load of understanding said data. The fourth criteria aims to present new information in ways that are familiar to the user, in order to make the newly provided information easier to absorb, thus reducing cognitive load. Names and labels should be representations of the conceptually related function in order to improve recollection and recognition. Similar to fusing, Grouping data aims to reduce the cognitive load of understanding data by grouping relevant data together for example within a screen. Where applicable one should prefer to use colours and graphics in order to limit the amount of raw data driven tasks, this reduces the amount of time necessary to be spent on assimilating data. Extraneous data should where applicable not be shown to the user, allowing the user to concentrate on critical assignments more easily. When displaying data to the user, the format of data should encompass varying formats and/or levels of detail in order to better accommodate a varying user base. In spite of the focus of many previous criteria seemingly limiting the redundancy of shown data, the tenth criteria is included in order to handle the sometimes necessary inclusion of more information than may be needed at any given time, in order to be consistent.[7] 
Expert Assessment In order to ensure the usefulness of a research platform in this field, we include some expert assessment of the platform. The profile of the experts includes firefighters, drone operators and general rescue workers. In order to perform the expert assessment we created a video demonstrating the platform as well as a use case of the system proposed by the platform. This video is shown to the experts and they 
will provide feedback. The video will be a visual representation of the scenario described earlier in section 3. Before making the video we developed a storyboard covering the interesting points of interest that the video should include, this storyboard can be seen in Figure 4. The main points covered during the expert assessment will be the viability of the system, exploratory questions widening our understanding of the problem domain and confirming our own initial suspicions on the topics of the questions. The expert review is conducted in the form of an qualitative interview to gather feedback directly from users working in the fields of interest of the problem domain as part of an empirical contribution as described by Jacob O. Wobbrock and Julie A. Kientz.[9] Our findings from these expert reviews will be discussed together with our remaining results. 
Our Implementation 
The implementation for the research platform is developed as two separate applications. One for controlling the drone and one responsible for the HMD information and path visualisation. The two systems communicate over a TCP connection where the drone is sending updates to the HMD. These updates include a video feed from the drone to the HMD as well as path information to visualise a path. On the HMD side of things we have functionality to visualise paths and on the drone side of things we have some functionality for using OpenCV as a means to detect survivors. As these functionalities were experimented with at the start of the project, they are, at the time of writing, separate experiments and not coupled as a system 
The drone application is capable of relaying a path, in the form of a sorted list of coordinates, through the TCP connection to one or more connected HMDs. It is also responsible for all communication to and from the drone. Although the vision is for the drone to be autonomous, the current implementation only supports manual control. Experimental support for relaying the video feed from the drone over the network has been included but, at the time of writing, has not been fully implemented in the current system. The drone also has a very basic OpenCV implementation for detecting people by analysing the video feed from the drone. There is more work to be done on this feature and in the current system it has not been tested. The application on the HMDs is responsible for all the visualisation that is being shown to the wearer based on input from the drone application. Current functionality includes the ability to generate visualisations for paths given a list of coordinates. 
In addition to the twopart system described above, we developed a separate system, a prototype, that was used in the 
making of a demonstrational video. The focus of this system was to allow us to easily set up environments that represents the look and feel of our vision. We also used this system in order to test different visualisations, based on findings in our related work, that were included in the video. The visualisations include spheres and arrows both of which have been found to infer direction. The video, which can be found on YouTube4, goes through a potential use case of the vision, in the main part of the video we show the implementation using spheres. Later in the video we show a side by side comparison of the spheres and arrows in order to get insights into which of these visualisations would be preferred and why as we had some initial thoughts on the matter and wanted to compare that to the expert assessments. 
4 Results The following sections will cover the results of expert reviews performed with a professional fire master and firefighter as well as a second person, who is an expert in the field of drone technology. The system presented in the concept video was generally well received and received praise both from the firefighter and the drone expert, how ever as can be seen in Figure 5 which covers the results of more strictly structured questionnaire type questions, the system does not, in the mind of the firefighter, seem to adequately help alleviate stress nor improve the safety of firefighters performing their job. During an initial conversation with one of the interviewees, we discussed the implications of the system, to which he pointed out that no matter the cost to personal safety, from the firefighters side, the firefighters are, and will always be, sent inside the burning building immediately, with no time to wait for a drone to report the position of a survivor. As such the purpose of the system described in section 3 changes slightly from enabling firefighters to locate survivors while not even necessarily being in the building, to enabling concurrent use with firefighters in order to minimise time spent inside a burning building while searching for survivors. 
The following sections will cover information learned directly as a result of less strictly structured parts of the expert reviews. The sections will be divided into issues to do with the drone, firefighter and context of the system respectively. 
Issues and Concerns for the Drone 
The size of the drone The interview with the drone expert yielded interesting considerations with regards to the size of the drone. While a small drone is generally more effective at entering smaller 
4https://www.youtube.com/watch?v=XIiL5Lnt7QA 
spaces as well as generally easier to pilot, larger drones are more often made of more durable materials like metals or carbon fiber which becomes relevant in that the drone in this case runs the risk of bumping into various debris while searching the building, larger drones are also generally more stable in violent air conditions than smaller drones. 
Drafts of air Pointed out by both the firefighter and the drone expert is the fact that the atmosphere inside burning buildings can be filled with turmoil and violent air drafts posing a hindrance to the flight of drones. 
Battery performance in high temperatures A concern addressed during the interview with the drone expert has to do with lithium batteries’ varying output discharge depending on the temperature of the lithium battery. 
Issues and Concerns for the Firefighter 
Spheres versus arrows With regards to the two different path visualisations presented in the concept video, the spheres and the arrows, the drone expert mentioned that while the spheres posed a more easily recognisable figure, the arrows might prove more capable of inferring direction, something that he deemed to be essential in the work of firefighting. When interviewing the firefighter though, this proved not necessarily to be the case, as he remarked that direction, and the inference of direction within the scope of this system, isn’t necessarily important seeing as the firefighters would never enter a burning building without their fire hose, which is tangentially used to infer direction of travel in that they would always just be able to follow the hose back out of the building. With regards to the question of spheres versus arrows how ever, the firefighter had no preference on the matter. 
Augmented reality components and their movement A concern first raised by the drone expert and later addressed by the firefighter was the concern of moving parts within the scope of augmented reality. The drone experts concern was that too much movement might prove distracting as he thought that firefighters probably instinctively saw unnatural movement as a sign of danger. When inquired on the subject how ever, the firefighter did not see moving parts as a problem or a distraction as he could not imagine having trouble distinguishing between augmented reality and actual reality. 
Figure 5: The figure displays the results of the rating part of the interviews conducted with the drone expert and firefighter. 
Human pilot versus Autonomous piloting With regards to human piloted drones, the firefighter pointed out that having human pilots eliminates the need for HMDside gestures completely. The firefighter deemed gestures to be a strenuous distraction which could prove troublesome in complete darkness, which is especially true when the firefighters are engaged in other activities. On the other hand, the firefighter thought that autonomous drone control posses less of a distraction thus freeing up resources with regards to rescue work. 
Stress as it relates to firefighting The firefighter could readily recognise stress as an important and detrimental factor in firefighting, in fact he had an acquaintance whom had just entered early retirement due to Posttraumatic Stress Disorder (PTSD), remarking that stress is certainly a very serious and ongoing subject in the field of rescue work as a firefighter. 
Information overload Information overloading was a concern raised in both interviews, met with very varying opinions. The drone expert thought that the system should be very wary of showing too much information at any one time as it could prove distracting and even dangerous if not handled correctly. On the other hand the firefighter thought that there was no such thing as too much information and if, at any point, too much or trivial information was displayed it would be no trouble for the firefighter to simply ignore it. 
Issues and Concerns for the Context 
The problem of doors One interesting point yielded by the interview with the firefighter arose when he told us that it was not uncommon to experience closed doors while searching for survivors, a fact that would most likely be detrimental to the work of the drone if encountered. In addition to doors being problematic for navigation the firefighter also explained that obstacle avoidance in general would be beneficial as to ensure that firefighters will not have to worry about the drone being stuck either because of an obstacle or as a result of colliding with one. 
Multiple drones Both the firefighter and the drone expert posed the very interesting idea of having multiple drones operating within the system at the same time, this is something that the drone expert thought would be more easily controlled through the use of human piloting, remarking that "".. it is possible to make the system so simple that a firefighter could control multiple drones at the same time."". 
Already existing firefighting drone solutions During the drone expert interview, he mentioned that autonomous drones are currently being used in the field of mining, in order to autonomously explore mining shafts, allowing for mining personnel to remain completely safe whilst exploration is underfoot. The drones work by having non predetermined paths computed in real time whilst exploring. The firefighter told us that a certain unit of firefighters exist in the Copenhagen branch of the Danish Emergency Management Agency whose sole responsibility it is to use drones for firefighting purposes. The main responsibilities of this branch seemed to cover exploration and surveying of plantation as well as high rise fires from outside in. 
Notable technologies for future work Through the interview with the firefighter we were pointed towards the delivery of thermal information being essential in already existing technologies aiming to aid firefighting and that amongst the information displayed to the firefighter through our system, thermal information was of the utmost importance. 
Extra ideas During both interviews several good ideas was discussed such as using sound as an alternative interfacing additions in order to infer information to do with everything from hazardous gasses, amount of survivors, the condition of the survivors and general information relevant to pathing. Likewise colour was suggested for much of the same reasons. 
5 Discussion 
To evaluate our work we will discuss the topics of our problem domain and the considerations that we have made in the development of our research platform and the knowledge we have obtained in the process. We will also propose our thoughts on future work, that could be beneficial for adding additional value to future iterations of this research platform as well as a potential final system. 
The Implementation The system as is, will make a good base for future iterations, as it applies the knowledge we have gathered over the course of the project. However, due to the fact that Unity is in the process of deprecating the old AR Application Programing Interface (API) that is currently built in, as well as due to the documentation for the new API being a lot more descriptive and thorough, it might be worthwhile investigating opportunities to switch the implementation to the new APIs, if possible. If that is not possible it may be worthwhile to start over using the new APIs entirely, depending on how much progress has been made since this initial experimentation. 
Implementation of Augmented Reality Headset A HoloLens are not durable enough nor made of the necessary materials in order to survive the hazardous environments found inside a burning building, implicitly agreed during the interviews has been the understanding that the final solution would involve the augmentation of fire resistant faceguards or facemasks with AR capabilities. 
Drone Size and Choice of Material The drone used in the current implementation needs to be made of more durable materials, during the interviews it was implicitly agreed upon that the current drone was an abstraction of the final fireproof drone which could be used in an actual implementation. The final drone should probably be made of fire retardant materials such as ceramics or steel yielding a more fire resistant drone than is currently the case. The changing of material also poses the restriction of more powerful rotors and thus more yield from the power supply. 
Lithium Batteries in Varying Temperatures The fact that a change of material might very possibly increase the power yield necessary to be generated by the power bank is only worsened by the fact that lithium batteries change their output discharge depending on the temperature of the batteries. In order to combat this, redundant power supplies might solve the problem of having an insufficient amount of power available while the use of voltage surge protectors might be able to help overcome the risk of internal power surges. Further research into the precise variation in discharge in lithium batteries is needed in order to more fully understand this problem. 
Spheres versus Arrows With regards to the case of spheres versus arrows, being used for path visualisation, the interview with the firefighter seems to suggest that either type is equally good at conveying directional information. The interview with the drone expert how ever addressed some valuable points, in that arrows 
by their mere shape can infer direction, an attribute that spheres does not possess, unless augmented with tangential features such as colour, animation or sound. That being said we deem arrows to be the superior path visualiser provided that measures to ensure visibility, such as arrow size and height above ground, is put into place. 
Information Overload While the drone expert felt very strongly about the dangers of having too much information displayed at any given point in time, the firefighter seemed much less passionate about its importance and postulated that too much information would simply lead to the users of the system ignoring the superfluous information. These are two very impactful and very contrasting opinions and while both participants provide valuable insights from each of their respective fields of expertise, further reviews involving firefighters from drone focused branches of the Danish Emergency Management Agency might be able to produce a more nuanced point of view on the matter. 
Stress and Firefighting Stress seemed a very important and current problem in the field of firefighting, and since the firefighter did not feel as though the system did much to help alleviate stress during the workflow of firefighting, the system, as well as the firefighters, could benefit from directing at least some of the development focus on alleviating stress, in that the reduction of stress could possibly increase effectivity reducing the amount of time spent in close proximity to hazardous environments. 
Human Piloting versus Autonomous Flight While the original intend of the system was to implement autonomous flight, the expert reviews have since then brought light upon the much simpler yet possibly just as effective use of human piloting in order to search the burning building using drones. On one side there is the added advantage of lower development time and an all around much simpler system to maintain with regards to a human piloted control system, and on the other hand is the potential of totally hands free searching of a building, freeing up resources which could potentially save human lives. 
The Problem of Doors While closed doors pose a, perhaps, insurmountable challenge in the way of surveying a building in its entirety for survivors, the system is still able to search paths not cut off by closed doors. Perhaps the usage of the system could entail a reduction of scope for the firefighters to only check paths 
where doors are closed, leaving the open doors to the drone system. 
Obstacle Avoidance With regards to drone navigation, several obstacles might present themselves while navigating, such as tables, chairs electronics etc. Generally speaking many obstacles one would encounter inside a burning building has flat surfaces, which makes the use of sonar a potentially viable solution for detecting these obstacles, first proposed by the drone expert, sonar very likely would prove suitable for the purpose of obstacle avoidance in the system, provided that autonomous flight is chosen as the drone control system. 
Limitations 
The limitations section will cover areas of the research platform where the current work is not perfect, but adequate for the purpose of serving as part of a research platform. The limitations section covers the drone autonomy, fire resistance and positioning. 
The drone is not currently autonomous, it is instead in its current implementation controlled by a person in real time through a PC and a live video feed. This is something that in future iterations of the research platform should be changed both in order to distinguish the research platform from already existing solutions on the market and in order to free up resources in any one given fire department. We have considered the usage of OpenCV in order to implement image recognition enabling the recognition of survivors. In the current iteration of the research platform, the drone itself is not fireproof, this, while not crucial to the development of firefighting aids, still presents a challenge in that the research platform will not necessarily provide precise data in actual fire hazardous environments. Future iterations of the research platform should consider using a more durable drone in order to better acquire data in realistic firefighting environments. In order to survive inside the hazardous environment of fires, firefighters use masks and various other kinds of headgear, this poses a problem for the current HMD in that it offers no fire protection what so ever. The implementation of the augmented reality device into the already used headgear of firefighters would improve the research platforms ability to provide accurate and useful data. 
Future Work 
The future work section will cover concerns needing resolution, in order to fully satisfy the requirements of the system described in section 3. 
As of right now the only camera attached to the drone is 
pointing straight forward and is totally static with respects to the drone frame. This is inadequate for the purpose of locating survivors in that the camera should be at an angle to better locate survivors of a fire who would most likely be lying down on the floor in order to avoid the toxic fumes and heat associated with fires. 
In some situations, the firefighters at the scene of a fire might wish to take direct control of the drone or for it to perform certain actions. This could be implemented through the use of new gestures which would need to be implemented. 
As of writing this, the server is not able to receive data from the HMD. In order for the HMD to communicate with the drone, necessitated if the aforementioned gesture suggestion is to be implemented, two way communication must be implemented. This can be handled by complimenting the current TCP connection system with an initial handshake as well as complimentary mirrored droneside receive functionality. 
Future research regarding the exploration of path visualisation using augmented reality for firefighters in an effort to find and rescue people in hazardous environments should involve exposing potential users of the system to the actual research platform directly, this could lead to crucial HCI minded contributions. Due to the 2020 COVID-19 pandemic, actually exposing potential users directly to the research platform has proven difficult and even problematic, alas future iterations of the research platform could, with very little adaptation, be exposed to a potential user base. We suggest future researchers pursuing the exploration of this problem domain to perform codesign sessions or user studies in order to receive more detailed cognitive feedback. We imagine that the precise nature of the proposed user review would borrow from the principles of heuristical inspection already specified in section 3 by Jill GerhardtPowals. We stipulate that such user reviews would reveal data not found during our presented expert reviews. 
6 Conclusion We have developed a research platform allowing people interesting in exploring the problem domain of technologies aiding firefighting. We also conducted two separate expert reviews, one with a drone expert and another with a firefighter. The two expert reviews yielded valuable information regarding the problem domain. During the development of our system we furthermore encountered some limitations as well as suggestions for future work which should be a point of focus for future iterations of the system. The research platform is able to provide valuable information 
in the ongoing effort to explore the realm of technologies aiding firefighting, the research platform is Open Source and is able to aid future research in said field. Furthermore the expert reviews aided in the understanding of firefighting, drone systems, augmented reality systems as well as any combination thereof. With regards to the problem statement, 
""How do we best minimise the amount of time firefighters have to spent in hazardous situations?"" 
We feel that our findings specified in section 4 and section 5 has contributed significantly towards the end goal of creating a system able to minimise the amount of time firefighters have to spent in hazardous environments enabling researchers to build upon our work in order to find the optimal solution. 
Acknowledgments 
We thank our supervisor, Timothy Robert Merritt, for his supervision on the project as a whole, but most importantly for his assistance in acquiring much of the equipment required to realize the project. 
References 
[1] Kelvin H Sung Aaron Hitchcock. 2018. Multiview augmented reality with a drone. https://www.researchgate.net/publication/329256719_ Multiview_augmented_reality_with_a_drone 
[2] Farzam Farbiz Wei Liu Yu Li Siewwan Fong Xubo Yang Sze Lee Teo Adrian David Cheok, Kokhwee Goh. 2004. Human pacman: a wide area sociophysical interactive entertainment system in mixed reality. https://dl.acm.org/doi/abs/10.1145/985921.985932 
[3] Dieter Schmalstieg Alessandro Mulloni, Hartmut Seichter. 2011. Handheld augmented reality indoor navigation with activitybased instructions. https://dl.acm.org/doi/abs/10.1145/2037373.2037406 
[4] Afshin Jamshidi Azin Karimzad Anzabi. 2019. Augmented Reality Based Thermal Drones for Victim Detection. https://www.researchgate.net/publication/337330902_Augmented_ Reality_Based_Thermal_Drones_for_Victim_Detection 
[5] Robert Van Eman. 2015. Finding Our Way In The Dark: A Firefighters Augmented Reality HeadsUpDevice. https://www.researchgate. net/publication/283492253_Finding_Our_Way_In_The_Dark_A_ Firefighters_Augmented_Reality_HeadsUpDevice 
[6] Andrew Hogue Bernadette Ann Murphy E. J. Weckman F. Michael WilliamsBell, Bill Kapralos. 2014. Using Serious Games and Virtual Simulation for Training in the Fire Service: A Review. https://www.researchgate.net/publication/265601002_Using_ Serious_Games_and_Virtual_Simulation_for_Training_in_the_Fire_ Service_A_Review 
[7] Jill GerhardtPowals. 1996. Cognitive Engineering Principles for Enhancing HumanComputer Performance. https://www.tandfonline. com/doi/abs/10.1080/10447319609526147 
[8] Rosen Ivanov. 2018. An algorithm for onthefly K shortest paths finding in multistorey buildings using a hierarchical topology model. https://www.tandfonline.com/doi/full/10.1080/13658816.2018. 1510126 
[9] Julie A. Kientz Jacob O. Wobbrock. 2016. Research contributions in humancomputer interaction. https://dl.acm.org/doi/10.1145/2907069 
[10] Arnold H. Buss Joerg C. G. Wellbrink. 2004. Vigilance Performance Modeled as a Complex Adaptive System with Listener Event Graph Objects (LEGOS). https://www.researchgate.net/publication/221529526_ Vigilance_Performance_Modeled_as_a_Complex_Adaptive_System_ with_Listener_Event_Graph_Objects_LEGOS#pf2 
[11] JeanLuc Dugelay Ludovic Apvrille, Tullio Joseph Tanzi. 2014. Autonomous Drones for Assisting Rescue Services within the context of Natural Disasters. https://www.researchgate.net/publication/ 271834672_Autonomous_Drones_for_Assisting_Rescue_Services_ within_the_context_of_Natural_Disasters 
[12] Xénophon Vaxevanoglou Sandrine Ponnelle. 1998. Coping with daily stress: Firefighters in emergency situations. https://www.researchgate.net/publication/290309267_Coping_ with_daily_stress_Firefighters_in_emergency_situations 
[13] Qwake Technologies. 2020. Qwake Technologies. https://www.qwake. tech/ 
[14] leading private sector companies The American Red Cross and a 32 Advisors Company federal agencies coordinated by Measure. 2015. Drones for Disaster Response and Relief Operations. https://www. issuelab.org/resources/21683/21683.pdf 
[15] Ran Dai Yue Zu. 2016. Distributed path planning for building evacuation guidance. https://www.tandfonline.com/doi/full/10.1080/ 23335777.2017.1326983 
Appendices 
Contextual Interview Questions 
Can you explain the system in the video you just saw? Can you talk about three, or more, of what you deem to be the most important considerations to be had, regarding such a system? 
What would you say were the main considerations behind having the drone be autonomous? What would you say were the main considerations behind having the drone be controlled by humans? Which hindrances can you imagine arising in constructing an autonomous drone able to navigate a burning building? Do you know of current drones that exist for this type of work? (Rescue work in hazardous environments) In your mind, what would be the best way to interact with the drone through the augmented reality headset? (for example hand gestures, voice, handheld controller, etc) In your mind, what would be the best way to symbolise the path travelled by the drone? (we used arrows and spheres, do you have a preference for either of these?) Can you imagine any other visualisations that would be more helpful? How would you signal, through a augmented reality headset, the location of the drone? How would you signal, through a augmented reality headset, the location of a survivor? How would you signal, through a augmented reality headset, a path? How would you signal, through a augmented reality headset, 
danger? 
The following statements are to be considered and rated on a scale of 1-7, 1 meaning that you very much disagree with the statement and 7 meaning that you very much agree with the statement. The system automated work for the firefighters, easing their workload. The system seemed easy to understand and use. The information displayed to the firefighter was clear and orderly. The information displayed to the firefighter was easy to understand. The system improved the safety of the firefighters. 
ing.The system will alleviate stress in the business of firefightI know or have heard of people suffering from stress related debilitation’s within the field of firefighting. 
Glossary 
API Application Programing Interface. 8 AR Augmented Reality. 1–5, 8 
HCI Human Computer Interaction. 3, 10 HMD Head Mounted Display. 1, 3, 4, 6, 7, 9, 10 HOG Histogram of Gradients. 2 
MR Mixed Reality. 4 
POIs Points Of Interest. 1 PTSD Posttraumatic Stress Disorder. 7 
UI User Interface. 3 
VR Virtual Reality. 4, 5 
",A Research Platform for Drone Assisted Firefighting.pdf,14
"SPIEDigitalLibrary.org/conferenceproceedingsofspie 
Experimentation for optimization of heterogeneous drone swarm configurations: terrain and distribution 
Ross Arnold, Elizabeth Mezzacappa, Melissa Jablonski, Benjamin Abruzzo, Jonathan Jablonski 
Ross Arnold, Elizabeth Mezzacappa, Melissa Jablonski, Benjamin Abruzzo, Jonathan Jablonski, ""Experimentation for optimization of heterogeneous drone swarm configurations: terrain and distribution,"" Proc. SPIE 11746, Artificial Intelligence and Machine Learning for MultiDomain Operations Applications III, 1174625 (12 April 2021); doi: 10.1117/12.2585589 
Event: SPIE Defense + Commercial Sensing, 2021, Online Only 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023  Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
Experimentation for optimization of heterogeneous drone swarm  
configurations: terrain and distribution   
Ross Arnolda,f , Elizabeth Mezzacappa*b,f, Melissa Jablonskic,f, Benjamin Abruzzod,f,   
Jonathan Jablonskie,f  
a Mortar and Common Fire Control Systems Division, FCDDACWFM, Building B31, Room 511,  Picatinny Arsenal, NJ 07806; bTactical Behavior Research Laboratory, FCDDACEQEA, Building  3518, Picatinny Arsenal, NJ 07806; c Reliability Management Branch, FCDDACEQSB, Building  62, Picatinny Arsenal, NJ 07806;  dPrecision Arms and Intelligent Sensors Division, FCDDACMFP, Building 6, Picatinny Arsenal, NJ 07806; eAnalysis and Evaluation Division, FCDDACMAA,  
Building, Building 94, Picatinny Arsenal, NJ 07806; fArmament Graduate School, Grimes Hall,  
Building 3411, Rm 202, Picatinny Arsenal NJ 07806  
ABSTRACT   
Computer simulation experimentation examined the effectiveness of different Unmanned Aircraft System (UAS) swarm  configurations for identification and localization of survivors after a natural disaster using the DroneLab application.   Swarms differed in terms of total number of drones and ratio of entities programmed to perform one of three different  “personalities”—Relay, Social, and Antisocial.   Relay behavior puts a high priority on maintaining proximity to the  centroid of the swarm while also maintaining a distance to closest neighbor drones equal to half of the maximum WiFi  range.  Antisocial drones prioritize an expanding behavior, increasing the spread of the swarm, while the Social behavior  prioritizes a contractive behavior resulting in a tighter swarm formation. All drones performed a local waypointbased  search behavior while conducting a spiralout search pattern upon detecting four or more survivors within a 10-meter  radius. Swarm configurations with different ratios of these behaving entities were assessed for mission completion, defined  as time to find 90% of the survivors.  Mission completions were recorded for four simulation scenarios consisting of two  terrains (urban/rural) with two different distributions of survivors (naturalistic/randomized).  Ten replications of 98  different drone configurations were evaluated.   
Statistically significant differences between time to mission completion between the terrains, between the two  distributions, and among the iterations were revealed.  Qualitative comparisons revealed differences in configurations that  performed the best in each terrain.  A few configurations performed well in all four scenarios.   Moreover, the minimum  number of entities needed for wellperforming swarms was indicated.  The work demonstrates the utility of computer  experimentation and statistical analyses for developing a framework for swarm design for operational effectiveness.   
Keywords: Swarms, statistical analyses, search and rescue, remote autonomous systems    
1. INTRODUCTION   
There is wide agreement within the United States Department of Defense (DoD) that collectives of artificially intelligent  agents, swarms, with various levels of autonomy will be employed for multidomain operations in the very near future  [1] [2].   As a consequence, the DoD requires research addressing quantitative methods for understanding how to build  drones and assemble swarms for specific missions.  This program of research includes the relationships between design  choices and measures of swarm performance and how those relationships change with specific missions and terrains.     Engineering design choices include parameters for individual entities as well as for the swarm as a whole.      
  
Artificial Intelligence and Machine Learning for MultiDomain Operations Applications III, edited by  Tien Pham, Latasha Solomon, Proc. of SPIE Vol. 11746, 1174625 · © 2021 SPIE  CCC code: 0277-786X/21/$21 · doi: 10.1117/12.2585589 
Proc. of SPIE Vol. 11746  1174625-1 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
  Modeling and simulation techniques have proven to be useful in understanding the relationships between small  Unmanned Aircraft System (UAS) swarm characteristics and swarm performance in computer simulated operations.   This paper demonstrates how the modeling and simulation approach, combined with statistical analyses can result in  insight that could be useful for decisionmakers when determining how to design, configure and deploy an autonomous  swarm for a mission scenario.  Performance of a UAS swarm in a search and rescue mission was compared for different  sized swarms, under two terrains, with two different distribution of targets in computer simulations.  The intent was to  identify the swarm configurations (i.e., swarm size, composition of behavior types) that performed best in each terrain  (urban, nonurban) and with different distributions of targets.  Based on those findings, the configurations of swarms that  performed best across all the terrains and target distributions were identified.        
2. METHOD  
2.1 Computer simulation:  DroneLab and entity “personality”  
The U.S. Army’s DroneLab simulation software was used for experimentation [3] [4] [5]. DroneLab is a software  application designed to facilitate simulation of large numbers of UAS operating collectively as a cohesive but  decentralized system.  The simulated scenario consisted of a search for survivors after major natural disasters (tsunami  and earthquake) in a mission space of approximately four square kilometers.  The entities in the search swarm were each  programmed with one of three roles, which can also be conceptualized as “personality” types.  The three roles were  developed by prioritizing different preprogrammed behaviors from a fixed set of options.  Examples from this fixed set  of options include behaviors such as collision avoidance, battery recharge, formation control, and waypoint navigation.  The three personality types developed for this experiment were titled Relay, Social Searcher, and Antisocial Searcher.  An entity assigned to the Relay role maintains a distance equal to half of the maximum range of the WiFi module  (approximately 400 meters) from the closest member of the swarm to provide a network infrastructure to enable other  agents to continue their behaviors while maintaining connectivity to the other swarm members. The Antisocial Searcher  drones prioritize a loose formation behavior, putting a high value on increasing the spread of the swarm thus maintains a  greater distance between themselves and all other agents of the swarm compared to the Social Searcher drones. The  Social Searcher role prioritizes a tighter formation between entities in the swarm.  All three roles trigger a spiralout  behavior upon detecting four or more survivors within a 10-meter radius.  This behavior was designed in order to more   rapidly locate other survivors that are likely to have congregated nearby [5].   2.2 Swarm Parameters:  Size and percentage of each personality type  
Test swarms were configured with two parameters.  The first swarm parameter was total number of entities in the  swarm. The second was relative proportion of entities assigned one of the three specific “personalities,” or roles, within  the swarm. All proportions of roles were considered with a swarm size in the range of 10 to 50 drones. The maximum  swarm size of 50 was selected as this was estimated to be beyond the point of saturation for the target area.  Ninetyeight  different configurations were included, with 10 iterations of each run.  Swarm performance was recorded in four  different scenarios which differed in two terrain/disaster combinations (ruraltsunami, urbanearthquake) and distribution  of survivors.      
2.3 Scenarios:  Terrain   
Of the two terrain sets, the Rural terrain is based on actual data gathered from primary sources onsite at Arahama,  Japan, which is a small coastal town destroyed by the 2011 tsunami that struck and disabled the Fukushima Daiichi  nuclear power plant, located approximately 100 kilometers to the south. The Urban terrain is reconstructed based on a  satellite photo of Kobe, a large city in Hyogo Prefecture, Japan.  The Kobe photo was taken hours after the Great  Hanshin earthquake of 1995 which caused largescale destruction in the city.  Both sets of terrain provide representations   of postdisaster sites that had undergone large scale searchandrescue operations.    
Proc. of SPIE Vol. 11746  1174625-2 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
  2.4  Scenarios: Survivor distribution  
Swarm performance in both terrains was tested against two different survivor distributions (i.e., Distribution A, B, see  Table 1). A total of 650 survivors were located in the rural terrain, 300 of which were randomly distributed.  The  remaining 350 survivors were placed in two different ways.  Distribution A in the rural terrain was based on real data of  survivors after a tsunami, where most people gathered on top of an elementary school.  Distribution B rural placements   were fictitious, but reasonably spread across the buildings and terrain.     A total of 2620 survivors were to be found in the urban earthquake scenario, where 2000 were randomly distributed in  the terrain.  The remaining 620 survivors were placed in again two different ways.  Distribution A placed survivors   concentrated in tall buildings.  Distribution B placed survivors in large clear areas, such as school fields.   Table 1: Scenarios    
Terrain  Distribution A   Distribution B   
 Ruraltsunami  (total of 650, 300  randomly placed)  
Real survivor data, collected from  interviews and information collected onsite, survivors concentrated on top of an  elementary school and several other  structures  
   Survivors spread across buildings and terrain  
 Urbanearthquake  (total of 2620  survivors, 2000  randomly placed)    
   Survivors concentrated in tall buildings  
   Survivors concentrated in large clear areas  
   2.5 Swarm Performance Metric  
 Time in seconds to locate 90% of survivors was the performance metric of interest.  2.6 Experiment and Analysis  
Performance (time to find 90% of survivors, t90%) was compared in 100 swarm configurations.  Swarm configurations  were based on the parameters of swarm size (10-50 entities) and percentage (0-100%) of each of the three different  personalities (Relay, Social, Antisocial).  Two of the 100 configurations returned spurious results and were deleted from  the analyses.  Each of the remaining 98 configurations was simulated 10 times in each of the four scenario conditions in  Table 1 (ruraltsunami Distribution A, ruraltsunami Distribution B, urbanearthquake Distribution A, urbanearthquake  Distribution B) for a total of 3920 simulation runs.    
To identify top performing configuration for each scenario, calculations of means and standard errors of t90% for each of  the 98 configurations for each of the conditions were calculated and compared.  Each of the configurations was ranked  according to the performance in each of the four scenarios in Table 1 and the top 20 performing configurations were  identified for each.  The configurations that were well performing across all four scenarios were identified.    
To explore the extent to which the scenario characteristics affect t90%, a general linear regression was applied using the  Statistical Product and Service Solutions (SPSS) software, predicting t90%; within subject factors were Terrain (ruraltsunami, urbanearthquake), Distribution (A,B), and Repetition Number (1-10).  Because each of the scenarios differed in  number of survivors, placements, and features (e.g., buildings, open spaces), performance differences can be expected.  
  
Proc. of SPIE Vol. 11746  1174625-3 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
3. RESULTS  
3.1 Performance of configurations in each of scenarios  
An overall qualitative analysis is presented in Figure 1.  The four graphs correspond to one of the four scenarios.  Each of  the 98 configurations is represented by a sphere on the graph.  The percentage of Relay, Social, and Antisocial entities is  represented on the axes, the size of the sphere indicates the size of the swarm.  The color indicates the ranking of the  configuration, from high performing (shorter times to find survivors) blue spheres to poor performing (longer time to find  survivors) red spheres.  Note that the pattern of colors changes in each of the graphs, showing that one swarm configuration  may perform well in one scenario and poorly in another.    
  
Figure 1:  Configurations ranked by t90% in each of the four scenarios     
Tables 2-5 show the top 20 swarm configurations that resulted in the fastest time to detect 90%^ of survivors (t90%).   Configuration characteristics (size, number of each personality type), average rescue time and associated standard  deviation, minimum and maximum time are listed for each of the swarm configurations.  Note that although average time  to locate is the primary metric of performance, the reliability of the system is also reflected in the standard deviation.   Decision makers may opt to configure a slower swarm in order to have a more reliable estimate of t90%.  Minimum and  maximum times are also useful with respect to planning, showing the range of times to expect until mission completion.   Finally, an estimate of the total number of drones associated with expected average times gives decision makers a range  of effective total numbers.  Not surprisingly, larger swarms generally outperform smaller swarms in a search and rescue  mission, though some smaller swarms are effective certain configurations. For example, configuration 13 in Table 2 only  contains 33 entities in its swarm, yet performs nearly as well as configuration 12 which contains 44 entities.   
Proc. of SPIE Vol. 11746  1174625-4 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
  Table 2: Swarm configurations with the shortest location times in the Rural Distribution A scenario    
Top 20  Config #  
# Drones  
Total  # Relay  # Social  
#  
Antisocial  
t90%  (sec)  
Standard  Deviation  
Minimum  Recorded  
Time  (sec)  
Maximum  Recorded  
Time  (sec)  
1  50  0  20  30  932.7  158.84  706  1220  
2  50  17  17  16  937.3  138.81  780  1273  
3  50  1  10  39  958.0  114.99  756  1100  
4  50  0  30  20  990.6  158.97  756  1306  
5  45  1  21  23  1006.1  118.01  783  1196  
6  49  13  12  24  1007.0  141.51  833  1200  
7  46  8  20  18  1013.7  180.88  763  1386  
8  47  21  5  21  1026.3  106.46  870  1193  
9  39  1  13  25  1026.7  166.93  853  1406  
10  43  9  9  25  1065.1  332.39  730  1923  
11  46  5  2  39  1106.2  234.35  870  1596  
12  44  7  6  31  1117.0  361.10  763  2063  
13  33  2  6  25  1119.3  327.27  716  1880  
14  41  12  3  26  1132.7  149.59  856  1360  
15  37  14  3  20  1140.6  122.65  936  1336  
16  39  9  0  30  1195.1  284.82  753  1820  
17  50  20  0  30  1197.0  258.94  706  1623  
18  50  31  6  13  1213.3  213.20  803  1530  
19  48  8  31  9  1221.7  224.23  936  1650  
20  41  6  22  13  1222.2  197.66  933  1626  
              
Proc. of SPIE Vol. 11746  1174625-5 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
   Table 3:  Swarm configurations with the shortest location times in the Rural Distribution B scenario    
Top 20  Config #  
# Drones  
Total  # Relay  # Social  
#  
Antisocial  
t90%  (sec)  
Standard  Deviation  
Minimum  Recorded  
Time  (sec)  
Maximum  Recorded  
Time  (sec)  
1  50  1  10  39  922.0  146.99  773  1216  
2  50  0  20  30  930.5  93.99  776  1126  
3  50  0  30  20  962.0  122.63  786  1163  
4  44  7  6  31  990.4  136.09  763  1130  
5  43  9  9  25  1027.3  159.14  780  1276  
6  49  13  12  24  1042.0  181.02  786  1306  
7  45  1  21  23  1064.6  243.34  703  1646  
8  46  5  2  39  1106.3  325.61  763  1650  
9  39  1  13  25  1106.6  248.29  856  1773  
10  46  8  20  18  1135.0  112.99  933  1333  
11  41  12  3  26  1140.5  184.43  856  1440  
12  48  8  31  9  1167.6  159.86  766  1326  
13  50  20  0  30  1178.5  266.07  870  1630  
14  36  9  14  13  1183.8  225.10  923  1676  
15  47  21  5  21  1203.1  170.84  893  1480  
16  50  17  17  16  1211.4  251.06  860  1686  
17  33  2  6  25  1232.6  130.28  1033  1383  
18  39  9  0  30  1237.6  385.25  866  2016  
19  38  4  23  11  1237.8  161.82  1000  1630  
20  37  14  3  20  1262.9  201.92  1006  1653  
  
              
  
Proc. of SPIE Vol. 11746  1174625-6 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
  Table 4:  Swarm configurations with the shortest location times in the Urban Distribution A scenario    
Top 20  Config #  
# Drones  
Total  # Relay  # Social  
#  
Antisocial  
t90%  (sec)  
Standard  Deviation  
Minimum  Recorded  
Time  (sec)  
Maximum  Recorded  
Time  (sec)  
1  50  31  6  13  1774.3  105.48  1556  1920  
2  50  30  0  20  1781.7  153.00  1450  1976  
3  50  20  0  30  1898.7  245.54  1600  2326  
4  48  37  10  1  2044.4  389.12  1533  2770  
5  47  21  5  21  2077.1  233.18  1720  2443  
6  50  30  20  0  2099.0  337.72  1693  2636  
7  43  25  3  15  2125.7  361.66  1830  3050  
8  47  25  16  6  2260.0  335.65  1946  3093  
9  46  33  8  5  2330.6  325.73  2006  3056  
10  50  17  17  16  2345.7  452.01  1790  3140  
11  34  25  3  6  2361.9  224.11  2006  2750  
12  41  20  11  10  2378.7  267.76  1990  2970  
13  40  34  2  4  2489.3  359.66  1860  2993  
14  49  13  12  24  2560.1  516.62  1860  3226  
15  50  1  10  39  2732.6  461.29  1966  3493  
16  50  0  0  50  2754.6  566.99  1810  3373  
17  50  20  30  0  2784.3  504.42  2026  3306  
18  50  50  0  0  2788.0  285.37  2370  3170  
19  35  23  10  2  2925.0  326.35  2393  3323  
20  33  18  5  10  2951.1  258.83  2433  3190  
             
      
Proc. of SPIE Vol. 11746  1174625-7 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
   Table 5: Swarm configurations with the shortest location times in the Urban Distribution B scenario    
Top 20  Config #  
# Drones  
Total  # Relay  # Social  
#  
Antisocial  
t90%  (sec)  
Standard  Deviation  
Minimum  Recorded  
Time  (sec)  
Maximum  Recorded  
Time  (sec)  
1  50  30  0  20  1754.0  169.00  1603  2103  
2  50  31  6  13  1790.8  70.46  1680  1890  
3  50  20  0  30  1958.0  292.13  1606  2540  
4  46  33  8  5  2056.9  194.01  1826  2343  
5  48  37  10  1  2110.6  289.33  1850  2626  
6  43  25  3  15  2188.5  367.88  1690  2926  
7  40  34  2  4  2210.5  369.88  1916  3130  
8  47  21  5  21  2219.4  452.80  1863  3133  
9  50  30  20  0  2243.4  347.61  1933  3143  
10  47  25  16  6  2372.5  350.69  1783  2963  
11  50  17  17  16  2487.7  416.39  1893  3060  
12  50  50  0  0  2648.6  361.66  2223  3370  
13  34  25  3  6  2719.7  264.88  2280  3116  
14  49  13  12  24  2756.4  503.91  2030  3246  
15  50  1  10  39  2761.9  452.41  2056  3483  
16  50  20  30  0  2868.6  403.00  1946  3246  
17  50  0  0  50  2894.1  178.57  2623  3166  
18  41  20  11  10  2910.6  254.01  2420  3170  
19  35  23  10  2  2951.1  296.35  2363  3210  
20  33  18  5  10  3065.0  318.69  2456  3496  
            
    
Proc. of SPIE Vol. 11746  1174625-8 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
  Five of the swarm configurations performed in the top 20 across all the scenarios, as shown in Figure 2.  Tentatively,  swarms configured in these manners may be expected to perform well across a broad range of conditions.    
  
   Figure 2:  Top performing configurations across all scenarios  3.2 Characteristics of scenarios and time to find survivors  
Figure 3 shows the means and standard errors for detecting survivors in each of the four scenarios.  Statistical analyses  reveal that there are significant differences in these times.  Results from the general linear regression revealed significant  main effects of Terrain (Rural, Urban, F(1,97)=282.71, p< .0001), Distribution (A,B; F(1,97)=35.16, p< .0001), and Run (1- 10; F(9,89)=2.35, p< .05) and an interaction effect of Terrain x Distribution (F(1,97)=8.25, p< .005).   These findings suggest  that time to find survivors is significantly different among the four scenarios, and is longer in the Urban versus the Rural  scenarios, and in Distribution B versus Distribution A.    
           
Proc. of SPIE Vol. 11746  1174625-9 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
    
  
Figure 3:  Comparison of times to detect across the four scenarios  
4. DISCUSSION  
As the previous sections indicate, modeling and simulation techniques can be used to explore the relationship between  swarm configuration and swarm performance in different scenarios.  Simple statistical calculations yield quantitative  metrics (average, standard deviation, range) which can be used to make decisions and analyses of alternatives.   Statistical analyses also reveal characteristics of the swarm and scenario that impact swarm performance, along with the   extent and reliability of those impacts.    More sophisticated analyses are available that would increase the power of prediction for swarm performance.  For  example, Design of Computer Experiments [6] or more specifically Design of Swarm Experiments  [7] offer the  possibility to consider additional parameters of study (e.g., size, WiFi range, velocity, interdrone distance,  communication, etc.). Using this approach, a scalar function relating the average time to find 90% of survivors is  statistically modeled with inputs including swarm size, role distribution, and other variables of interest. The resulting  function is used to estimate the performance of swarm configurations not explicitly simulated and can be used for  optimization studies and sensitivity analyses. This is contrasted by the statistical analysis used in this work which solely  considers individual simulations and is, thus, limited in that the entire design space of swarm configurations is not  considered. Future experimentation using these more advanced methods will give engineers and planners far more tools  to configure swarms and to plan operations.    
5. CONCLUSION  
Computer modeling and simulation results subjected to statistical analyses offer design engineers and mission planners a  method for exploring different swarm configurations for different operational scenarios.  Using these methods, swarm  configurations that performed universally well across all simulated scenarios were discovered.  Additionally, under certain  configurations, swarms of smaller size (lesser number of entities) were found to perform as well as, or better than, swarms  of larger size up to a factor of one third.  These results indicate that decisionmakers, when made aware of the optimal  configurations for a given mission, may be able to decrease their deployed number of swarm assets by 33% while retaining  a similar level of mission capability.  As a result, logistics footprint and operational burden can be decreased significantly.   Further research is warranted.   
    
Proc. of SPIE Vol. 11746  1174625-10 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
  
  
  
REFERENCES  
  [1]  Defense Science Board, ""Report of the Defense Science Board Summer Study on Autonomy,"" Office of the Under  
Secretary of Defense for Acquisition, Technology and Logistics, Washington, DC, 2016.  
[2]  G. J. Herrera, J. A. Dechant, E. K. Green and E. A. Klein, ""Technology Trends in Small Unmanned Aircraft  
Systems (sUAS) and CounterUAS: A Five Year Outlook,"" Institute for Defense Analyses, Alexandria, VA, 2017.  
[3]  R. Arnold, J. Jablonski, B. Abruzzo and E. Mezzacappa, ""Heterogeneous UAV MultiRole Swarming Behaviors for  
Search and Rescue,"" in Proceedings of the IEEE CogSIMA 2020, 2020.   
[4]  R. Arnold, E. Mezzacappa, B. Abruzzo and J. Jablonski, ""MultiRole UAV Swarm Behaviors for Wide Area Search  
using Emergent Intelligence.,"" in Proceedings of the IEEE WorldS4 2020, 2020.   
[5]  R. Arnold, H. Yamaguchi and T. Tanaka, ""Search and rescue with autonomous flying robots through behavior- 
based cooperative intelligence,"" Journal of International Humanitarian Action, vol. 3, p. 18, 2018.   
[6]  T. J. Santner, B. J. Williams and W. Notz, Design and analysis of computer experiments, Springer, 2003.   [7]  M. Jablonski, J. Jablonski, R. Arnold, B. Abruzzo and E. Mezzacappa, ""The Design of Swarm Experiments: A  
Formal Method to Approach Swarm Systems Research,"" (under review).   
    
                          
Proc. of SPIE Vol. 11746  1174625-11 
Downloaded From: https://www.spiedigitallibrary.org/conferenceproceedingsofspie on 26 Sep 2023 Terms of Use: https://www.spiedigitallibrary.org/termsofuse 
",Experimentation for optimization.pdf,15
"Received October 21, 2019, accepted November 4, 2019, date of publication November 18, 2019, date of current version December 2, 2019. 
Digital Object Identifier 10.1109/ACCESS.2019.2953900 
The StateoftheArt of Human–Drone Interaction: A Survey 
DANTE TEZZA , (Member, IEEE), AND MARVIN ANDUJAR , (Member, IEEE) Computer Science and Engineering Department, University of South Florida, Tampa, FL 33559, USA 
Corresponding author: Dante Tezza (dtezza@mail.usf.edu) 
ABSTRACT Drones have expanded from military operations to performing a broad range of civilian applications. As drone usage increases, humans will interact with such systems more often, therefore, it is important to achieve a natural humandrone interaction. Although some knowledge can be derived from the ﬁeld of humanrobot interaction, drones can ﬂy in a 3D space, which essentially changes how humans can interact with them, making humandrone interaction a ﬁeld of its own. This paper is the ﬁrst survey on the emerging ﬁeld of humandrone interaction focusing on multirotor systems, providing an overview of existing literature and the current state of the art in the ﬁeld. This work begins with an analysis and comparison of the drone models that are commonly used by endusers and researchers in the ﬁeld of humandrone interaction. Following, the current state of the ﬁeld is discussed, including the roles of humans in HDI, innovative control methods, remaining aspects of interaction, and novelty drone prototypes and applications. This paper concludes by presenting a discussion of current challenges and future work in the ﬁeld of humandrone interaction. 
INDEX TERMS Drone, humancomputer interaction, humandrone interaction, humanintheloop, humanrobot interaction, unmanned aerial vehicle. 
I. INTRODUCTION Drones, also known as unmanned aerial vehicles (UAV), are robots capable of ﬂying autonomously or through different control modalities such as joysticks, smartphones, the human brain, voice, gestures, and others. Until the early 2000s, drones were complex systems commonly seen in the military world and outofreach for civilians. Modern advancements in hardware and software technologies allow the development of smaller, easier to control, and lower cost systems. Drones are now found performing a broad range of civilian activities (e.g. photography during extreme sports activities, construction surveillance, racing, agriculture, among others) and their usage is expected to keep increasing in the near future. The United States Federal Aviation Administration (FAA) expects that by 2022 the number of registered drones in their database might be as high as 3.8 million units [1]. The FAA drone registration forecast can be seen in Table 1, the baseline number was calculated by FAA through observation of the number of registrations, expert opinions, review of industry forecast, and market/industry research. However, the report does not 
The associate editor coordinating the review of this manuscript and 
approving it for publication was Maurizio Magarini . 
TABLE 1. Drone registration forecast according to the FAA with a low base and high estimates for drone registration until 2022 [1]. 
specify how the low and base scenarios are calculated and does not provide a low scenario for commercial drones. This paper focuses on the interaction among humans and multirotor drones, which are capable of ﬂying in a 3D space, hovering, and vertical takeoff and landing. Such drones can range from small toygrade remotecontrolled aircraft to fullyautonomous systems capable of decisionmaking and carrying a large variety of sensors. Multirotor drones are widely used for photography, structural inspections, and sports; however, their application goes far beyond. As an example, monitoring of animals’ distributions is a timedemanding task that is usually performed by foot or manual 
167438 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ VOLUME 7, 2019 
analysis of videos; drones can greatly automatize this process assisting in nature conservation [2]. Another example of how these systems can be applied is to assist in natural disasters response, as they can be used to search for victims, delivery of supplements, or even as ﬁre extinguishers [3]–[5]. Humanrobot interaction can be deﬁned as ‘‘a ﬁeld of study dedicated to understanding, designing, and evaluating robotic systems for use by or with humans’’ [6], analogously, humandrone interaction (HDI) can be deﬁned as the study ﬁeld focused on understanding, designing, and evaluating drone systems for use by or with human users. Although some knowledge can be derived from the ﬁeld of humanrobot interaction, drone’s unique characteristic to freely ﬂy in a 3D space, and unprecedented shape makes humandrone interaction a research topic of its own. Humandrone interaction is a broad research ﬁeld, for instance, a researcher can design new drones’ shapes with friendlylike appearance, while another researcher can focus on designing new user interfaces that allow nonskilled pilots to accurately operate drones without extensive training. As drone usage continues to increase, humans are likely to be seeing or using drones in their everyday lives, therefore it becomes important to study the interaction among humans and such systems. As HDI is a relatively new ﬁeld, this work was motivated to summarize the existing literature and present the stateoftheart in the ﬁeld. This survey shows that the current state of the art research in humandrone interaction consists of evaluating and developing new control modalities, enhancing humandrone communication, evaluating interaction distance, and developing new use cases. These four major ﬁelds of HDI are displayed in Fig. 1 and discussed throughout this paper. Additionally, recent research in HRI has been focusing on social robots [7], similarly, HDI research is starting to focus on social drones. It is important to mention that this paper does not discuss ﬁxedwing drones, which are less applicable to HDI due to their limited maneuverability and lack of hovering ability. Researchers that are not familiar with the fundamentals of drone technologies can acquire the necessary background to understand the concepts presented in this paper in [8]. The remaining of this paper is organized as follow. Section II presents an analysis of which drone models are commonly used by hobbyists, professional pilots, and by researchers. This section also presents a comparison of 
FIGURE 1. The four major fields of HumanDrone Interaction research. 
features of the most common drone models, giving insights on their capabilities and limitations. Section III discusses the roles of humans in HDI, and the evolution of research in the ﬁeld by presenting the number of publications in the ﬁeld chronologically. Section IV presents innovative drone control methods, including gesture, speech, braincomputer interfaces, and others. Section V discusses remaining aspects of humandrone interaction, such as comfortable distances, communication, and emotion encoding. Following, Section VI presents research studies with innovative drone prototypes and usecases in the HDI ﬁeld. In section VII, it is presented the current challenges and future directions in HDI. Concluding, Section VIII summarizes this survey paper. 
II. DRONE MODELS This section presents an analysis of the most commonly used multirotor drones by end users and researchers, as well as a comparison of these model’s speciﬁcations. 
A. POPULAR DRONE MODELS Today’s market presents a wide variety of commercialofftheshelf drone models. The United States Federal Aviation Administration (FAA) requires any drone weighing at least 0.55 pounds to be registered on their database. Analyzing the FAA database allows us to estimate the number of drones being used and which are the most popular manufacturers and models. Until November 2017, a total of 836,796 aircraft were registered for hobbyists and 106,739 for commercial purposes [9]. The application areas where commercial drones are being mostly used can be seen in Table 2. Fig. 2 describes the number of registrations per manufacturer until November of 2017 (last public release of the database). The graph shows that most registered drones are manufactured by DJI and according to [10] they control 72% of the commercial drone global market. The easiness of use and a large variety of models for different applications (hobbyists, professional cinematography, and industrial applications) are possible explanations for why DJI dominates the commercial market. 
FIGURE 2. Number of drones per manufacturer registered in the FAA until November 2017. Only 30 mostly registered drone models are taken into consideration [9], [11]. 
VOLUME 7, 2019 167439 
D. Tezza, M. Andujar: StateoftheArt of Human–Drone Interaction 
TABLE 2. Common uses of commercial drones according to the FAA [1]. 
B. DRONE MODELS USED ON PREVIOUS RESEARCH From the papers reviewed in this survey, 56 of them described a project where a drone or a prototype was used; the remainder of the papers discussed other aspects of HDI (e.g. statistics and use case scenarios). The models of the drones used in these papers can be seen in Fig. 3. As shown in the ﬁgure, most studies (57%) used the Parrot ARDrone as a base for their prototype or to conduct an experiment. This shows a considerable difference from the drones used by endusers where DJI dominates the market (see Fig. 2). Parrot drones provide an easytouse software API allowing for quick prototyping, which is likely the reason they are the researcher’s ﬁrst choice. A comprehensive overview of the ARDrone including its hardware, software, and instructions on how to use it as a research platform can be found in [12]. Although the Parrot ARDrone is widely used for research, this model was discontinued by the manufacturer. 
FIGURE 3. Analysis of drone models used during the research studies reviewed in this paper. 
C. DRONE MODELS COMPARISON The speciﬁcations of the currently available commercialofftheshelf drone models can be seen in Table 3. As shown in the table, current drones can reach 31 minutes of ﬂight time, speeds up to 93 kilometers per hour, 8 kilometers of range and their price range from $99 to $2999. 
III. HUMANDRONE INTERACTION RESEARCH A. ROLES OF HUMANS IN HDI Humans play different roles when interacting with drone systems. Their role is dependent on the drone’s application and its level of autonomy. The human can act as an active 
controller,in which the pilot directly controls the drone with a control interface to perform a task. For example, to take pictures of a landscape or participate in a drone racing competition. Another role is of a recipient, in this case, the user does not control the drone, but it beneﬁts from interacting with it. As an example, a user walking in the street might be approached by a drone holding an advertisement screen [13], or even a user receiving a package delivered by a drone [14], [15]. Humans can also interact as social companions with drones, in this case, the user might or might not be able to control the drone movement, but it holds a social interaction with it. An example of such interaction is Joggobot, in which the drone ﬂies along with humans that are jogging providing a social interaction [16]. Lastly, autonomous drones require users to act as supervisors.Although modern drone systems can ﬂy fully autonomous, a human is still required either to preprogram the drone behavior (e.g. plan ﬂight mission) or to supervise the ﬂight itself (e.g. monitor an autonomous inspection ﬂight in realtime) in case of emergency. Additionally, it is worth mentioning that even applications in which the drone is acting autonomously, a human can still be in the loop in the form of a recipient, for example, to receive the package of a drone delivery service, or to read the information of an advertisement drone. These roles of humans in HDI are summarized in Table 4. 
B. HDI RESEARCH OVER TIME The ﬁeld of HDI is relatively new in the research community. A search in Google Scholar ﬁltering the results by publication year can demonstrate how this ﬁeld has evolved over time. The number of search results for ‘‘humandrone interaction’’ by year is shown in Fig. 4. Until the year 2014, there were only two results for this search, but the number changes to 180 when publications up to 2018 are included. Different search queries containing a combination of the words ‘‘drones, UAV, and humanintheloop’’ were also searched, but the results were mostly related to drone technologies for warfare and ethics related to drone usage. Therefore, for purposes of demonstrating how the HDI ﬁeld evolved over time, Fig. 4 only contains results for the query ‘‘humandrone interaction’’. 
FIGURE 4. Number of publications found on Google Scholar using ‘‘humandrone interaction’’ as a query. 
167440 VOLUME 7, 2019 
TABLE 3. Comparison of commercialoftheshelf drone models. 
TABLE 4. Roles of humans in HDI. 
IV. INNOVATIVE CONTROL INTERFACES Originally, drones were used for military applications and required highly trained pilots for operation due to its complex user interfaces. As drone technologies became more ubiquitous and affordable, researchers started to shift interface design towards modern user interfaces that no longer limits drone control solely using a remote controller or a ground control station. These innovative methods, which are also known as natural user interfaces (NUI) allow users to interact with drones through gesture, speech, gaze, touch, and even using braincomputer interfaces (BCIs) such as electroencephalography (EEG). Each control interface impacts how 
the pilot interacts with the drone in various aspects, such as training period, accuracy, latency, interaction distance; in this section, we present innovative control modalities and how they affect the pilot’s experience. A summary of the advantages and disadvantages of each NUI, as well as the traditional remotecontroller interface, can be seen in Table 5. Each NUI is further discussed in the subsection below. The main goal of natural user interfaces is to achieve an intuitive control method, which is deﬁned as an interface that works at user’s expectations [17]. Natural user interfaces allow nonexpert users to control drones with a shorter training period, reduced workload and possibly even decrease aircraft crashes. A previous study proposed a series of design guidelines for natural user interfaces: (1) input vocabulary must employ mental models that are known to the targeted user, (2) natural behavior of users must be analyzed to discover userdeﬁned mental models, (3) mixing gestures from different mental models in an input vocabulary should be avoided, (4) and important aspects (such as physiological, cultural differences, application, and ergonomics) must be considered in the ﬁnal input vocabulary [17]. Additionally, the concept of using mental models when designing a user interface can enhance the ﬁnal product. Mental models for 
VOLUME 7, 2019 167441 
D. Tezza, M. Andujar: StateoftheArt of Human–Drone Interaction 
TABLE 5. Control interface summary. 
HDI are classiﬁed as imitative, instrumented and intelligent [17]. An imitative control happens when the drone simple copies an action performed by the user, for example, the aircraft can ﬂy translating the pilot’s hand movement directly to aircraft attitude. Instrumented interaction requires an intermediate link, which can be a physical object or an imaginary one; such as the use of a physical or imaginary joystick. An intelligent type of interaction allows the operator to treat the system as an intelligent system and send highlevel commands such as a ‘‘follow me’’ command. An elicitation study evaluated natural humandrone interaction techniques using a Wizard of Oz (WoZ) approach [18]. In this user study, 19 participants were asked to complete a set of tasks of different complexities using any interaction technique they considered natural. A DJI Phantom 2 with propeller guards was used in this study and participants were informed that its behavior was being simulated. To avoid verbal biasing, each task was written on a card that participants were asked to pick out of a stack. During the ﬁrst phase, participants had to perform 18 tasks without receiving any instructions, at the end of each interaction they were asked to recall and explain their actions using a posttask and think aloud technique and rate the session in terms of suitability and simplicity. During a second study phase, participants were given interaction suggestions and asked to perform 4 tasks; this phase allowed an evaluation to see if participants changed their interaction strategy. Each session was recorded, it lasted between 1 to 1.5 hours, and participants were rewarded $15 for their time. At the end of the experiment, a total of 216 unique interactions were observed. From these interactions, it was observed that 86% were gesturebased, 38% were soundbased, and 26% used both gesture and sound (multimodal). This study also observed that 90% of the participants felt in control of the drone, and 95% felt that the humandrone interaction was natural. Safety is a concern of studies performed with ﬂying robots, nonetheless, 
in this study 16 out of 19 participants reported feeling safe when interacting with the drone. The above study was replicated at a Chinese university to evaluate how cultural differences impact HDI [19]. The same methodology and drone model were used during the study replica, which was performed with 16 participants. Similarly, 86% of Chinese participants performed gesturebased interaction, however a higher number used speech (58%) and multimodal interaction (45%). A comparison between the results achieved in the USA and China shows that Chinese participants are more prompt to multimodal interaction than American users, conﬁrming the hypothesis that cultural differences impacts HDI. A modiﬁed version of this study was performed to evaluate if participants will instinctively use touch as a means of interacting with the drone [20]. This was a betweensubjects study with 24 participants divided into two groups, one of them interacting with a standard Parrot ARDrone, while the other group interacted with a modiﬁed safetotouch version of the same model (protective cage). The methodology of the study differed from the original [18] as participants were not informed that the drone was being controlled through a WoZ, instead of cards participants were given visual images of the drone state before and after the task, and only 12 tasks were performed. Among the 12 participants that interacted with the safetotouch 58% of them used touch, and 39% of all interaction was touchbased, suggesting that it is a natural interaction technique. Feedback provided by participants states that interacting with the safetotouch drone was signiﬁcantly less mentally demanding than the unmodiﬁed drone. 
A. GESTURE Studies demonstrate that when users are asked to interact with a drone without any instruction, gesture interaction is the primary choice of most users [18]–[20]. Collocated interaction with drones is more natural using gestures rather than a joystick, which positively affects the social aspects of HDI [21]. Previous studies suggest four design rules for gesturebased systems: (1) gestures should be natural and easy to perform, (2) information in the captured images should be related to gestures, (3) a clear distinction between the background and the gesturing body is necessary, (4) and data processing shall be done with minimal delay [22]. A straightforward approach to implement a gesturebased system is to use a Kinect device to extract spatial information and recognize postures. This approach was successfully used for both collocated [23], [24] and teleoperation control [22]. To create an immersive ﬂight experience, a Kinect device can be combined with an Oculus Rift virtual headset to control drones [25]. A setback from using a Kinect device for humandrone interaction is that the system endtoend latency is relatively high, measured at around 300 milliseconds [22], [23]. Handgestures were also successfully used to control drones, either through a Leap motion controller [26], [27] or electromyography (EMG) signals captured with a BioSleeve [28]. 
167442 VOLUME 7, 2019 
TABLE 6. Summary of gesture controlled multirotor drones. 
Gesture recognition can also be achieved using cameras and computer vision. Many drones already have an onboard camera, which can be used for gesture recognition without requiring extra sensors that increases payload [29], [30]. Face pose detection can be used to estimate the drone’s relative position to the user [31], the user can move the drone by simply pointing at the desired direction. A prototype was built using the Parrot ARDrone and its onboard camera. The drone ﬁrst detects a human face, initializing the interaction. A ViolaJones face detector algorithm is used, and a Kalman ﬁlter is also used to track the human face and reduce false positives. A face score system was developed, which detects the direction of the user’s face and allows the drone to estimate its position and distance with respect to the user. The pilot can send the drone to a speciﬁc direction by pointing his hand, colored gloves were used to allow easier detection of the gestures. The integration of face recognition and gesture was also studied by [32], where users can select which drones to add or remove from a group of drones through face engagement and gesture control the selected quadcopters through. This approach allows the creation of teams by individually adding or removing drones from the group through faceengagement and hand gestures. Face engagement is deﬁned as the process of using eye and gaze contact to establish an interaction with the system. Prototypes were built using Parrot ARDrones that communicated to each other and the GCS through WiFi connection. Each drone uses OpenCV to detect and track the user’s face. A distributed election among the drones detects which quadcopter the user is wishing to engage. The drone with the most accurate face detection is considered active and proceeds to detect user’s hand gestures. A user can add a drone to a team by looking at the desired drone and raising his right hand, similarly, a drone can be removed from a group by raising a left hand. The user can start a mission by waving both hands, and the command will be sent to all drones in the group. For this experiment, the mission was to simply perform a ﬂip action, but it can be elaborated 
to more complex autonomous missions at a later work. For distinction purposes, the drones currently on the team ﬂy at an altitude 0.2 meters higher than others. During the ﬁrst experiment, a markerbased localization algorithm was used to position the three used drones, individual engagement and team control was considered successful in 10 out of 15 trials. A second experiment was performed with only two drones and a featurebased localization algorithm instead for positioning, in this case, 8 out of 10 trials for engagement and team control were successful. A summary of papers and user studies related to gesture control can be seen in Table 6, which also describes the prototype and additional hardware speciﬁcation. This survey on gesture control suggests that it is an intuitive control modality, with the advantages of easy control, and shorter training periods. Additionally, it presents the beneﬁt of not requiring the user to hold external devices such as a joystick and can even be used with the onboard sensors solely. However, this method might not be the best approach to applications that require ﬁne and precise control, as it does suffer from a higher latency and lower accuracy than other methods such as a remote controller. 
B. SPEECH Studies designed to elicit natural user interfaces showed that 38% of American users [18] and 58% of Chinese users [19] used speech as a method of interaction; which suggests that it is an intuitive control method (see Section IV). However, speech control has not yet been thoroughly explored by HDI researchers. Speech control can be considered easier than other methods (e.g. remote controller), as all that it is required is for the pilot to memorize voice commands, leading to shorter training periods. However, similarly to gesture control, the speech recognition can add latency to the system, limiting its applications. Additionally, controlling drones through speech presents some unique challenges, the fastspinning propellers creates a loud noise that can 
VOLUME 7, 2019 167443 
D. Tezza, M. Andujar: StateoftheArt of Human–Drone Interaction 
TABLE 7. Summary of brain controlled multirotor drones. 
decrease the accuracy of voice recognition. Another issue is that users are limited to collocated interaction if the voicerecognition is being performed onboard, as the drone needs to be close to the user to acquire the voice commands. This issue does not affect systems where a groundcontrol station is used to decode voice commands and control the drone. A prototype built using the Parrot ARDrone and Aerostack framework can be controlled through voice commands [27]. In this project, the pilot uses speech to interact with a ground control station, which sends control commands to the drone. Voice processing is done using the ROS package implementation of the Pocket Sphinx library, a speech recognition system developed at Carnegie Mellon University. The current version has a control dictionary of 15 commands but can be further expanded to include new functionalities. The software package listens for a simple one to threeword tasks, feedback is provided through a voice synthesizer when a detection is made, and a command sent to the drone. Users that tried the system provided positive feedback, and most of them expressed that talking to a drone was like interacting with a pet. Speech has also been used as an input channel for multimodal ground control stations [33], and direct multimodal interaction [29], [34]. As these projects include more than one interaction technique, they are further explained in the multimodal section below. 
C. BRAINCOMPUTERINTERFACE/BRAINCONTROLLED DRONES Braincomputer interface devices have broad potential as assistive technologies and novelty control methods. Since 2010 researchers have been exploring the use of BCI’s to control unmanned aircraft (ﬁxedwing), and in 2013 the ﬁrst braincontrolled multirotor project was published [35]. Additionally, the ﬁrst braincontrolled drone race was held in 2016 at the University of Florida [36], followed by a race at the University of South Florida [37] and the University of Alabama [38]. To control drones with brainsignals the pilot must wear some form of BCI headset, the most common being Electroencephalography (EEG) headsets. These devices measure the brain’s electrical activity on human’s scalp, which are decoded using machine learning algorithms to control physical systems using brainwaves. 
A summary of braincontrolled drones’ literature can be seen in Table 7, including the drone models, and BCI device speciﬁcations. The table shows that most projects use EEG based BCIs. Additionally, there are hybrid BCI approaches, which combine EEGs with other physiological sensors and other forms of control that is not brainbased. A comprehensive survey on braincomputer interfaces for unmanned aerial vehicle control can be found in [39]. Although successful control was achieved for up to three degrees of freedom, the authors conclude that interaction with BCIs is still more limited than other control interfaces and further research is required to increase ﬁdelity and robustness of these systems before they migrate from laboratories to user’s homes [39]. Once BCI reliability and accuracy achieve similar levels to other control modalities, this type of control will allow handsfree interaction and accessibility for disabled users. Additionally, BCI’s can be used to measure the pilot’s mental state and adapt the drone behavior and interface accordingly, for example, the system could adapt its user interface if it detects that the pilot workload levels are too high. 
D. MULTIMODAL Integrating different interaction methods can combine the advantages of each. Furthermore, previous work demonstrates that 45% of Chinese users [18] and 26% of American users [19] naturally use multimodal interaction with drones. A summary of the papers related to multimodal control can be seen in Table 8, along with the prototype speciﬁcations, in which control modalities are involved and performed user studies. A multimodal approach can be used to create a direct interaction with drones, for example by taking off and landing through speech and controlling movement by gesture [29], [34]. In these studies, a quadcopter prototype can be controlled solely by using onboard sensors to detect gesture and speech interactions, without the need for any external devices. Users can command takeoff and land through audio commands, which was implemented using Julius, an opensource voice recognition library. Takeoff is accomplished by voice command, but the propeller sound during ﬂight creates too much noise for voice recognition, therefore the landing command is a whistling sound which has a distinct frequency from the propellers sound. Movement during ﬂight 
167444 VOLUME 7, 2019 
TABLE 8. Summary of multimodal controlled multirotor drones. 
is controlled by following hand movements wearing colored gloves. During the ﬂight, if a hand is not recognized, the quadcopter simply hovers on the location. A pilot evaluation was performed with four participants, two of them being familiar with the project while the other two were not, and all participants were able to successfully control the system. Virtual reality environments have been used for an exploratory study to investigate multimodal interaction to control a swarm of drones [50]. This project presented a virtual environment of grass ﬁelds with landmarks (trees, wood and rock piles), where the users must control a swarm of 10 drones to search and rescue an individual. Users can control the drones using gestures and speech, but a Wizard of Oz technique was used to simulate the drone response. A ﬁrst exploratory study was performed with 10 participants, each having 1 practice and 4 experimental trials. Participants were asked to control the team of drones over the landmarks until the target was found, time was emphasized as the experiment was simulating a search and rescue scenario. Observed interaction was classiﬁed as pointing and herding gesture actions, and high and lowlevel voice commands. User feedback stated that depth perception was an issue, as they could not precisely estimate the location and speed of the drones. A second experiment was performed using the same procedures as the previous, but participants were given a set of gesture and speech commands they could use to control the swarm. The command set was built based on the feedback received from the ﬁrst experiment. To address the depth perception issue, half of the participants used a modiﬁed version of the system where fake shadows were projected in the ground below the drone. Results of the experiments state four design implications: (1) commands should be clear and well deﬁned, but allowing some level of ﬂexibility; highlevel commands such as ‘‘go to that tree’’ require less interaction effort and should be given priority; (2) feedback is very important, users need to be informed that the system received and understood the command; (3) depth perception is an issue on this type of virtual reality environment, but adding shadows to the drones signiﬁcantly addressed the issue; and (4) participants wanted more data in the interface. There are three ﬂows of information between users and ground control stations, which can be summarized as follow [33]. The ﬁrst ﬂow is from the operator to the ground control station (GCS), as the user controls the system. 
The second ﬂow goes from the GCS to the user, providing system data, state, and feedback. Lastly, the operator state can also be used to provide information to the GCS, for example by adapting ground control stations using heartbeat or EEG signals as passive input channels. Multimodal interaction can enhance all three data-ﬂows by increasing the number of communication channels, addressing high information loads, and allowing communication within a variety of environmental constraints [33]. 
E. OTHER CONTROL INTERFACES Manufacturers recently started to design safetotouch drones, features such as hand landing are already available to commercial drones like the DJI Spark, Ryze Tello, and others. A study designed to elicit natural user interfaces found that 58% of participants used touch interaction when the drone was enclosed in a safetotouch frame (see Section IV), and that interacting with a safetotouch drone is mentally less demanding than a traditional design [20]. Another control modality is to use Gaze tracking to generate commands. Gaze allows two directional control, which can be combined with different input channels to ﬂy drones in a 3D space. A comparison of different combinations of gaze and keyboard control found that the best approach is to use gaze to control pitch and yaw while controlling roll and altitude through the keyboard [35]. Birdly is a simulator that creates a virtual reality environment to allows users to ﬂy like a bird. A modiﬁed version of Birdly described in [51] allows users to control an actual drone through body movements and the camera gimbal through head movement, creating an immersive ﬂight experience. The author’s goal was to develop an effective and natural embodied humandrone interaction that is easier to train and more immersive than traditional interfaces such as joysticks, keyboards, and touchscreens. To control the drone the user lays down on the simulator with arms wide open as it was about to ﬂy like a bird. Roll and pitch movement is controlled by hand movements and the camera gimbal is controlled by head movement. The video stream from the drone is displayed on the user goggles, the simulator platform tilting provides vestibular feedback according to the drone attitude, and a fan pointed at the user adapts its speed accordingly to the drone speed. The authors decided to use a multirotor drone modiﬁed to act as a ﬁxedwing drone as it 
VOLUME 7, 2019 167445 
D. Tezza, M. Andujar: StateoftheArt of Human–Drone Interaction 
ﬂies more similarly like a bird and provides an easier mapping to Birdly commands. A ﬁrst experiment was performed with 42 participants using a virtual reality environment built with Unity 3D. During the experiment, 2 users felt nauseous and didn’t complete the tasks. Out of the 40 participants, 20 tried the system utilizing a traditional joystick control and 20 tried using Birdly. Out of each group, 10 participants controlled the drone utilizing angle mode, and 10 controlled using acro mode. The best results were achieved using Birdly with angle control mode. Positive feedback was provided as Birdly allowed an immersive, enjoyable, and natural experience. A system prototype was built allowing the control of a drone utilizing Birdly and angle control mode, following the results achieved during the ﬁrst experiment. To validate the system, a user with no prior experience was asked to ﬂy the system for 5 minutes. He was able to accurately control the drone through the predetermined ﬂight path. The participant also provided a feedback score of 7 out of 7 for an enjoyable experience, 6 out 7 that he was in control of the ﬂight trajectory and 5 out of 7 as the sensation of ﬂight. Flying head is another project that provides an innovative drone control method [52]. The drone movement is synchronized with the user head movement in terms of horizontal, vertical position, and yaw rotation. As the user head moves or rotates the head, the drone imitates the movement. Altitude can also be commanded through a Wii remote controller, allowing altitudes higher the user height. The drone’s camera image is streamed to a headmounted display (HMD), allowing the user to see the ﬁrstperson view. The prototype consists of a Parrot ARDrone, an HMC Sony HMZT1 display, and an OptiTrack S250e motion capture system. Two user studies with 6 participants were performed to compare Flying head against a joystick control. During the ﬁrst study, each participant had 3 sessions to control the drone to acquire 4 static markers placed around the room using both Flying Head and the joystick. The time required to complete the task using Flying Head was on average 40.8 seconds, while interaction with the joystick took 80.1 seconds. These results show the efﬁciency of Flying head as a control modality, a paired ttest from the average of each participant time resulted in a pvalue of 0.007. During a second experiment, participants had to follow a moving target (toy train) around an oval lap using both Flying head and joystick control methods. Again, each participant performed 3 sessions and the percentage of time each was able to accurately follow the target was calculated. Flying head allowed participants to follow the train on average 59.3% of the time, while joystick control had a result of 35.8%; a paired ttest resulted in a pvalue of 0.012. Another user study was performed with Flying Head to evaluate the system against a method where operators control the drone by moving a scaleddown dummy aircraft with his/her hand [53]. In this experiment the movement acquired by ﬂying head was translated to drone movement in a 1:2 scale, and the dummy drone movement was scaled 1:2.5. Again, ﬂying head provided better results as the average completion time for the required task with Flying head was 53.1s 
and 99.1seconds with the handsynchronization method, which led to a pvalue smaller than .01 in a paired ttest. Headsynchronization methods can be explored and used to create new aerial sports that allow users to overcome the humanphysical limitation. [54]. An innovative control method is to utilize a scaleddown 3D printed map of the ﬂying terrain and allow the pilot to draw the desired drone’s ﬂight path on the physical map model [55]. The drawn ﬂight path and video stream from the drone is superimposed on the map using either an augmented reality handheld or headmounted display (HMD). A proof of concept prototype was built using the Parrot Bebop drone, an iPad air as the handheld device and an Epson Moverio HMD. The user can interact with the 3D map using a pen to plan the ﬂight and watch it once the drone starts to ﬂy autonomously. In this project, the user interface is fully developed but the drone interface has not yet been developed, therefore as a proof of concept a Wizard of Oz approach was used to simulate the drone behavior. 
V. DISTANCE, COMMUNICATION AND EMOTION ENCODING As drone usage continues to increase, it becomes important to understand the different aspects of humandrone interaction. This section covers areas related to comfortable distances, humandrone communication, and emotion encoding in drones. 
A. INTERACTION DISTANCE The interaction distance between a drone and a human must be considered for smooth social interaction [56]. In the elicitation studies described in Section IV, the authors also analyzed the distance participants kept from the drones during the experiments. In [18], 37% of American users stayed within the intimate space from the drone (45cm), 47% stayed in the personal space (1.2 m) and the remaining 16% interacted within the social space (3.7 m). In [19], the authors demonstrate that Chinese participants interacted at a closer distance: 50% at the intimate space, 38% at the personal space, 6% at the social in the public space (> 3.7 m). Such results demonstrate that the majority of users are comfortable getting to a close interaction distance with drones and that cultural factors inﬂuence proxemics in HDI, as Chinese participants got to closer proximity than American participants in studies [18], [19]. Another user study where drones approached users at different heights (1.80m and 2.13m) concluded that height played no signiﬁcant impact in comfortable approach distance [57]. A humancentered design approach was used to prototype a social drone in [56]. At the ﬁrst stage, 20 participants of a design study were asked to draw what they believed a social drone would look like starting from a DJI Phantom 3 silhouette and to answer a survey about the features such a drone would have. Results showed a strong agreement in a few aspects: an oval or round shape around the drone, facial features, colorful, and the use of a screen and audio for 
167446 VOLUME 7, 2019 
humandrone interaction. A second phase consisted of a focus group with 5 participants, who were given 20minutes to draw a social drone using the attributes and characteristics elicited during the ﬁrst phase. Results from both the design study and focus group were taken into consideration to implement a social drone prototype, which was used for a proxemics study. Results demonstrated that users allowed the social prototype to get 30% closer (average 41inches) than a nonsocial prototype (average 54 inches). The same study observed that petowners and male users are more likely to have a closer interaction than nonpet owners and female users. 
B. DRONE FEEDBACK Studies have previously explored methods for acknowledgment of mutual attention between a drone and its user. A proposed system allows users to call the drone attention by waving its hands, the drone acknowledges that the user has its attention by wobbling sideways in the air and further interaction can start. This topic is also discussed in [58], where a comparison of four different drone acknowledgment gestures shows that users prefer a rotation in the yaw axis to indicate acknowledgment. The authors provide a series of design guidelines for drone acknowledgment gestures such as the preferred distance (1.80m), a clear distinction between gestures and other ﬂight maneuvers, and gesture speed (yaw: 100deg/sec pitch: 66deg/sec roll:133deg/sec). Natural humandrone communication is also necessary to achieve good interaction, especially in a collocated space. Previous research explored a drone’s ability to communicate its intent to users [59]. In this study, the authors deﬁned drone motion as a composition of trajectory, velocity, and orientation. To express intent through motion the authors manipulated movement primitives using arc trajectories, easing in and out of velocity proﬁles, anticipatory motions, and different combinations of the previous. A user study with a Unity simulation was performed to elicit the most efﬁcient motion manipulations to demonstrate intent. Using the highest accuracy results achieved during the ﬁrst study, a ﬂying prototype was built with manipulations for the following tasks: approach person (easy inout), avoid person (arc + easeinout), depart person (easy inout), approach object (anticipate), and depart object (arc + easeinout). A second userstudy was performed to test the prototype with 24 participants. Results demonstrated that all three hypotheses were correct: (1) collocated individuals will prefer working with a drone using manipulated ﬂight paths, rather than baseline paths, (2) collocated individuals will view manipulated drone ﬂight paths to be more natural and intuitive than baseline motions, and (3) collocated individuals will feel safer interacting with a drone using manipulated ﬂight paths than baseline paths. This study successfully demonstrated the ability to enhance humandrone interaction by manipulating ﬂightpath to communicate intent. While a drone’s ability to ﬂy in a 3D space present unique advantages, they also pose a challenge in achieving effective humanrobot interaction [60]. Current drones lack the ability 
to communicate effectively with its user, which makes it hard to interpret the system and can even lead to accidents. Another study related to humandrone communication explored efﬁ- cient approaches to communicate the directionality of ﬂying robots through visual feedback [60]. The authors developed a prototype ring consisted of 64 LED lights that are used to communicate directionality and can be attached to drones. Four highlevel signal models were designed to inform directionality: blinker, beacon, thruster, and gaze. The blinker approach consists of blinking onequarter of the ring at a 1Hz frequency to communicate intent to move in a speciﬁc direction, like an automobile blinker. The beacon can be compared to a light beacon that points the direction the drone is moving towards; a wider portion of the ring was lighten using a gradient brightness decreasing from the pointing direction to the sides. The thruster is an analogy to the light and ﬂame produced in jet engines, therefore a small region of the ring in the opposite side of the direction was powered at highintensity. Gaze was inspired by human eyes, two small areas on the ring (eyes) looking in the direction that the drone will ﬂy. A user study with 16 participants was performed to evaluate the different lighting patterns. Users provided feedback after observing the drone perform different tasks utilizing each pattern. Participants were able to interpret directionality quickly and more accurate when the ring was used compared to a traditional drone. Best results were achieved using gaze, blinker and thruster models. 
C. REMOTE COMMUNICATION Humans can interact with drones in a collocated space, or remotely. Remote interaction requires a wireless communication link between the human and the system, such as a direct radiofrequency link. More recently, drones are now capable of carrying modems and connecting to internet networks such as 5G, which allows humans to interact with their drones from anywhere using a reliable and lowlatency network, if there is signal coverage. These capabilities expand the spectrum of applications in which drones can be used. As an example, it allows drones to be used in package delivery systems such as described in [14], [15]. Additionally, drones can be used as a network station themselves, allowing for an ultra-ﬂexible and costeffective approach to provide wireless services [61], [62]. Internet connection for drones also bene- ﬁts the creation of drone swarms, as it provides an easy way to create a network for drone communication. Such connectivity allows ﬂights beyondlineofsight (BLOS), which also presents certain disadvantages, especially on safety and security aspects. In most countries, BLOS ﬂights are not permitted without previous authorizations, mainly due to safety risks (e.g. crashes with manned aircraft). Additionally, longerrange ﬂights increase the chance of physical (e.g. ﬁrearm shots) and cyber (e.g. signal jamming) attacks. For example in [63], the authors discuss cyberphysical attacks on drone delivery systems, in which the vendor wants to deliver a package in the shortest possible time while an attacker wants to cause delays. 
VOLUME 7, 2019 167447 
D. Tezza, M. Andujar: StateoftheArt of Human–Drone Interaction 
D. EMOTION ENCODING Emotion encoding in robots can be beneﬁcial as it increases social interaction and enhances communication; equivalently to how humans perceive dog happiness when they wag their tails. Drones lack humanoid characteristics such as a torso, legs, and arms; therefore, emotion encoding must be made through a combination of movement parameters, such as velocity, acceleration, and trajectory [64]. A previous study encoded emotion in a drone prototype and a user study was conducted to discover if users could read the robot emotion [66]. The authors ﬁrst elicited 8 stereotypes of personality and their respective emotion characteristics (brave, dopey/sleep, grumpy, happy, sad, scared, shy). For each stereotype, a drone interaction proﬁle was created, listing characteristics such as fast vs slow, instant vs delayed, gentle vs powerful and so on. Taking the stereotypes and proﬁles into consideration the authors merged some similar characteristics and created four emotion proﬁles for drones: exhausted, antisocial, adventurer hero, sneakyspy (not implemented in the user study). For each of the three proﬁles implemented the ﬂight characteristics were adjusted accordingly to: speed, reaction time, altitude, and special movements (ﬂip, starts and stops, wobbles). Participants were asked to observe the drone and answer a questionnaire with the emotional state they believed the drone to be. Results demonstrated that emotion was correctly recognized 60% of the time when using a single keyword (exhausted, antisocial, adventurer) and 85% when a second keyword was used (sleepy, sad, grumpy and so on). Previous research aimed to investigate the impact of encoding emotional states in drones, and how it inﬂuences user emotion [66]. The authors 3D printed a prototype named Daedalus, which is capable of head movement (pitch and roll) as well as changing eye colors (white and red). Four expressions were preprogrammed into Daedalus: (1) head roll with white eyes and propeller off, (2) head pitch with white eyes and propeller off, (3) head roll with white eyes and propeller on, (4) head pitch with red eyes and propeller on. An experiment was performed with 30 students; each participant would stay 0.5-0.7 meters away from the drone and observe while it performed one of the preprogrammed actions. After observing Daedalus, the participants had to answer (1) what do you feel? and (2) what do you think the robot is feeling? by selecting one of 7 cards (happiness, sadness, anger, fear, disgust, surprise and contempt). Results demonstrated that positive expressions were perceived with the propellers off (happiness and surprise), but the opposite was found with operating propellers (mostly fear and anger). It was also found that red eyes (4th movement) were highly impacted participants as 50% selected they were in fear, and 80% selected that the robot was angry. Results achieved in these studies demonstrate the potential of emotion encoding to provide a richer humandrone interaction experience. 
VI. INNOVATIVE PROTOTYPES AND USE CASES Drones are currently being used for a broad range of applications, but researchers are further exploring new ways in which 
these systems can be useful. This section presents novel drone prototypes and envisioned use cases. Prototypes with similar purposes were aggregated in categories, which are discussed in the following subsections. 
A. FLYING USER INTERFACES This subsection presents drone prototypes designed to enhance and add mobility to user interfaces. Drone’s ability to ﬂy in a 3D space allows it to position itself anywhere around the user at any orientation. This fact can be explored to augment user interfaces as drones can be used as a new medium for both input and output of information [13]. Researchers have previously classiﬁed the phases of ﬂying interfaces into three categories: approaching the user, interaction, and leaving phase; which can be used to call its attention [67]. These systems can be used for controlling crowds during emergency situations, provide feedback and guidance to athletes during sports, or even act as a tourguide to outdoors activities [68], [69]. Previous work explored the use of two drones as ﬂying displays, one carrying a projector while another carries a projection screen [67]. This approach can be used as a new model for public display in urban environments as it allows the display to grab attention by approaching the user, interacting and leaving. A prototype using two Parrot ARDrones with a modiﬁed stabilization algorithm was built and evaluated. The relation between the drones was based on a masterslave relationship, the projector drone follows the path from the screen drone using visual markers and computer vision to position itself to display the image properly. The ﬁrst experiment compared the modiﬁed drone’s ability to hover in place against a nonmodiﬁed version. The prototype was able to hover signiﬁcantly more accurate than the nonmodiﬁed drone (p < 0.05; Wilcoxon Rank test) even though they were carrying heavier payloads (screen and projector). The second experiment compared the projection accuracy when using/not using visual markers for localization. The system with a visual marker achieved signiﬁcantly higher accuracy (p < 0.05; Fisher’s exact test). The prototype was used during a research forum in Tokyo as a demonstration and successfully caught the attendee’s attention. A custombuilt octocopter carrying a smartphone and video projector was successfully used to display images and SMS texts on arbitrary surfaces [70]. For evaluation purposes, a ﬂying experiment was performed outdoors displaying the received messages on a building wall. The ﬂight lasted 7 minutes and approximately 40 people were standing 15 meters away. During the experiment, a total of 23 messages were displayed and at the end 14 participants provided feedback during an interview. Users found the system to be a fun experience capable of grabbing attention and envisioned usecases such as interactive storytelling and advertisement applications. In [71] the authors describe two ﬂying display prototypes designed for both indoors and outdoors interaction. The ﬁrst prototype is meant to be used indoors and it consists of adding 
167448 VOLUME 7, 2019 
both a screen and a mobile projector to the same drone. This system was tested during a banquet (200 persons) and an exhibition opening (30 persons) where participants were able to text a message to be displayed on the drone while it hovered around the location. The results were positive as participants engaged in social interaction as a group, every text displayed on the drone made participants shout, laugh and clap. The second prototype is another example where two drones are used outdoors for carrying a projector and a screen canvas, the system was tested but there was no user study performed. This setup allows for outdoor projection only during nights due to illumination constraints. A device with a stronger illumination screen such as an iPad can also be attached to drones to mitigate the issue of illumination and allow indoor use during daylight [68], [69]. In this project, an iPad was attached to an octocopter. A demonstration was performed to 12 users. Feedback acquired through interviews and questionnaires stated large potential for the application [69]. A reading test was performed to evaluate how the motion of the display would affect reading performance [68]. In this experiment, twelve participants had to read characters from the screen during 4 different occasions: (1) system sitting on a table, (2) hovering, (3) ﬂying pass by the user, (4) user walking behind the moving display at a constant speed. Results showed a signiﬁcantly less reading accuracy when both the user and displays are moving (case 4) and did not show a signiﬁcant difference between cases 1, 2 and 3. The authors conclude that when both display and users are in motion the font or content size should be increased. 
B. SOCIAL COMPANIONS This section discusses drone prototypes that explore social interactions with users. Exercising bring many health beneﬁts for humans and technology has been previously used to enhance jogging experience, for example as in mobile applications and sports watches. Previous work has explored the use of quadcopters as a jogging companion, and the prototype was named Joggobot [16]. While existing systems focus on keeping track and enhancing the performance of the runner, the authors of Joggobot focused on enhancing the social aspects of jogging. The prototype was built with the Parrot ARDrone, which takesoff and ﬂies 3 meters ahead of the runner; the prototype can only accompany a user in a straightline and the user must wear a tshirt with a visual marker allowing the drone to easily locate itself in respect of the user. There was no formal evaluation of the system but preliminary insights by users who tried it demonstrated positive feedback towards the concept. Users noted that the system helped distract them from exhaustion and challenged to increase efforts. Some users preferred Joggobot to follow the user pace and guidance, while some users liked the idea of the robot being in control of the pace as it can motivate and compel users to follow it. Users also expressed a need for means to communicate with the robot, and the authors suggested 
using heart rate monitoring as an implicit communication tool as Joggobot could pace the jogging according to health recommendations. A technical speciﬁcation of a custombuilt quadcopter designed to be a jogging companion is provided in [72]. The authors tested 6 different custom design to achieve a conﬁguration that best compromise between performance, safety, and stability for outdoor ﬂying. The ﬁnal design consists of: a Safeﬂight Quadcopter 500mm frame, GemFan 10inch propellers, Sunnysky x2212 980kv brushless motors, a pixhawk ﬂight controller, uBlox GPS, and HMC5883 magnetometer and a 915MHz telemetry link, the prototype is controlled through the APM Planner control ground station. Drones can also be used as companions for visually impaired persons to provide navigation assistance [73]. This study envisions a drone system that standsby on a wearable bracelet until its assistance is required. A blind user would command the drone to a destination using voice commands, and the drone would provide guidance until the target is reached. The user could follow the drone through the auditory feedback provided by the noise of the spinning propellers. Once a command is received, the drone calculates the distance to the target location and guides the user ﬂying at a set distance ahead avoiding obstacles. A Bluetooth connection to the bracelet would allow the drone to adapt its distance and speed relative to the user. The two envisioned use cases are to guide users to a speciﬁc location or to assist in ﬁnding misplaced objects using computer vision algorithms. Although the system is not yet developed, a preliminary study was performed with a blind user and a Wizard of Oz approach to control a miniature drone. The participant was able to successfully follow the drone as envisioned and provided positive feedback about the project idea. Researchers also envisioned the use of drones as an agent to support a clean environment [74]. In this application, the drone would ﬁnd trash items on the ﬂoor, persuade users to pick it up and guide them to the nearest trash bin. To simulate the system behavior a researcher controlled a Parrot ARdrone through a smartphone application while being recorded. The video was edited to add lighting effects, ambient street sounds, and motor noise. Three videos were created, and the drone had a different persuasion technique on each: visual, audio, and a combination of both. An online betweensubject study with 82 participants was performed; each participant watched one of the videos and ﬁlled out a survey afterward. Although results analysis did not ﬁnd an effect of the interaction modality (visual, audio, both) on user’s compliance, other factors were observed. Females participants perceived the drone as friendlier than males. Also, participants from developing countries rated the perceived persuasion, compliance, pleasantness, and sensibility of the drone signiﬁcantly higher than participants from developed countries, suggesting that cultural factors impact humandrone interaction. 
VOLUME 7, 2019 167449 
D. Tezza, M. Andujar: StateoftheArt of Human–Drone Interaction 
C. ARTS AND SPORTS This section presents how drones can be used to create artwork and new sports. Drawing on landscaping always raised interest among people and as drones become popular more people will be able to watch landscapes from the sky [75]. To create art pieces in a landscape requires a large amount of planning and time used to require planning and time, but with the use of drones, it can be an easy and fast task. An approach to quickly create landscape art is described in [75] as follows: a user can draw a sketch on a mobile phone screen that displays the video stream from a drone camera. The user than ﬂies the drone above the area where the work will be created. While the drone hovers above the area, the user follows the screen drawing by walking and putting markers on the ground. At this point the user can land the drone and mow the grass following the previously marked points, creating the landscape work. Using this approach and a commercialofftheshelf drone, a user was able to create by himself a large smiley face on a grass ﬁeld in less than 30 minutes, two trees were incorporated in the drawing like the eyes of the smiley face. Technology can be used to assist sports as referee’s tools for instant replay or to augment the entertainment, but drones can take such capabilities even further. A project known as HoverBall envisions augmenting sports by utilizing a drone as a ball capable of changing its physics dynamics [76]. The drone is enclosed in a circular cage and can change physics dynamics such as gravity, speed, and trajectory of the ball. Hoverball allows the design of new sports and modiﬁcation of current sports to enhance player experience. As an example, Hoverball could decrease the speed of the ball or gravity effect during a Volleyball game to allow unskilled players or even kids and the elderly to play the game. Such a system could be used to create games that take into consideration players skill and adapt the difﬁculty to allow skilled and unskilled users to play together. An initial HoverBall prototype was built using a Crazyﬂie nanoquadcopter enclosed in a Styrofoam grid shell and an Optitrack s250e motion tracking system. The initial version allows three simple throw interactions: hover, glide, and boomerang; but there was no formal evaluation of the system. The authors concluded that drones have the potential to augment sports, but research is necessary to deal with some current constraints, such as payload requirements to build a strong cage capable of absorbing impacts and increased ﬂight time. 
D. HAPTIC FEEDBACK FOR VIRTUAL REALITY This section presents two projects where drones were used to create haptic feedback for virtual reality (VR) systems. Current VR systems can provide immersive visual and sound experiences, but it lacks the ability to provide tactile feedback. As drones can ﬂy in a 3D space, they can be used to provide tactile feedback by touching the user at any location and speed to provide adequate experience. Small quadcopters have been used to provide haptic feedback for virtual reality 
games [77]. In this project, small drones are used to ﬂy into users at varying speeds while they are immersed in a virtual environment system. To provide safety to participants the drone has a protective cage, therefore, the spinning propellers cannot hurt the user. The prototype was built using a Parrot Rolling Spider drone, able to carry 10grams and accelerating up to 18km/h; the theoretical energy capable of impacting the user is 0.8125Joules, which presents no harm to the user. Different tips can be attached to the drone, depending on the virtual environment to provide adequate feedback. The ﬁrst prototyped game consists of a Mayan city in the jungle, the drones provide feedback in three scenarios: acting as bumblebees that are attacking the user, arrows that are shot by creatures at the player, and bricks and woods falling in the user as the ruins collapse. In all scenarios, the small drones ﬂy around with a speciﬁc tip and bounces at the user providing the tactile feedback. Until this point, there was no user study performed for this project. Drones have also been used to provide encounteredtype of haptic feedback [78]. In this study, a prototype was built by attaching a lightweight ﬂat object to a quadcopter. While the user is immersed in a virtual environment, the drone position itself allowing the user to touch it using a grasping object to provide adequately haptic feedback. In this case, the user would use a wand to touch the ﬂat object, which was a sheet of paper. Although the resistance force of the paper was not enough to be perceived when the drone is not in movement, the airﬂow created during ﬂight pushes the paper creating a resistance force measured at 0.118N (+- 0.036N), which is enough to be perceived. An experiment with 4 participants was performed to test the system. All participants were able to feel the haptic feedback when immersed in the virtual environment. The experiment asked participants to draw a straightline using the wand while immersed in the virtual environment. Results showed that participants were more accurate when haptic feedback was provided. 
E. OTHER PROTOTYPES The ﬁrst project presented in this section uses a drone to record videos and display a realistic experience for sports spectators, the second project uses drones to stream in realtime a selfimage to an athlete for training purposes, and lastly a project to augment humans’ mobility and perceptibility is presented. A project named Flying Eyes explored the use of drones to autonomously track humans in a 3D space with a speciﬁc camera path, to create a realistic experience for spectators [79]. Flying eyes is an alternative to the expensive and big systems used to record videos in 3D spaces, such as the cameras used in soccer games. The system uses an autonomous quadcopter that executes computer vision algorithms to track individuals and ﬂy different paths controlling the camera position and orientation towards the target. For the initial prototype, the user must wear a distinct color which will be used by the computer vision algorithms for detection, tracking and distance estimation. The computer 
167450 VOLUME 7, 2019 
vision algorithms are executed on the ground station, which communicates with the drone over a WiFi connection. Flying eyes calculates the drone positioning and two different ﬂight paths named tracking and circling. When tracking mode is used the system follows the target and captures the image from behind. Circling mode differs because the drone ﬂies in a circular path around the subject getting a different image perspective. The prototype was built using a Parrot ARDrone and tested with a single user, results suggest that a better mechanism is necessary to estimate the distance from the user. The authors plan to develop a secondgeneration prototype using a Mikrokopter model. The mental image of one’s self can help athletes to sharpen their skills and improve performance. Drones can be used as sports ﬂying assistant by providing an athlete his external image through a headmounted display or handheld device [80]. The proposed system autonomously tracks the target athlete, commanding the camera angle and position. The athlete can see the image at a later moment (delayed), in realtime through a headmounted display or through a handheld device. and compute camera orientation. The speed, relative position to the user, and height of the drone can be controlled through a mobile application. The current prototype was built using a Parrot ARDrone and tested with a single user using both delayed and realtime video stream. Delayed video stream was tested with a soccer player and a user running upstairs; the current prototype was not fast enough to follow the user in the ﬁrst case and did not provide adequate height control above the stairs, therefore further research would be required to select an appropriate drone for such uses. In realtime display mode, the system was successfully tested with a user swinging a baseball bat and a jogging activity while the image was displayed in a headmounted display. A research project named FlyingBuddy demonstrates how drones can be used to augment human mobility and perceptibility [73]. The prototype can ﬂy manually or autonomously, and an iPhone was attached to the drone providing extra sensors and a communication link, a second iPhone is used as a client to control the system. The authors of FlyingBuddy elicited 4 different user scenarios for the system. The ﬁrst scenario ‘‘ﬂying to buy’’ allows the users to locate nearby stores, create a videocall with the salesman, and ﬁnally carry the product back to the buyer autonomously. Another scenario ‘‘ﬂying to see’’ gives user abilities to explore and views beyond what they could see normally. The system could also be used to report automobile accidents, taking and sending pictures and precise locations to EMS allowing a better assessment of the situation and improving response time. Finally, FlyingBuddy can also be used as a mobile camera, taking pictures from different perspectives that we couldn’t reach due to physical constraints. A prototype was built with a Parrot ARDrone, two iPhone 4, and custombuilt software; but there was no user study performed with the prototype. 
VII. CHALLENGES AND FUTURE WORK Humandrone interaction is naturally limited by the challenges faced in general drone systems. For instance, safety is a major concern for both ﬁelds, as the fastspinning propellers can cause damage and injuries in case of accidents. This constraint is especially important for collocated interaction. As previously discussed in this paper, recent drones and prototypes allow safe touchinteraction (due to measures such as propellerguards). It is likely that this will become a trend and that future work in HDI will be done in the area of safetotouch drones. Another challenge caused by hardware limitation is short ﬂight times. As discussed in Section 2, current drone models have ﬂight times limited to 31minutes or less. However, this challenge cannot be easily be mitigated by HDI research as it is directly related to hardware and battery components. As research in related ﬁelds leads to longer ﬂight times, HDI researchers will be able to design and study longer interactions. Similarly, as the payload capabilities of drones increase due to hardware advancements, they will be able to carry more sensors and actuators, which will also allow HDI researchers to design new systems and interactions. An important sub-ﬁeld of humandrone interaction is the study of control modalities used to send commands to the drone, which can be a challenging task to the user. Controlling a drone safely and accurately might require long training periods and dedication. This paper reviews the research performed to create natural user interfaces such as gesture, speech, touch, and others. Although the majority of current HDI papers focus on control modalities, further research would still be beneﬁcial as we believe this ﬁeld is crucial to enhance humandrone interaction. Control modalities are the direct interface between the user and the drone, and improvements in such a link can lead to more accurate control and decrease training periods. Additionally, control modalities have the potential to decrease the pilot workload, possibly leading to a decrease in accidents and safer systems. It is expected that as researchers better understand HDI and with advancements in hardware technologies, they will be able to improve current control modalities and ﬁnd new ones. Drones are likely to become ubiquitous to society, especially as they start to be used in a broader application spectrum. The prototypes reviewed in this paper demonstrated that their usage goes far beyond traditional uses such as photography and inspection. Additionally, to HDI research leading to new uses, it is also worth mentioning that advancements in hardware and software technology will allow drones to be used on applications not yet envisioned. In the near future, drones will be extensively used in the ﬁelds of public advertising, deliveries, sports entertainment, emergency response, and to augment human capabilities. Furthermore, drone’s popularity will increase once we better understand how society accepts these systems, therefore, future researchers could contribute by studying how societies and different 
VOLUME 7, 2019 167451 
D. Tezza, M. Andujar: StateoftheArt of Human–Drone Interaction 
cultures view drones. Future work can be done to elicit design guidelines to ensure that future drone technologies are well accepted by society. This paper presents an analysis of which are the most common drone models used for research. Although models manufactured by DJI dominate the commercial market, they are not as common among researchers. Over half of the studies reviewed in this paper used the Parrot ARDrone, however, this model is already discontinued, therefore it is expected that its use on research will start to decrease. Therefore, it is likely that a new drone model will emerge as researchers ﬁrst choice; at this point it is unclear if one of Parrot’s new models will become such drone. The analysis suggests that there is a lack of a drone platform speciﬁcally designed for humandrone interaction research. Therefore, the future development of an opensource drone speciﬁcally designed for this HDI research would be a strong contribution to the ﬁeld. This platform would serve as a standard research tool, and it would allow researchers to easily integrate their own work and research. 
VIII. CONCLUSION As drones shifted from military technologies to the civilian world, they are now used in a wide variety of applications. As drone technology matures, these systems are becoming cheaper, easier to operate, and popular among a large number of users. Drone usage is expected to keep growing, and it is likely that such systems will become ubiquitous to society. Therefore, it is important to study the ﬁeld of humandrone interaction to understand how users interact with these systems. This paper deﬁnes the ﬁeld of humandrone interaction using an analogy to the wellestablished ﬁeld of humanrobot interaction. Proceeding, it presents the ﬁrst comprehensive survey on the emerging ﬁeld of humandrone interaction. This survey presents a comparison of modern commercialofftheshelf multirotor drones, providing readers with both the maximum speciﬁcations and limitations of current drone models. Additionally, it presents a comparison of which models are mostly used by endusers versus models used by researchers. It is also discussed how HDI research evolved over the past years and what is the current stateoftheart in the ﬁeld. Traditionally drones were controlled using either a joystick, a ground control station, or a smartphone application; however, research in HDI led to new natural user interfaces such as gesturebased control, braincomputer interfaces, speech, and others. Research in HDI goes beyond control modalities only, this paper also covers additional aspects of humandrone interaction, including a review of proxemics studies, and emotion encoding in drones. Furthermore, innovative prototypes found in literature and envisioned usecases are also presented, which allows us to envision future drone applications and HDI directions. Lastly, this paper presents a discussion on the current challenges and expectations for future research in the ﬁeld of HDI. Concluding, humandrone interaction is an emerging ﬁeld that it is likely to keep growing. This survey shows that 
the stateoftheart research consists mainly of evaluating and developing new control modalities, designing new applications where humans interact with drones, and enhancing such interaction by understanding how humans perceive the interaction (i.e. comfortable distances, communication). This paper serves both as a survey in the ﬁeld of humandrone interaction, and as an introduction to researchers who would like to contribute to the ﬁeld. 
REFERENCES 
[1] Federal Aviation Administration. (Jan. 2019). Unmanned Aircraft Systems Forecast. [Online]. Available: https://www.faa.gov/data_research/ aviation/aerospace_forecasts/media/Unmanned_Aircraft_Systems.pdf [2] J. C. van Gemert, C. R. Verschoor, P. Mettes, K. Epema, L. P. Koh, and S. Wich, ‘‘Nature conservation drones for automatic localization and counting of animals,’’ in Proc. Workshop Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2014, pp. 255–270. [3] L. Apvrille, T. Tanzi, and J.-L. Dugelay, ‘‘Autonomous drones for assisting rescue services within the context of natural disasters,’’ in Proc. 31st URSI IEEE Gen. Assembly Sci. Symp. (URSI GASS), Aug. 2014, pp. 1–4. [4] P. Doherty and P. Rudol, ‘‘A UAV search and rescue scenario with human body detection and geolocalization,’’ in Proc. Australas. Joint Conf. Artif. Intell. Berlin, Germany: Springer, 2007, pp. 1–13. [5] M. Manimaraboopathy, H. S. Christopher, and S. Vignesh, ‘‘Unmanned ﬁre extinguisher using quadcopter,’’ Int. J. Smart Sens. Intell. Syst., vol. 10, pp. 471–481, Sep. 2017. [6] M. Goodrich and A. Schultz, ‘‘Humanrobot interaction: A survey,’’ Found. Trends Human Comput. Interaction, vol. 1, no. 3, pp. 203–275, 2008. [7] I. Leite, C. Martinho, and A. Paiva, ‘‘Social robots for longterm interaction: A survey,’’ Int. J. Social Robot., vol. 5, no. 2, pp. 291–308, 2013. [8] D. Tezza. (2019). Fundamentals of MultiRotor Drones. Accessed: Aug. 25, 2019. [Online]. Available: https://medium.com/ @dantetezza/fundamentalsofmultirotordrones-979c579ba960?sk= 76a4874a6a5234905fedee6b69a4ab6a [9] D. Gettinger and A. H. Michel, ‘‘Drone registrations, a preliminary analysis,’’ Center Study Drone Bard College, AnnandaleonHudson, NY, USA, Tech. Rep., Nov. 2017. [Online]. Available: http://dronecenter. bard.edu/droneregistration [10] A. Fitzpatrick, ‘‘Drones are here to stay. Get used to it,’’ Time USA, May 31, 2018. [Online]. Available: https://time.com/longform/timethedroneage/ [11] Federal Aviation Administration. (Jan. 2019). Yearly Aircraft Registration Database. [Online]. Available: https://www.faa.gov/licenses_certiﬁcates/ aircraft_certiﬁcation/aircraft_registry/releasable_aircraft_download/ [12] T. Krajník, V. Vonásek, D. Fišer, and J. Faigl, ‘‘ARdrone as a platform for robotic research and education,’’ in Proc. Int. Conf. Res., 2011, pp. 172–186. [13] M. Funk, ‘‘Humandrone interaction: Let’s get ready for ﬂying user interfaces!’’ Interactions, vol. 25, no. 3, pp. 78–81, 2018. [14] M. McFarland, ‘‘Google drones will deliver Chipotle burritos at Virginia Tech,’’ CNN Money, Sep. 2016. [15] Amazon. (2016). Amazon Prime Air. [Online]. Available: https://www. amazon.com/b?node=8037720011 [16] E. Graether and F. Mueller, ‘‘Joggobot: A ﬂying robot as jogging companion,’’ in Proc. ACM CHI Extended Abstr. Hum. Factors Comput. Syst., 2012, pp. 1063–1066. [17] E. Peshkova, M. Hitz, and B. Kaufmann, ‘‘Natural interaction techniques for an unmanned aerial vehicle system,’’ IEEE Pervasive Comput., vol. 16, no. 1, pp. 34–42, Jan./Mar. 2017. [18] J. R. Cauchard, K. Y. Zhai, and J. A. Landay, ‘‘Drone & me: An exploration into natural humandrone interaction,’’ in Proc. ACM Int. Joint Conf. Pervasive Ubiquitous Comput., 2015, pp. 361–365. [19] J. A. Landay, L. E. Jane, L. E. Ilene, and J. R. Cauchard, ‘‘Drone & Wo: Cultural inﬂuences on humandrone interaction techniques,’’ in Proc. ACM CHI Conf. Hum. Factors Comput. Syst., 2017, pp. 6794–6799. [20] P. Abtahi, D. Y. Zhao, L. E. Jane, and J. A. Landay, ‘‘Drone near me: Exploring touchbased humandrone interaction,’’ Proc. ACM Interact., Mobile, Wearable Ubiquitous Technol., vol. 1, no. 3, p. 34, 2017. [21] W. S. Ng and E. Sharlin, ‘‘Collocated interaction with ﬂying robots,’’ in Proc. IEEE ROMAN, Jul./Aug. 2011, pp. 143–149. 
167452 VOLUME 7, 2019 
[22] A. Mashood, H. Noura, I. Jawhar, and N. Mohamed, ‘‘A gesture based kinect for quadrotor control,’’ in Proc. IEEE Int. Conf. Inf. Commun. Technol. Res. (ICTRC), May 2015, pp. 298–301. [23] A. Sanna, F. Lamberti, G. Paravati, and F. Manuri, ‘‘A kinectbased natural interface for quadrotor control,’’ Entertainment Comput., vol. 4, no. 3, pp. 179–186, 2013. [24] M. Chilmonczyk, ‘‘Kinect control of a quadrotor UAV,’’ Univ. South Florida, Tampa, FL, USA, Tech. Rep., Apr. 2014. [25] K. Ikeuchi, T. Otsuka, A. Yoshii, M. Sakamoto, and T. Nakajima, ‘‘KinecDrone: Enhancing somatic sensation to ﬂy in the sky with Kinect and AR.Drone,’’ in Proc. ACM 5th Augmented Hum. Int. Conf., 2014, Art. no. 53. [26] A. Sarkar, K. A. Patel, R. K. G. Ram, and G. K. Capoor, ‘‘Gesture control of drone using a motion controller,’’ in Proc. IEEE Int. Conf. Ind. Inform. Comput. Syst. (CIICS), Mar. 2016. pp. 1–5. [27] R. A. S. Fernández, J. L. SanchezLopez, C. Sampedro, H. Bavle, M. Molina, and P. Campoy, ‘‘Natural user interfaces for humandrone multimodal interaction,’’ in Proc. IEEE Int. Conf. Unmanned Aircr. Syst. (ICUAS), Jun. 2016, pp. 1013–1022. [28] A. Stoica, F. Salvioli, and C. Flowers, ‘‘Remote control of quadrotor teams, using hand gestures,’’ in Proc. ACM/IEEE Int. Conf. Hum.-Robot Interact., Mar. 2014, pp. 296–297. [29] K. Miyoshi, R. Konomura, and K. Hori, ‘‘Above your hand: Direct and natural interaction with aerial robot,’’ in Proc. ACM SIGGRAPH Emerg. Technol., 2014, Art. no. 8. [30] T. Sun, S. Nie, D.-Y. Yeung, and S. Shen, ‘‘Gesturebased piloting of an aerial robot using monocular vision,’’ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), May/Jun. 2017, pp. 5913–5920. [31] J. Nagi, A. Giusti, G. A. Di Caro, and L. M. Gambardella, ‘‘Human control of UAVs using face pose estimates and hand gestures,’’ in Proc. ACM/IEEE Int. Conf. Hum.-Robot Interact., Mar. 2014, pp. 252–253. [32] V. M. Monajjemi, J. Wawerla, R. Vaughan, and G. Mori, ‘‘HRI in the sky: Creating and commanding teams of UAVs with a visionmediated gestural interface,’’ in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), Nov. 2013, pp. 617–623. [33] I. Maza, F. Caballero, R. Molina, N. Peña, and A. Ollero, ‘‘Multimodal interface technologies for UAV ground control stations,’’ in Proc. 2nd Int. Symp. UAVs, Reno, NV, USA. Dordrecht, The Netherlands: Springer, Jun. 2009, pp. 371–391. [34] K. Miyoshi, R. Konomura, and K. Hori, ‘‘Entertainment multirotor robot that realises direct and multimodal interaction,’’ in Proc. 28th Int. BCS Hum. Comput. Interact. Conf. HCI, Sand, Sea SkyHoliday (HCI), 2014, pp. 218–221. [35] K. LaFleur, K. Cassady, A. Doud, K. Shades, E. Rogin, and B. He, ‘‘Quadcopter control in threedimensional space using a noninvasive motor imagerybased braincomputer interface,’’ J. Neural Eng., vol. 10, no. 4, Aug. 2013, Art. no. 046003. [36] F. Tepper. (2016). University of Florida Held the World’s First BrainControlled Drone RaceTechCrunch. Accessed: Aug. 24, 2019. [Online]. Available: https://techcrunch.com/2016/04/25/universityof-ﬂoridaheldtheworlds-ﬁrstbraincontrolleddronerace/ [37] AUVSI News. (2019). University of South Florida’s BrainDrone Race Welcomes Diversity and Inclusivity. Accessed: Aug. 24, 2019. [Online]. Available: https://www.auvsi.org/industrynews/universitysouth-ﬂoridasbraindroneracewelcomesdiversityandinclusivity/ [38] D. Taylor. (2019). UA Students Combine Computer Science, the Brain to Fly Drones.-Tuscaloosa News. Accessed: Aug. 24, 2019. [Online]. Available: https://www.tuscaloosanews.com/news/20190405/uastudentscombinecomputersciencebrainto-ﬂydrones [39] A. Nourmohammadi, M. Jafari, and T. O. Zander, ‘‘A survey on unmanned aerial vehicle remote control using brain–computer interface,’’ IEEE Trans. HumanMach. Syst., vol. 48, no. 4, pp. 337–348, Aug. 2018. [40] N. Kos’myna, F. TarpinBernard, and B. Rivet, ‘‘Bidirectional feedback in motor imagery BCIs: Learn to control a drone within 5 minutes,’’ in Proc. ACM CHI Extended Abstracts Hum. Factors Comput. Syst., 2014, pp. 479–482. [41] N. Kosmyna, F. TarpinBernard, and B. Rivet, ‘‘Drone, your brain, ring course: Accept the challenge and prevail!’’ in Proc. ACM Int Joint Conf Pervasive Ubiquitous Comput., Adjunct Publication, 2014, pp. 243–246. [42] N. KosMyna, F. TarpinBernard, and B. Rivet, ‘‘Towards brain computer interfaces for recreational activities: Piloting a drone,’’ in HumanComputer Interaction—INTERACT. Berlin, Germany: SpringerVerlag, 2015, pp. 506–925. 
[43] B. H. Kim, M. Kim, and S. Jo, ‘‘Quadcopter ﬂight control using a lowcost hybrid interface with EEGbased classiﬁcation and eye tracking,’’ Comput. Biol. Med., vol. 51, pp. 82–92, Aug. 2014. [44] T. Shi, H. Wang, and C. Zhang, ‘‘Brain Computer Interface system based on indoor semiautonomous navigation and motor imagery for Unmanned Aerial Vehicle control,’’ Expert Syst. Appl., vol. 42, no. 9, pp. 4196–4206, 2015. [45] J. F. B. Coenen, ‘‘UAV BCI Comparison of manual and pedal control systems for 2D ﬂight performance of users with simultaneous BCI control,’’ Ph.D. dissertation, Fac. Social Sci., Radboud Univ., Nijmegen, The Netherlands, Jun. 2015. [46] M. J. Khan, K.-S. Hong, N. Naseer, and M. R. Bhutta, ‘‘Hybrid EEGNIRS based BCI for quadcopter control,’’ in Proc. 54th Annu. Conf. Soc. Instrum. Control Eng. Jpn. (SICE), Hangzhou, China, 2015, pp. 1177–1182. [47] J.-S. Lin and Z.-Y. Jiang, ‘‘Implementing remote presence using quadcopter control by a noninvasive BCI device,’’ Comput. Sci. Inf. Technol., vol. 3, no. 4, pp. 122–126, 2015. [48] D. Zhang and M. M. Jackson, ‘‘Quadcopter navigation using Google Glass and braincomputer interface,’’ M.S. thesis, Georgia Inst. Technol., Atlanta, GA, USA, 2015. [49] D. Tezza, S. Garcia, T. Hossain, and M. Andujar, ‘‘Brain eRacing: An exploratory study on virtual braincontrolled drones,’’ in Virtual, Augmented and Mixed Reality. Applications and Case Studies (Lecture Notes in Computer Science), vol. 11575, J. Chen and G. Fragomeni, Eds. Cham, Switzerland: Springer, 2019, pp. 150–162. [50] G. Jones, N. Berthouze, R. Bielski, and S. Julier, ‘‘Towards a situated, multimodal interface for multiple UAV control,’’ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), May 2010, pp. 1739–1744. [51] J. P. Hansen, A. Alapetite, I. S. MacKenzie, and E. Møllenbach, ‘‘The use of gaze to control drones,’’ in Proc. ACM Symp. Eye Tracking Res. Appl., 2014, pp. 27–34. [52] K. Higuchi and J. Rekimoto, ‘‘Flying head: A head motion synchronization mechanism for unmanned aerial vehicle control,’’ in Proc. ACM CHI Extended Abstracts Hum. Factors Comput. Syst., 2013, pp. 2029–2038. [53] K. Higuchi, K. Fujii, and J. Rekimoto, ‘‘Flying head: A headsynchronization mechanism for ﬂying telepresence,’’ in Proc. IEEE 23rd Int. Conf. Artif. Reality TeleExistence (ICAT), 2013, pp. 28–34. [54] H. Hayakawa, C. L. Fernando, M. H. D. Saraiji, K. Minamizawa, and S. Tachi, ‘‘Telexistence drone: Design of a ﬂight telexistence system for immersive aerial sports experience,’’ in Proc. ACM 6th Augmented Hum. Int. Conf., 2015, pp. 171–172. [55] N. Li, S. Cartwright, A. S. Nittala, E. Sharlin, and M. C. Sousa, ‘‘Flying frustum: A spatial interface for enhancing humanUAV awareness,’’ in Proc. ACM 3rd Int. Conf. Hum.-Agent Interact., 2015, pp. 27–31. [56] A. Yeh, P. Ratsamee, K. Kiyokawa, Y. Uranishi, T. Mashita, H. Takemura, M. Fjeld, and M. Obaid, ‘‘Exploring proxemics for humandrone interaction,’’ in Proc. ACM 5th Int. Conf. Hum. Agent Interact., 2017, pp. 81–88. [57] B. A. Duncan and R. R. Murphy, ‘‘Comfortable approach distance with small unmanned aerial vehicles,’’ in Proc. IEEE ROMAN, Aug. 2013, pp. 786–792. [58] W. Jensen, S. Hansen, and H. Knoche, ‘‘Knowing you, seeing me: Investigating user preferences in dronehuman acknowledgement,’’ in Proc. ACM CHI Conf. Hum. Factors Comput. Syst., 2018, Art. no. 365. [59] D. Szaﬁr, B. Mutlu, and T. Fong, ‘‘Communication of intent in assistive free ﬂyers,’’ in Proc. ACM/IEEE Int. Conf. Hum.-Robot Interact., Mar. 2014, pp. 358–365. [60] D. Szaﬁr, B. Mutlu, and T. Fong, ‘‘Communicating directionality in ﬂying robots,’’ in Proc. 10th Annu. ACM/IEEE Int. Conf. Hum.-Robot Interact., 2015, pp. 19–26. [61] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, ‘‘Unmanned aerial vehicle with underlaid devicetodevice communications: Performance and tradeoffs,’’ IEEE Trans. Wireless Commun., vol. 15, no. 6, pp. 3949–3963, Jun. 2016. [62] M. Mozaffari, A. T. Z. Kasgari, W. Saad, M. Bennis, and M. Debbah, ‘‘Beyond 5G with UAVs: Foundations of a 3D wireless cellular network,’’ IEEE Trans. Wireless Commun., vol. 18, no. 1, pp. 357–372, Jan. 2019. [63] A. Sanjab, W. Saad, and T. Başar, ‘‘Prospect theory for enhanced cyberphysical security of drone delivery systems: A network interdiction game,’’ in Proc. IEEE Int. Conf. Commun. (ICC), Paris, France, May 2017, pp. 1–6. [64] C. Hieida, H. Matsuda, S. Kudoh, and T. Suehiro, ‘‘Action elements of emotional body expressions for ﬂying robots,’’ in Proc. 11th ACM/IEEE Int. Conf. Hum.-Robot Interact. (HRI), Mar. 2016, pp. 439–440. 
VOLUME 7, 2019 167453 
D. Tezza, M. Andujar: StateoftheArt of Human–Drone Interaction 
[65] J. R. Cauchard, K. Y. Zhai, M. Spadafora, and J. A. Landay, ‘‘Emotion encoding in HumanDrone interaction,’’ in Proc. 11th ACM/IEEE Int. Conf. Hum.-Robot Interact. (HRI), Mar. 2016, pp. 263–270. [66] D. Arroyo, C. Lucho, S. J. Roncal, and F. Cuellar, ‘‘Daedalus: A sUAV for humanrobot interaction,’’ in Proc. ACM/IEEE Int. Conf. Hum.-Robot Interact., Mar. 2014, pp. 116–117. [67] H. Nozaki, ‘‘Flying display: A movable display pairing projector and screen in the air,’’ in Proc. CHI Extended Abstr. Hum. Factors Comput. Syst., 2014, pp. 909–914. [68] S. Schneegass, F. Alt, J. Scheible, and A. Schmidt, ‘‘Midair displays: Concept and ﬁrst experiences with free-ﬂoating pervasive displays,’’ in Proc. ACM Int. Symp. Pervasive Displays, 2014, Art. no. 27. [69] S. Schneegass, F. Alt, J. Scheible, A. Schmidt, and H. Su, ‘‘Midair displays: Exploring the concept of free-ﬂoating public displays,’’ in Proc. CHI Extended Abstracts Hum. Factors Comput. Syst., 2014, pp. 2035–2040. [70] J. Scheible, A. Hoth, J. Saal, and H. Su, ‘‘DisplayDrone: A ﬂying robot based interactive display,’’ in Proc. 2nd ACM Int. Symp. Pervas. Displays, 2013, pp. 49–54. [71] J. Scheible and M. Funk, ‘‘Insitudisplaydrone: Facilitating colocated interactive experiences via a ﬂying screen,’’ in Proc. 5th ACM Int. Symp. Pervas. Displays, 2016, pp. 251–252. [72] F. Mueller and M. Muirhead, ‘‘Understanding the design of a ﬂying jogging companion,’’ in Proc. 27th Annu. ACM Symp. User Interface Softw. Technol., 2014, pp. 81–82. [73] M. Avila, M. Funk, and N. Henze, ‘‘DroneNavigator: Using drones for navigating visually impaired persons,’’ in Proc. 17th Int. ACM SIGACCESS Conf. Comput. Accessibility, 2015, pp. 327–328. [74] M. Obaid, O. Mubin, C. A. Basedow, A. A. Ünlüer, M. J. Bergström, and M. Fjeld, ‘‘A drone agent to support a clean environment,’’ in Proc. ACM 3rd Int. Conf. Hum.-Agent Interact., 2015, pp. 55–61. [75] J. Scheible and M. Funk, ‘‘DroneLandArt: Landscape as organic pervasive display,’’ in Proc. 5th ACM Int. Symp. Pervasive Displays, 2016, pp. 255–256. [76] K. Nitta, K. Higuchi, and J. Rekimoto, ‘‘HoverBall: Augmented sports with a ﬂying ball,’’ in Proc. ACM 5th Augmented Hum. Int. Conf., 2014, Art. no. 13. [77] P. Knierim, T. Kosch, V. Schwind, M. Funk, F. Kiss, S. Schneegass, and N. Henze, ‘‘Tactile drones—Providing immersive tactile feedback in virtual reality through quadcopters,’’ in Proc. ACM CHI Conf. Extended Abstracts Hum. Factors Comput. Syst., 2017, pp. 433–436. [78] K. Yamaguchi, G. Kato, Y. Kuroda, K. Kiyokawa, and H. Takemura, ‘‘A nongrounded and encounteredtype haptic display using a drone,’’ in Proc. ACM Symp. Spatial User Interact., 2016, pp. 43–46. [79] K. Higuchi, T. Shimada, and J. Rekimoto, ‘‘Flying sports assistant: External visual imagery representation for sports training,’’ in Proc. ACM 2nd Augmented Hum. Int. Conf., 2011, Art. no. 7. [80] D. He, H. Ren, W. Hua, G. Pan, S. Li, and Z. Wu, ‘‘FlyingBuddy: Augment human mobility and perceptibility,’’ in Proc. ACM 13th Int. Conf. Ubiquitous Comput., 2011, pp. 615–616. 
DANTE TEZZA received the B.Sc. degree in computer engineering and the M.Sc. degree in software engineering from St. Mary’s University, San Antonio, TX, USA. He is currently pursuing the Ph.D. degree with the Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA. He was with industry developing drone systems. He is currently a Research Assistant with the NeuroMachine Interaction Laboratory, under the supervision of Dr. M. Andujar. His current research interests include drone technologies and human–drone interaction. In addition, during his M.Sc. program, he developed an Innovative Software Architecture that facilitates drone usage in STEM Education and Research. 
MARVIN ANDUJAR received the Ph.D. degree in humancentered computing from the University of Florida, Gainesville, FL, USA. He is currently an Assistant Professor with the Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA. His current research interests include affective brain–computer interfaces where he focuses on measuring and decoding the user’s affective state from the brain during human–machine interaction. He was recognized as a National Science Foundation Graduate Research Fellow, a GEM Fellow, a Generation’s Google Scholar, and an Intel Scholar. His effort on brain–computer interfaces has led toward multiple publications in journals and conferences, obtain external funding of $300 000 from the CEO of Intel along with his colleagues, and cofounded the world’s ﬁrst BrainDrone Race. The race was showcased in more than 550 news outlets, including The New York Times, Associated Press, Discovery Channel, The Verge, and Engadget. 
167454 VOLUME 7, 2019 
",The State-of-the-Art of Human–Drone Interaction  A Survey.pdf,16
"Received: 27 September 2020 | Revised: 6 January 2021 | Accepted: 25 January 2021 
DOI: 10.1002/int.22380 
R E S E A R C H A R T I C L E 
Adversarial retraining attack of asynchronous advantage actor‐critic based pathfinding 
Chen Tong1 | Liu Jiqiang1 | Xiang Yingxiao1 | Niu Wenjia1 | Tong Endong1 | Wang Shuoru1 | 
Li He1 | Chang Liang2 | Li Gang3 | Chen Qi Alfred4 
1Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, Beijing, China 
2Guangxi Key Laboratory of Trusted Software, Guilin University of Electronic Technology, Guilin, China 
3School of Information Technology, Deakin University, Geelong, Australia 
4Donald Bren School of Information and Computer Sciences (ICS), University of California, Irvine, California, USA 
Correspondence Niu Wenjia and Tong Endong, Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, 3 Shangyuan Village, Haidian District, 100044 Beijing, China. Email: niuwj@bjtu.edu.cn (N. W.), edtong@bjtu.edu.cn (T. E.) 
Funding information 
The National Natural Science Foundation of China, Grant/Award Numbers: 61972025, 61802389, 61672092, U1811264, 61966009; National Key R&D Program of China, Grant/Award Numbers: 2020YFB1005604, 2020YFB2103800; Fundamental Research Funds for the Central Universities of China, Grant/Award Numbers: 2018JBZ103, 2019RC008; Science and Technology on Information Assurance Laboratory, Guangxi Key Laboratory of Trusted Software, Grant/Award Number: KX201902 
Abstract Pathfinding becomes an important component in 
many real‐world scenarios, such as popular warehouse systems and autonomous aircraft towing ve- 
hicles. With the development of reinforcement 
learning (RL) especially in the context of asynchronous advantage actor‐critic (A3C), pathfinding is 
undergoing a revolution in terms of efficient parallel learning. Similar to other artificial intelligence‐based 
applications, A3C‐based pathfinding is also threatened by the adversarial attack. In this paper, we are 
the first to study the adversarial attack to A3C, that 
can unexpectedly wake up longtime retraining mechanism until successful pathfinding. We also dis- 
cover an attack example generation to launch the attack based on gradient band, in which only one 
baffle of extremely few unit lengths can successfully 
perform the attack. Experiments with detailed analysis are conducted to show a high attack success rate 
of 95% with an average baffle length of 2.95. We also 
K E Y W O R D S 
A3C, evasion attack, pathfinding, reinforcement learning, retraining attack 
1 | INTRODUCTION 
Pathfinding, also called pathing, is responsible for finding the path from the start location to the destination in a given map. It is increasingly becoming a vital component in many real‐world applications, such as famous Kiva (Amazon Robotics) warehouse systems,1 and some autonomous aircraft towing vehicles.2 In the pathfinding research family, multiagent pathfinding3 
and any‐angle pathfinding4 are attracting considerable research interests from artificial intelligence (AI), robotics, theoretical computer science, and operations research. Recently, pathfinding is moving into a new direction of unknown environment‐oriented pathfinding, with the push of reinforcement learning (RL), that learns an optimal policy for agents through exhausting trials of action exploration in the unknown environment to maximize cumulative rewards. With the development of deep learning, the RL also develops new algorithms such as Q‐learning‐based5 deep Q network,6 deep deterministic policy gradient,7 and asynchronous advantage actor‐critic (A3C).8 Due to the special parallel learning framework of A3C, A3C becomes the most promising RL in learning efficiency in a large environment. Instead of single‐ agent learning, A3C‐based pathfinding asynchronously executes multiple agents in parallel to interact with the environment. Such parallelism makes it possible to make fast cooperative learning and push A3C‐based pathfinding into broader real‐world scenarios. A3C has great potential to promote pathfinding. However, as an RL of machine learning in AI, the security of A3C whether it also opens a new door for cyber attacks is still an open issue. Recently, as described in Berkley's report,9 several AI security vulnerabilities have been exposed and exploited by the adversarial attack10. For instance, a special stop sign can deceive to output a recognition of the speed limit of 60 miles,11 and a designed panda image can have a gibbon recognition with a great confidence.12 Many studies have revealed adversarial attacks on RL, for example, poisoning attack13 and evasion attack.14 These studies highlight the RL security challenges. However, to the best of our knowledge, the former work focuses on the vulnerability analysis on nonparallel RL learning and overlook such a security issue of emerging RL mechanism. Thus, it is highly important to understand its potential security vulnerabilities of A3C parallel learning so that they can be proactively addressed before widespread deployment. In this study, we perform the first security analysis on the emerging A3C parallel learning in real pathfinding. Our analysis focuses on the adversarial evasion attack on the retraining mechanism of RL. RL retraining actually has a nonstop connection between training process and testing process, while the retraining of supervised learning has to manually operate according to testing results. Our analysis results are expected to serve as a guideline for understanding whether and why the current implementation of A3C‐based pathfinding is essentially vulnerable, as well as revealing and verifying it through high‐success vulnerability exploitation. The attack requirement in our study is limited that only one baffle of extremely few unit 
2324 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
FIGURE 1 An example to show a success of evasion attack by setting an attack baffle 2 in A3C usage mode. (a) Pathfinding of A3C training mode through three agents' parallel learning; (b) pathfinding of A3C usage mode when encountering evasion attack of baffle 1 and baffle 2, respectively. A3C, asynchronous advantage actor‐critic [Color figure can be viewed at wileyonlinelibrary.com] 
TONG ET AL. | 2325 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
1. We give the very first attempt to discover the adversarial evasion attack to the vulnerability of retraining trigger of parallel A3C learning in pathfinding. This is a novel adversarial attack to parallel RL in the specific application. 2. We propose a gradient ascend‐ and exhaustive‐based approach to launch a retraining attack based on our vulnerability analysis. 3. We evaluate our approach empirically in real pathfinding application. Based on massive processing on four initial data sets of 100 samples, we deep further reveal and verify the retraining attack involving evaluating attack success rate, attack cost in the context of different agent numbers and retraining trigger threshold, and others. 
The remainder of this paper is organized as follows. Section 2 introduces the preliminaries. Section 3 describes the retraining attack analysis. We propose corresponding exploit construction in Section 4. Experiments and detailed analysis are reported in Section 5. Section 6 gives defense suggestions. Section 7 discusses the related works. Finally, we conclude the paper in Section 8. 
2 | PRELIMINARY 
2.1 | A3C value functions and advantage function 
The final goal of the A3C algorithm is to learn a policy π, whose input is the observation of agents (also called actors) represented as a state a, and output is a vector of action probability for any state. Then, the probability to choose at at t time, can use the policy 
∣ ∣ π a s p a s ( ) = ( ). t t t t (1) 
From task beginning to the end, agent will form a trajectory τ s a s a s a = { , , , , …, , } T T 1 1 2 2 . Then we denote τi as the ith trajectory. State‐action value function. Given a state s and action a, state‐action value function Q ( ,s a ) π 
is to output an expected cumulated reward (total reward) based on sampled trajectories. 
 ⋯ ∣ Q s a r γr γ r s a ( , ) = ( + + + , ) π t t t t t t t +1 +2 2 +3 . Further, we can denote it as a simple formula: 
 ∣ ∈ ( ) Q s a γ r s a t t t T ( , ) = , , ′ { , + 1, …, } π t t t t nt t t ′− ′+1 , in which γ∈ [0, 1] is a discount factor of 
reward according to Markov decision process (MDP).15 For simplicity, 
∑∑∑ ≈ (∣ ) Q s a N γ r p a s ( , ) 1 ( ) . π t t n 
N 
t 
T 
t t 
T t t t nt t n 
=1 =1 ′= 
′− ′+1 
n 
(2) 
State value function. According to Bellman equation16 (dynamic programming equation), we have the state value function 
2326 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
t 
(3) 
Also,  Q s a r V s ( , ) = [ + ( )] π t t t π t+1 . Advantage function. To learn a good policy πθ, it is natural to update the θ to maximize the expected total reward of all sampled trajectories, namely ∑ R R τ p ¯ = ( ) θ θ τ τ( ), thus a gradient 
ascend can be used ← ∇ θ θ +η R ¯θ. In A3C, the advantage function is defined as: 
r V s V s + ( ) − ( ) nt π t n π nt +1 , and the advantage function‐based gradient is 
∑∑ ∇ ≈ ∇ ( ) ( ) ( ) R N r V s V s p a s ¯ 1 + ( ) − log . θ θ n 
N 
t 
T 
nt π t n π nt nt t n 
=1 =1 +1 
n 
(4) 
2.2 | Space of state and action in pathfinding 
State space and action space are highly important for A3C working in pathfinding. State space. SNM is a state matrix for pathfinding environment of a N ×M grid map, in which sij is a vector of two tuples: ⟨ ⟩ ∈ avail s coordinate s avail ( ), ( ) , {0, 1} ij ij , and the state 
coordinate is denoted by i j ( , ). The unit length can be defined as the distance between i j ( , ) 
and i j ( ± 1, ), or the one between i j ( , ) and i j ( , ± 1). 
⎡ 
⎣ 
⎢⎢ ⎢ 
⎤ 
⎦ 
⎥⎥ ⎥ 
⋯ ⋯ ⋮ ⋮ ⋱ ⋮ ⋯ 
S 
s s s s s s 
s s s 
= . NM 
M 
M 
N N N M 
00 01 0( −1) 
10 11 1( −1) 
( −1)0 ( −1)1 ( −1)( −1) 
(5) 
If there is an obstacle in sij, the avail is 1, otherwise avail = 0, thus the total state space has a size calculated as follows. 
∑ T avail s = ( ). s n m 
N M 
ij =0, =0 
( −1)( −1) 
(6) 
Action space. For pathfinding in our work, we limit four moving actions for any state: 
a s∈ a a a a ( ) { , , , } 1 2 3 4 , a1 = up, a2 = down, a3 = left, and a4 = right. Thus, at t time, the state 
s t=s ij, and the action a =a s ( ) t t . 
3 | RETRAINING ATTACK ANALYSIS 
3.1 | Target multiagent A3C parallel learning 
A3C utilizes a hierarchical actor‐critic framework and enables asynchronous parallel training. Concretely, at the bottom layer, there are multiple agents in parallel to interact with the environment, and each agent has a pair of actor network and critic network, which is also called local actor‐critic network; all agents further connect to a global actor‐critic network at an upper layer. For simplicity, we still employ a common two‐layer structure to discuss in the paper. As Figure 2 shows, each agenti copies the global parameters θ before learning, then agenti interacts with the environmenti for sampling diverse data. Each agenti will compute the 
TONG ET AL. | 2327 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
3.2 | Retraining trigger for switching usage mode to training mode 
For computational resource saving, in A3C‐based pathfinding implementation, there are two modes to switch: training mode and usage mode. Training mode. The training mode is used for efficient parallel training from the different start locations to the same destination. As Figure 2 shows, a complex network is utilized for iterative collaborative training by multiple agents. In simplicity, we assume that the training converges condition is a fixed large constant (e.g., 10,000) of accumulated training episode number. In RL, the term “episode” is always used to represent a time interval (length) that contains each subsequence of agent–environment interactions between initial and terminal states. Usage mode. The A3C usage mode is started when a converged A3C training is achieved. In such mode, the learned policy can be downloaded into any new joined agent, and any 
FIGURE 2 The asynchronous advantage actor‐critic framework with asynchronous parallel training [Color figure can be viewed at wileyonlinelibrary.com] 
2328 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
3.3 | Vulnerability to evasion attack 
Amid a N ×M finite state space with ∣ a s( )∣ action space, for each state s∈ N ×M t , there is a vector of action probability generated by policy ∣ ∈ π a s a a s ( ), ( ) i t i t , that is π s( ) = t ∣ ∣ ∣ ∣ ∣ ( ( )) π a s π a s π a s ( ), ( ), …, t t a s t 1 2 ( ) t . We use σπ s( ) t to represent the unbiased SD of action probability vector π s( ) t at state st. We have 
∑ ∣ ∣ ∣ ∣ ∣ σ π s p a s μ π s ( ( )) = ( ( ) − ( ( ))) t a s k a s k t t ( )1 −1 =1 ( ) 2 
t 
t , in which ≈ ∑ ∣ ∣ ∣ ∣ ∣ μ π s p a s ( ( )) ( ) t a s k a s k t 1 ( ) =1 ( ) 
t 
t 
Then the t time matrix of action probability deviation can be constructed as follows: 
⎡ 
⎣ 
⎢⎢⎢ ⎢ 
⎤ 
⎦ 
⎥⎥⎥ ⎥ 
⋯ ⋯ ⋮ ⋮ ⋱ ⋮ ⋯ 
M 
σ π s σ π s σ π s σ π s σ π s σ π s 
σ π s σ π s σ π s 
= 
( ( )) ( ( )) ( ( )) ( ( )) ( ( )) ( ( )) 
( ( )) ( ( )) ( ( )) 
. t 
t t t M 
t t t M 
t N t N t N M 
00 01 0( −1) 
10 11 1( −1) 
( −1)0 ( −1)1 ( −1)( −1) 
(7) 
In Mt, if σ π s ( t( )) ij has a big value, then we believe that corresponding state sij is vulnerable to evasion attack, through set a new baffle to probably trigger unexpected retraining. This is because the action exploration is dependent on the action probability deviation through using the function ∣ numpy random choice a s π a s . . ( ( ), ( )) i . High σ π s ( t( )) ij means an action exploration with high probability of single action choosing as an over confidence. 
4 | RETRAINING ATTACK CONSTRUCTION 
Our approach to construct retraining attack is outlined in Figure 3. We can see that our approach primarily consists of three steps, that are capturing value table, producing gradient band, and baffle generation. First, we need the whole value table of A3C after converged training, then a gradient band can be fixed as a boundary constraint of baffle generation. At the last step, based on real feedback and continuous iterations, we utilize the exhaustive method to properly determine the single baffle's location and length so as to build an attack example of the retraining attack. 
4.1 | Capturing the value table 
The global actor‐critic network does not train itself but manages accumulated updates and then cooperates with the local actor‐critic networks for improving training efficiency and maintaining A3C training stability. 
TONG ET AL. | 2329 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
⎡ 
⎣ 
⎢⎢⎢ ⎢ 
⎤ 
⎦ 
⎥⎥⎥ ⎥ 
⋯ ⋯ ⋮ ⋮ ⋱ ⋮ ⋯ 
V 
V s V s V s V s V s V s 
V s V s V s 
= 
( ) ( ) ( ) ( ) ( ) ( ) 
( ) ( ) ( ) 
. 
π π π M 
π π π M 
π N π N π N M 
00 01 0( −1) 
10 11 1( −1) 
( −1)0 ( −1)1 ( −1)( −1) 
(8) 
V can be visualized, as both the contour map and three‐dimensional surface. As shown in Figure 4, the first subgraph is the original map with a start point and a destination for pathfinding. After A3C‐based pathfinding, for the V we obtained, the corresponding contour map and the three‐dimensional surface can be used for revealing value distribution. In the contour map, the red line denotes the fastest way of value increment from the start position. In other words, from the start position to the destination, if we follow the gradient direction and fit each 
FIGURE 3 Architecture for the gradient band‐based retraining attack model. A3C, asynchronous advantage actor‐critic [Color figure can be viewed at wileyonlinelibrary.com] 
FIGURE 4 A visualization of the value table V in a well‐trained asynchronous advantage actor‐critic pathfinding [Color figure can be viewed at wileyonlinelibrary.com] 
2330 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
4.2 | Fitting the value ascent function 
We capture L optimal trajectories τ τ τ τ = { , , …, L} 1 2 from the start position to destination of A3C model after the training converged. Extracting states from τ and constructing a trajectory state collection S S S S = { , , …, L} 1 2 . Thus, we can get the frequency states set from τ, which can be represented as: 
⎩⎨⎧ ⎣⎢⎢ ⎦⎥⎥ ⎭⎬⎫ ∑ ≥ ≤ ≤ ≥ S s G s L k L or V s avg V ′ = ( ) 32 (1 ), ( ) ( ) , ij k ij ij (9) 
in which G ks( ) = 1 ij if and only if s∈ S ij k, otherwise G ks( ) = 0 ij ; V s( ) ij is the value of sij 
obtained from value table V , avg V ( ) means the average value of V . Next we construct our training sample as ∈ ∈ x i s S y j s S ( = ( ′), = ( ′)) ij ij , and the training set size is S| ′|. For simplicity, we use the least square (LS) method17 to fit the value ascent 
function. In LS, we set a linear composition of p primary functions of power functions for one input x, thus we have Ax =y as follows. 
⎡ 
⎣ 
⎢⎢⎢⎢ ⎢ 
⎤ 
⎦ 
⎥⎥⎥⎥ ⎥ 
⎡ 
⎣ 
⎢⎢⎢ ⎢ 
⎤ 
⎦ 
⎥⎥⎥ ⎥ 
⎡ 
⎣ 
⎢⎢⎢ ⎢ 
⎤ 
⎦ 
⎥⎥⎥ ⎥ 
⋯ 
⋯ ⋮ ⋮ ⋮ ⋱ ⋮ ⋯ 
⋮ ⋮ 
∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 
x x x 
x x x 
x x x 
a a 
a 
y y 
y 
1 
1 
1 
= , 
p 
p 
S S pS p S 
1 21 1 
2 22 2 
′ 2′ ′ 
0 
1 
1 
2 
′ 
(10) 
in which ∣ ∣ a a a y y y x y = [ , , …, ] , = [ , , …, ] p T S 
T 0 1 1 2 ′ . if (A A ) T −1 exists, we can get the fitted parameters as follows: 
A A A x y = ( ) . T −1T (11) 
For p setting, we can determine through comparing the loss of different p by formula (12) in real training. 
⎣⎡ ⎦⎤ ∑ ⋯ 
∣ ∣ ( ) Q y a a x a x = − + + + . 
i 
S 
i i p i p 2 
=1 
′ 
0 1 2 (12) 
Finally, we have the value ascent function ⋯ f x a a x a x ( ) = + + + p p 0 1 . 
TONG ET AL. | 2331 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
For blocks, we extract their intersection points with all grid lines including vertical lines and horizon ones. These points are grouped into one set E e e e = { , , …, k} 0 1 2 , in which 
e x e y e = ( ( ), ( )) k k k . Then we have the ⋃ E E X Y X Y = {(0, 0), ( , 0), (0, ), ( , )} max max max max 0 . Based on the f function, we can further separate E into two subset EH and EL, and ⋃ E E E = H L. 
∣ ≥ E e y e f x e = { ( ) ( ( ))} H i i i and ∣ E e y e f x e = { ( ) < ( ( ))} L i i i . Next, we calculated two minimum distances from f curve to points of EH 
and EL, respectively. Given any point ei, we assume that there is a point r of f with coordinates x r ( ( ),y r ( )) that has the shortest distance to ei. Thus, we have following equations to get r. 
⎩⎨⎧ T P f x r ( ( )) =y = 0,, r r 
r (13) 
where the tangent vector ( ) T = − , 1 r df x r dx ( ( )) at x r ( ( ),y r ( )) is orthogonal to the vector 
P x r x e y r y e = ( ( ) − ( ), ( ) − ( )) r i i T. After r is determined, we can get the shortest distance between f and ei of EH as follows. 
d e f x r x e y r y e ( , ) = ( ( ) − ( )) + ( ( ) − ( )) . i i i 2 2 (14) 
As last, we can get the minimum distance from f to the points of EL and EH, respectively by formula (15). 
∣ ∈ ∣ ∈ D min d e f e E D min d e f e E = { ( , ) }, = { ( , ) }. L i i L H j j H (15) 
As Figure 5 shows, based on the f (see the red curve), through checking the value of DL and DH, we can determine three cases and produce corresponding bands that we call them as gradient bands (see the green band), which cover most optimal pathfindings for converged A3C model. The detailed producing algorithm is presented as follows (Algorithm 1). 
FIGURE 5 The gradient bands towards three different cases [Color figure can be viewed at 
wileyonlinelibrary.com] 
2332 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Input: f D D , H, L taking the value ascent functionf , and the minimum distance fromf to the points of EH and EL, which are DH, and DLas input Output: Bgb//an area with boundaries of fL and fH 
2.1. //calculating the angle for shift direction ∣ ∣ ∈ ∘ ∘ ψ arctan ψ = , [0 , 90 ) df x r dx ( ( )) 
3. //Case I: ≠ ≠ D 0,D 0 L H 
4. if D≠ 0 L && ≠ D 0 H then 5. f f x D sinψ = ( + × ) L L 
6. f f x D sinψ = ( − × ) H H 
7. end if 8. //Case II: ≠ D = 0,D 0 L H 
9. if D = 0 L && ≠ D 0 H then 10. f =f L 
11. f f x D sinψ = ( − × ) H H 
12. end if 13. //Case III: D≠ 0,D = 0 L H 
14. if D≠ 0 L && D H= 0 then 15. f f x D sinψ = ( + × ) L L 
16. f =f H 
17. end if 18. return ≤ ≤ B x y f x y f x = {( , ), ( ) ( )} gb i i L i i H i 
We can see that through ψ calculation, we can determine the shift of f , and achieve shifted 
fH and fL to form an area of band. 
4.4 | Attack example generation 
Given the gradient band Bgb in a N ×M map, we can obtain an original candidate set 
CA a a a = { , , …, } M 1 2 −2 of attack examples to trigger retraining. For simplicity, such examples can be set as horizontal baffles with Bgb discretely along vertical Y axis of grid, and we have CA M | | = − 2, if we ignore the start and the destination of pathfinding. Thus, a baffle of attack 
example ai is denoted by a horizontal segment a i=AB , in which ⌈ ⌉ x A y A f y y ( ( ), ( )) = ( ( ) , ) L i i −1 
and ⌊ ⌋ x B y B f y y ( ( ), ( )) = ( ( ) , ) H i i −1 . We use f −1 to represent the inverse function of f , ⌈ ⌉ to 
represent the top integral function and ⌊ ⌋ to represent the low integral function. The length of baffle ai can be calculated as follows. 
⎡⎢ ⎤⎥ ⎢⎣ ⎥⎦ ∣ ∣ ∣ ∣ a x A x B f y f y = ( ) − ( ) = ( ) − ( ) . i L i H i −1 −1 (16) 
Next, we will design an automatic algorithm (See Algorithm 2) to discover the optimal retraining attack among CA set based on Definition 1 and Definition 2. 
TONG ET AL. | 2333 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Input: CA a a a = { , , …, } M 1 2 −2 //taking the candidate set for attack examples as input Output: a*//the optimal attack example 1. //initializing the optimal attack example 2. a null * = 
3. //traversing all candidate attack examples in CA 4. for i∈ {1, 2, …,M − 2} then 5. //initializing the baffle's left end 6. ⌈ ⌉ x A f y ( ) = ( ) L i −1 
7. repeat 8. //initializing the baffle's right end 9. x B ( ) =x A ( ) + 1 
10. repeat 11. //determining the attack exampleaiand the 12.coordinates for ai's left and right end a =AB i 
13. x A y A x A y ( ( ), ( )) = ( ( ), ) i 
14. x B y B x B y ( ( ), ( )) = ( ( ), ) i 
15. //calculating Feffect, Fcost, and F based on attack example ai 
16. 
⋅ 
⋅ F a effect( ) = i T K T 
K T 
retraining+ A C model A C 
model A C 
3 3 
3 
17. ∣ ∣ F a cost( ) = i Na i 
18. F F a βF a = ( ( ) − ( )) effect i cost i 2 
19. if current F is maximum then 20. //recording the current optimal attack exampleai 
21. a * =a i 
22. end if 23. //determining the baffle's right end exhaustively 24. x B ( ) =x B ( ) + 1 
25. until ⌊ ⌋ x B f y ( ) > ( ) ) H i −1 //finish until baffle's right end shift out the upper boundary 26. //determining the baffle's left end exhaustively 27. x A ( ) =x A ( ) + 1 
28. until ⌊ ⌋ x A f y ( ) = ( ) ) H i −1 //finish until baffle's left end shift out the upper boundary 29. end for 30. return a* 
Definition 1 (Attack effect Feffect). In A3C, for baffle ai, F a effect( ) i reflects its attack effect and is calculated by the time ratio of average retraining time of K agents with model‐ based pathfinding time to the pathfinding time, which can be calculated as: 
⋅ 
⋅ ∈ F a T K T 
K T i M ( ) = + , {1, 2, …, − 2}, effect i retraining A C model A C 
model A C 
3 3 
3 (17) 
where Tretraining A C 3 reflects the retraining time to a new converge, Tmodel 
A C 3 is the total time of A3C model‐based pathfinding successfully from start to destination. The unit of T is 
2334 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Definition 2 (Attack cost Fcost). Fcost is the ratio of the length of attack baffle to the gird length N, which can be calculated as: 
∣ ∣ ∈ F a Na i M ( ) = , {1, 2, …, − 2}, cost i i (18) 
where F a 0 < cost( ) < 1 i , the smaller Fcost means a smaller attack cost. 
At last, we can find out the optimal ai attack with maximum attack effect and minimum attack cost by formula (19). 
a F a βF a * = arg max( ( ) − ( )) . 
a effect i cost i 2 
i 
(19) 
The β is responsible for adjusting the influence of Fcost. For implementation, the detailed algorithm to discover optimal a* is shown in Algorithm 2. According to the formula (19), this algorithm tries to explore each discrete baffle of Y axis with different length, evaluating attack effect and cost to discover the optimal baffle as the real attack example to perform. Also, we analysis the time complexity of Algorithm 2. Under the grid map of N ×M , an original candidate set CA can be obtained based on the gradient band produced by Algorithm 1, and CA M | | = − 2. To discover the optimal attack example a*, we traverse all candidate 
attack examples in CA, and exhaust all possible baffle lengths within the gradient band. We 
assume that ⌊ ⌋ ⌈ ⌉∣ ∈ w f y f y i M = max{ ( ) − ( ) {1, 2, …, − 2}} H i L i 
−1 −1 , thus, the time complexity for 
Algorithm 2 is a level of O w⋅ M ( ( − 2)) 2 . We can see that the key factors for optimal attack 
example computation include two parts: the grid map size N ×M , and the gradient band's maximum width w. The smaller the grid map size is, and the more accurate the gradient band production, the time complexity for optimal attack example computation is lower. 
5 | EXPERIMENTAL EVALUATION 
5.1 | Experimental setup 
The platform and experimental environment configuration is shown in Table 1. Grid map data set. According to the grid map size, we build four data sets: 
D D D D , , , 5×5 10×10 15×15 20×20. The number of samples chosen for each D is equal to 20. For randomness, we use a common python function named random.randint to form a random matrix filled with 0 or 1 value, in which 0 means no block exists and 1 means a block exists. Through running 100 times of random randint N N N . (0, 2, ( , )), = 5, 10, 15, 20, we have four initial data sets of 100 samples in D D D D , , , init init init init 5×5 10×10 15×15 20×20, respectively, for further fine filtering. For the pathfinding success and block density, we perform hand filtering on initial data set: (1) filter out those samples in which there are no path from start to the destination; (2) filter out 
TONG ET AL. | 2335 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
1) Attack success rate (ASR). 
ASR is the ratio of the number of successful optimal attack examples to the total number of optimal attack examples, which can be calculated as follows: 
∑ 
∣ ∣ 
∣ ∣ ASR g 
D = , N i D i 
N N 
=1 
× 
N N × 
(20) 
where N ×N is the map size,∣ ∣ DN ×N denotes the samples' number in DN N × , gi can be calculated as follows: 
g sign F a = ( ( *) − 1), i effect (21) 
in which when F > 1 effect a ( *) the g = 1 i , else g = 0 i . 
TABLE 1 Experimental environment configuration 
Platform Experimental environment Environment configuration 
A3C Operating system Ubuntu 16.04.6 LTS 
CPU Intel (R) Core (TM) i7‐9700F CPU @ 3.00 GHz 
RAM 32 GB 
GPU MSI GeForce RTX 2070 VENTUS 
Graphic memory 151MiB 
Software Python 2.7 
2336 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
We use F a effect( ) to measure the attack effect caused by baffle a, and F a cost( ) to measure the attack cost. They have been defined Section 4.4. 
5.2 | Experimental results 
5.2.1 | ASR and average length of optimal baffle 
DTable 2 presents the detailed comparison with average length of optimal baffle among ,N = 5, 10, 15, 20 N ×N under different agent number (2, 3, 4, 8) and ρ = 2, 3, 4, 5. We can see that the maximum ASR reaches 95% when the agent number is 6 and 8, in which the minimal average baffle length is 2.95 in D5×5. The minimal ASR appears in the D15×15 when the agent number is 2 and ρ = 5, and its value is 0.30. Obviously, the lower threshold ρ causes an easier trigger of retraining. With the increment of ρ, the ASR decreases. For a fixed ρ, ASR increases with the increment of agent number. We find that due to more parallel training, the action 
TABLE 2 ASR comparison with average length of optimal baffle among DN ×N under different agent number, ρ 
Grid map 
5 × 5 10 × 10 15 × 15 20 × 20 
Agent number ρ ASR 
Average baffle length ASR 
Average baffle length ASR 
Average baffle length ASR 
Average baffle length 
2 2 0.80 2.75 0.80 4.65 0.75 6.95 0.85 8.15 
3 0.70 2.45 0.75 4.15 0.70 6.75 0.70 7.65 
4 0.60 2.25 0.55 3.25 0.55 5.85 0.60 6.85 
5 0.40 1.85 0.35 2.70 0.30 4.55 0.40 5.65 
4 2 0.85 2.80 0.90 4.70 0.80 7.00 0.90 8.65 
3 0.70 2.50 0.75 4.30 0.75 6.95 0.75 7.95 
4 0.65 2.30 0.60 3.45 0.60 6.10 0.60 7.00 
5 0.45 2.05 0.40 2.85 0.35 4.75 0.40 6.00 
6 2 0.95 2.95 0.95 4.85 0.90 7.25 0.90 8.60 
3 0.75 2.55 0.80 4.35 0.75 6.90 0.80 8.05 
4 0.65 2.35 0.70 3.85 0.60 6.15 0.70 7.50 
5 0.50 2.15 0.50 3.05 0.45 5.00 0.50 6.15 
8 2 0.95 3.05 0.95 5.15 0.95 7.30 0.95 8.85 
3 0.80 2.70 0.85 4.65 0.8 7.10 0.90 8.20 
4 0.70 2.50 0.70 4.00 0.70 6.65 0.75 7.80 
5 0.60 2.20 0.60 3.30 0.50 5.25 0.60 6.75 
TONG ET AL. | 2337 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
5.2.2 | Average attack effect Feffect and attack cost Fcost 
Table 3 shows the detailed comparison of average optimal F (μ F ( *)) with average 
Feffect(μ F ( ) effect ) and Fcost (μ F ( ) cost ) among DN N × , N = 5, 10, 15, 20 under different agent number (2, 4, 6, 8) and β = 5, 10, 15. We can see that the maximum μ F ( *) and μ F ( ) effect reach 73539.93 and 248.83, respectively, when the agent number is 2, β = 10 in D20×20. The minimal μ F ( *) and 
(A) (B) 
(C) (D) 
FIGURE 6 Attack success rate variation with ρ increment [Color figure can be viewed at 
wileyonlinelibrary.com] 
2338 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
β = 15 under D15×15. The minimal μ F ( cost) appears in the D5×5 when agent number is 8 and β = 15. Obviously, the larger grid map will cause an increment of retraining time. With the increment of the grid map size, the Feffect increases. For a fixed grid map, due to the retraining acceleration of multiple agents, thus the increment of agent number, can cause to decrease the Feffect. As Figure 8 shows, the grid map size and agent number have observed influence on Feffect. With the increment of grid map size, the Feffect increases. The Feffect increment of 33.49%, 19.86%, and 14.52% are formed from D5×5 to D10×10, D10×10 to D15×15, D15×15 to D20×20, respectively. We can see that the decrease amount of Feffect when agent numbers from 2 to 4 is larger than the one when agent numbers from 4 to 6 and 6 to 8. The β shows a bigger weight on baffle size in larger grid map, especially for those A3C training based on more agents. This not only ensures a proper attack cost consideration on baffle size, but also does not affect to takes the retraining time as main component of optimal function. 
(A) (B) 
(C) (D) 
FIGURE 7 Distribution of average baffle length under different attack success rate [Color figure can be viewed at wileyonlinelibrary.com] 
TONG ET AL. | 2339 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Grid map 
5 × 5 10 × 10 15 × 15 20 × 20 
Agent number β μ F ( *) μ F ( effect) μF cost ( ) μ F ( *) μ F ( effect) μ F ( cost) μ F ( *) μ F ( effect) μ F ( cost) μ F ( *) μ F ( effect) μ F ( ) cost 
2 5 27957.20 148.86 0.47 47328.52 195.65 0.37 66849.93 237.83 0.36 73236.18 245.78 0.35 
10 31986.32 152.34 0.43 56524.64 196.24 0.34 67909.99 238.45 0.35 73539.93 247.83 0.33 
15 28369.89 151.33 0.39 48966.48 196.11 0.33 60979.63 236.12 0.32 73284.73 246.11 0.33 
4 5 6471.74 74.13 0.48 12949.06 99.49 0.38 17434.37 118.42 0.39 22712.42 128.32 0.37 
10 5493.88 71.65 0.44 14340.25 99.85 0.36 17630.75 119.11 0.35 24481.94 129.44 0.36 
15 6530.86 72.45 0.40 13540.09 98.54 0.33 17366.66 118.39 0.34 21098.55 126.51 0.34 
6 5 2713.03 46.37 0.50 4308.13 61.93 0.40 7268.70 75.04 0.39 18457.60 121.05 0.38 
10 2329.52 46.01 0.46 4681.56 63.71 0.37 7551.82 76.94 0.38 18277.76 122.99 0.35 
15 2232.94 47.69 0.42 4125.98 60.33 0.35 7604.47 77.12 0.35 17251.71 120.61 0.33 
8 5 1295.28 35.45 0.52 2809.93 49.55 0.43 3565.07 56.72 0.41 4599.61 61.23 0.40 
10 1600.61 36.49 0.51 3071.08 51.62 0.41 3472.98 56.11 0.40 4767.40 63.45 0.38 
15 1261.86 35.12 0.49 3033.66 52.10 0.37 3757.27 58.06 0.39 4850.73 64.13 0.36 
2340 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
6 | DEFENSE SUGGESTIONS 
As shown in our study, to proactively address the retraining attack of A3C, this section discusses defense suggestions based on the insights from our analysis. Enabling dynamic trigger threshold of retraining. The results from the experiment show that small threshold ρ(such as ρ = 2, 3) can cause a retraining trigger through adversarial baffle setting, while usually in normal condition without above evasion attack, such small threshold works well for ensuring A3C model's performance through timely A3C model's by retraining parameter updating. However, it is not plausible to set a ρ initially with a big value, which will make it difficult to start the retraining mechanism, causing the lack of robustness towards exception of the running environment. Moreover, such a threshold should be protected as an important hyperparameter for 
FIGURE 8 Comparison of μ F ( ) effect and μ F ( ) cost for different β [Color figure can be viewed at 
wileyonlinelibrary.com] 
FIGURE 9 Value distribution of F and ASR in different DN ×N [Color figure can be viewed at 
wileyonlinelibrary.com] 
TONG ET AL. | 2341 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
7 | RELATED WORK 
7.1 | Adversarial examples in evasion attack 
A lot of studies have investigated the adversarial attack in different scenarios. These studies highlight the security challenges and the severe consequences, in which most efforts can be thought of making almost invisible or inaudible perturbations into original inputs for data spoofing. These inputs are primarily distributed to the types of image and audio. For example, Kurakin et al.11 showed the adversarial image examples can perform deception with 87% success rate from the printer's picture to the real‐world photo. A face image was modified such that it can change the gender prediction, whereas that face's biometric utility remained intact.18,19 Eykholt et al.20 introduced adversarial attacks from physical road signs, which seems no abnormality as we see. For audio, Carlini et al.21 produced adversarial voice commands that people can not hear. Carlini et al.22 constructed targeted adversarial audio examples under any given audio waveform by adding audio noises. Lately, black‐box targeted attacks were performed by simulating transformations caused by playback or recording in the physical world.23,24 Taori et al.25 showed a 89.25% targeted attack similarity in audio with 35% targeted attack success rate. Above adversarial example belongs to an implicit adversarial example with the invisible or inaudible data spoofing. In comparison, a more recent study tried to construct dominant adversarial examples with more obvious perturbations. Elsayed et al.26 directly embedded a obvious whole image as noise into the original image. Apart from image and audio, some recorded network features were also modified to perform adversarial attacks against intrusion detection.27 Our paper is the first study that exposes concrete data spoofing of dominant adversarial example by adding short baffle in pathfinding application. Compared to implicit image or audio attack example, baffle‐based dominant attack example can be easily set, and thus is able to bring practical and severe consequence as well, which should not be overlooked. 
2342 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Before our study, the vulnerability exploits of AI models especially in machine learning subdomain, research to let machine trained to recognize patterns, have been studied a lot. These efforts highlight and reveal valuable vulnerabilities to those famous machine learning models, such as support vector machine,28 deep natural networks (DNNs),29 convolutional neural network,12 recurrent neural network,30 and long short term memory.30 
There are many studies that explore the security problem in reinforcement learning. Behzadan and Munir et al.31 discovered that the self‐driving platooning vehicles can collide with each other when their observation data are manipulated. Drones equipped with RL techniques can be commanded to collide to a crowd or a building.32,33 Everitt et al.33 and Wang et al.34 investigated RL algorithms under corrupted reward signals. Lin et al.35 and Behzadan and Munir36 focused on deep RL which involves DNNs for function approximation. Huang and Zhu et al.32 studied RL under malicious falsification on cost signals and introduced a quantitative framework of attack models to understand the vulnerabilities of RL. Ma et al.37 focused on security threats on batch RL and control where the attacker aims to poison the learned policy. In comparison, we are the first to study RL security on parallel training‐based A3C under the experimental scenario of pathfinding. Moreover, this does not belong to a kind of poisoning attack but an evasion attack, which is much more realistic in a black‐box attack. Compared to the policy attack in related work, we also reveal an attack on retraining, an important mechanism for integrating multiagent A3C training and single‐agent A3C model usage. To our best knowledge, this is the very first work. To summary, as Table 4 shows, the experimental scenarios among state‐of‐the‐art works in the field of adversarial attack mainly including computer vision, audio, intrusion detection, control system, Atari game, tabular certainty‐equivalence, and linear quadratic regulator. And our work gives the very first attempt to reveal the vulnerability of A3C under the experimental scenario of pathfinding. Moreover, in former work on aiming reinforcement learning mostly focuses on poison attack. While the evasion attack developed in this study is much more realistic in the black‐box attack. In addition, our work proposes a kind of dominant adversarial example, which can be set easier, and is able to bring a more practical consequence. In comparison, we give the very first attempt to reveal the vulnerability of the retraining trigger of A3C in the pathfinding. Moreover, we propose a kind of evasion attack, namely retraining attack, which poses a threat to the effectiveness of A3C under the scenario of pathfinding. However, the retraining attack proposed in this paper focuses on the vulnerability of parallel learning A3C in pathfinding only, thus there exist certain limitations on the available application scenarios of this approach. In the future, we plan to develop a more sophisticated attack approach, which mainly focuses on the design vulnerability of the parallel learning algorithm itself. So that our experiment can be implemented on a wider range of application scenarios. 
8 | CONCLUSION 
In this study, we perform the first attempt to discover a retraining attack to the vulnerability of retraining trigger of parallel A3C learning in pathfinding. Targeting such vulnerability exploitation, we propose a gradient ascent‐ and exhaustive‐based approach to launch a retraining attack. We implement an A3C‐based pathfinding application as our experimental environment, and the massive experiments with a different combination of agent number, retraining trigger 
TONG ET AL. | 2343 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Work Target model Experimental scenario Example's type Attack's type 
Mirjalili et al.19 DNN—IntraFace Computer vision—Gender prediction for face image Implicit Evasion attack 
Eykholt et al.20 CNN—LISA&GTSRB Computer vision—Physical road sign recognition Implicit Evasion attack 
Taori et al.25 DRN—Speech recognition system Audio—Audio recognition Implicit Evasion attack 
Elsayed et al.26 DNN—Google Inception Net v3 Computer vision—Image classification Dominant Evasion attack 
Corona et al.27 Intrusion detection system Intrusion detection Dominant Evasion attack 
Huang et al.32 Q‐Learning Control system—Water reservoir control Implicit Poison attack 
Lin et al.35 DQN Atari game Implicit Poison attack 
Ma et al.37 Batch reinforcement learning Tabular certainty‐equivalence ānd Linear quadratic regulator Implicit Poison attack 
Our work A3C Pathfinding Dominant Evasion attack 
Abbreviations: A3C, asynchronous advantage actor‐critic; CNN, convolutional neural network; DDN, deep natural network; DRN, deep recurrent network; DQN, deep Q‐network. 
2344 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
ACKNOWLEDGEMENTS The National Natural Science Foundation of China (61972025, 61802389, 61672092, U1811264, and 61966009), the National Key R&D Program of China (2020YFB1005604, 2020YFB2103800), the Fundamental Research Funds for the Central Universities of China (2018JBZ103 and 2019RC008), Science and Technology on Information Assurance Laboratory, Guangxi Key Laboratory of Trusted Software (KX201902). 
ORCID Chen Tong http://orcid.org/0000-0003-3851-2135 
Liu Jiqiang http://orcid.org/0000-0003-1147-4327 
Xiang Yingxiao http://orcid.org/0000-0002-8679-7000 
Niu Wenjia http://orcid.org/0000-0003-4706-4266 
Tong Endong http://orcid.org/0000-0003-0348-2108 
Wang Shuoru http://orcid.org/0000-0002-8003-6745 
Li He http://orcid.org/0000-0002-4073-5816 
Chang Liang http://orcid.org/0000-0002-7262-4707 
REFERENCES 1. Wurman PR, D'Andrea R, Mountz M. Coordinating hundreds of cooperative, autonomous vehicles in warehouses. AI Magazine. 2008;29(1):1‐9. 2. Morris R, Pasareanu CS, Luckow KS, et al. Planning, scheduling and monitoring for airport surface operations. In: AAAI Workshop: Planning for Hybrid Systems. AAAI; 2016:608‐614. 3. Andreychuk A, Yakovlev K, Atzmon D, Stern R. Multi‐agent pathfinding (mapf) with continuous time. In: Proceedings of the Twenty‐Eighth International Joint Conference on Artificial Intelligence. 2019:39‐45. 4. Harabor DD, Grastien A. An optimal any‐angle pathfinding algorithm. In: ICAPS. 2013:308‐311. 5. Watkins C. Learning from Delayed Rewards. Cambridge University; 1989. 6. Mnih V, Kavukcuoglu K, Silver D, et al. Playing atari with deep reinforcement learning. In: Neural Information Processing Systems. 2013:1‐9. 7. Lillicrap TP, Hunt JJ, Pritzel A, et al. Continuous control with deep reinforcement learning. In: International Conference on Learning Representations. 2016:168‐174. 8. Mnih V, Badia AP, Mirza M, et al. Asynchronous methods for deep reinforcement learning. In: International Conference on Machine Learning. 2016:1928‐1937. 9. Ren T. Berkeley released ai system challenge report, 2017. 10. Szegedy C, Zaremba W, Sutskever I, et al. Intriguing properties of neural networks. In: International Conference on Learning Representations. 2013:1‐10. 11. Kurakin A, Goodfellow I, Bengio S. Adversarial examples in the physical world. In: International Conference on Learning Representations. 2017:1‐14. 12. Goodfellow IJ, Shlens J, Szegedy C. Explaining and harnessing adversarial examples. In: International Conference on Learning Representations. 2015:1‐11. 
TONG ET AL. | 2345 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
How to cite this article: Tong C, Jiqiang L, Yingxiao X, et al. Adversarial retraining attack of asynchronous advantage actor‐critic based pathfinding. Int J Intell Syst. 2021;36: 2323‐2346. https://doi.org/10.1002/int.22380 
2346 | TONG ET AL. 
 1098111x, 2021, 5, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/int.22380 by Aalborg University Library, Wiley Online Library on [25/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
",Adversarial retraining attack of asynchronousadvantage actor‐critic based pathfinding.pdf,17
"Robert I. CITRON 1*, Peter JENNISKENS 2,3, Christopher WATKINS4, Sravanthi SINHA5, Amar SHAH6, Chedy RAISSI7, Hadrien DEVILLEPOIX 8, and Jim ALBERS2 
1Department of Earth and Planetary Sciences, University of California, Davis, Davis, California 95616, USA 2SETI Institute, Mountain View, California 94043, USA 3NASA Ames Research Center, Moffett Field, California 94035, USA 4Scientiﬁc Computing, Commonwealth Scientiﬁc and Industrial Research Organisation, Clayton, Victoria 3181, Australia 5Holberton School of Software Engineering, San Francisco, California 94111, USA 6Department of Engineering, Computational and Biological Learning, Cambridge University, Cambridge CB2 1PZ, UK 7Institut National de Recherche en Informatique et en Automatique, Villersl�esNancy 54506, France 8Space Science & Technology Centre, School of Earth and Planetary Sciences, Curtin University, GPO Box U1987, Perth, Western Australia 6845, Australia *Corresponding author. Email: rcitron@ucdavis.edu 
(Received 01 October 2020; revision accepted 25 April 2021) 
Abstract–The recovery of freshly fallen meteorites from tracked and triangulated meteors is critical to determining their source asteroid families. Even though our ability to locate meteorite falls continues to improve, the recovery of meteorites remains a challenge due to large search areas with terrain and vegetation obscuration. To improve the efﬁciency of meteorite recovery, we have tested the hypothesis that meteorites can be located using machine learning techniques and an autonomous drone. To locate meteorites autonomously, a quadcopter drone ﬁrst conducts a grid survey acquiring topdown images of the strewn ﬁeld from a low altitude. The droneacquired images are then analyzed using a machine learning classiﬁer to identify meteorite candidates for followup examination. Here, we describe a proofofconcept meteorite classiﬁer that deploys offline a combination of different convolution neural networks to recognize meteorites from images taken by drones in the ﬁeld. The system was implemented in a conceptual drone setup and tested in the suspected strewn ﬁeld of a recent meteorite fall near Walker Lake, Nevada. 
INTRODUCTION 
In order to better understand the early evolution of the solar system, there is an ongoing effort to determine the composition of some 40 asteroid families in the asteroid belt. Remote sensing information complemented with data gleaned from laboratory studiesis of meteorites. To put that meteoritic data in context, the approach orbit of freshly fallen meteorites is measured to determine which asteroid family might have produced the meteoritic debris from a particular collision event. This is a statistical effort to determine the inclination and semimajor axis distribution of the approach orbits of meteorites of the same type with the same collision age. The inclination identiﬁes the inclination of the source region, while the semimajor axis points to the delivery resonance (Jenniskens 2013, 2018). 
If the meteorite can be recovered, the corresponding ﬁreball’s light curve and deceleration proﬁle also provide information regarding the deposition of the meteoroid’s kinetic energy into the Earth’s atmosphere, which is a function of the meteoroid’s density and internal strength. That information can be used to improve predictions of the altitude where various types of meteors fragment, which is critical to understanding events that lead to damaging airbursts (Popova et al. 2013). Building a statistical database of asteroid family compositions requires a large number of meteorites with known trajectories to be located in the ﬁeld. Yet so far, in only 40 cases have meteorites been recovered from observed falls in which the ﬁreball trajectory was measured sufﬁciently to determine the preimpact orbit (Jenniskens 2013; Borovi�cka et al. 2015). In many of 
Meteoritics & Planetary Science 56, Nr 6, 1073–1085 (2021) doi: 10.1111/maps.13663 
1073 © 2021 The Meteoritical Society 
Prior Work 
The use of drone surveys to locate freshly fallen meteorites has gained increasing traction in recent years (Citron et al. 2017; Zender et al. 2018; Alowais et al. 2019; Anderson et al. 2020). The general method of autonomous meteorite detection using drones involves applying a machine learning classiﬁer to images acquired from a lowaltitude survey (Citron et al. 2017). The drone ﬁrst surveys the ﬁeld obtaining topdown images of the search area. With a sufﬁcient ﬁeld of view and image resolution, each 1–3 cm meteorite fragment 
should be resolved by >20 pixels in width, sufﬁcient for image classiﬁcation. A trained machine learning classiﬁer is then used to identify any potential meteorites in each image. Training a machine learning classiﬁer to detect freshly fallen meteorites presents a challenge. The main identifying characteristic of a meteorite fragment is that it should appear to not belong in the surrounding terrain. Although freshly fallen meteorites should appear quite distinct from native rocks, meteorites are diverse and it is impossible to know the characteristics of the next freshly fallen meteorites or the surrounding terrain. While meteorites typically are darker than native rocks due to the fusion crust obtained during atmospheric entry, some meteorites lack a fusion crust because of subsequent breakup. Meteorites are also only sparsely distributed throughout the strewn ﬁeld, with search areas up to several square kilometers potentially containing only a few meteorites 1–3 cm in size. A machine learning classiﬁer therefore must return sufﬁciently low falsepositive detections to make a square kilometer survey feasible, while also being versatile enough to detect meteorites of a variety of types on completely new terrains. The construction of a machine learning classiﬁer to identify meteorites in various terrains has been explored by several groups (Citron et al. 2017; Alowais et al. 2019; Anderson et al. 2019, 2020). The most suitable machine learning classiﬁers for meteorite identiﬁcation etare based on convolutional neural networks (LeCun al. 1998), which have achieved stateoftheart performance in a variety of computer visionbased tasks (Gu et al. 2018). Classiﬁers based on convolution neural networks avoid problems associated with hand picking features, which would be inadequate for meteorite identiﬁcation because of the diverse properties of meteorites. The key beneﬁt of a neural networkbased classiﬁer is that it is trained to learn the features most relevant for its task jointly with the output predictions it makes. Thus, a general classiﬁer can be constructed using deep learning algorithms to locate freshly fallen meteorites on most terrains. There are two main types of machine learning models applied to object recognition: binary image classiﬁers or object detection networks. A binary image classiﬁer analyzes an entire image and determines what object it represents. Binary image classiﬁers are trained on sets of “positive” and “negative” images, and when a trained binary classiﬁcation network is applied to a new image, it determines the likelihood the image falls in the “positive” or “negative” category (i.e., if the image is of a meteorite or not). For a binary classiﬁer to work, the meteorite must encompass most of the image, meaning that a large image acquired by a drone must be spliced 
1074 R. I. Citron et al. 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
deeper neural networks capable of advanced object detection independent of the image size. Thus, instead of training a binary image classiﬁer to examine many small patches (<200 pixels in width; e.g., Citron et al. 2017; Anderson et al. 2020), an effective object detection network can be trained to recognize small meteorites in full images. Utilizing these recent advances, we trained a residual neural networkbased object detection network to recognize meteorites in droneacquired images and test its accuracy in the ﬁeld. Here, as a proof of concept, we constructed an object detection network using images of meteorites collected from an autonomous drone. We chose an inexpensive offtheshelf drone model (3DR) and camera (GoPro Hero4) in hope that our algorithm could later be applied on a much bigger scale by owners of such equipment. We show that classiﬁcation of meteorites in drone images is possible, and tested our model at the location of a recent suspected strewn ﬁeld in Walker Lake, Nevada. 
METHODS 
Constructing the Machine Learning Classiﬁer 
Training Data Set Any machine learning classiﬁer requires a large training data set of thousands of positive and negative images. Because a future meteorite fall could involve any type of meteorite fragment and terrain type, we constructed a data set containing a variety of meteorite types on a diverse set of terrains. A classiﬁer trained on such an image set could be readily applied to the classiﬁcation of images from a future fresh meteorite fall. We used three sources for meteorite images to construct a large training data set: (1) placing a limited collection of eight meteorites in our possession on various local terrains and taking overhead images of them, (2) placing our collection of eight meteorites in a recent meteorite strewn ﬁeld and imaging them from above with a drone, and (3) using overhead shots of freshly fallen meteorites obtained from a search of publicly available internet resources. For collecting images of meteorites on local terrains, we used our collection of eight meteorites 10– 100 g in mass (1–4 cm in diameter) obtained fresh from the 1992 Mbale meteorite fall (Jenniskens et al. 1994). The meteorites were deployed on various local grass, dirt, sand, and rocky terrains, and imaged with smart phone cameras and a DSLR from above at a similar height as the expected drone images. Each image was spliced into smaller patches of 1000 9 600 pixels. Although the object detection network described in the Training the Object Detection Network section can 
Autonomous meteorite recovery 1075 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
into smaller 1000 9 600 pixel image patches before being added to the training data set. Example training data from our ﬁeld test are shown in Fig. 1b. We used 82 image patches obtained from the dronemounted camera during our ﬁeld test in Creston, California. The third source of training images was obtained from an internet search for pictures of freshly fallen meteorites. Images of meteorites obtained from internet resources were critical because we did not want to overtrain our object detection network on the eight meteorites in our collection. Including images of meteorites from various falls worldwide in our data set allowed us to train our model on a variety of different meteorite types and background terrains, enhancing our ability to deploy our machine learning algorithm at the site of a future fresh fall. Each meteorite image obtained from the internet search was rescaled or cropped to 1000 9 600 pixels, so the training data set contained similar image sizes. Overall, 
(a) 
(b) 
(c) 
Fig. 1. Examples of images used to train our object detection data set. a) Images of two meteorites from our collection placed on local terrains and captured from above with a DSLR camera. b) Images of two meteorites from our collection captured during a ﬁeld test in Creston, California, with a GoPro Hero4 camera mounted on a 3DR Solo Drone. c) Example images of freshly fallen meteorites obtained from publicly available internet resources. (Color ﬁgure can be viewed at wileyonlinelibrary.com.) 
1076 R. I. Citron et al. 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Training the Object Detection Network 
For meteorite detection, we used the RetinaNet object detection network (Lin et al. 2017). RetinaNet is a highly efﬁcient object detection network built upon the ResNet deep residual neural network architecture (He et al. 2016). The implementation of RetinaNet utilizes the Keras (Chollet 2015), Caffe (Jia et al. 2014), and Tensorﬂow (Abadi et al. 2015) software packages. We used the ResNet-50 backend (He et al. 2016), which is pretrained using a data set of roughly 15 million images called ImageNet (Russakovsky et al. 2015). This approach means that from initializing our training phase, the object detection network would already be familiar with objects such as grass, rocks, hay, etc., and would not have to learn their features from scratch. We took the pretrained model and trained it on our data set of meteorite images for several epochs with RetinaNet in order to obtain a ﬁnal object detection model. We trained the model for three epochs until the classiﬁcation loss was <0.05 to obtain high accuracy without over-ﬁtting our training data. Once trained, RetinaNet could process a 1000 9 600 pixel image in 135�146 ms on our ﬁeld laptop equipped with an NVIDIA GeForce GTX 980 graphics card. Each image taken with the dronemounted GoPro camera was 4000 9 3000 pixels (12 Mp) and was spliced into 47 1000 9 600 pixel patches using a 200 pixel spacing to avoid splitting any potential meteorites across image patches. This resulted in a total processing time of 6.6 s for each full GoPro image. In our ﬁeld test (see the Field Test and Results section), each 20–25 min ﬂight acquired an average of 200 images, for a processing time of 22 min per ﬂight. A day’s worth of data (10–15 ﬂights) could therefore be processed in 4–6 h. 
While RetinaNet obtained a low classiﬁcation loss, ﬁeld deployment inevitably involves many falsepositive identiﬁcations because many terrain types contain dark rocks that might be mistaken for meteorites. It is important to emphasize that the goal of the object detection network is not to return only a few images of suspected meteorites with 100% accuracy, but instead return 100s of candidate image patches that might contain a meteorite fragment within a given search area. As long as the number of candidate patches is manageable (hundreds of images and not tens of thousands), the user can scan through the image patches and ﬂag particularly strong candidates for followup examination. 
Drone Hardware and Field Deployment 
In our deployment, we used the 3DR Solo quadcopter drone ﬁtted with a gimbalmounted GoPro HERO 4 camera. Because of the hilly terrain in much of California, we upgraded the 3DR Solo drone with a laser altimeter and the PixHawk GreenCube ﬂight controller capable of running Arducopter 3.4. This allowed the drone to use true terrain following to maintain a constant elevation above the ground. We upgraded the GoPro with a narrowangle lens offering an 87 degree ﬁeld of view. Flying at a height of 3 m with the 4000 9 3000 pixel GoPro camera yields a resolution of 0.97 mm per pixel. At 6 m, the resolution is 1.95 mm per pixel. To obtain the desired resolution of ~20 pixels across a meteorite fragment, the drone was ﬂown at 3 m when searching for smaller fragments ~2 cm across (10–20 g) and 6 m when searching for larger fragments >4 cm across (75–150 g). Each drone survey was programmed with the Mission Planner software, which allowed us to prefetch the map data prior to ﬁeld deployment. During ﬁeld deployment, we programmed the drone to ﬂy a gridsearch pattern and take photos from a constant altitude of 2�6 m. The battery limited ~25 min ﬂight time of the 3DR Solo drone resulted in ~130–250 images per survey, depending on the ﬂight altitude and search pattern. Because our object detection classiﬁer identiﬁes many false meteorite candidates, the user must scan through the processed images to determine which candidates are worth followup examination during a future ﬁeld visit. Each image is tagged with a timestamp that can be compared to the drone’s GPS log to determine the approximate location where the image was acquired. Although in principle it is possible to process the images and scan for candidates in the ﬁeld, we found this was more easily accomplished out of the ﬁeld. During ﬁeld deployment, we acquired images for a full day and processed the images during the evening. 
Autonomous meteorite recovery 1077 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
FIELD TEST AND RESULTS 
Potential Meteorite Fall at Walker Lake, Nevada 
On July 14, 2019, at 09:36:50 UTC, a bright meteor of 4.1 s duration was captured by four stations of the NASA Meteorite Tracking and Recovery Network, part of the Global Fireball Observatory. The ﬁreball track was detected from stations at Lick Observatory, Mt. Umunhum, Sunnyvale, and the Allen Telescope Array (Fig. 2). The images were calibrated and the meteor track extracted (Devillepoix et al. 2020). The beginning height of the meteor was at 82.054 km and the end height was at 27.066 km over the Sierra Nevada mountains. The approach orbit was asteroidal, with a = 2.358 � 0.032 AU and i = 10.72 � 0.08°. The entry speed was relatively high at 21.937 � 0.114 km s�1, but the surviving mass was calculated to be as much as 35.3 � 3.7 kg (EKS model; Devillepoix et al. 2020). The meteor had a 41° slope with lateral uncertainty in the trajectory of order 250 m due to a relatively large minimum distance of 275 km between the meteor and the stations. Analysis of the wind drift during the dark ﬂight calculations was done using the Oakland Wind sonde data before and after the time of fall, using the software WIND with an assumed density of 3.2 g cm�3 
(Jenniskens et al. 2019). This resulted in expected fragments of 1 g, 10 g, 100 g, and 1 kg located near Walker Lake, Nevada within the Walker River Indian 
Reservation of the Walker River Paiute Tribe, headquartered at Schurz, Nevada. The expected strewn ﬁeld location is shown in Fig. 3. Solutions were calculated with descent starting from both the ﬁnal ﬂare (white markers) and the ﬁnal observed point of the trajectory (blue). In the latter case, winds would have blown small meteorites toward the larger ones, shortening the strewn ﬁeld. The area is a former lake bed of Walker Lake and has sand dunes and stretches of rockstrewn ﬂat areas. The vegetation is minimal, but increases in density toward the river. 
(a) (b) (c) 
(d) (e) 
Fig. 2. NASA Meteorite Tracking and Recovery Network imaging of July 14, 2019 bolide from (a) Lick Observatory, (b) Mount Umunhum, (c) Alan Telescope Array, (d) Sunnyvale—early part of the bolide, and (e) Sunnyvale—late part of the bolide. (Color ﬁgure can be viewed at wileyonlinelibrary.com.) 
Fig. 3. Map of the expected Walker Lake strewn ﬁeld. The white line indicates the ﬁnal part of the meteor trajectory based on NASA Meteorite Tracking and Recovery imagery. The yellow line is an extension. The dots mark the anticipated location for meteorites falling from the ﬁnal ﬂare (white markers) and the end point (blue markers), based on Oakland wind sonde data and the WIND dark ﬂight model. Sites A and B are marked with yellow text. (Color ﬁgure can be viewed at wileyonlinelibrary.com.) 
1078 R. I. Citron et al. 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
We received permission from the Walker River Northern Paiute Tribe to conduct a ﬁeld test of our meteorite classiﬁcation system in Walker Lake, Nevada, the site of a meteorite fall on July 14, 2019 (Fig. 3). Overall, we conducted 10 test ﬂights at two different sites within the expected strewn ﬁeld. Site A was near the 10 g marker in Fig. 3, where we expected 1–20 g fragments. Site B was near the 100 g marker in Fig. 3, where we expected 50–150 g fragments. Due to the smaller expected fragment size at Site A, we ﬂew the drone at a lower altitude (2–3 m). At Site A, we also deployed our collection of eight meteorites during each test ﬂight to conﬁrm our ability to detect the meteorites used to train our classiﬁer on a new terrain type. At Site B, we conducted higher altitude (3–6 m) surveys to cover a larger area and potentially ﬁnd a single large fragment ~100 g. A summary of the 10 test ﬂights and results is shown in Table 1. Four tests were conducted at Site A, three at 2 m and one at 3 m. Six tests ﬂights were conducted at Site B, two at 3 m and four at 6 m. During each test ﬂight, the drone acquired images for ~20 min, limited by the battery life of the drone. Each test ﬂight acquired 129–288 full GoPro images. These full 4000 9 3000 pixel images were spliced into overlapping image patches of 1000 9 600 pixels. The image patches were analyzed with the trained RetinaNet model. An image patch was classiﬁed as “positive” if it contained an object classiﬁed as a meteorite with a score of 0.5 or higher, and all other image patches were classiﬁed as “negative.” The number of positive and negative image patches is listed in Table 1. Each test ﬂight resulted in a large number of image patches classiﬁed as positive. This was particularly true for the lowaltitude ﬂights, where there were on average ~2650 positive patches per test ﬂight (~28% of all patches). This is mostly due to the large number of other rocks in 
the survey area, and illustrates the difﬁculty of locating meteorites remotely on rocky terrain. The higher altitude ﬂights (6 m) resulted in less positive patches, with an average of ~1112 positive patches per test ﬂight (~12% of the all patches). Test ﬂight 5 contained around three times as many positive patches as the other 6 m ﬂights, which was likely due to the rockier terrain in the test ﬂight 5 path. The results in Table 1 show that RetinaNet returns a large number of false positives for a given search area. For the 2�3 m test ﬂights, the number of positive patches returned was prohibitive, meaning that the thousands of positive patches ﬂagged for followup is too cumbersome to sort through and identify expected meteorites. For the 6 m test ﬂights, the number of positive patches was sufﬁciently low to allow a user to quickly scan through the positive image patches and identify fragments for followup with an additional ﬁeld visit. Apart from generally determining the number of positive patches during ﬁeld surveys, we conducted two critical tests to determine the feasibility of using our system in the ﬁeld. The ﬁrst test was to deploy our collection of known meteorites in the drone survey path and assess the ability of our machine learning algorithm to correctly identify these fragments on novel terrain. The second test was to scan through the most likely meteorite candidates ﬂagged by our trained RetinaNet model and attempt to relocate them in the ﬁeld. For this test, we determined the geolocation of each meteorite candidate by crossreferencing the image timestamp with the drone GPS log. The test of our ability to correctly identify known meteorites on novel terrain was conducted at Site A, where we deployed several of the eight meteorite samples in our collection in the drone ﬂight path during each drone survey. An example image from such a test is shown in Fig. 4. As described in the Methods section, each full GoPro image was split into smaller 1000 9 600 pixel patches that were then analyzed with the object detection model. Examples of image patches containing correctly identiﬁed meteorites and false positives are shown in Fig. 5. The two meteorites from Fig. 4 were correctly identiﬁed in the smaller image patches and given a model score of 1.0 by the object detection algorithm (Fig. 5a). Several false positives were also identiﬁed with two examples shown in Fig. 5b, which were given scores of 0.89 and 0.76. Overall, of the four lowaltitude test ﬂights where we deployed meteorites in the survey path, our object detection model correctly located all of the deployed meteorites with corresponding model scores of 0.97–1. This is not surprising because the meteorites we deployed in the ﬁeld were the same meteorites used to 
Table 1. Field test runs. 
Test Site Altitude (m) Images Total patches Positives Negatives 
1 A 2 256 12,240 3553 8687 2 A 2 245 11,232 2322 8910 3 A 2 288 13,824 1328 12,496 4 A 3 145 6960 679 6281 5 B 3 135 6480 2019 4461 6 B 3 129 6192 2769 3423 7 B 6 214 10,272 2377 7895 8 B 6 175 8400 875 7525 9 B 6 190 9120 399 8721 10 B 6 225 10,800 800 10,000 
Autonomous meteorite recovery 1079 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
able to be located in the ﬁeld after georeferencing the ﬂagged images. As a proof of concept, this illustrated the ability of our method to locate meteorite candidates in the ﬁeld which were previously ﬂagged in images acquired from prior surveys. 
DISCUSSION 
Our ﬁeld deployment highlighted several areas of improvement required to make drone searches for meteorites more effective. The primary area of improvement is the object detection classiﬁer. Because our local collection of meteor fragments was limited to eight samples, our training data were biased toward locating these eight samples, even with the inclusion of additional meteorite images acquired from an internet search. Expanding the training data set to include more meteorite images from the internet and more droneacquired images of meteorites other than the eight we had in our collection would increase the diversity of meteorites in the training data. Every time the drone system is deployed in the ﬁeld, it generates new data that can later be added to the training data to create a newly trained object detection model. Bringing new meteorites not previously in our collection and deploying them during future ﬁeld tests would allow for continued improvement of our trained object detection model. This option, however, is limited by the availability of additional meteorites to bring into the ﬁeld. We also noticed during our tests that the accuracy of the classiﬁer was related to several variables, including the resolution of the training and test images. It is possible that an improved camera resolution or more stable height control could decrease the amount of false positives. In addition to updating the training data set, we can also improve the object detection network and method of training the ﬁnal model. For example, we could determine if certain iterations of training data result in more efﬁcient object detection by training RetinaNet with varied training data sets and adjusting factors such as the ratio of images acquired from the internet to images of our collection of eight fragments, image resolution, or the overall size of the training data. We could also test other object detection networks to determine if certain network architectures are more efﬁcient when trained on our data set. Although RetinaNet represents the state of the art in machine learning and is widely used in computer vision and object detection, machine learning is a rapidly evolving ﬁeld and it is possible that a newer machine learning approach could be even more effective. Future work could more directly compare the effectiveness of various types of object detection networks. 
Fig. 4. Example image of two test meteorites placed in terrain new to the machine learning algorithm, during a ﬁeld test near Walker Lake, Nevada. The meteorites are marked with orange ﬂags. (Color ﬁgure can be viewed at wileyonlinelibrary.com.) 
1080 R. I. Citron et al. 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
horizon, in order to avoid long shadows that might disrupt the object detection algorithm. The model was trained largely on images of our collection of eight meteorites that were captured in mostly clear conditions so should be most suitable for similar conditions. However, many of the meteorite images obtained from the internet could have been captured in cloudy conditions (although it is difﬁcult to tell from the topdown images without further context), implying that our model could contain some degree of training to detect meteorites in cloudy weather. Cloudy skies would reduce sharp shadows, which could eliminate some false detections; however, the lack of sunlight could also make differentiating various types of rocks more difﬁcult. In particular, the fusion crust on freshly fallen meteorites often has a glassy appearance that is easier to identify in welllit conditions. In order to expand our model to varying lighting conditions, we could add training images acquired with varied cloud cover into our training data set. We could also artiﬁcially decrease or increase the brightness of the training images during data augmentation to introduce more varied lighting conditions into our model. In addition to improving the machine learning algorithm, our ﬁeld test identiﬁed several hardware improvements necessary to make drone surveys fully effective. We found that surveys at 2 or 3 m covered too small of an area to be particularly efﬁcient. These 
(a) 
(b) 
Fig. 5. Example 1000 9 600 pixel image patches from Fig. 4 containing (a) correctly identiﬁed meteorites (score = 1.0) that we placed in the test area from our collection and (b) false positives (scores = 0.89 and 0.76). (Color ﬁgure can be viewed at wile yonlinelibrary.com.) 
Fig. 6. Example image showing a candidate meteorite identiﬁed during the higher altitude ﬁeld test ﬂights. The large image shows the full 4000 9 3000 pixel GoPro image obtained from the drone, with the candidate meteorite circled in blue. The inset shows the enlarged 1000 9 600 pixel image patch run through the object detection model with the meteorite outlined in a blue box (model score = 1.0). (Color ﬁgure can be viewed at wileyonlinelibrary.com.) 
Autonomous meteorite recovery 1081 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
1 to a subset more easily parsed to ﬁnd the best followup candidates. In our ﬁeld test, we examined all candidates with a score >0.5; however, a higher cutoff value would reduce the time spent searching for followup candidates. Because we only tested the model on meteorites used in our training set, it is unclear what model score our algorithm would assign to novel meteorites. Testing our model with ﬁeld data from additional meteorites not used in the training data could constrain the expected range of scores for new meteorite targets. This would more accurately assess the performance of the machine learning algorithm and help reﬁne future search efforts. The original intention of the project was to process the images onsite with the object detection classiﬁer so that ﬂagged positives could immediately be closely inspected in situ during the same ﬁeld trip. However, our ﬁeld test found this to be ineffective because outdoor lighting conditions made it difﬁcult to scan through images on the laptop in the ﬁeld. The relatively fast image processing time of ~6 s means that in theory images could be processed at the same rate that they are acquired with drone ﬂights. However, this requires a larger team and more equipment (such as a power source for the laptop) so that one user could run the classiﬁer on the images while another user conducts the next test ﬂight. Overall, the object detection network deployed in this work presented a major improvement over the direct image classiﬁer used in our previous ﬁeld test (Citron et al. 2017). Our results showed that the trained RetinaNet object detection network was highly effective on our data set, and could locate meteorites on terrain not used for training the model. RetinaNet reduced the image processing time and increased the accuracy of our model. Although our model still detected many false positives, for 6 m ﬂights, there was a sufﬁciently low number of positive image patches that a user could scan through the data in a reasonable timeframe and ﬂag 
(a) (b) (c) 
Fig. 7. Images of meteorite candidates taken closeup with a DSLR camera after being geolocated in the ﬁeld. Panels (a), (b), and (c) correspond to candidates in the drone imagery identiﬁed in Figs. 6 and S1 and S2, respectively. (Color ﬁgure can be viewed at wileyonlinelibrary.com.) 
1082 R. I. Citron et al. 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
CONCLUSIONS 
We demonstrated that as a proof of concept, it is possible to identify meteorites in the ﬁeld by applying an object detection classiﬁer to images acquired with an autonomous drone. As a ﬁrst step at automating meteorite detection, we constructed a large data set of meteorite images and trained an object detection network. We demonstrated the accuracy of our classiﬁer in the ﬁeld over terrain not used in our training data set and were able to locate meteorite candidates identiﬁed by our classiﬁer during subsequent ﬁeld trips. With a larger training data set, updated classiﬁcation scheme, and improved imaging hardware, machine learning coupled to an autonomous drone survey could prove a valuable tool for increasing the number of meteorites found from fresh falls. This is particularly important for fresh meteorite falls where only small fragments are expected. These falls are unlikely to draw the attention of meteorite hunters, but if the fall is imaged with an allsky survey, locating fragments is essential to augmenting the number of freshly fallen meteorites with entry orbits computed from imaged ﬁreball trajectories. By constructing a more efﬁcient classiﬁer for locating meteorites in the ﬁeld, it may be possible to increase the number of meteorites found that can be associated with imaged ﬁreball trajectories, increasing our understanding of the composition of meteors and their parent asteroid bodies. 
Acknowledgments—We thank Ilias Fernini and an anonymous reviewer for comments that helped to improve the manuscript and editor Mike Zolensky for handling the manuscript. This project was completed as part of the 2016 NASA Frontier Development Lab, which was supported by the NASA Ofﬁce of the Chief Technologist. We would like to thank Jason Utas for assistance with our prior Creston, CA, ﬁeld study. We also thank Fabio Teixeira and Brian Lim of Hypercube and Carlos Uranga for help testing various drone designs. We thank James Parr and Jordan McRae, codirectors of FDL; as well as Jim Adams, deputy chief technologist for NASA; Bruce Pittman, Chief Systems Engineer at NASA Ames Research Center; Bill Diamond, CEO of the SETI Institute; Debbie Kolyer, Grants Manager of the SETI Institute; Jonathan Knowles, explorer in Residence at Autodesk; Alison Lowndes, Deep Learning Solutions for NVIDIA; Eric Dahlstrom, President of the International Space 
University; Yarin Gal, University of Cambridge Research Fellow. The ﬁeldwork was coordinated with the tribal leaders of the Walker River Northern Paiute Tribe. We thank especially Cheri Clenneding and her interns for assisting in the search. The Global Fireball Observatory is supported by the Australian Research Council. RC and PJ are supported by NASA grant 80NSSC18K0854. 
Data Availability Statement—Data used in this study are available on request. 
Editorial Handling—Dr. Michael Zolensky 
REFERENCES 
Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G. S., Davis A., Dean J., Devin M., Ghemawat S., Goodfellow I., Harp A., Irving G., Isard M., Jia Y., Jozefowicz R., Kaiser L., Kudlur M., Levenberg J., Man�e D., Monga R., Moore S., Murray D., Olah C., Schuster M., Shlens J., Steiner B., Sutskever I., Talwar K., Tucker P., Vanhoucke V., Vasudevan V., Vi�egas F., Vinyals O., Warden P., Wattenberg M., Wicke M., Yu Y., and Zheng X. 2015. TensorFlow: Largescale machine learning on heterogeneous systems. tensorflow. org. Accessed May 24, 2020. Alowais A., Naseem S., Dawdi T., Abdisalam M., Elkalyoubi Y., Adwan A., Hassan K., and Fernini I. 2019. Meteorite hunting using deep learning and UAVs. 2nd International Conference on Signal Processing and Information Security (ICSPIS). pp. 1–4. Anderson S. L., Bland P. A., Towner M. C., and Paxman J. P. 2019. Utilizing drones and machine learning for meteorite searching and recovery (abstract #2426). 50th Lunar and Planetary Science Conference. Anderson S., Towner M., Bland P., Haikings C., Volante W., Sansom E., Devillepoix H., Shober P., Hartig B., Cupak M., JansenSturgeon T., Howie R., Benedix G., and Deacon G. 2020. Machine learning for semiautomated meteorite recovery. Meteoritics & Planetary Science 55:2461–2471. https://doi.org/10.1111/maps.13593 Borovi�cka J., Spurnỳ P., and Brown P. 2015. Small nearearth asteroids as a source of meteorites. In Asteroids edited by Michel P., DeMeo F. E., and Tuscon B. W. F. Tucson, Arizona: University of Arizona Press. https://doi.org/10. 2458/azu_uapress_9780816532131-ch014 Chollet F. 2015. Keras: Deep learning library for Theano and TensorFlow. Github Repository. Citron R. I., Shah A., Sinha S., Watkins C., and Jenniskens P. 2017. Meteorite recovery using an autonomous drone and machine learning (abstract #2528). 48th Lunar and Planetary Science Conference. Devillepoix H., Cup�ak M., Bland P. A., Sansom E. K., Towner M. C., Howie R. M., Hartig B., JansenSturgeon T., Shober P. M., Anderson S. l., Benedix G. K., Busan D., Sayers R., Jenniskens P., Albers J., Herd C., Hill P., Brown P. G., Krzeminski Z., Osinski G. R., Aoudjehane H. C., Benkhaldoun Z., Jabiri A., Guennoun M., Barka A., Darhmaoui H., Daly L., Collins G. S., McMullan S., Suttle M. D., Ireland T., Bonning G., Baeza L., Alrefay T. 
Autonomous meteorite recovery 1083 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Lawton J. A., Barnes D., Steele A., Rochette P., Verosub K. l., Gattacceca J., Cooper G., Glavin D. P., Burton A. S., Dworkin J. P., Elsila J. E., Pizzarello S., Ogliore R., SchmittKopplin P., Harir M., Hertkorn N., Verchovsky A., Grady M., Nagao K., Okazaki R., Takechi H., Hiroi T., Smith K., Silber E. A., Brown P. G., Albers J., Klotz D., Hankey M., Matson R., Fries J. A., Walker R. J., Puchtel I., Lee C.-T A., Erdman M. E., Eppich G. R., Roeske S., Gabelica Z., Lerche M., Nuevo M., Girten B., and Worden S. P. 2012. Radarenabled recovery of the Sutter’s Mill meteorite, a carbonaceous chondrite regolith breccia. Science 338:1583–1587. https://doi.org/10.1126/ science.1227163 Jenniskens P., Utas J., Yin Q.-Z., Matson R. D., Fries M., Howell J. A., Free D., Albers J., Devillepoix H., Bland P., Miller A., Verish R., Garvie L. A. J., Zolensky M. E., Ziegler K., Sanborn M. E., Verosub K. L., Rowland D. J., Ostrowski D. R., Bryson K., Laubenstein M., Zhou Q., Li Q.-L., Li X.-H., Liu Y. U., Tang G.-Q., Welten K., Caffee M. W., Meier M. M. M., Plant A. A., Maden C., Busemann H., and Granvik M. 2019. The Creston, California, meteorite fall and the origin of L chondrites. Meteoritics & Planetary Science 54:699–720. https://doi. org/10.1111/maps.13235 Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick R., Guadarrama S., and Darrell T. 2014. Caffe: Convolutional architecture for fast feature embedding. Proceedings of the 22nd ACM international conference on Multimedia. pp. 675–678. LeCun Y., Bottou L., Bengio Y., and Haffner P. 1998. Gradientbased learning applied to document recognition. Proceedings of the IEEE 86:2278–2324. Lin T. Y., Goyal P., Girshick R., He K., and Doll�ar P. 2017. Focal loss for dense object detection. Proceedings of the IEEE International Conference on Computer Vision. pp. 2980–2988. Popova O. P., Jenniskens P., Emel’yanenko V., Kartashova A., Biryukov E., Khaibrakhmanov S., Shuvalov V., Rybnov Y., Dudorov A., Grokhovsky V. I., Badyukov D. D., Yin Q.- Z., Gural P. S., Albers J., Granvik M., Evers L. G., Kuiper J., Kharlamov V., Solovyov A., Rusakov Y. S., Korotkiy S., Serdyuk I., Korochantsev A. v., Larionov M. Y., Glazachev D., Mayer A. E., Gisler G., Gladkovsky S. V., Wimpenny J., Sanborn M. E., Yamakawa A., Verosub K. l., Rowland D. J., Roeske S., Botto N. W., Friedrich J. M., Zolensky M. E., Le L., Ross D., Ziegler K., Nakamura T., Ahn I., Lee J. I., Zhou Q., Li X.-H., Li Q.-l., Liu Y., Tang G.-Q., Hiroi T., Sears D., Weinstein I. A., Vokhmintsev A. S., Ishchenko A. V., SchmittKopplin P., Hertkorn N., Nagao K., Haba M. K., Komatsu M., and Mikouchi T. 2013. Chelyabinsk airburst, damage assessment, meteorite recovery, and characterization. Science 342:1069–1073. https://doi.org/10.1126/science.1242642 Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A. C., and FeiFei L. 2015. Imagenet large scale visual recognition challenge. International Journal of Computer Vision 115:211–252. https://doi.org/10.1007/s11263-015- 0816-y Zender J., Rudawska R., Koschny D., Drolshagen G., Netjes G. J., Bosch M., Bijl R., Crevecoeur R., and Bettonvil F. 2018. Meteorite detection with airborne support—A study case. Proceedings of the International Meteor Conference. pp. 145–152. 
1084 R. I. Citron et al. 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
Additional supporting information may be found in the online version of this article. Fig. S1. Example image showing a candidate meteorite identiﬁed during the higher altitude ﬁeld test ﬂights. The large image shows the full 4000 9 3000 pixel GoPro image obtained from the drone, with the candidate meteorite circled in blue. The inset shows the enlarged 1000 9 600 pixel image patch run through 
the object detection model with the meteorite outlined in a blue box (model score = 0.998). Fig. S2. Example image showing a candidate meteorite identiﬁed during the higher altitude ﬁeld test ﬂights. The large image shows the full 4000 9 3000 pixel GoPro image obtained from the drone, with the candidate meteorite circled in blue. The inset shows the enlarged 1000 9 600 pixel image patch run through the object detection model with the meteorite outlined in a blue box (model score = 0.992). 
Autonomous meteorite recovery 1085 
 19455100, 2021, 6, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/maps.13663 by Aalborg University Library, Wiley Online Library on [26/09/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/termsandconditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 
",Recovery of meteorites using an autonomous drone and machine learning.pdf,18
